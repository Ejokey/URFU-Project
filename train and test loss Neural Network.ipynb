{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "\n",
    "1. Generate train and test data.\n",
    "2. Define a class NN of one-hidden neural net. The __init__ function takes 3 parameters (inputsize, hiddensize, outputsize).\n",
    "\n",
    "2. Define a list hidden_size = [2, ...., 33, ....128] with a step 5. Be sure that list contains 33, which corresponds to the case of interpolation threshold, i.e. the number of parameters equals to the number of training points.\n",
    "3. \n",
    "\n",
    "for h in hidden_size:\n",
    "    define model(inputsize, hiddensize, outputsize)\n",
    "    for e in epochs:\n",
    "        forward step: output = model(input)\n",
    "        compute loss \n",
    "        backpropagate loss (use backward)\n",
    "        \n",
    "    get train loss (after training): train_out = model(input)\n",
    "    train_loss = criterion(train_out, labels)\n",
    "    get test_loss: test_out = model(input)\n",
    "    test_loss = criterion(test_out, labels)\n",
    "       \n",
    "4. Plot saved list of train and test losses, x axis reveals each h, y axis is loss values.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import multivariate_normal\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.random.normal(size = 200)\n",
    "x_vals = np.array(x_values, dtype=np.float32)\n",
    "r = np.random.normal(0, 3, size = 200)\n",
    "\n",
    "y_values = [3*i + 5 for i in x_values] + r\n",
    "y_vals = np.array(y_values, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = (train_test_split(x_vals, y_vals, test_size=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train).reshape(-1, 1)\n",
    "x_test = (x_test).reshape(-1, 1)\n",
    "y_train = (y_train).reshape(-1, 1)\n",
    "y_test = (y_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 1)\n",
      "(160, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12a5ed5a100>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize):\n",
    "        super(NN, self).__init__()\n",
    "        self.linear = nn.Sequential(torch.nn.Linear(inputSize, hiddenSize),\n",
    "                                    torch.nn.Linear(hiddenSize, outputSize))\n",
    "                                    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1\n",
    "hidden_sizes = 128\n",
    "outputDim = 1      \n",
    "learningRate = 0.001 \n",
    "epochs = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 2 | epoch: 0, train loss: 56.78473663330078, test loss: 51.037025451660156\n",
      "h: 2 | epoch: 1, train loss: 56.281982421875, test loss: 50.57836151123047\n",
      "h: 2 | epoch: 2, train loss: 55.7906494140625, test loss: 50.129554748535156\n",
      "h: 2 | epoch: 3, train loss: 55.31019973754883, test loss: 49.6901741027832\n",
      "h: 2 | epoch: 4, train loss: 54.84014129638672, test loss: 49.2597770690918\n",
      "h: 2 | epoch: 5, train loss: 54.37998580932617, test loss: 48.83795928955078\n",
      "h: 2 | epoch: 6, train loss: 53.929298400878906, test loss: 48.42433547973633\n",
      "h: 2 | epoch: 7, train loss: 53.487632751464844, test loss: 48.01853561401367\n",
      "h: 2 | epoch: 8, train loss: 53.05458450317383, test loss: 47.62021255493164\n",
      "h: 2 | epoch: 9, train loss: 52.6297721862793, test loss: 47.22903060913086\n",
      "h: 2 | epoch: 10, train loss: 52.21282958984375, test loss: 46.84467697143555\n",
      "h: 2 | epoch: 11, train loss: 51.80338668823242, test loss: 46.46684265136719\n",
      "h: 2 | epoch: 12, train loss: 51.40114212036133, test loss: 46.095237731933594\n",
      "h: 2 | epoch: 13, train loss: 51.00574493408203, test loss: 45.729610443115234\n",
      "h: 2 | epoch: 14, train loss: 50.616912841796875, test loss: 45.36967086791992\n",
      "h: 2 | epoch: 15, train loss: 50.23434066772461, test loss: 45.01517868041992\n",
      "h: 2 | epoch: 16, train loss: 49.857765197753906, test loss: 44.665916442871094\n",
      "h: 2 | epoch: 17, train loss: 49.486907958984375, test loss: 44.321624755859375\n",
      "h: 2 | epoch: 18, train loss: 49.12153244018555, test loss: 43.982112884521484\n",
      "h: 2 | epoch: 19, train loss: 48.761390686035156, test loss: 43.64716339111328\n",
      "h: 2 | epoch: 20, train loss: 48.40625762939453, test loss: 43.316566467285156\n",
      "h: 2 | epoch: 21, train loss: 48.055908203125, test loss: 42.99015426635742\n",
      "h: 2 | epoch: 22, train loss: 47.71013641357422, test loss: 42.6677360534668\n",
      "h: 2 | epoch: 23, train loss: 47.368736267089844, test loss: 42.349143981933594\n",
      "h: 2 | epoch: 24, train loss: 47.031517028808594, test loss: 42.0341911315918\n",
      "h: 2 | epoch: 25, train loss: 46.698299407958984, test loss: 41.72274398803711\n",
      "h: 2 | epoch: 26, train loss: 46.3689079284668, test loss: 41.414642333984375\n",
      "h: 2 | epoch: 27, train loss: 46.04316329956055, test loss: 41.109745025634766\n",
      "h: 2 | epoch: 28, train loss: 45.720916748046875, test loss: 40.807899475097656\n",
      "h: 2 | epoch: 29, train loss: 45.40200424194336, test loss: 40.50897979736328\n",
      "h: 2 | epoch: 30, train loss: 45.08628463745117, test loss: 40.212867736816406\n",
      "h: 2 | epoch: 31, train loss: 44.77361297607422, test loss: 39.919437408447266\n",
      "h: 2 | epoch: 32, train loss: 44.4638557434082, test loss: 39.628562927246094\n",
      "h: 2 | epoch: 33, train loss: 44.15687561035156, test loss: 39.34013366699219\n",
      "h: 2 | epoch: 34, train loss: 43.852561950683594, test loss: 39.054054260253906\n",
      "h: 2 | epoch: 35, train loss: 43.55078125, test loss: 38.770206451416016\n",
      "h: 2 | epoch: 36, train loss: 43.25142288208008, test loss: 38.488502502441406\n",
      "h: 2 | epoch: 37, train loss: 42.954383850097656, test loss: 38.20884323120117\n",
      "h: 2 | epoch: 38, train loss: 42.659549713134766, test loss: 37.931148529052734\n",
      "h: 2 | epoch: 39, train loss: 42.36682891845703, test loss: 37.655330657958984\n",
      "h: 2 | epoch: 40, train loss: 42.07612228393555, test loss: 37.381290435791016\n",
      "h: 2 | epoch: 41, train loss: 41.7873420715332, test loss: 37.108970642089844\n",
      "h: 2 | epoch: 42, train loss: 41.500389099121094, test loss: 36.838287353515625\n",
      "h: 2 | epoch: 43, train loss: 41.21519088745117, test loss: 36.56915283203125\n",
      "h: 2 | epoch: 44, train loss: 40.93166732788086, test loss: 36.301536560058594\n",
      "h: 2 | epoch: 45, train loss: 40.64973068237305, test loss: 36.03533935546875\n",
      "h: 2 | epoch: 46, train loss: 40.36931610107422, test loss: 35.77051544189453\n",
      "h: 2 | epoch: 47, train loss: 40.09035873413086, test loss: 35.50700378417969\n",
      "h: 2 | epoch: 48, train loss: 39.81277847290039, test loss: 35.24474334716797\n",
      "h: 2 | epoch: 49, train loss: 39.53652572631836, test loss: 34.98368835449219\n",
      "h: 2 | epoch: 50, train loss: 39.261539459228516, test loss: 34.72378158569336\n",
      "h: 2 | epoch: 51, train loss: 38.987754821777344, test loss: 34.464969635009766\n",
      "h: 2 | epoch: 52, train loss: 38.715118408203125, test loss: 34.207218170166016\n",
      "h: 2 | epoch: 53, train loss: 38.44358444213867, test loss: 33.95048904418945\n",
      "h: 2 | epoch: 54, train loss: 38.17310333251953, test loss: 33.694725036621094\n",
      "h: 2 | epoch: 55, train loss: 37.90362548828125, test loss: 33.439903259277344\n",
      "h: 2 | epoch: 56, train loss: 37.63511657714844, test loss: 33.185977935791016\n",
      "h: 2 | epoch: 57, train loss: 37.36752700805664, test loss: 32.93291473388672\n",
      "h: 2 | epoch: 58, train loss: 37.10082244873047, test loss: 32.68069076538086\n",
      "h: 2 | epoch: 59, train loss: 36.83496856689453, test loss: 32.42927551269531\n",
      "h: 2 | epoch: 60, train loss: 36.56992721557617, test loss: 32.17863464355469\n",
      "h: 2 | epoch: 61, train loss: 36.30567169189453, test loss: 31.92875099182129\n",
      "h: 2 | epoch: 62, train loss: 36.042179107666016, test loss: 31.67959213256836\n",
      "h: 2 | epoch: 63, train loss: 35.77941131591797, test loss: 31.431142807006836\n",
      "h: 2 | epoch: 64, train loss: 35.51735305786133, test loss: 31.18338966369629\n",
      "h: 2 | epoch: 65, train loss: 35.2559814453125, test loss: 30.936309814453125\n",
      "h: 2 | epoch: 66, train loss: 34.99527359008789, test loss: 30.689889907836914\n",
      "h: 2 | epoch: 67, train loss: 34.73521423339844, test loss: 30.444107055664062\n",
      "h: 2 | epoch: 68, train loss: 34.47578811645508, test loss: 30.198955535888672\n",
      "h: 2 | epoch: 69, train loss: 34.216976165771484, test loss: 29.954437255859375\n",
      "h: 2 | epoch: 70, train loss: 33.95877456665039, test loss: 29.710529327392578\n",
      "h: 2 | epoch: 71, train loss: 33.701168060302734, test loss: 29.467220306396484\n",
      "h: 2 | epoch: 72, train loss: 33.44414520263672, test loss: 29.22452163696289\n",
      "h: 2 | epoch: 73, train loss: 33.187713623046875, test loss: 28.9824161529541\n",
      "h: 2 | epoch: 74, train loss: 32.931854248046875, test loss: 28.74091148376465\n",
      "h: 2 | epoch: 75, train loss: 32.676570892333984, test loss: 28.4999942779541\n",
      "h: 2 | epoch: 76, train loss: 32.42186737060547, test loss: 28.259674072265625\n",
      "h: 2 | epoch: 77, train loss: 32.16773223876953, test loss: 28.019954681396484\n",
      "h: 2 | epoch: 78, train loss: 31.914175033569336, test loss: 27.780834197998047\n",
      "h: 2 | epoch: 79, train loss: 31.66119956970215, test loss: 27.542327880859375\n",
      "h: 2 | epoch: 80, train loss: 31.4088077545166, test loss: 27.304424285888672\n",
      "h: 2 | epoch: 81, train loss: 31.157012939453125, test loss: 27.067148208618164\n",
      "h: 2 | epoch: 82, train loss: 30.905818939208984, test loss: 26.83049964904785\n",
      "h: 2 | epoch: 83, train loss: 30.65523338317871, test loss: 26.594491958618164\n",
      "h: 2 | epoch: 84, train loss: 30.4052734375, test loss: 26.359130859375\n",
      "h: 2 | epoch: 85, train loss: 30.15595054626465, test loss: 26.124439239501953\n",
      "h: 2 | epoch: 86, train loss: 29.907268524169922, test loss: 25.89042091369629\n",
      "h: 2 | epoch: 87, train loss: 29.659252166748047, test loss: 25.657089233398438\n",
      "h: 2 | epoch: 88, train loss: 29.41192054748535, test loss: 25.424463272094727\n",
      "h: 2 | epoch: 89, train loss: 29.165283203125, test loss: 25.192562103271484\n",
      "h: 2 | epoch: 90, train loss: 28.919361114501953, test loss: 24.961400985717773\n",
      "h: 2 | epoch: 91, train loss: 28.674175262451172, test loss: 24.731000900268555\n",
      "h: 2 | epoch: 92, train loss: 28.42974853515625, test loss: 24.50136947631836\n",
      "h: 2 | epoch: 93, train loss: 28.18609619140625, test loss: 24.272539138793945\n",
      "h: 2 | epoch: 94, train loss: 27.943246841430664, test loss: 24.044530868530273\n",
      "h: 2 | epoch: 95, train loss: 27.701221466064453, test loss: 23.81735610961914\n",
      "h: 2 | epoch: 96, train loss: 27.46004295349121, test loss: 23.591039657592773\n",
      "h: 2 | epoch: 97, train loss: 27.219738006591797, test loss: 23.36561393737793\n",
      "h: 2 | epoch: 98, train loss: 26.9803409576416, test loss: 23.141094207763672\n",
      "h: 2 | epoch: 99, train loss: 26.74186134338379, test loss: 22.917503356933594\n",
      "h: 3 | epoch: 0, train loss: 41.01143264770508, test loss: 38.0799560546875\n",
      "h: 3 | epoch: 1, train loss: 40.7164421081543, test loss: 37.794342041015625\n",
      "h: 3 | epoch: 2, train loss: 40.42300033569336, test loss: 37.51026153564453\n",
      "h: 3 | epoch: 3, train loss: 40.13102340698242, test loss: 37.227622985839844\n",
      "h: 3 | epoch: 4, train loss: 39.840431213378906, test loss: 36.94635772705078\n",
      "h: 3 | epoch: 5, train loss: 39.55113983154297, test loss: 36.6663818359375\n",
      "h: 3 | epoch: 6, train loss: 39.26308059692383, test loss: 36.387638092041016\n",
      "h: 3 | epoch: 7, train loss: 38.97618103027344, test loss: 36.110042572021484\n",
      "h: 3 | epoch: 8, train loss: 38.69037628173828, test loss: 35.83354187011719\n",
      "h: 3 | epoch: 9, train loss: 38.40559387207031, test loss: 35.55807113647461\n",
      "h: 3 | epoch: 10, train loss: 38.121788024902344, test loss: 35.28357696533203\n",
      "h: 3 | epoch: 11, train loss: 37.83888244628906, test loss: 35.00999069213867\n",
      "h: 3 | epoch: 12, train loss: 37.55684280395508, test loss: 34.737274169921875\n",
      "h: 3 | epoch: 13, train loss: 37.275611877441406, test loss: 34.46537399291992\n",
      "h: 3 | epoch: 14, train loss: 36.995140075683594, test loss: 34.19423294067383\n",
      "h: 3 | epoch: 15, train loss: 36.71538162231445, test loss: 33.92382049560547\n",
      "h: 3 | epoch: 16, train loss: 36.4362907409668, test loss: 33.654090881347656\n",
      "h: 3 | epoch: 17, train loss: 36.157840728759766, test loss: 33.385009765625\n",
      "h: 3 | epoch: 18, train loss: 35.879981994628906, test loss: 33.116539001464844\n",
      "h: 3 | epoch: 19, train loss: 35.60268783569336, test loss: 32.84864044189453\n",
      "h: 3 | epoch: 20, train loss: 35.325931549072266, test loss: 32.5812873840332\n",
      "h: 3 | epoch: 21, train loss: 35.04967498779297, test loss: 32.314453125\n",
      "h: 3 | epoch: 22, train loss: 34.773902893066406, test loss: 32.04811477661133\n",
      "h: 3 | epoch: 23, train loss: 34.49858856201172, test loss: 31.782236099243164\n",
      "h: 3 | epoch: 24, train loss: 34.22370910644531, test loss: 31.516815185546875\n",
      "h: 3 | epoch: 25, train loss: 33.94924545288086, test loss: 31.251827239990234\n",
      "h: 3 | epoch: 26, train loss: 33.675193786621094, test loss: 30.987247467041016\n",
      "h: 3 | epoch: 27, train loss: 33.401527404785156, test loss: 30.723064422607422\n",
      "h: 3 | epoch: 28, train loss: 33.12824249267578, test loss: 30.459278106689453\n",
      "h: 3 | epoch: 29, train loss: 32.855323791503906, test loss: 30.19586753845215\n",
      "h: 3 | epoch: 30, train loss: 32.58277893066406, test loss: 29.932830810546875\n",
      "h: 3 | epoch: 31, train loss: 32.31059646606445, test loss: 29.6701602935791\n",
      "h: 3 | epoch: 32, train loss: 32.03877258300781, test loss: 29.407855987548828\n",
      "h: 3 | epoch: 33, train loss: 31.76731300354004, test loss: 29.14591407775879\n",
      "h: 3 | epoch: 34, train loss: 31.496219635009766, test loss: 28.88433837890625\n",
      "h: 3 | epoch: 35, train loss: 31.225494384765625, test loss: 28.623123168945312\n",
      "h: 3 | epoch: 36, train loss: 30.955150604248047, test loss: 28.36228370666504\n",
      "h: 3 | epoch: 37, train loss: 30.685192108154297, test loss: 28.101831436157227\n",
      "h: 3 | epoch: 38, train loss: 30.415637969970703, test loss: 27.841766357421875\n",
      "h: 3 | epoch: 39, train loss: 30.1464900970459, test loss: 27.582096099853516\n",
      "h: 3 | epoch: 40, train loss: 29.87777328491211, test loss: 27.32284164428711\n",
      "h: 3 | epoch: 41, train loss: 29.609500885009766, test loss: 27.064016342163086\n",
      "h: 3 | epoch: 42, train loss: 29.341693878173828, test loss: 26.805627822875977\n",
      "h: 3 | epoch: 43, train loss: 29.074371337890625, test loss: 26.547710418701172\n",
      "h: 3 | epoch: 44, train loss: 28.80755615234375, test loss: 26.290271759033203\n",
      "h: 3 | epoch: 45, train loss: 28.541275024414062, test loss: 26.033334732055664\n",
      "h: 3 | epoch: 46, train loss: 28.275548934936523, test loss: 25.776920318603516\n",
      "h: 3 | epoch: 47, train loss: 28.010412216186523, test loss: 25.521060943603516\n",
      "h: 3 | epoch: 48, train loss: 27.745891571044922, test loss: 25.265769958496094\n",
      "h: 3 | epoch: 49, train loss: 27.48201560974121, test loss: 25.01108741760254\n",
      "h: 3 | epoch: 50, train loss: 27.218820571899414, test loss: 24.757030487060547\n",
      "h: 3 | epoch: 51, train loss: 26.95633316040039, test loss: 24.50364112854004\n",
      "h: 3 | epoch: 52, train loss: 26.694591522216797, test loss: 24.250938415527344\n",
      "h: 3 | epoch: 53, train loss: 26.433639526367188, test loss: 23.998964309692383\n",
      "h: 3 | epoch: 54, train loss: 26.173507690429688, test loss: 23.74774932861328\n",
      "h: 3 | epoch: 55, train loss: 25.914234161376953, test loss: 23.497323989868164\n",
      "h: 3 | epoch: 56, train loss: 25.655858993530273, test loss: 23.247726440429688\n",
      "h: 3 | epoch: 57, train loss: 25.39842414855957, test loss: 22.99899673461914\n",
      "h: 3 | epoch: 58, train loss: 25.141969680786133, test loss: 22.751171112060547\n",
      "h: 3 | epoch: 59, train loss: 24.886539459228516, test loss: 22.504283905029297\n",
      "h: 3 | epoch: 60, train loss: 24.632179260253906, test loss: 22.258377075195312\n",
      "h: 3 | epoch: 61, train loss: 24.378931045532227, test loss: 22.013490676879883\n",
      "h: 3 | epoch: 62, train loss: 24.126832962036133, test loss: 21.76966667175293\n",
      "h: 3 | epoch: 63, train loss: 23.87594223022461, test loss: 21.52694320678711\n",
      "h: 3 | epoch: 64, train loss: 23.626298904418945, test loss: 21.285364151000977\n",
      "h: 3 | epoch: 65, train loss: 23.377946853637695, test loss: 21.044971466064453\n",
      "h: 3 | epoch: 66, train loss: 23.130935668945312, test loss: 20.805809020996094\n",
      "h: 3 | epoch: 67, train loss: 22.885313034057617, test loss: 20.567913055419922\n",
      "h: 3 | epoch: 68, train loss: 22.64112663269043, test loss: 20.331331253051758\n",
      "h: 3 | epoch: 69, train loss: 22.398418426513672, test loss: 20.096105575561523\n",
      "h: 3 | epoch: 70, train loss: 22.157243728637695, test loss: 19.86227798461914\n",
      "h: 3 | epoch: 71, train loss: 21.91764259338379, test loss: 19.629899978637695\n",
      "h: 3 | epoch: 72, train loss: 21.67966651916504, test loss: 19.398998260498047\n",
      "h: 3 | epoch: 73, train loss: 21.443357467651367, test loss: 19.169635772705078\n",
      "h: 3 | epoch: 74, train loss: 21.208770751953125, test loss: 18.94184112548828\n",
      "h: 3 | epoch: 75, train loss: 20.975942611694336, test loss: 18.71565818786621\n",
      "h: 3 | epoch: 76, train loss: 20.74492645263672, test loss: 18.491130828857422\n",
      "h: 3 | epoch: 77, train loss: 20.515766143798828, test loss: 18.268306732177734\n",
      "h: 3 | epoch: 78, train loss: 20.288501739501953, test loss: 18.047216415405273\n",
      "h: 3 | epoch: 79, train loss: 20.06318473815918, test loss: 17.827909469604492\n",
      "h: 3 | epoch: 80, train loss: 19.8398494720459, test loss: 17.61042022705078\n",
      "h: 3 | epoch: 81, train loss: 19.618549346923828, test loss: 17.394790649414062\n",
      "h: 3 | epoch: 82, train loss: 19.399316787719727, test loss: 17.18105697631836\n",
      "h: 3 | epoch: 83, train loss: 19.18219566345215, test loss: 16.969257354736328\n",
      "h: 3 | epoch: 84, train loss: 18.967227935791016, test loss: 16.759431838989258\n",
      "h: 3 | epoch: 85, train loss: 18.75444984436035, test loss: 16.55160903930664\n",
      "h: 3 | epoch: 86, train loss: 18.543903350830078, test loss: 16.345829010009766\n",
      "h: 3 | epoch: 87, train loss: 18.335615158081055, test loss: 16.142127990722656\n",
      "h: 3 | epoch: 88, train loss: 18.129627227783203, test loss: 15.940534591674805\n",
      "h: 3 | epoch: 89, train loss: 17.92597198486328, test loss: 15.741081237792969\n",
      "h: 3 | epoch: 90, train loss: 17.724681854248047, test loss: 15.543797492980957\n",
      "h: 3 | epoch: 91, train loss: 17.525789260864258, test loss: 15.348713874816895\n",
      "h: 3 | epoch: 92, train loss: 17.329320907592773, test loss: 15.155856132507324\n",
      "h: 3 | epoch: 93, train loss: 17.13530921936035, test loss: 14.965253829956055\n",
      "h: 3 | epoch: 94, train loss: 16.943775177001953, test loss: 14.77692985534668\n",
      "h: 3 | epoch: 95, train loss: 16.754745483398438, test loss: 14.590907096862793\n",
      "h: 3 | epoch: 96, train loss: 16.5682430267334, test loss: 14.407208442687988\n",
      "h: 3 | epoch: 97, train loss: 16.384292602539062, test loss: 14.225851058959961\n",
      "h: 3 | epoch: 98, train loss: 16.202911376953125, test loss: 14.04686164855957\n",
      "h: 3 | epoch: 99, train loss: 16.024118423461914, test loss: 13.870256423950195\n",
      "h: 4 | epoch: 0, train loss: 39.953575134277344, test loss: 36.6727180480957\n",
      "h: 4 | epoch: 1, train loss: 39.731544494628906, test loss: 36.457611083984375\n",
      "h: 4 | epoch: 2, train loss: 39.510047912597656, test loss: 36.24307632446289\n",
      "h: 4 | epoch: 3, train loss: 39.289031982421875, test loss: 36.02906036376953\n",
      "h: 4 | epoch: 4, train loss: 39.06846237182617, test loss: 35.81551742553711\n",
      "h: 4 | epoch: 5, train loss: 38.848297119140625, test loss: 35.60242462158203\n",
      "h: 4 | epoch: 6, train loss: 38.62847900390625, test loss: 35.38971710205078\n",
      "h: 4 | epoch: 7, train loss: 38.408973693847656, test loss: 35.17737579345703\n",
      "h: 4 | epoch: 8, train loss: 38.18974685668945, test loss: 34.96535110473633\n",
      "h: 4 | epoch: 9, train loss: 37.970760345458984, test loss: 34.753604888916016\n",
      "h: 4 | epoch: 10, train loss: 37.751976013183594, test loss: 34.542118072509766\n",
      "h: 4 | epoch: 11, train loss: 37.53335952758789, test loss: 34.330833435058594\n",
      "h: 4 | epoch: 12, train loss: 37.31487274169922, test loss: 34.11973571777344\n",
      "h: 4 | epoch: 13, train loss: 37.096492767333984, test loss: 33.908790588378906\n",
      "h: 4 | epoch: 14, train loss: 36.87818145751953, test loss: 33.69796371459961\n",
      "h: 4 | epoch: 15, train loss: 36.659912109375, test loss: 33.48723602294922\n",
      "h: 4 | epoch: 16, train loss: 36.441654205322266, test loss: 33.276573181152344\n",
      "h: 4 | epoch: 17, train loss: 36.223388671875, test loss: 33.06595230102539\n",
      "h: 4 | epoch: 18, train loss: 36.005088806152344, test loss: 32.855350494384766\n",
      "h: 4 | epoch: 19, train loss: 35.7867317199707, test loss: 32.644737243652344\n",
      "h: 4 | epoch: 20, train loss: 35.56828689575195, test loss: 32.434104919433594\n",
      "h: 4 | epoch: 21, train loss: 35.34973907470703, test loss: 32.22341537475586\n",
      "h: 4 | epoch: 22, train loss: 35.131072998046875, test loss: 32.012657165527344\n",
      "h: 4 | epoch: 23, train loss: 34.91226577758789, test loss: 31.801822662353516\n",
      "h: 4 | epoch: 24, train loss: 34.69329833984375, test loss: 31.590877532958984\n",
      "h: 4 | epoch: 25, train loss: 34.47416305541992, test loss: 31.37982177734375\n",
      "h: 4 | epoch: 26, train loss: 34.25483322143555, test loss: 31.168630599975586\n",
      "h: 4 | epoch: 27, train loss: 34.03531265258789, test loss: 30.957286834716797\n",
      "h: 4 | epoch: 28, train loss: 33.815574645996094, test loss: 30.74579429626465\n",
      "h: 4 | epoch: 29, train loss: 33.595619201660156, test loss: 30.534130096435547\n",
      "h: 4 | epoch: 30, train loss: 33.37542724609375, test loss: 30.32229232788086\n",
      "h: 4 | epoch: 31, train loss: 33.15500259399414, test loss: 30.110271453857422\n",
      "h: 4 | epoch: 32, train loss: 32.93433380126953, test loss: 29.89805030822754\n",
      "h: 4 | epoch: 33, train loss: 32.713409423828125, test loss: 29.68564224243164\n",
      "h: 4 | epoch: 34, train loss: 32.49224090576172, test loss: 29.473026275634766\n",
      "h: 4 | epoch: 35, train loss: 32.270809173583984, test loss: 29.260208129882812\n",
      "h: 4 | epoch: 36, train loss: 32.04912567138672, test loss: 29.047176361083984\n",
      "h: 4 | epoch: 37, train loss: 31.82718849182129, test loss: 28.83394432067871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 4 | epoch: 38, train loss: 31.604995727539062, test loss: 28.62050437927246\n",
      "h: 4 | epoch: 39, train loss: 31.382549285888672, test loss: 28.406856536865234\n",
      "h: 4 | epoch: 40, train loss: 31.15985679626465, test loss: 28.193012237548828\n",
      "h: 4 | epoch: 41, train loss: 30.936925888061523, test loss: 27.978958129882812\n",
      "h: 4 | epoch: 42, train loss: 30.713760375976562, test loss: 27.764719009399414\n",
      "h: 4 | epoch: 43, train loss: 30.490367889404297, test loss: 27.550289154052734\n",
      "h: 4 | epoch: 44, train loss: 30.26675796508789, test loss: 27.335689544677734\n",
      "h: 4 | epoch: 45, train loss: 30.042943954467773, test loss: 27.12091064453125\n",
      "h: 4 | epoch: 46, train loss: 29.818933486938477, test loss: 26.905975341796875\n",
      "h: 4 | epoch: 47, train loss: 29.594745635986328, test loss: 26.69089698791504\n",
      "h: 4 | epoch: 48, train loss: 29.37038803100586, test loss: 26.475683212280273\n",
      "h: 4 | epoch: 49, train loss: 29.145883560180664, test loss: 26.26034164428711\n",
      "h: 4 | epoch: 50, train loss: 28.921245574951172, test loss: 26.044902801513672\n",
      "h: 4 | epoch: 51, train loss: 28.696491241455078, test loss: 25.82936668395996\n",
      "h: 4 | epoch: 52, train loss: 28.47164535522461, test loss: 25.6137638092041\n",
      "h: 4 | epoch: 53, train loss: 28.246728897094727, test loss: 25.39810562133789\n",
      "h: 4 | epoch: 54, train loss: 28.02175521850586, test loss: 25.182411193847656\n",
      "h: 4 | epoch: 55, train loss: 27.796756744384766, test loss: 24.966707229614258\n",
      "h: 4 | epoch: 56, train loss: 27.571752548217773, test loss: 24.751007080078125\n",
      "h: 4 | epoch: 57, train loss: 27.346771240234375, test loss: 24.535335540771484\n",
      "h: 4 | epoch: 58, train loss: 27.121835708618164, test loss: 24.319717407226562\n",
      "h: 4 | epoch: 59, train loss: 26.896976470947266, test loss: 24.10418128967285\n",
      "h: 4 | epoch: 60, train loss: 26.672222137451172, test loss: 23.88875389099121\n",
      "h: 4 | epoch: 61, train loss: 26.44760513305664, test loss: 23.673452377319336\n",
      "h: 4 | epoch: 62, train loss: 26.2231502532959, test loss: 23.458314895629883\n",
      "h: 4 | epoch: 63, train loss: 25.9988956451416, test loss: 23.243364334106445\n",
      "h: 4 | epoch: 64, train loss: 25.774871826171875, test loss: 23.02863311767578\n",
      "h: 4 | epoch: 65, train loss: 25.551111221313477, test loss: 22.814151763916016\n",
      "h: 4 | epoch: 66, train loss: 25.327651977539062, test loss: 22.59994888305664\n",
      "h: 4 | epoch: 67, train loss: 25.10452651977539, test loss: 22.386058807373047\n",
      "h: 4 | epoch: 68, train loss: 24.881771087646484, test loss: 22.17251205444336\n",
      "h: 4 | epoch: 69, train loss: 24.659427642822266, test loss: 21.95934295654297\n",
      "h: 4 | epoch: 70, train loss: 24.437528610229492, test loss: 21.746591567993164\n",
      "h: 4 | epoch: 71, train loss: 24.216115951538086, test loss: 21.534284591674805\n",
      "h: 4 | epoch: 72, train loss: 23.995227813720703, test loss: 21.322465896606445\n",
      "h: 4 | epoch: 73, train loss: 23.774904251098633, test loss: 21.111164093017578\n",
      "h: 4 | epoch: 74, train loss: 23.55518913269043, test loss: 20.90042495727539\n",
      "h: 4 | epoch: 75, train loss: 23.336118698120117, test loss: 20.69027328491211\n",
      "h: 4 | epoch: 76, train loss: 23.11773681640625, test loss: 20.480754852294922\n",
      "h: 4 | epoch: 77, train loss: 22.900083541870117, test loss: 20.271909713745117\n",
      "h: 4 | epoch: 78, train loss: 22.683204650878906, test loss: 20.063770294189453\n",
      "h: 4 | epoch: 79, train loss: 22.46714210510254, test loss: 19.85638427734375\n",
      "h: 4 | epoch: 80, train loss: 22.251937866210938, test loss: 19.649782180786133\n",
      "h: 4 | epoch: 81, train loss: 22.037635803222656, test loss: 19.44400405883789\n",
      "h: 4 | epoch: 82, train loss: 21.824277877807617, test loss: 19.23909568786621\n",
      "h: 4 | epoch: 83, train loss: 21.611909866333008, test loss: 19.035091400146484\n",
      "h: 4 | epoch: 84, train loss: 21.400577545166016, test loss: 18.832033157348633\n",
      "h: 4 | epoch: 85, train loss: 21.190319061279297, test loss: 18.62995719909668\n",
      "h: 4 | epoch: 86, train loss: 20.981182098388672, test loss: 18.428909301757812\n",
      "h: 4 | epoch: 87, train loss: 20.77320671081543, test loss: 18.228923797607422\n",
      "h: 4 | epoch: 88, train loss: 20.56644058227539, test loss: 18.03004264831543\n",
      "h: 4 | epoch: 89, train loss: 20.360923767089844, test loss: 17.83230209350586\n",
      "h: 4 | epoch: 90, train loss: 20.156700134277344, test loss: 17.6357479095459\n",
      "h: 4 | epoch: 91, train loss: 19.95380973815918, test loss: 17.440414428710938\n",
      "h: 4 | epoch: 92, train loss: 19.752300262451172, test loss: 17.246334075927734\n",
      "h: 4 | epoch: 93, train loss: 19.552204132080078, test loss: 17.053556442260742\n",
      "h: 4 | epoch: 94, train loss: 19.353572845458984, test loss: 16.862110137939453\n",
      "h: 4 | epoch: 95, train loss: 19.156442642211914, test loss: 16.672040939331055\n",
      "h: 4 | epoch: 96, train loss: 18.96084976196289, test loss: 16.483379364013672\n",
      "h: 4 | epoch: 97, train loss: 18.7668399810791, test loss: 16.296157836914062\n",
      "h: 4 | epoch: 98, train loss: 18.574447631835938, test loss: 16.110416412353516\n",
      "h: 4 | epoch: 99, train loss: 18.383708953857422, test loss: 15.926191329956055\n",
      "h: 5 | epoch: 0, train loss: 41.91807174682617, test loss: 38.66259002685547\n",
      "h: 5 | epoch: 1, train loss: 41.57352828979492, test loss: 38.32054138183594\n",
      "h: 5 | epoch: 2, train loss: 41.23027801513672, test loss: 37.980010986328125\n",
      "h: 5 | epoch: 3, train loss: 40.88825607299805, test loss: 37.640899658203125\n",
      "h: 5 | epoch: 4, train loss: 40.5473747253418, test loss: 37.30312728881836\n",
      "h: 5 | epoch: 5, train loss: 40.20756912231445, test loss: 36.96662902832031\n",
      "h: 5 | epoch: 6, train loss: 39.86876678466797, test loss: 36.6313362121582\n",
      "h: 5 | epoch: 7, train loss: 39.53090286254883, test loss: 36.29718780517578\n",
      "h: 5 | epoch: 8, train loss: 39.19392395019531, test loss: 35.9641227722168\n",
      "h: 5 | epoch: 9, train loss: 38.8577766418457, test loss: 35.63208770751953\n",
      "h: 5 | epoch: 10, train loss: 38.522403717041016, test loss: 35.301029205322266\n",
      "h: 5 | epoch: 11, train loss: 38.18776321411133, test loss: 34.97090148925781\n",
      "h: 5 | epoch: 12, train loss: 37.85382080078125, test loss: 34.64168167114258\n",
      "h: 5 | epoch: 13, train loss: 37.520538330078125, test loss: 34.31331253051758\n",
      "h: 5 | epoch: 14, train loss: 37.18788146972656, test loss: 33.98577117919922\n",
      "h: 5 | epoch: 15, train loss: 36.8558235168457, test loss: 33.65901184082031\n",
      "h: 5 | epoch: 16, train loss: 36.52433395385742, test loss: 33.33302307128906\n",
      "h: 5 | epoch: 17, train loss: 36.19340515136719, test loss: 33.00779724121094\n",
      "h: 5 | epoch: 18, train loss: 35.863014221191406, test loss: 32.68328857421875\n",
      "h: 5 | epoch: 19, train loss: 35.53314971923828, test loss: 32.3594970703125\n",
      "h: 5 | epoch: 20, train loss: 35.20379638671875, test loss: 32.036407470703125\n",
      "h: 5 | epoch: 21, train loss: 34.87495803833008, test loss: 31.714014053344727\n",
      "h: 5 | epoch: 22, train loss: 34.546630859375, test loss: 31.392314910888672\n",
      "h: 5 | epoch: 23, train loss: 34.218814849853516, test loss: 31.07131004333496\n",
      "h: 5 | epoch: 24, train loss: 33.891510009765625, test loss: 30.75099754333496\n",
      "h: 5 | epoch: 25, train loss: 33.56473922729492, test loss: 30.431381225585938\n",
      "h: 5 | epoch: 26, train loss: 33.23850631713867, test loss: 30.112478256225586\n",
      "h: 5 | epoch: 27, train loss: 32.91282653808594, test loss: 29.794292449951172\n",
      "h: 5 | epoch: 28, train loss: 32.58771896362305, test loss: 29.47684097290039\n",
      "h: 5 | epoch: 29, train loss: 32.263206481933594, test loss: 29.160137176513672\n",
      "h: 5 | epoch: 30, train loss: 31.939306259155273, test loss: 28.844207763671875\n",
      "h: 5 | epoch: 31, train loss: 31.61605453491211, test loss: 28.529077529907227\n",
      "h: 5 | epoch: 32, train loss: 31.293476104736328, test loss: 28.214763641357422\n",
      "h: 5 | epoch: 33, train loss: 30.971607208251953, test loss: 27.90130043029785\n",
      "h: 5 | epoch: 34, train loss: 30.650482177734375, test loss: 27.58871078491211\n",
      "h: 5 | epoch: 35, train loss: 30.330135345458984, test loss: 27.27704429626465\n",
      "h: 5 | epoch: 36, train loss: 30.0106143951416, test loss: 26.966312408447266\n",
      "h: 5 | epoch: 37, train loss: 29.69195556640625, test loss: 26.656570434570312\n",
      "h: 5 | epoch: 38, train loss: 29.37420654296875, test loss: 26.347850799560547\n",
      "h: 5 | epoch: 39, train loss: 29.05741310119629, test loss: 26.04019546508789\n",
      "h: 5 | epoch: 40, train loss: 28.741628646850586, test loss: 25.733652114868164\n",
      "h: 5 | epoch: 41, train loss: 28.426898956298828, test loss: 25.42825698852539\n",
      "h: 5 | epoch: 42, train loss: 28.11328125, test loss: 25.12406349182129\n",
      "h: 5 | epoch: 43, train loss: 27.800823211669922, test loss: 24.821123123168945\n",
      "h: 5 | epoch: 44, train loss: 27.48959732055664, test loss: 24.51947593688965\n",
      "h: 5 | epoch: 45, train loss: 27.179641723632812, test loss: 24.219181060791016\n",
      "h: 5 | epoch: 46, train loss: 26.87103271484375, test loss: 23.9202880859375\n",
      "h: 5 | epoch: 47, train loss: 26.563817977905273, test loss: 23.62285041809082\n",
      "h: 5 | epoch: 48, train loss: 26.258068084716797, test loss: 23.326923370361328\n",
      "h: 5 | epoch: 49, train loss: 25.953838348388672, test loss: 23.032562255859375\n",
      "h: 5 | epoch: 50, train loss: 25.651203155517578, test loss: 22.739818572998047\n",
      "h: 5 | epoch: 51, train loss: 25.350210189819336, test loss: 22.448755264282227\n",
      "h: 5 | epoch: 52, train loss: 25.050939559936523, test loss: 22.159433364868164\n",
      "h: 5 | epoch: 53, train loss: 24.75345230102539, test loss: 21.871898651123047\n",
      "h: 5 | epoch: 54, train loss: 24.457813262939453, test loss: 21.58622169494629\n",
      "h: 5 | epoch: 55, train loss: 24.164091110229492, test loss: 21.302452087402344\n",
      "h: 5 | epoch: 56, train loss: 23.87234878540039, test loss: 21.02065658569336\n",
      "h: 5 | epoch: 57, train loss: 23.582653045654297, test loss: 20.740888595581055\n",
      "h: 5 | epoch: 58, train loss: 23.29507064819336, test loss: 20.46320152282715\n",
      "h: 5 | epoch: 59, train loss: 23.009662628173828, test loss: 20.18766212463379\n",
      "h: 5 | epoch: 60, train loss: 22.726505279541016, test loss: 19.914321899414062\n",
      "h: 5 | epoch: 61, train loss: 22.445653915405273, test loss: 19.643238067626953\n",
      "h: 5 | epoch: 62, train loss: 22.167179107666016, test loss: 19.37447738647461\n",
      "h: 5 | epoch: 63, train loss: 21.89113998413086, test loss: 19.108081817626953\n",
      "h: 5 | epoch: 64, train loss: 21.617595672607422, test loss: 18.8441162109375\n",
      "h: 5 | epoch: 65, train loss: 21.346614837646484, test loss: 18.582626342773438\n",
      "h: 5 | epoch: 66, train loss: 21.078258514404297, test loss: 18.323665618896484\n",
      "h: 5 | epoch: 67, train loss: 20.81257438659668, test loss: 18.06728744506836\n",
      "h: 5 | epoch: 68, train loss: 20.549631118774414, test loss: 17.81354522705078\n",
      "h: 5 | epoch: 69, train loss: 20.289478302001953, test loss: 17.56248664855957\n",
      "h: 5 | epoch: 70, train loss: 20.03217315673828, test loss: 17.31415367126465\n",
      "h: 5 | epoch: 71, train loss: 19.77776527404785, test loss: 17.06859588623047\n",
      "h: 5 | epoch: 72, train loss: 19.526309967041016, test loss: 16.82585906982422\n",
      "h: 5 | epoch: 73, train loss: 19.277851104736328, test loss: 16.585983276367188\n",
      "h: 5 | epoch: 74, train loss: 19.032440185546875, test loss: 16.34900665283203\n",
      "h: 5 | epoch: 75, train loss: 18.790119171142578, test loss: 16.114971160888672\n",
      "h: 5 | epoch: 76, train loss: 18.550931930541992, test loss: 15.88391399383545\n",
      "h: 5 | epoch: 77, train loss: 18.314918518066406, test loss: 15.65587043762207\n",
      "h: 5 | epoch: 78, train loss: 18.082117080688477, test loss: 15.430865287780762\n",
      "h: 5 | epoch: 79, train loss: 17.85256004333496, test loss: 15.208938598632812\n",
      "h: 5 | epoch: 80, train loss: 17.626285552978516, test loss: 14.99010944366455\n",
      "h: 5 | epoch: 81, train loss: 17.403324127197266, test loss: 14.774412155151367\n",
      "h: 5 | epoch: 82, train loss: 17.183700561523438, test loss: 14.561868667602539\n",
      "h: 5 | epoch: 83, train loss: 16.96744155883789, test loss: 14.352499008178711\n",
      "h: 5 | epoch: 84, train loss: 16.754573822021484, test loss: 14.146319389343262\n",
      "h: 5 | epoch: 85, train loss: 16.545114517211914, test loss: 13.943349838256836\n",
      "h: 5 | epoch: 86, train loss: 16.339080810546875, test loss: 13.74360466003418\n",
      "h: 5 | epoch: 87, train loss: 16.136493682861328, test loss: 13.547098159790039\n",
      "h: 5 | epoch: 88, train loss: 15.937357902526855, test loss: 13.353838920593262\n",
      "h: 5 | epoch: 89, train loss: 15.741689682006836, test loss: 13.16383171081543\n",
      "h: 5 | epoch: 90, train loss: 15.54949951171875, test loss: 12.977084159851074\n",
      "h: 5 | epoch: 91, train loss: 15.3607816696167, test loss: 12.793600082397461\n",
      "h: 5 | epoch: 92, train loss: 15.175544738769531, test loss: 12.613377571105957\n",
      "h: 5 | epoch: 93, train loss: 14.993791580200195, test loss: 12.436415672302246\n",
      "h: 5 | epoch: 94, train loss: 14.815513610839844, test loss: 12.262712478637695\n",
      "h: 5 | epoch: 95, train loss: 14.640707015991211, test loss: 12.092259407043457\n",
      "h: 5 | epoch: 96, train loss: 14.469369888305664, test loss: 11.925049781799316\n",
      "h: 5 | epoch: 97, train loss: 14.301485061645508, test loss: 11.761072158813477\n",
      "h: 5 | epoch: 98, train loss: 14.137042045593262, test loss: 11.600316047668457\n",
      "h: 5 | epoch: 99, train loss: 13.97602653503418, test loss: 11.442766189575195\n",
      "h: 6 | epoch: 0, train loss: 42.78407287597656, test loss: 38.765541076660156\n",
      "h: 6 | epoch: 1, train loss: 42.42034149169922, test loss: 38.42323303222656\n",
      "h: 6 | epoch: 2, train loss: 42.059364318847656, test loss: 38.083396911621094\n",
      "h: 6 | epoch: 3, train loss: 41.701011657714844, test loss: 37.745880126953125\n",
      "h: 6 | epoch: 4, train loss: 41.34513854980469, test loss: 37.41059112548828\n",
      "h: 6 | epoch: 5, train loss: 40.99163818359375, test loss: 37.07741928100586\n",
      "h: 6 | epoch: 6, train loss: 40.64038848876953, test loss: 36.74625778198242\n",
      "h: 6 | epoch: 7, train loss: 40.2912712097168, test loss: 36.41701126098633\n",
      "h: 6 | epoch: 8, train loss: 39.9442024230957, test loss: 36.0895881652832\n",
      "h: 6 | epoch: 9, train loss: 39.59906768798828, test loss: 35.76390075683594\n",
      "h: 6 | epoch: 10, train loss: 39.25578308105469, test loss: 35.43987274169922\n",
      "h: 6 | epoch: 11, train loss: 38.91425704956055, test loss: 35.1174201965332\n",
      "h: 6 | epoch: 12, train loss: 38.57441329956055, test loss: 34.796485900878906\n",
      "h: 6 | epoch: 13, train loss: 38.236175537109375, test loss: 34.47697830200195\n",
      "h: 6 | epoch: 14, train loss: 37.89946746826172, test loss: 34.158851623535156\n",
      "h: 6 | epoch: 15, train loss: 37.564231872558594, test loss: 33.84204864501953\n",
      "h: 6 | epoch: 16, train loss: 37.23038864135742, test loss: 33.52650451660156\n",
      "h: 6 | epoch: 17, train loss: 36.89789962768555, test loss: 33.212188720703125\n",
      "h: 6 | epoch: 18, train loss: 36.56669998168945, test loss: 32.899024963378906\n",
      "h: 6 | epoch: 19, train loss: 36.236751556396484, test loss: 32.586997985839844\n",
      "h: 6 | epoch: 20, train loss: 35.90800094604492, test loss: 32.27605438232422\n",
      "h: 6 | epoch: 21, train loss: 35.580413818359375, test loss: 31.966161727905273\n",
      "h: 6 | epoch: 22, train loss: 35.25395584106445, test loss: 31.657299041748047\n",
      "h: 6 | epoch: 23, train loss: 34.928585052490234, test loss: 31.349435806274414\n",
      "h: 6 | epoch: 24, train loss: 34.604286193847656, test loss: 31.042530059814453\n",
      "h: 6 | epoch: 25, train loss: 34.28102493286133, test loss: 30.736597061157227\n",
      "h: 6 | epoch: 26, train loss: 33.95878982543945, test loss: 30.431589126586914\n",
      "h: 6 | epoch: 27, train loss: 33.63755416870117, test loss: 30.12749671936035\n",
      "h: 6 | epoch: 28, train loss: 33.31731414794922, test loss: 29.824321746826172\n",
      "h: 6 | epoch: 29, train loss: 32.99805450439453, test loss: 29.522043228149414\n",
      "h: 6 | epoch: 30, train loss: 32.679771423339844, test loss: 29.220666885375977\n",
      "h: 6 | epoch: 31, train loss: 32.362457275390625, test loss: 28.920190811157227\n",
      "h: 6 | epoch: 32, train loss: 32.046119689941406, test loss: 28.620615005493164\n",
      "h: 6 | epoch: 33, train loss: 31.73075294494629, test loss: 28.32193946838379\n",
      "h: 6 | epoch: 34, train loss: 31.416366577148438, test loss: 28.024173736572266\n",
      "h: 6 | epoch: 35, train loss: 31.10297203063965, test loss: 27.727325439453125\n",
      "h: 6 | epoch: 36, train loss: 30.790578842163086, test loss: 27.431415557861328\n",
      "h: 6 | epoch: 37, train loss: 30.479206085205078, test loss: 27.13644027709961\n",
      "h: 6 | epoch: 38, train loss: 30.168865203857422, test loss: 26.842437744140625\n",
      "h: 6 | epoch: 39, train loss: 29.859577178955078, test loss: 26.54941749572754\n",
      "h: 6 | epoch: 40, train loss: 29.551366806030273, test loss: 26.257396697998047\n",
      "h: 6 | epoch: 41, train loss: 29.244258880615234, test loss: 25.966405868530273\n",
      "h: 6 | epoch: 42, train loss: 28.938278198242188, test loss: 25.676471710205078\n",
      "h: 6 | epoch: 43, train loss: 28.633459091186523, test loss: 25.387615203857422\n",
      "h: 6 | epoch: 44, train loss: 28.329830169677734, test loss: 25.099876403808594\n",
      "h: 6 | epoch: 45, train loss: 28.027423858642578, test loss: 24.813278198242188\n",
      "h: 6 | epoch: 46, train loss: 27.726276397705078, test loss: 24.527851104736328\n",
      "h: 6 | epoch: 47, train loss: 27.426427841186523, test loss: 24.243640899658203\n",
      "h: 6 | epoch: 48, train loss: 27.127918243408203, test loss: 23.960681915283203\n",
      "h: 6 | epoch: 49, train loss: 26.830780029296875, test loss: 23.679004669189453\n",
      "h: 6 | epoch: 50, train loss: 26.535064697265625, test loss: 23.39865493774414\n",
      "h: 6 | epoch: 51, train loss: 26.240814208984375, test loss: 23.119667053222656\n",
      "h: 6 | epoch: 52, train loss: 25.948074340820312, test loss: 22.842090606689453\n",
      "h: 6 | epoch: 53, train loss: 25.65688705444336, test loss: 22.565967559814453\n",
      "h: 6 | epoch: 54, train loss: 25.367305755615234, test loss: 22.29133415222168\n",
      "h: 6 | epoch: 55, train loss: 25.07938003540039, test loss: 22.01824378967285\n",
      "h: 6 | epoch: 56, train loss: 24.793148040771484, test loss: 21.746736526489258\n",
      "h: 6 | epoch: 57, train loss: 24.508670806884766, test loss: 21.47686004638672\n",
      "h: 6 | epoch: 58, train loss: 24.225996017456055, test loss: 21.208660125732422\n",
      "h: 6 | epoch: 59, train loss: 23.945173263549805, test loss: 20.942190170288086\n",
      "h: 6 | epoch: 60, train loss: 23.666261672973633, test loss: 20.6774845123291\n",
      "h: 6 | epoch: 61, train loss: 23.389301300048828, test loss: 20.41460609436035\n",
      "h: 6 | epoch: 62, train loss: 23.11435317993164, test loss: 20.153587341308594\n",
      "h: 6 | epoch: 63, train loss: 22.84147071838379, test loss: 19.894489288330078\n",
      "h: 6 | epoch: 64, train loss: 22.570701599121094, test loss: 19.637351989746094\n",
      "h: 6 | epoch: 65, train loss: 22.30209732055664, test loss: 19.38222312927246\n",
      "h: 6 | epoch: 66, train loss: 22.035715103149414, test loss: 19.129154205322266\n",
      "h: 6 | epoch: 67, train loss: 21.771602630615234, test loss: 18.878189086914062\n",
      "h: 6 | epoch: 68, train loss: 21.509811401367188, test loss: 18.629375457763672\n",
      "h: 6 | epoch: 69, train loss: 21.25039291381836, test loss: 18.38275909423828\n",
      "h: 6 | epoch: 70, train loss: 20.993398666381836, test loss: 18.138389587402344\n",
      "h: 6 | epoch: 71, train loss: 20.738874435424805, test loss: 17.896299362182617\n",
      "h: 6 | epoch: 72, train loss: 20.486873626708984, test loss: 17.656543731689453\n",
      "h: 6 | epoch: 73, train loss: 20.23743438720703, test loss: 17.419160842895508\n",
      "h: 6 | epoch: 74, train loss: 19.990610122680664, test loss: 17.184192657470703\n",
      "h: 6 | epoch: 75, train loss: 19.746444702148438, test loss: 16.951677322387695\n",
      "h: 6 | epoch: 76, train loss: 19.504980087280273, test loss: 16.721660614013672\n",
      "h: 6 | epoch: 77, train loss: 19.266258239746094, test loss: 16.494169235229492\n",
      "h: 6 | epoch: 78, train loss: 19.03032112121582, test loss: 16.269258499145508\n",
      "h: 6 | epoch: 79, train loss: 18.797204971313477, test loss: 16.046945571899414\n",
      "h: 6 | epoch: 80, train loss: 18.566951751708984, test loss: 15.8272705078125\n",
      "h: 6 | epoch: 81, train loss: 18.339595794677734, test loss: 15.610269546508789\n",
      "h: 6 | epoch: 82, train loss: 18.115169525146484, test loss: 15.395967483520508\n",
      "h: 6 | epoch: 83, train loss: 17.893707275390625, test loss: 15.184399604797363\n",
      "h: 6 | epoch: 84, train loss: 17.67523956298828, test loss: 14.975587844848633\n",
      "h: 6 | epoch: 85, train loss: 17.459793090820312, test loss: 14.769561767578125\n",
      "h: 6 | epoch: 86, train loss: 17.247394561767578, test loss: 14.5663423538208\n",
      "h: 6 | epoch: 87, train loss: 17.038070678710938, test loss: 14.365951538085938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 6 | epoch: 88, train loss: 16.83184242248535, test loss: 14.168413162231445\n",
      "h: 6 | epoch: 89, train loss: 16.62872886657715, test loss: 13.97374153137207\n",
      "h: 6 | epoch: 90, train loss: 16.428752899169922, test loss: 13.781956672668457\n",
      "h: 6 | epoch: 91, train loss: 16.23192596435547, test loss: 13.593069076538086\n",
      "h: 6 | epoch: 92, train loss: 16.03826332092285, test loss: 13.40709400177002\n",
      "h: 6 | epoch: 93, train loss: 15.847776412963867, test loss: 13.224042892456055\n",
      "h: 6 | epoch: 94, train loss: 15.660479545593262, test loss: 13.043922424316406\n",
      "h: 6 | epoch: 95, train loss: 15.476373672485352, test loss: 12.866740226745605\n",
      "h: 6 | epoch: 96, train loss: 15.295465469360352, test loss: 12.692502975463867\n",
      "h: 6 | epoch: 97, train loss: 15.117761611938477, test loss: 12.521210670471191\n",
      "h: 6 | epoch: 98, train loss: 14.943258285522461, test loss: 12.352865219116211\n",
      "h: 6 | epoch: 99, train loss: 14.771960258483887, test loss: 12.187463760375977\n",
      "h: 7 | epoch: 0, train loss: 39.56805419921875, test loss: 35.494083404541016\n",
      "h: 7 | epoch: 1, train loss: 39.23872756958008, test loss: 35.18613052368164\n",
      "h: 7 | epoch: 2, train loss: 38.91215133666992, test loss: 34.8806266784668\n",
      "h: 7 | epoch: 3, train loss: 38.58821487426758, test loss: 34.57746887207031\n",
      "h: 7 | epoch: 4, train loss: 38.26679992675781, test loss: 34.276554107666016\n",
      "h: 7 | epoch: 5, train loss: 37.947792053222656, test loss: 33.977779388427734\n",
      "h: 7 | epoch: 6, train loss: 37.6310920715332, test loss: 33.68105697631836\n",
      "h: 7 | epoch: 7, train loss: 37.31659698486328, test loss: 33.38630676269531\n",
      "h: 7 | epoch: 8, train loss: 37.004215240478516, test loss: 33.09343338012695\n",
      "h: 7 | epoch: 9, train loss: 36.693843841552734, test loss: 32.80236053466797\n",
      "h: 7 | epoch: 10, train loss: 36.38542556762695, test loss: 32.51301956176758\n",
      "h: 7 | epoch: 11, train loss: 36.078857421875, test loss: 32.225337982177734\n",
      "h: 7 | epoch: 12, train loss: 35.77406692504883, test loss: 31.939254760742188\n",
      "h: 7 | epoch: 13, train loss: 35.47099685668945, test loss: 31.654699325561523\n",
      "h: 7 | epoch: 14, train loss: 35.16956329345703, test loss: 31.37160873413086\n",
      "h: 7 | epoch: 15, train loss: 34.86971664428711, test loss: 31.089950561523438\n",
      "h: 7 | epoch: 16, train loss: 34.571388244628906, test loss: 30.809650421142578\n",
      "h: 7 | epoch: 17, train loss: 34.274532318115234, test loss: 30.530675888061523\n",
      "h: 7 | epoch: 18, train loss: 33.97909164428711, test loss: 30.25296974182129\n",
      "h: 7 | epoch: 19, train loss: 33.68502426147461, test loss: 29.976505279541016\n",
      "h: 7 | epoch: 20, train loss: 33.39228057861328, test loss: 29.70122718811035\n",
      "h: 7 | epoch: 21, train loss: 33.10082244873047, test loss: 29.427114486694336\n",
      "h: 7 | epoch: 22, train loss: 32.81061935424805, test loss: 29.154138565063477\n",
      "h: 7 | epoch: 23, train loss: 32.521629333496094, test loss: 28.88225746154785\n",
      "h: 7 | epoch: 24, train loss: 32.233829498291016, test loss: 28.6114501953125\n",
      "h: 7 | epoch: 25, train loss: 31.94718360900879, test loss: 28.341699600219727\n",
      "h: 7 | epoch: 26, train loss: 31.66167640686035, test loss: 28.072978973388672\n",
      "h: 7 | epoch: 27, train loss: 31.377283096313477, test loss: 27.805267333984375\n",
      "h: 7 | epoch: 28, train loss: 31.093982696533203, test loss: 27.538564682006836\n",
      "h: 7 | epoch: 29, train loss: 30.811763763427734, test loss: 27.27284812927246\n",
      "h: 7 | epoch: 30, train loss: 30.530614852905273, test loss: 27.008102416992188\n",
      "h: 7 | epoch: 31, train loss: 30.25052261352539, test loss: 26.744333267211914\n",
      "h: 7 | epoch: 32, train loss: 29.971487045288086, test loss: 26.481517791748047\n",
      "h: 7 | epoch: 33, train loss: 29.693490982055664, test loss: 26.219669342041016\n",
      "h: 7 | epoch: 34, train loss: 29.416540145874023, test loss: 25.958776473999023\n",
      "h: 7 | epoch: 35, train loss: 29.140636444091797, test loss: 25.698848724365234\n",
      "h: 7 | epoch: 36, train loss: 28.865779876708984, test loss: 25.43988037109375\n",
      "h: 7 | epoch: 37, train loss: 28.591976165771484, test loss: 25.181888580322266\n",
      "h: 7 | epoch: 38, train loss: 28.319225311279297, test loss: 24.924869537353516\n",
      "h: 7 | epoch: 39, train loss: 28.047550201416016, test loss: 24.668838500976562\n",
      "h: 7 | epoch: 40, train loss: 27.77695083618164, test loss: 24.413803100585938\n",
      "h: 7 | epoch: 41, train loss: 27.507442474365234, test loss: 24.159780502319336\n",
      "h: 7 | epoch: 42, train loss: 27.239044189453125, test loss: 23.90677833557129\n",
      "h: 7 | epoch: 43, train loss: 26.97176742553711, test loss: 23.654817581176758\n",
      "h: 7 | epoch: 44, train loss: 26.70563316345215, test loss: 23.403911590576172\n",
      "h: 7 | epoch: 45, train loss: 26.440662384033203, test loss: 23.154090881347656\n",
      "h: 7 | epoch: 46, train loss: 26.176876068115234, test loss: 22.90536117553711\n",
      "h: 7 | epoch: 47, train loss: 25.9143009185791, test loss: 22.657752990722656\n",
      "h: 7 | epoch: 48, train loss: 25.6529541015625, test loss: 22.411287307739258\n",
      "h: 7 | epoch: 49, train loss: 25.392871856689453, test loss: 22.16598892211914\n",
      "h: 7 | epoch: 50, train loss: 25.13407325744629, test loss: 21.9218807220459\n",
      "h: 7 | epoch: 51, train loss: 24.8765926361084, test loss: 21.67899513244629\n",
      "h: 7 | epoch: 52, train loss: 24.62045669555664, test loss: 21.43735122680664\n",
      "h: 7 | epoch: 53, train loss: 24.36569595336914, test loss: 21.196989059448242\n",
      "h: 7 | epoch: 54, train loss: 24.112346649169922, test loss: 20.957927703857422\n",
      "h: 7 | epoch: 55, train loss: 23.860437393188477, test loss: 20.72020149230957\n",
      "h: 7 | epoch: 56, train loss: 23.610002517700195, test loss: 20.483840942382812\n",
      "h: 7 | epoch: 57, train loss: 23.36107635498047, test loss: 20.248872756958008\n",
      "h: 7 | epoch: 58, train loss: 23.113697052001953, test loss: 20.015336990356445\n",
      "h: 7 | epoch: 59, train loss: 22.86789894104004, test loss: 19.783267974853516\n",
      "h: 7 | epoch: 60, train loss: 22.623714447021484, test loss: 19.552684783935547\n",
      "h: 7 | epoch: 61, train loss: 22.38118553161621, test loss: 19.323633193969727\n",
      "h: 7 | epoch: 62, train loss: 22.140344619750977, test loss: 19.096141815185547\n",
      "h: 7 | epoch: 63, train loss: 21.90123176574707, test loss: 18.870241165161133\n",
      "h: 7 | epoch: 64, train loss: 21.663883209228516, test loss: 18.64597511291504\n",
      "h: 7 | epoch: 65, train loss: 21.428335189819336, test loss: 18.423372268676758\n",
      "h: 7 | epoch: 66, train loss: 21.194625854492188, test loss: 18.202463150024414\n",
      "h: 7 | epoch: 67, train loss: 20.962793350219727, test loss: 17.983280181884766\n",
      "h: 7 | epoch: 68, train loss: 20.73287582397461, test loss: 17.765865325927734\n",
      "h: 7 | epoch: 69, train loss: 20.504905700683594, test loss: 17.550241470336914\n",
      "h: 7 | epoch: 70, train loss: 20.278921127319336, test loss: 17.336450576782227\n",
      "h: 7 | epoch: 71, train loss: 20.054962158203125, test loss: 17.124523162841797\n",
      "h: 7 | epoch: 72, train loss: 19.833057403564453, test loss: 16.914487838745117\n",
      "h: 7 | epoch: 73, train loss: 19.61324691772461, test loss: 16.706378936767578\n",
      "h: 7 | epoch: 74, train loss: 19.395565032958984, test loss: 16.500226974487305\n",
      "h: 7 | epoch: 75, train loss: 19.180044174194336, test loss: 16.296062469482422\n",
      "h: 7 | epoch: 76, train loss: 18.966718673706055, test loss: 16.093915939331055\n",
      "h: 7 | epoch: 77, train loss: 18.7556209564209, test loss: 15.893816947937012\n",
      "h: 7 | epoch: 78, train loss: 18.54677963256836, test loss: 15.69579029083252\n",
      "h: 7 | epoch: 79, train loss: 18.340225219726562, test loss: 15.499865531921387\n",
      "h: 7 | epoch: 80, train loss: 18.135990142822266, test loss: 15.306065559387207\n",
      "h: 7 | epoch: 81, train loss: 17.934101104736328, test loss: 15.114419937133789\n",
      "h: 7 | epoch: 82, train loss: 17.734582901000977, test loss: 14.924951553344727\n",
      "h: 7 | epoch: 83, train loss: 17.537464141845703, test loss: 14.73768424987793\n",
      "h: 7 | epoch: 84, train loss: 17.342769622802734, test loss: 14.552640914916992\n",
      "h: 7 | epoch: 85, train loss: 17.150522232055664, test loss: 14.369839668273926\n",
      "h: 7 | epoch: 86, train loss: 16.960742950439453, test loss: 14.189303398132324\n",
      "h: 7 | epoch: 87, train loss: 16.773456573486328, test loss: 14.011049270629883\n",
      "h: 7 | epoch: 88, train loss: 16.58867835998535, test loss: 13.835095405578613\n",
      "h: 7 | epoch: 89, train loss: 16.406429290771484, test loss: 13.661455154418945\n",
      "h: 7 | epoch: 90, train loss: 16.226720809936523, test loss: 13.490150451660156\n",
      "h: 7 | epoch: 91, train loss: 16.049577713012695, test loss: 13.321187973022461\n",
      "h: 7 | epoch: 92, train loss: 15.87500286102295, test loss: 13.154582023620605\n",
      "h: 7 | epoch: 93, train loss: 15.703015327453613, test loss: 12.990346908569336\n",
      "h: 7 | epoch: 94, train loss: 15.533624649047852, test loss: 12.828489303588867\n",
      "h: 7 | epoch: 95, train loss: 15.366841316223145, test loss: 12.669013023376465\n",
      "h: 7 | epoch: 96, train loss: 15.202667236328125, test loss: 12.511935234069824\n",
      "h: 7 | epoch: 97, train loss: 15.041112899780273, test loss: 12.35725212097168\n",
      "h: 7 | epoch: 98, train loss: 14.882182121276855, test loss: 12.204970359802246\n",
      "h: 7 | epoch: 99, train loss: 14.72587776184082, test loss: 12.055095672607422\n",
      "h: 8 | epoch: 0, train loss: 58.377601623535156, test loss: 54.05120086669922\n",
      "h: 8 | epoch: 1, train loss: 57.367591857910156, test loss: 53.11443328857422\n",
      "h: 8 | epoch: 2, train loss: 56.38641357421875, test loss: 52.20344924926758\n",
      "h: 8 | epoch: 3, train loss: 55.43244171142578, test loss: 51.316795349121094\n",
      "h: 8 | epoch: 4, train loss: 54.5041389465332, test loss: 50.45311737060547\n",
      "h: 8 | epoch: 5, train loss: 53.6001091003418, test loss: 49.61119079589844\n",
      "h: 8 | epoch: 6, train loss: 52.719017028808594, test loss: 48.78984069824219\n",
      "h: 8 | epoch: 7, train loss: 51.85966110229492, test loss: 47.98799133300781\n",
      "h: 8 | epoch: 8, train loss: 51.0208854675293, test loss: 47.20464324951172\n",
      "h: 8 | epoch: 9, train loss: 50.201629638671875, test loss: 46.4388427734375\n",
      "h: 8 | epoch: 10, train loss: 49.400901794433594, test loss: 45.689727783203125\n",
      "h: 8 | epoch: 11, train loss: 48.6177864074707, test loss: 44.956451416015625\n",
      "h: 8 | epoch: 12, train loss: 47.851417541503906, test loss: 44.23828125\n",
      "h: 8 | epoch: 13, train loss: 47.10097885131836, test loss: 43.53449630737305\n",
      "h: 8 | epoch: 14, train loss: 46.36573791503906, test loss: 42.84440231323242\n",
      "h: 8 | epoch: 15, train loss: 45.644981384277344, test loss: 42.16741943359375\n",
      "h: 8 | epoch: 16, train loss: 44.938072204589844, test loss: 41.50292205810547\n",
      "h: 8 | epoch: 17, train loss: 44.24437713623047, test loss: 40.85041046142578\n",
      "h: 8 | epoch: 18, train loss: 43.56334686279297, test loss: 40.209346771240234\n",
      "h: 8 | epoch: 19, train loss: 42.894447326660156, test loss: 39.57927322387695\n",
      "h: 8 | epoch: 20, train loss: 42.23717498779297, test loss: 38.95974349975586\n",
      "h: 8 | epoch: 21, train loss: 41.59107208251953, test loss: 38.3503532409668\n",
      "h: 8 | epoch: 22, train loss: 40.955711364746094, test loss: 37.750701904296875\n",
      "h: 8 | epoch: 23, train loss: 40.33069610595703, test loss: 37.16045379638672\n",
      "h: 8 | epoch: 24, train loss: 39.71564865112305, test loss: 36.579261779785156\n",
      "h: 8 | epoch: 25, train loss: 39.110225677490234, test loss: 36.0068244934082\n",
      "h: 8 | epoch: 26, train loss: 38.51410675048828, test loss: 35.44285202026367\n",
      "h: 8 | epoch: 27, train loss: 37.92698669433594, test loss: 34.88707733154297\n",
      "h: 8 | epoch: 28, train loss: 37.348594665527344, test loss: 34.33924865722656\n",
      "h: 8 | epoch: 29, train loss: 36.77867889404297, test loss: 33.79914855957031\n",
      "h: 8 | epoch: 30, train loss: 36.21699523925781, test loss: 33.26655960083008\n",
      "h: 8 | epoch: 31, train loss: 35.66332244873047, test loss: 32.74127960205078\n",
      "h: 8 | epoch: 32, train loss: 35.117469787597656, test loss: 32.22314453125\n",
      "h: 8 | epoch: 33, train loss: 34.57924270629883, test loss: 31.711965560913086\n",
      "h: 8 | epoch: 34, train loss: 34.04846954345703, test loss: 31.20761489868164\n",
      "h: 8 | epoch: 35, train loss: 33.52499771118164, test loss: 30.709936141967773\n",
      "h: 8 | epoch: 36, train loss: 33.00868606567383, test loss: 30.218801498413086\n",
      "h: 8 | epoch: 37, train loss: 32.49939727783203, test loss: 29.73410415649414\n",
      "h: 8 | epoch: 38, train loss: 31.99701499938965, test loss: 29.255727767944336\n",
      "h: 8 | epoch: 39, train loss: 31.501434326171875, test loss: 28.783584594726562\n",
      "h: 8 | epoch: 40, train loss: 31.012548446655273, test loss: 28.317584991455078\n",
      "h: 8 | epoch: 41, train loss: 30.5302791595459, test loss: 27.85764503479004\n",
      "h: 8 | epoch: 42, train loss: 30.054540634155273, test loss: 27.4036922454834\n",
      "h: 8 | epoch: 43, train loss: 29.585256576538086, test loss: 26.955673217773438\n",
      "h: 8 | epoch: 44, train loss: 29.12237548828125, test loss: 26.513525009155273\n",
      "h: 8 | epoch: 45, train loss: 28.665828704833984, test loss: 26.077198028564453\n",
      "h: 8 | epoch: 46, train loss: 28.2155704498291, test loss: 25.646648406982422\n",
      "h: 8 | epoch: 47, train loss: 27.771554946899414, test loss: 25.221830368041992\n",
      "h: 8 | epoch: 48, train loss: 27.3337459564209, test loss: 24.802722930908203\n",
      "h: 8 | epoch: 49, train loss: 26.902103424072266, test loss: 24.389278411865234\n",
      "h: 8 | epoch: 50, train loss: 26.476598739624023, test loss: 23.981487274169922\n",
      "h: 8 | epoch: 51, train loss: 26.057209014892578, test loss: 23.57931900024414\n",
      "h: 8 | epoch: 52, train loss: 25.643909454345703, test loss: 23.182750701904297\n",
      "h: 8 | epoch: 53, train loss: 25.236677169799805, test loss: 22.791770935058594\n",
      "h: 8 | epoch: 54, train loss: 24.835500717163086, test loss: 22.406362533569336\n",
      "h: 8 | epoch: 55, train loss: 24.44036293029785, test loss: 22.02651596069336\n",
      "h: 8 | epoch: 56, train loss: 24.05124855041504, test loss: 21.652223587036133\n",
      "h: 8 | epoch: 57, train loss: 23.668148040771484, test loss: 21.283466339111328\n",
      "h: 8 | epoch: 58, train loss: 23.291057586669922, test loss: 20.920244216918945\n",
      "h: 8 | epoch: 59, train loss: 22.919958114624023, test loss: 20.562545776367188\n",
      "h: 8 | epoch: 60, train loss: 22.554847717285156, test loss: 20.21036720275879\n",
      "h: 8 | epoch: 61, train loss: 22.19571876525879, test loss: 19.86370086669922\n",
      "h: 8 | epoch: 62, train loss: 21.842554092407227, test loss: 19.522539138793945\n",
      "h: 8 | epoch: 63, train loss: 21.495349884033203, test loss: 19.18687629699707\n",
      "h: 8 | epoch: 64, train loss: 21.154102325439453, test loss: 18.856700897216797\n",
      "h: 8 | epoch: 65, train loss: 20.818796157836914, test loss: 18.532011032104492\n",
      "h: 8 | epoch: 66, train loss: 20.489421844482422, test loss: 18.212797164916992\n",
      "h: 8 | epoch: 67, train loss: 20.165966033935547, test loss: 17.899045944213867\n",
      "h: 8 | epoch: 68, train loss: 19.84841537475586, test loss: 17.590749740600586\n",
      "h: 8 | epoch: 69, train loss: 19.536758422851562, test loss: 17.287893295288086\n",
      "h: 8 | epoch: 70, train loss: 19.230976104736328, test loss: 16.990467071533203\n",
      "h: 8 | epoch: 71, train loss: 18.931049346923828, test loss: 16.698448181152344\n",
      "h: 8 | epoch: 72, train loss: 18.636959075927734, test loss: 16.411827087402344\n",
      "h: 8 | epoch: 73, train loss: 18.348682403564453, test loss: 16.130578994750977\n",
      "h: 8 | epoch: 74, train loss: 18.06619644165039, test loss: 15.854682922363281\n",
      "h: 8 | epoch: 75, train loss: 17.789470672607422, test loss: 15.584124565124512\n",
      "h: 8 | epoch: 76, train loss: 17.518482208251953, test loss: 15.318867683410645\n",
      "h: 8 | epoch: 77, train loss: 17.253192901611328, test loss: 15.05888843536377\n",
      "h: 8 | epoch: 78, train loss: 16.993576049804688, test loss: 14.804161071777344\n",
      "h: 8 | epoch: 79, train loss: 16.739591598510742, test loss: 14.554651260375977\n",
      "h: 8 | epoch: 80, train loss: 16.491201400756836, test loss: 14.310325622558594\n",
      "h: 8 | epoch: 81, train loss: 16.248363494873047, test loss: 14.071151733398438\n",
      "h: 8 | epoch: 82, train loss: 16.011035919189453, test loss: 13.83708381652832\n",
      "h: 8 | epoch: 83, train loss: 15.77917194366455, test loss: 13.6080904006958\n",
      "h: 8 | epoch: 84, train loss: 15.552726745605469, test loss: 13.384122848510742\n",
      "h: 8 | epoch: 85, train loss: 15.331645011901855, test loss: 13.165143013000488\n",
      "h: 8 | epoch: 86, train loss: 15.115878105163574, test loss: 12.951095581054688\n",
      "h: 8 | epoch: 87, train loss: 14.905367851257324, test loss: 12.741938591003418\n",
      "h: 8 | epoch: 88, train loss: 14.700057983398438, test loss: 12.537619590759277\n",
      "h: 8 | epoch: 89, train loss: 14.499890327453613, test loss: 12.338088989257812\n",
      "h: 8 | epoch: 90, train loss: 14.304800033569336, test loss: 12.143289566040039\n",
      "h: 8 | epoch: 91, train loss: 14.114728927612305, test loss: 11.953165054321289\n",
      "h: 8 | epoch: 92, train loss: 13.929606437683105, test loss: 11.767657279968262\n",
      "h: 8 | epoch: 93, train loss: 13.749368667602539, test loss: 11.586708068847656\n",
      "h: 8 | epoch: 94, train loss: 13.573946952819824, test loss: 11.410259246826172\n",
      "h: 8 | epoch: 95, train loss: 13.403268814086914, test loss: 11.238243103027344\n",
      "h: 8 | epoch: 96, train loss: 13.237261772155762, test loss: 11.070598602294922\n",
      "h: 8 | epoch: 97, train loss: 13.075854301452637, test loss: 10.907262802124023\n",
      "h: 8 | epoch: 98, train loss: 12.918972969055176, test loss: 10.74816608428955\n",
      "h: 8 | epoch: 99, train loss: 12.766539573669434, test loss: 10.593240737915039\n",
      "h: 9 | epoch: 0, train loss: 45.58155059814453, test loss: 41.05755615234375\n",
      "h: 9 | epoch: 1, train loss: 44.97550964355469, test loss: 40.49211120605469\n",
      "h: 9 | epoch: 2, train loss: 44.38087463378906, test loss: 39.936973571777344\n",
      "h: 9 | epoch: 3, train loss: 43.797149658203125, test loss: 39.39168930053711\n",
      "h: 9 | epoch: 4, train loss: 43.22382354736328, test loss: 38.85582733154297\n",
      "h: 9 | epoch: 5, train loss: 42.66044235229492, test loss: 38.328983306884766\n",
      "h: 9 | epoch: 6, train loss: 42.10658264160156, test loss: 37.81077194213867\n",
      "h: 9 | epoch: 7, train loss: 41.56183624267578, test loss: 37.30084991455078\n",
      "h: 9 | epoch: 8, train loss: 41.02581024169922, test loss: 36.79886245727539\n",
      "h: 9 | epoch: 9, train loss: 40.498165130615234, test loss: 36.304500579833984\n",
      "h: 9 | epoch: 10, train loss: 39.97855758666992, test loss: 35.81746292114258\n",
      "h: 9 | epoch: 11, train loss: 39.46666717529297, test loss: 35.33747100830078\n",
      "h: 9 | epoch: 12, train loss: 38.962196350097656, test loss: 34.864280700683594\n",
      "h: 9 | epoch: 13, train loss: 38.46487808227539, test loss: 34.39760971069336\n",
      "h: 9 | epoch: 14, train loss: 37.97443771362305, test loss: 33.937259674072266\n",
      "h: 9 | epoch: 15, train loss: 37.49064636230469, test loss: 33.483001708984375\n",
      "h: 9 | epoch: 16, train loss: 37.013267517089844, test loss: 33.034629821777344\n",
      "h: 9 | epoch: 17, train loss: 36.54208755493164, test loss: 32.59197235107422\n",
      "h: 9 | epoch: 18, train loss: 36.076904296875, test loss: 32.154823303222656\n",
      "h: 9 | epoch: 19, train loss: 35.61753463745117, test loss: 31.723037719726562\n",
      "h: 9 | epoch: 20, train loss: 35.1638069152832, test loss: 31.296457290649414\n",
      "h: 9 | epoch: 21, train loss: 34.715553283691406, test loss: 30.87493896484375\n",
      "h: 9 | epoch: 22, train loss: 34.27263259887695, test loss: 30.458349227905273\n",
      "h: 9 | epoch: 23, train loss: 33.83489227294922, test loss: 30.046550750732422\n",
      "h: 9 | epoch: 24, train loss: 33.40221405029297, test loss: 29.639455795288086\n",
      "h: 9 | epoch: 25, train loss: 32.9744758605957, test loss: 29.236934661865234\n",
      "h: 9 | epoch: 26, train loss: 32.55155944824219, test loss: 28.838909149169922\n",
      "h: 9 | epoch: 27, train loss: 32.13336944580078, test loss: 28.445266723632812\n",
      "h: 9 | epoch: 28, train loss: 31.719818115234375, test loss: 28.05593490600586\n",
      "h: 9 | epoch: 29, train loss: 31.310810089111328, test loss: 27.670841217041016\n",
      "h: 9 | epoch: 30, train loss: 30.906274795532227, test loss: 27.289907455444336\n",
      "h: 9 | epoch: 31, train loss: 30.50613021850586, test loss: 26.9130802154541\n",
      "h: 9 | epoch: 32, train loss: 30.110321044921875, test loss: 26.540292739868164\n",
      "h: 9 | epoch: 33, train loss: 29.718786239624023, test loss: 26.171499252319336\n",
      "h: 9 | epoch: 34, train loss: 29.33147621154785, test loss: 25.806650161743164\n",
      "h: 9 | epoch: 35, train loss: 28.948345184326172, test loss: 25.44570541381836\n",
      "h: 9 | epoch: 36, train loss: 28.5693416595459, test loss: 25.088626861572266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 9 | epoch: 37, train loss: 28.194448471069336, test loss: 24.735380172729492\n",
      "h: 9 | epoch: 38, train loss: 27.823617935180664, test loss: 24.385944366455078\n",
      "h: 9 | epoch: 39, train loss: 27.45682716369629, test loss: 24.040285110473633\n",
      "h: 9 | epoch: 40, train loss: 27.094051361083984, test loss: 23.69839096069336\n",
      "h: 9 | epoch: 41, train loss: 26.735275268554688, test loss: 23.3602352142334\n",
      "h: 9 | epoch: 42, train loss: 26.3804874420166, test loss: 23.025798797607422\n",
      "h: 9 | epoch: 43, train loss: 26.029659271240234, test loss: 22.695087432861328\n",
      "h: 9 | epoch: 44, train loss: 25.682796478271484, test loss: 22.36808204650879\n",
      "h: 9 | epoch: 45, train loss: 25.339885711669922, test loss: 22.04476547241211\n",
      "h: 9 | epoch: 46, train loss: 25.00092315673828, test loss: 21.72515106201172\n",
      "h: 9 | epoch: 47, train loss: 24.665908813476562, test loss: 21.409223556518555\n",
      "h: 9 | epoch: 48, train loss: 24.334835052490234, test loss: 21.096988677978516\n",
      "h: 9 | epoch: 49, train loss: 24.007715225219727, test loss: 20.788440704345703\n",
      "h: 9 | epoch: 50, train loss: 23.68454360961914, test loss: 20.48358726501465\n",
      "h: 9 | epoch: 51, train loss: 23.365327835083008, test loss: 20.18242835998535\n",
      "h: 9 | epoch: 52, train loss: 23.050073623657227, test loss: 19.884967803955078\n",
      "h: 9 | epoch: 53, train loss: 22.738784790039062, test loss: 19.591205596923828\n",
      "h: 9 | epoch: 54, train loss: 22.431474685668945, test loss: 19.301158905029297\n",
      "h: 9 | epoch: 55, train loss: 22.12814712524414, test loss: 19.014822006225586\n",
      "h: 9 | epoch: 56, train loss: 21.828811645507812, test loss: 18.73220443725586\n",
      "h: 9 | epoch: 57, train loss: 21.53347396850586, test loss: 18.45331382751465\n",
      "h: 9 | epoch: 58, train loss: 21.242149353027344, test loss: 18.178157806396484\n",
      "h: 9 | epoch: 59, train loss: 20.9548397064209, test loss: 17.906740188598633\n",
      "h: 9 | epoch: 60, train loss: 20.671558380126953, test loss: 17.639066696166992\n",
      "h: 9 | epoch: 61, train loss: 20.39230728149414, test loss: 17.375141143798828\n",
      "h: 9 | epoch: 62, train loss: 20.117101669311523, test loss: 17.114974975585938\n",
      "h: 9 | epoch: 63, train loss: 19.845943450927734, test loss: 16.858570098876953\n",
      "h: 9 | epoch: 64, train loss: 19.578838348388672, test loss: 16.60591697692871\n",
      "h: 9 | epoch: 65, train loss: 19.315793991088867, test loss: 16.357038497924805\n",
      "h: 9 | epoch: 66, train loss: 19.05681610107422, test loss: 16.1119327545166\n",
      "h: 9 | epoch: 67, train loss: 18.801902770996094, test loss: 15.870585441589355\n",
      "h: 9 | epoch: 68, train loss: 18.55105972290039, test loss: 15.633012771606445\n",
      "h: 9 | epoch: 69, train loss: 18.304285049438477, test loss: 15.399208068847656\n",
      "h: 9 | epoch: 70, train loss: 18.061580657958984, test loss: 15.169168472290039\n",
      "h: 9 | epoch: 71, train loss: 17.822940826416016, test loss: 14.942889213562012\n",
      "h: 9 | epoch: 72, train loss: 17.588367462158203, test loss: 14.720372200012207\n",
      "h: 9 | epoch: 73, train loss: 17.357851028442383, test loss: 14.501599311828613\n",
      "h: 9 | epoch: 74, train loss: 17.131383895874023, test loss: 14.28657341003418\n",
      "h: 9 | epoch: 75, train loss: 16.908964157104492, test loss: 14.075282096862793\n",
      "h: 9 | epoch: 76, train loss: 16.690574645996094, test loss: 13.867713928222656\n",
      "h: 9 | epoch: 77, train loss: 16.476207733154297, test loss: 13.663851737976074\n",
      "h: 9 | epoch: 78, train loss: 16.26584815979004, test loss: 13.463688850402832\n",
      "h: 9 | epoch: 79, train loss: 16.059480667114258, test loss: 13.267210006713867\n",
      "h: 9 | epoch: 80, train loss: 15.857089042663574, test loss: 13.074397087097168\n",
      "h: 9 | epoch: 81, train loss: 15.658655166625977, test loss: 12.885229110717773\n",
      "h: 9 | epoch: 82, train loss: 15.464157104492188, test loss: 12.699691772460938\n",
      "h: 9 | epoch: 83, train loss: 15.273574829101562, test loss: 12.517759323120117\n",
      "h: 9 | epoch: 84, train loss: 15.086880683898926, test loss: 12.339414596557617\n",
      "h: 9 | epoch: 85, train loss: 14.904052734375, test loss: 12.164624214172363\n",
      "h: 9 | epoch: 86, train loss: 14.725065231323242, test loss: 11.993369102478027\n",
      "h: 9 | epoch: 87, train loss: 14.549883842468262, test loss: 11.825624465942383\n",
      "h: 9 | epoch: 88, train loss: 14.378480911254883, test loss: 11.661355972290039\n",
      "h: 9 | epoch: 89, train loss: 14.210827827453613, test loss: 11.50053596496582\n",
      "h: 9 | epoch: 90, train loss: 14.046880722045898, test loss: 11.343132972717285\n",
      "h: 9 | epoch: 91, train loss: 13.886614799499512, test loss: 11.189115524291992\n",
      "h: 9 | epoch: 92, train loss: 13.729990005493164, test loss: 11.038450241088867\n",
      "h: 9 | epoch: 93, train loss: 13.576967239379883, test loss: 10.89110279083252\n",
      "h: 9 | epoch: 94, train loss: 13.427505493164062, test loss: 10.74703598022461\n",
      "h: 9 | epoch: 95, train loss: 13.28156852722168, test loss: 10.606212615966797\n",
      "h: 9 | epoch: 96, train loss: 13.139114379882812, test loss: 10.46859359741211\n",
      "h: 9 | epoch: 97, train loss: 13.000094413757324, test loss: 10.334142684936523\n",
      "h: 9 | epoch: 98, train loss: 12.864468574523926, test loss: 10.202813148498535\n",
      "h: 9 | epoch: 99, train loss: 12.73219108581543, test loss: 10.07457447052002\n",
      "h: 10 | epoch: 0, train loss: 45.78995895385742, test loss: 40.77288055419922\n",
      "h: 10 | epoch: 1, train loss: 45.186073303222656, test loss: 40.23199462890625\n",
      "h: 10 | epoch: 2, train loss: 44.59223175048828, test loss: 39.69951629638672\n",
      "h: 10 | epoch: 3, train loss: 44.007972717285156, test loss: 39.17505645751953\n",
      "h: 10 | epoch: 4, train loss: 43.43286895751953, test loss: 38.658260345458984\n",
      "h: 10 | epoch: 5, train loss: 42.86650848388672, test loss: 38.14879608154297\n",
      "h: 10 | epoch: 6, train loss: 42.30852127075195, test loss: 37.646331787109375\n",
      "h: 10 | epoch: 7, train loss: 41.758541107177734, test loss: 37.15058898925781\n",
      "h: 10 | epoch: 8, train loss: 41.216243743896484, test loss: 36.66128158569336\n",
      "h: 10 | epoch: 9, train loss: 40.68131637573242, test loss: 36.17815399169922\n",
      "h: 10 | epoch: 10, train loss: 40.153472900390625, test loss: 35.70096969604492\n",
      "h: 10 | epoch: 11, train loss: 39.63243865966797, test loss: 35.229496002197266\n",
      "h: 10 | epoch: 12, train loss: 39.11796951293945, test loss: 34.76353073120117\n",
      "h: 10 | epoch: 13, train loss: 38.60981369018555, test loss: 34.30288314819336\n",
      "h: 10 | epoch: 14, train loss: 38.10777282714844, test loss: 33.84736633300781\n",
      "h: 10 | epoch: 15, train loss: 37.61162567138672, test loss: 33.396820068359375\n",
      "h: 10 | epoch: 16, train loss: 37.12120056152344, test loss: 32.95108413696289\n",
      "h: 10 | epoch: 17, train loss: 36.636314392089844, test loss: 32.51001739501953\n",
      "h: 10 | epoch: 18, train loss: 36.156803131103516, test loss: 32.073490142822266\n",
      "h: 10 | epoch: 19, train loss: 35.682533264160156, test loss: 31.641376495361328\n",
      "h: 10 | epoch: 20, train loss: 35.213348388671875, test loss: 31.213577270507812\n",
      "h: 10 | epoch: 21, train loss: 34.7491455078125, test loss: 30.789993286132812\n",
      "h: 10 | epoch: 22, train loss: 34.289798736572266, test loss: 30.370519638061523\n",
      "h: 10 | epoch: 23, train loss: 33.835208892822266, test loss: 29.9550838470459\n",
      "h: 10 | epoch: 24, train loss: 33.38528060913086, test loss: 29.543615341186523\n",
      "h: 10 | epoch: 25, train loss: 32.93993377685547, test loss: 29.13604164123535\n",
      "h: 10 | epoch: 26, train loss: 32.499088287353516, test loss: 28.732295989990234\n",
      "h: 10 | epoch: 27, train loss: 32.06268310546875, test loss: 28.332347869873047\n",
      "h: 10 | epoch: 28, train loss: 31.63064956665039, test loss: 27.936147689819336\n",
      "h: 10 | epoch: 29, train loss: 31.202951431274414, test loss: 27.543643951416016\n",
      "h: 10 | epoch: 30, train loss: 30.779529571533203, test loss: 27.15481185913086\n",
      "h: 10 | epoch: 31, train loss: 30.3603572845459, test loss: 26.769641876220703\n",
      "h: 10 | epoch: 32, train loss: 29.945398330688477, test loss: 26.388086318969727\n",
      "h: 10 | epoch: 33, train loss: 29.534631729125977, test loss: 26.010141372680664\n",
      "h: 10 | epoch: 34, train loss: 29.128026962280273, test loss: 25.63579750061035\n",
      "h: 10 | epoch: 35, train loss: 28.725582122802734, test loss: 25.26504898071289\n",
      "h: 10 | epoch: 36, train loss: 28.327280044555664, test loss: 24.897891998291016\n",
      "h: 10 | epoch: 37, train loss: 27.933115005493164, test loss: 24.53432846069336\n",
      "h: 10 | epoch: 38, train loss: 27.5430850982666, test loss: 24.17435073852539\n",
      "h: 10 | epoch: 39, train loss: 27.157196044921875, test loss: 23.817981719970703\n",
      "h: 10 | epoch: 40, train loss: 26.775455474853516, test loss: 23.465221405029297\n",
      "h: 10 | epoch: 41, train loss: 26.397863388061523, test loss: 23.116092681884766\n",
      "h: 10 | epoch: 42, train loss: 26.024438858032227, test loss: 22.770601272583008\n",
      "h: 10 | epoch: 43, train loss: 25.655193328857422, test loss: 22.428768157958984\n",
      "h: 10 | epoch: 44, train loss: 25.290142059326172, test loss: 22.090621948242188\n",
      "h: 10 | epoch: 45, train loss: 24.929309844970703, test loss: 21.756168365478516\n",
      "h: 10 | epoch: 46, train loss: 24.57271385192871, test loss: 21.42544174194336\n",
      "h: 10 | epoch: 47, train loss: 24.22037124633789, test loss: 21.09845542907715\n",
      "h: 10 | epoch: 48, train loss: 23.872304916381836, test loss: 20.77524757385254\n",
      "h: 10 | epoch: 49, train loss: 23.528545379638672, test loss: 20.455829620361328\n",
      "h: 10 | epoch: 50, train loss: 23.189111709594727, test loss: 20.140228271484375\n",
      "h: 10 | epoch: 51, train loss: 22.854028701782227, test loss: 19.82847785949707\n",
      "h: 10 | epoch: 52, train loss: 22.52332305908203, test loss: 19.520605087280273\n",
      "h: 10 | epoch: 53, train loss: 22.197019577026367, test loss: 19.216623306274414\n",
      "h: 10 | epoch: 54, train loss: 21.875141143798828, test loss: 18.916561126708984\n",
      "h: 10 | epoch: 55, train loss: 21.55771255493164, test loss: 18.620450973510742\n",
      "h: 10 | epoch: 56, train loss: 21.2447566986084, test loss: 18.32831382751465\n",
      "h: 10 | epoch: 57, train loss: 20.936298370361328, test loss: 18.040172576904297\n",
      "h: 10 | epoch: 58, train loss: 20.632354736328125, test loss: 17.75604820251465\n",
      "h: 10 | epoch: 59, train loss: 20.33295249938965, test loss: 17.475961685180664\n",
      "h: 10 | epoch: 60, train loss: 20.038105010986328, test loss: 17.199934005737305\n",
      "h: 10 | epoch: 61, train loss: 19.747835159301758, test loss: 16.927978515625\n",
      "h: 10 | epoch: 62, train loss: 19.4621524810791, test loss: 16.660123825073242\n",
      "h: 10 | epoch: 63, train loss: 19.181076049804688, test loss: 16.39637565612793\n",
      "h: 10 | epoch: 64, train loss: 18.904617309570312, test loss: 16.136751174926758\n",
      "h: 10 | epoch: 65, train loss: 18.632783889770508, test loss: 15.881254196166992\n",
      "h: 10 | epoch: 66, train loss: 18.365585327148438, test loss: 15.629905700683594\n",
      "h: 10 | epoch: 67, train loss: 18.103031158447266, test loss: 15.382711410522461\n",
      "h: 10 | epoch: 68, train loss: 17.84511947631836, test loss: 15.139671325683594\n",
      "h: 10 | epoch: 69, train loss: 17.591854095458984, test loss: 14.900793075561523\n",
      "h: 10 | epoch: 70, train loss: 17.343236923217773, test loss: 14.666078567504883\n",
      "h: 10 | epoch: 71, train loss: 17.099260330200195, test loss: 14.435523986816406\n",
      "h: 10 | epoch: 72, train loss: 16.85991859436035, test loss: 14.209127426147461\n",
      "h: 10 | epoch: 73, train loss: 16.625205993652344, test loss: 13.986885070800781\n",
      "h: 10 | epoch: 74, train loss: 16.39510726928711, test loss: 13.768786430358887\n",
      "h: 10 | epoch: 75, train loss: 16.16961669921875, test loss: 13.55482292175293\n",
      "h: 10 | epoch: 76, train loss: 15.948709487915039, test loss: 13.34498405456543\n",
      "h: 10 | epoch: 77, train loss: 15.732373237609863, test loss: 13.139253616333008\n",
      "h: 10 | epoch: 78, train loss: 15.52058219909668, test loss: 12.937614440917969\n",
      "h: 10 | epoch: 79, train loss: 15.313314437866211, test loss: 12.740043640136719\n",
      "h: 10 | epoch: 80, train loss: 15.110544204711914, test loss: 12.546526908874512\n",
      "h: 10 | epoch: 81, train loss: 14.912243843078613, test loss: 12.357037544250488\n",
      "h: 10 | epoch: 82, train loss: 14.718381881713867, test loss: 12.171552658081055\n",
      "h: 10 | epoch: 83, train loss: 14.528921127319336, test loss: 11.990038871765137\n",
      "h: 10 | epoch: 84, train loss: 14.343831062316895, test loss: 11.812471389770508\n",
      "h: 10 | epoch: 85, train loss: 14.16307258605957, test loss: 11.638813018798828\n",
      "h: 10 | epoch: 86, train loss: 13.986602783203125, test loss: 11.469036102294922\n",
      "h: 10 | epoch: 87, train loss: 13.814382553100586, test loss: 11.30309772491455\n",
      "h: 10 | epoch: 88, train loss: 13.64636516571045, test loss: 11.140968322753906\n",
      "h: 10 | epoch: 89, train loss: 13.482503890991211, test loss: 10.98260498046875\n",
      "h: 10 | epoch: 90, train loss: 13.32275676727295, test loss: 10.827962875366211\n",
      "h: 10 | epoch: 91, train loss: 13.167065620422363, test loss: 10.677003860473633\n",
      "h: 10 | epoch: 92, train loss: 13.01538372039795, test loss: 10.529682159423828\n",
      "h: 10 | epoch: 93, train loss: 12.867657661437988, test loss: 10.385953903198242\n",
      "h: 10 | epoch: 94, train loss: 12.723832130432129, test loss: 10.245773315429688\n",
      "h: 10 | epoch: 95, train loss: 12.58385181427002, test loss: 10.109088897705078\n",
      "h: 10 | epoch: 96, train loss: 12.447656631469727, test loss: 9.975851058959961\n",
      "h: 10 | epoch: 97, train loss: 12.315192222595215, test loss: 9.846012115478516\n",
      "h: 10 | epoch: 98, train loss: 12.186396598815918, test loss: 9.71951961517334\n",
      "h: 10 | epoch: 99, train loss: 12.061208724975586, test loss: 9.596322059631348\n",
      "h: 11 | epoch: 0, train loss: 45.628482818603516, test loss: 40.80493927001953\n",
      "h: 11 | epoch: 1, train loss: 45.00543212890625, test loss: 40.23656463623047\n",
      "h: 11 | epoch: 2, train loss: 44.394107818603516, test loss: 39.67839431762695\n",
      "h: 11 | epoch: 3, train loss: 43.79397201538086, test loss: 39.129974365234375\n",
      "h: 11 | epoch: 4, train loss: 43.204524993896484, test loss: 38.59087371826172\n",
      "h: 11 | epoch: 5, train loss: 42.62528991699219, test loss: 38.060691833496094\n",
      "h: 11 | epoch: 6, train loss: 42.055824279785156, test loss: 37.539031982421875\n",
      "h: 11 | epoch: 7, train loss: 41.495704650878906, test loss: 37.025550842285156\n",
      "h: 11 | epoch: 8, train loss: 40.944549560546875, test loss: 36.51991271972656\n",
      "h: 11 | epoch: 9, train loss: 40.40198516845703, test loss: 36.02179718017578\n",
      "h: 11 | epoch: 10, train loss: 39.86768341064453, test loss: 35.530921936035156\n",
      "h: 11 | epoch: 11, train loss: 39.3412971496582, test loss: 35.04700469970703\n",
      "h: 11 | epoch: 12, train loss: 38.82253646850586, test loss: 34.56977081298828\n",
      "h: 11 | epoch: 13, train loss: 38.311134338378906, test loss: 34.09901428222656\n",
      "h: 11 | epoch: 14, train loss: 37.806800842285156, test loss: 33.634483337402344\n",
      "h: 11 | epoch: 15, train loss: 37.30929946899414, test loss: 33.175968170166016\n",
      "h: 11 | epoch: 16, train loss: 36.81840515136719, test loss: 32.72328186035156\n",
      "h: 11 | epoch: 17, train loss: 36.333900451660156, test loss: 32.276222229003906\n",
      "h: 11 | epoch: 18, train loss: 35.85558319091797, test loss: 31.834636688232422\n",
      "h: 11 | epoch: 19, train loss: 35.38325881958008, test loss: 31.398357391357422\n",
      "h: 11 | epoch: 20, train loss: 34.91676712036133, test loss: 30.967248916625977\n",
      "h: 11 | epoch: 21, train loss: 34.45594024658203, test loss: 30.541156768798828\n",
      "h: 11 | epoch: 22, train loss: 34.000633239746094, test loss: 30.119958877563477\n",
      "h: 11 | epoch: 23, train loss: 33.55070114135742, test loss: 29.703542709350586\n",
      "h: 11 | epoch: 24, train loss: 33.106021881103516, test loss: 29.291797637939453\n",
      "h: 11 | epoch: 25, train loss: 32.666481018066406, test loss: 28.884634017944336\n",
      "h: 11 | epoch: 26, train loss: 32.231971740722656, test loss: 28.481937408447266\n",
      "h: 11 | epoch: 27, train loss: 31.802383422851562, test loss: 28.08364486694336\n",
      "h: 11 | epoch: 28, train loss: 31.377639770507812, test loss: 27.689678192138672\n",
      "h: 11 | epoch: 29, train loss: 30.957651138305664, test loss: 27.29996109008789\n",
      "h: 11 | epoch: 30, train loss: 30.542346954345703, test loss: 26.914438247680664\n",
      "h: 11 | epoch: 31, train loss: 30.13165855407715, test loss: 26.533050537109375\n",
      "h: 11 | epoch: 32, train loss: 29.725528717041016, test loss: 26.155752182006836\n",
      "h: 11 | epoch: 33, train loss: 29.323902130126953, test loss: 25.782495498657227\n",
      "h: 11 | epoch: 34, train loss: 28.92673110961914, test loss: 25.41324234008789\n",
      "h: 11 | epoch: 35, train loss: 28.533971786499023, test loss: 25.047954559326172\n",
      "h: 11 | epoch: 36, train loss: 28.145593643188477, test loss: 24.686613082885742\n",
      "h: 11 | epoch: 37, train loss: 27.761560440063477, test loss: 24.32918357849121\n",
      "h: 11 | epoch: 38, train loss: 27.381847381591797, test loss: 23.975648880004883\n",
      "h: 11 | epoch: 39, train loss: 27.00642967224121, test loss: 23.625988006591797\n",
      "h: 11 | epoch: 40, train loss: 26.63529396057129, test loss: 23.280193328857422\n",
      "h: 11 | epoch: 41, train loss: 26.268421173095703, test loss: 22.93824577331543\n",
      "h: 11 | epoch: 42, train loss: 25.905797958374023, test loss: 22.600143432617188\n",
      "h: 11 | epoch: 43, train loss: 25.547420501708984, test loss: 22.265878677368164\n",
      "h: 11 | epoch: 44, train loss: 25.193283081054688, test loss: 21.93544578552246\n",
      "h: 11 | epoch: 45, train loss: 24.843381881713867, test loss: 21.60885238647461\n",
      "h: 11 | epoch: 46, train loss: 24.497713088989258, test loss: 21.28609275817871\n",
      "h: 11 | epoch: 47, train loss: 24.15628433227539, test loss: 20.9671688079834\n",
      "h: 11 | epoch: 48, train loss: 23.819095611572266, test loss: 20.65208625793457\n",
      "h: 11 | epoch: 49, train loss: 23.48615074157715, test loss: 20.34085464477539\n",
      "h: 11 | epoch: 50, train loss: 23.157459259033203, test loss: 20.033479690551758\n",
      "h: 11 | epoch: 51, train loss: 22.833026885986328, test loss: 19.729963302612305\n",
      "h: 11 | epoch: 52, train loss: 22.51285743713379, test loss: 19.43031883239746\n",
      "h: 11 | epoch: 53, train loss: 22.196964263916016, test loss: 19.134552001953125\n",
      "h: 11 | epoch: 54, train loss: 21.885356903076172, test loss: 18.842681884765625\n",
      "h: 11 | epoch: 55, train loss: 21.578041076660156, test loss: 18.554698944091797\n",
      "h: 11 | epoch: 56, train loss: 21.275028228759766, test loss: 18.2706298828125\n",
      "h: 11 | epoch: 57, train loss: 20.976329803466797, test loss: 17.9904727935791\n",
      "h: 11 | epoch: 58, train loss: 20.681949615478516, test loss: 17.714242935180664\n",
      "h: 11 | epoch: 59, train loss: 20.391902923583984, test loss: 17.441944122314453\n",
      "h: 11 | epoch: 60, train loss: 20.10619354248047, test loss: 17.173585891723633\n",
      "h: 11 | epoch: 61, train loss: 19.8248291015625, test loss: 16.909177780151367\n",
      "h: 11 | epoch: 62, train loss: 19.54781723022461, test loss: 16.648717880249023\n",
      "h: 11 | epoch: 63, train loss: 19.275163650512695, test loss: 16.392223358154297\n",
      "h: 11 | epoch: 64, train loss: 19.00687026977539, test loss: 16.139686584472656\n",
      "h: 11 | epoch: 65, train loss: 18.742938995361328, test loss: 15.891118049621582\n",
      "h: 11 | epoch: 66, train loss: 18.483379364013672, test loss: 15.646512985229492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 11 | epoch: 67, train loss: 18.22818374633789, test loss: 15.405881881713867\n",
      "h: 11 | epoch: 68, train loss: 17.97734832763672, test loss: 15.169215202331543\n",
      "h: 11 | epoch: 69, train loss: 17.730876922607422, test loss: 14.936511039733887\n",
      "h: 11 | epoch: 70, train loss: 17.488765716552734, test loss: 14.70776653289795\n",
      "h: 11 | epoch: 71, train loss: 17.25100326538086, test loss: 14.482978820800781\n",
      "h: 11 | epoch: 72, train loss: 17.0175838470459, test loss: 14.262136459350586\n",
      "h: 11 | epoch: 73, train loss: 16.788497924804688, test loss: 14.045234680175781\n",
      "h: 11 | epoch: 74, train loss: 16.563730239868164, test loss: 13.832258224487305\n",
      "h: 11 | epoch: 75, train loss: 16.34326934814453, test loss: 13.623201370239258\n",
      "h: 11 | epoch: 76, train loss: 16.12710189819336, test loss: 13.41804313659668\n",
      "h: 11 | epoch: 77, train loss: 15.915204048156738, test loss: 13.216772079467773\n",
      "h: 11 | epoch: 78, train loss: 15.707559585571289, test loss: 13.019366264343262\n",
      "h: 11 | epoch: 79, train loss: 15.50414752960205, test loss: 12.82581615447998\n",
      "h: 11 | epoch: 80, train loss: 15.30494213104248, test loss: 12.636090278625488\n",
      "h: 11 | epoch: 81, train loss: 15.1099214553833, test loss: 12.450169563293457\n",
      "h: 11 | epoch: 82, train loss: 14.919052124023438, test loss: 12.268030166625977\n",
      "h: 11 | epoch: 83, train loss: 14.732307434082031, test loss: 12.08964729309082\n",
      "h: 11 | epoch: 84, train loss: 14.549657821655273, test loss: 11.91499137878418\n",
      "h: 11 | epoch: 85, train loss: 14.371068000793457, test loss: 11.74403190612793\n",
      "h: 11 | epoch: 86, train loss: 14.196504592895508, test loss: 11.576742172241211\n",
      "h: 11 | epoch: 87, train loss: 14.025927543640137, test loss: 11.413084983825684\n",
      "h: 11 | epoch: 88, train loss: 13.85930347442627, test loss: 11.253029823303223\n",
      "h: 11 | epoch: 89, train loss: 13.696589469909668, test loss: 11.096537590026855\n",
      "h: 11 | epoch: 90, train loss: 13.537742614746094, test loss: 10.943574905395508\n",
      "h: 11 | epoch: 91, train loss: 13.382722854614258, test loss: 10.79410457611084\n",
      "h: 11 | epoch: 92, train loss: 13.231480598449707, test loss: 10.64808464050293\n",
      "h: 11 | epoch: 93, train loss: 13.083974838256836, test loss: 10.505473136901855\n",
      "h: 11 | epoch: 94, train loss: 12.940156936645508, test loss: 10.366231918334961\n",
      "h: 11 | epoch: 95, train loss: 12.799975395202637, test loss: 10.230316162109375\n",
      "h: 11 | epoch: 96, train loss: 12.663385391235352, test loss: 10.097679138183594\n",
      "h: 11 | epoch: 97, train loss: 12.5303316116333, test loss: 9.968281745910645\n",
      "h: 11 | epoch: 98, train loss: 12.400762557983398, test loss: 9.842074394226074\n",
      "h: 11 | epoch: 99, train loss: 12.274625778198242, test loss: 9.719009399414062\n",
      "h: 12 | epoch: 0, train loss: 55.916297912597656, test loss: 50.51276397705078\n",
      "h: 12 | epoch: 1, train loss: 54.77008819580078, test loss: 49.485755920410156\n",
      "h: 12 | epoch: 2, train loss: 53.65825653076172, test loss: 48.48810958862305\n",
      "h: 12 | epoch: 3, train loss: 52.578887939453125, test loss: 47.518226623535156\n",
      "h: 12 | epoch: 4, train loss: 51.5301628112793, test loss: 46.574581146240234\n",
      "h: 12 | epoch: 5, train loss: 50.510467529296875, test loss: 45.65580749511719\n",
      "h: 12 | epoch: 6, train loss: 49.51824951171875, test loss: 44.76062774658203\n",
      "h: 12 | epoch: 7, train loss: 48.55210494995117, test loss: 43.8878288269043\n",
      "h: 12 | epoch: 8, train loss: 47.61073303222656, test loss: 43.036338806152344\n",
      "h: 12 | epoch: 9, train loss: 46.69291305541992, test loss: 42.205116271972656\n",
      "h: 12 | epoch: 10, train loss: 45.79751968383789, test loss: 41.39321517944336\n",
      "h: 12 | epoch: 11, train loss: 44.92352294921875, test loss: 40.599769592285156\n",
      "h: 12 | epoch: 12, train loss: 44.0699462890625, test loss: 39.823951721191406\n",
      "h: 12 | epoch: 13, train loss: 43.235904693603516, test loss: 39.06501770019531\n",
      "h: 12 | epoch: 14, train loss: 42.42056655883789, test loss: 38.322261810302734\n",
      "h: 12 | epoch: 15, train loss: 41.6231575012207, test loss: 37.595035552978516\n",
      "h: 12 | epoch: 16, train loss: 40.84296417236328, test loss: 36.88273620605469\n",
      "h: 12 | epoch: 17, train loss: 40.079322814941406, test loss: 36.1848030090332\n",
      "h: 12 | epoch: 18, train loss: 39.331626892089844, test loss: 35.50070571899414\n",
      "h: 12 | epoch: 19, train loss: 38.599308013916016, test loss: 34.829978942871094\n",
      "h: 12 | epoch: 20, train loss: 37.881813049316406, test loss: 34.172176361083984\n",
      "h: 12 | epoch: 21, train loss: 37.17867660522461, test loss: 33.52687072753906\n",
      "h: 12 | epoch: 22, train loss: 36.48944091796875, test loss: 32.89368438720703\n",
      "h: 12 | epoch: 23, train loss: 35.81367874145508, test loss: 32.272274017333984\n",
      "h: 12 | epoch: 24, train loss: 35.15099334716797, test loss: 31.662296295166016\n",
      "h: 12 | epoch: 25, train loss: 34.50102996826172, test loss: 31.063446044921875\n",
      "h: 12 | epoch: 26, train loss: 33.86345291137695, test loss: 30.47544288635254\n",
      "h: 12 | epoch: 27, train loss: 33.237945556640625, test loss: 29.898019790649414\n",
      "h: 12 | epoch: 28, train loss: 32.62422561645508, test loss: 29.330942153930664\n",
      "h: 12 | epoch: 29, train loss: 32.022010803222656, test loss: 28.77398109436035\n",
      "h: 12 | epoch: 30, train loss: 31.431072235107422, test loss: 28.2269344329834\n",
      "h: 12 | epoch: 31, train loss: 30.85116195678711, test loss: 27.689599990844727\n",
      "h: 12 | epoch: 32, train loss: 30.282079696655273, test loss: 27.161800384521484\n",
      "h: 12 | epoch: 33, train loss: 29.723617553710938, test loss: 26.643381118774414\n",
      "h: 12 | epoch: 34, train loss: 29.17559242248535, test loss: 26.134159088134766\n",
      "h: 12 | epoch: 35, train loss: 28.637828826904297, test loss: 25.634023666381836\n",
      "h: 12 | epoch: 36, train loss: 28.11016845703125, test loss: 25.142826080322266\n",
      "h: 12 | epoch: 37, train loss: 27.59246826171875, test loss: 24.66043472290039\n",
      "h: 12 | epoch: 38, train loss: 27.08457374572754, test loss: 24.18674659729004\n",
      "h: 12 | epoch: 39, train loss: 26.58636474609375, test loss: 23.72164535522461\n",
      "h: 12 | epoch: 40, train loss: 26.09771156311035, test loss: 23.265029907226562\n",
      "h: 12 | epoch: 41, train loss: 25.61849021911621, test loss: 22.816802978515625\n",
      "h: 12 | epoch: 42, train loss: 25.148609161376953, test loss: 22.376874923706055\n",
      "h: 12 | epoch: 43, train loss: 24.687946319580078, test loss: 21.94515609741211\n",
      "h: 12 | epoch: 44, train loss: 24.236408233642578, test loss: 21.52156639099121\n",
      "h: 12 | epoch: 45, train loss: 23.793895721435547, test loss: 21.106021881103516\n",
      "h: 12 | epoch: 46, train loss: 23.36031723022461, test loss: 20.698448181152344\n",
      "h: 12 | epoch: 47, train loss: 22.935583114624023, test loss: 20.298770904541016\n",
      "h: 12 | epoch: 48, train loss: 22.51960563659668, test loss: 19.90692710876465\n",
      "h: 12 | epoch: 49, train loss: 22.112300872802734, test loss: 19.5228328704834\n",
      "h: 12 | epoch: 50, train loss: 21.713581085205078, test loss: 19.14642906188965\n",
      "h: 12 | epoch: 51, train loss: 21.323368072509766, test loss: 18.777637481689453\n",
      "h: 12 | epoch: 52, train loss: 20.941577911376953, test loss: 18.416400909423828\n",
      "h: 12 | epoch: 53, train loss: 20.568130493164062, test loss: 18.062646865844727\n",
      "h: 12 | epoch: 54, train loss: 20.202945709228516, test loss: 17.7163028717041\n",
      "h: 12 | epoch: 55, train loss: 19.845937728881836, test loss: 17.377307891845703\n",
      "h: 12 | epoch: 56, train loss: 19.497028350830078, test loss: 17.045589447021484\n",
      "h: 12 | epoch: 57, train loss: 19.1561336517334, test loss: 16.72108268737793\n",
      "h: 12 | epoch: 58, train loss: 18.823169708251953, test loss: 16.403715133666992\n",
      "h: 12 | epoch: 59, train loss: 18.498056411743164, test loss: 16.093416213989258\n",
      "h: 12 | epoch: 60, train loss: 18.180700302124023, test loss: 15.79011058807373\n",
      "h: 12 | epoch: 61, train loss: 17.871021270751953, test loss: 15.49372673034668\n",
      "h: 12 | epoch: 62, train loss: 17.568923950195312, test loss: 15.204193115234375\n",
      "h: 12 | epoch: 63, train loss: 17.27432632446289, test loss: 14.921429634094238\n",
      "h: 12 | epoch: 64, train loss: 16.98712921142578, test loss: 14.645355224609375\n",
      "h: 12 | epoch: 65, train loss: 16.707244873046875, test loss: 14.375898361206055\n",
      "h: 12 | epoch: 66, train loss: 16.434574127197266, test loss: 14.112974166870117\n",
      "h: 12 | epoch: 67, train loss: 16.169025421142578, test loss: 13.85649585723877\n",
      "h: 12 | epoch: 68, train loss: 15.910493850708008, test loss: 13.606389999389648\n",
      "h: 12 | epoch: 69, train loss: 15.65888500213623, test loss: 13.362556457519531\n",
      "h: 12 | epoch: 70, train loss: 15.414093017578125, test loss: 13.124918937683105\n",
      "h: 12 | epoch: 71, train loss: 15.176020622253418, test loss: 12.89338493347168\n",
      "h: 12 | epoch: 72, train loss: 14.944555282592773, test loss: 12.667865753173828\n",
      "h: 12 | epoch: 73, train loss: 14.719596862792969, test loss: 12.448266983032227\n",
      "h: 12 | epoch: 74, train loss: 14.501035690307617, test loss: 12.234498977661133\n",
      "h: 12 | epoch: 75, train loss: 14.288763046264648, test loss: 12.026468276977539\n",
      "h: 12 | epoch: 76, train loss: 14.082667350769043, test loss: 11.824077606201172\n",
      "h: 12 | epoch: 77, train loss: 13.882641792297363, test loss: 11.62723159790039\n",
      "h: 12 | epoch: 78, train loss: 13.688570022583008, test loss: 11.435832023620605\n",
      "h: 12 | epoch: 79, train loss: 13.500340461730957, test loss: 11.249781608581543\n",
      "h: 12 | epoch: 80, train loss: 13.317838668823242, test loss: 11.068986892700195\n",
      "h: 12 | epoch: 81, train loss: 13.140953063964844, test loss: 10.893342971801758\n",
      "h: 12 | epoch: 82, train loss: 12.969568252563477, test loss: 10.722747802734375\n",
      "h: 12 | epoch: 83, train loss: 12.803564071655273, test loss: 10.557107925415039\n",
      "h: 12 | epoch: 84, train loss: 12.642831802368164, test loss: 10.396320343017578\n",
      "h: 12 | epoch: 85, train loss: 12.487252235412598, test loss: 10.240279197692871\n",
      "h: 12 | epoch: 86, train loss: 12.336710929870605, test loss: 10.088894844055176\n",
      "h: 12 | epoch: 87, train loss: 12.191091537475586, test loss: 9.942057609558105\n",
      "h: 12 | epoch: 88, train loss: 12.050281524658203, test loss: 9.799670219421387\n",
      "h: 12 | epoch: 89, train loss: 11.914163589477539, test loss: 9.66163158416748\n",
      "h: 12 | epoch: 90, train loss: 11.782624244689941, test loss: 9.527841567993164\n",
      "h: 12 | epoch: 91, train loss: 11.655550003051758, test loss: 9.398202896118164\n",
      "h: 12 | epoch: 92, train loss: 11.53282642364502, test loss: 9.272616386413574\n",
      "h: 12 | epoch: 93, train loss: 11.414342880249023, test loss: 9.150979042053223\n",
      "h: 12 | epoch: 94, train loss: 11.299986839294434, test loss: 9.033197402954102\n",
      "h: 12 | epoch: 95, train loss: 11.189648628234863, test loss: 8.919174194335938\n",
      "h: 12 | epoch: 96, train loss: 11.083220481872559, test loss: 8.80881404876709\n",
      "h: 12 | epoch: 97, train loss: 10.98059368133545, test loss: 8.702016830444336\n",
      "h: 12 | epoch: 98, train loss: 10.881660461425781, test loss: 8.598699569702148\n",
      "h: 12 | epoch: 99, train loss: 10.786316871643066, test loss: 8.498760223388672\n",
      "h: 13 | epoch: 0, train loss: 37.74980545043945, test loss: 33.40814971923828\n",
      "h: 13 | epoch: 1, train loss: 36.907005310058594, test loss: 32.638423919677734\n",
      "h: 13 | epoch: 2, train loss: 36.0864143371582, test loss: 31.888309478759766\n",
      "h: 13 | epoch: 3, train loss: 35.28721618652344, test loss: 31.15712547302246\n",
      "h: 13 | epoch: 4, train loss: 34.50867462158203, test loss: 30.444232940673828\n",
      "h: 13 | epoch: 5, train loss: 33.750083923339844, test loss: 29.7490291595459\n",
      "h: 13 | epoch: 6, train loss: 33.01079177856445, test loss: 29.070964813232422\n",
      "h: 13 | epoch: 7, train loss: 32.290184020996094, test loss: 28.40951919555664\n",
      "h: 13 | epoch: 8, train loss: 31.58770179748535, test loss: 27.76420021057129\n",
      "h: 13 | epoch: 9, train loss: 30.902807235717773, test loss: 27.13456153869629\n",
      "h: 13 | epoch: 10, train loss: 30.235004425048828, test loss: 26.520153045654297\n",
      "h: 13 | epoch: 11, train loss: 29.583831787109375, test loss: 25.92059898376465\n",
      "h: 13 | epoch: 12, train loss: 28.948837280273438, test loss: 25.335506439208984\n",
      "h: 13 | epoch: 13, train loss: 28.329614639282227, test loss: 24.764522552490234\n",
      "h: 13 | epoch: 14, train loss: 27.725778579711914, test loss: 24.20730972290039\n",
      "h: 13 | epoch: 15, train loss: 27.136951446533203, test loss: 23.66354751586914\n",
      "h: 13 | epoch: 16, train loss: 26.562786102294922, test loss: 23.132936477661133\n",
      "h: 13 | epoch: 17, train loss: 26.002954483032227, test loss: 22.615192413330078\n",
      "h: 13 | epoch: 18, train loss: 25.457138061523438, test loss: 22.11003303527832\n",
      "h: 13 | epoch: 19, train loss: 24.925039291381836, test loss: 21.617202758789062\n",
      "h: 13 | epoch: 20, train loss: 24.406370162963867, test loss: 21.136451721191406\n",
      "h: 13 | epoch: 21, train loss: 23.900859832763672, test loss: 20.66753578186035\n",
      "h: 13 | epoch: 22, train loss: 23.40823745727539, test loss: 20.21022605895996\n",
      "h: 13 | epoch: 23, train loss: 22.92824935913086, test loss: 19.76430892944336\n",
      "h: 13 | epoch: 24, train loss: 22.460655212402344, test loss: 19.329547882080078\n",
      "h: 13 | epoch: 25, train loss: 22.00521469116211, test loss: 18.9057559967041\n",
      "h: 13 | epoch: 26, train loss: 21.561702728271484, test loss: 18.492712020874023\n",
      "h: 13 | epoch: 27, train loss: 21.129886627197266, test loss: 18.09023666381836\n",
      "h: 13 | epoch: 28, train loss: 20.709550857543945, test loss: 17.698122024536133\n",
      "h: 13 | epoch: 29, train loss: 20.300485610961914, test loss: 17.31618881225586\n",
      "h: 13 | epoch: 30, train loss: 19.90247917175293, test loss: 16.94424819946289\n",
      "h: 13 | epoch: 31, train loss: 19.515323638916016, test loss: 16.58211898803711\n",
      "h: 13 | epoch: 32, train loss: 19.138822555541992, test loss: 16.2296199798584\n",
      "h: 13 | epoch: 33, train loss: 18.772777557373047, test loss: 15.886579513549805\n",
      "h: 13 | epoch: 34, train loss: 18.416982650756836, test loss: 15.552821159362793\n",
      "h: 13 | epoch: 35, train loss: 18.07126235961914, test loss: 15.228170394897461\n",
      "h: 13 | epoch: 36, train loss: 17.735408782958984, test loss: 14.912454605102539\n",
      "h: 13 | epoch: 37, train loss: 17.40923500061035, test loss: 14.60551929473877\n",
      "h: 13 | epoch: 38, train loss: 17.092559814453125, test loss: 14.307182312011719\n",
      "h: 13 | epoch: 39, train loss: 16.785194396972656, test loss: 14.017278671264648\n",
      "h: 13 | epoch: 40, train loss: 16.486949920654297, test loss: 13.7356538772583\n",
      "h: 13 | epoch: 41, train loss: 16.197643280029297, test loss: 13.462133407592773\n",
      "h: 13 | epoch: 42, train loss: 15.917094230651855, test loss: 13.196560859680176\n",
      "h: 13 | epoch: 43, train loss: 15.645120620727539, test loss: 12.938766479492188\n",
      "h: 13 | epoch: 44, train loss: 15.381536483764648, test loss: 12.688600540161133\n",
      "h: 13 | epoch: 45, train loss: 15.126169204711914, test loss: 12.445889472961426\n",
      "h: 13 | epoch: 46, train loss: 14.878832817077637, test loss: 12.21048355102539\n",
      "h: 13 | epoch: 47, train loss: 14.639355659484863, test loss: 11.982221603393555\n",
      "h: 13 | epoch: 48, train loss: 14.407559394836426, test loss: 11.760945320129395\n",
      "h: 13 | epoch: 49, train loss: 14.183263778686523, test loss: 11.546499252319336\n",
      "h: 13 | epoch: 50, train loss: 13.966299057006836, test loss: 11.338725090026855\n",
      "h: 13 | epoch: 51, train loss: 13.756489753723145, test loss: 11.137472152709961\n",
      "h: 13 | epoch: 52, train loss: 13.553665161132812, test loss: 10.942587852478027\n",
      "h: 13 | epoch: 53, train loss: 13.357653617858887, test loss: 10.753915786743164\n",
      "h: 13 | epoch: 54, train loss: 13.168283462524414, test loss: 10.571306228637695\n",
      "h: 13 | epoch: 55, train loss: 12.985386848449707, test loss: 10.394612312316895\n",
      "h: 13 | epoch: 56, train loss: 12.808801651000977, test loss: 10.22368335723877\n",
      "h: 13 | epoch: 57, train loss: 12.63835620880127, test loss: 10.05837345123291\n",
      "h: 13 | epoch: 58, train loss: 12.473894119262695, test loss: 9.898538589477539\n",
      "h: 13 | epoch: 59, train loss: 12.315248489379883, test loss: 9.744035720825195\n",
      "h: 13 | epoch: 60, train loss: 12.162264823913574, test loss: 9.594720840454102\n",
      "h: 13 | epoch: 61, train loss: 12.014781951904297, test loss: 9.450453758239746\n",
      "h: 13 | epoch: 62, train loss: 11.872644424438477, test loss: 9.311098098754883\n",
      "h: 13 | epoch: 63, train loss: 11.735699653625488, test loss: 9.176518440246582\n",
      "h: 13 | epoch: 64, train loss: 11.603797912597656, test loss: 9.046577453613281\n",
      "h: 13 | epoch: 65, train loss: 11.476786613464355, test loss: 8.921144485473633\n",
      "h: 13 | epoch: 66, train loss: 11.354524612426758, test loss: 8.800092697143555\n",
      "h: 13 | epoch: 67, train loss: 11.236865043640137, test loss: 8.683287620544434\n",
      "h: 13 | epoch: 68, train loss: 11.123664855957031, test loss: 8.570609092712402\n",
      "h: 13 | epoch: 69, train loss: 11.014788627624512, test loss: 8.461932182312012\n",
      "h: 13 | epoch: 70, train loss: 10.910097122192383, test loss: 8.357133865356445\n",
      "h: 13 | epoch: 71, train loss: 10.809457778930664, test loss: 8.256096839904785\n",
      "h: 13 | epoch: 72, train loss: 10.712738037109375, test loss: 8.158702850341797\n",
      "h: 13 | epoch: 73, train loss: 10.619811058044434, test loss: 8.064841270446777\n",
      "h: 13 | epoch: 74, train loss: 10.530550003051758, test loss: 7.974396705627441\n",
      "h: 13 | epoch: 75, train loss: 10.44483470916748, test loss: 7.887261867523193\n",
      "h: 13 | epoch: 76, train loss: 10.362543106079102, test loss: 7.803328037261963\n",
      "h: 13 | epoch: 77, train loss: 10.283556938171387, test loss: 7.722495079040527\n",
      "h: 13 | epoch: 78, train loss: 10.207764625549316, test loss: 7.644654273986816\n",
      "h: 13 | epoch: 79, train loss: 10.135052680969238, test loss: 7.569711208343506\n",
      "h: 13 | epoch: 80, train loss: 10.065309524536133, test loss: 7.497566223144531\n",
      "h: 13 | epoch: 81, train loss: 9.998434066772461, test loss: 7.428128242492676\n",
      "h: 13 | epoch: 82, train loss: 9.934319496154785, test loss: 7.361299991607666\n",
      "h: 13 | epoch: 83, train loss: 9.8728666305542, test loss: 7.296994686126709\n",
      "h: 13 | epoch: 84, train loss: 9.81397819519043, test loss: 7.2351250648498535\n",
      "h: 13 | epoch: 85, train loss: 9.757558822631836, test loss: 7.175604820251465\n",
      "h: 13 | epoch: 86, train loss: 9.703516006469727, test loss: 7.118351936340332\n",
      "h: 13 | epoch: 87, train loss: 9.65176010131836, test loss: 7.063287258148193\n",
      "h: 13 | epoch: 88, train loss: 9.602205276489258, test loss: 7.0103302001953125\n",
      "h: 13 | epoch: 89, train loss: 9.554765701293945, test loss: 6.959407806396484\n",
      "h: 13 | epoch: 90, train loss: 9.509361267089844, test loss: 6.910445213317871\n",
      "h: 13 | epoch: 91, train loss: 9.465911865234375, test loss: 6.863370418548584\n",
      "h: 13 | epoch: 92, train loss: 9.424341201782227, test loss: 6.818115234375\n",
      "h: 13 | epoch: 93, train loss: 9.384575843811035, test loss: 6.774613857269287\n",
      "h: 13 | epoch: 94, train loss: 9.346542358398438, test loss: 6.732801914215088\n",
      "h: 13 | epoch: 95, train loss: 9.310171127319336, test loss: 6.692613124847412\n",
      "h: 13 | epoch: 96, train loss: 9.275398254394531, test loss: 6.653990745544434\n",
      "h: 13 | epoch: 97, train loss: 9.242156982421875, test loss: 6.616870880126953\n",
      "h: 13 | epoch: 98, train loss: 9.2103853225708, test loss: 6.581203460693359\n",
      "h: 13 | epoch: 99, train loss: 9.180021286010742, test loss: 6.546929836273193\n",
      "h: 14 | epoch: 0, train loss: 39.66518020629883, test loss: 35.66232681274414\n",
      "h: 14 | epoch: 1, train loss: 39.109413146972656, test loss: 35.14813995361328\n",
      "h: 14 | epoch: 2, train loss: 38.56214141845703, test loss: 34.641456604003906\n",
      "h: 14 | epoch: 3, train loss: 38.02305221557617, test loss: 34.14202117919922\n",
      "h: 14 | epoch: 4, train loss: 37.491859436035156, test loss: 33.64958572387695\n",
      "h: 14 | epoch: 5, train loss: 36.968299865722656, test loss: 33.163917541503906\n",
      "h: 14 | epoch: 6, train loss: 36.452125549316406, test loss: 32.684810638427734\n",
      "h: 14 | epoch: 7, train loss: 35.94310760498047, test loss: 32.212059020996094\n",
      "h: 14 | epoch: 8, train loss: 35.4410285949707, test loss: 31.7454776763916\n",
      "h: 14 | epoch: 9, train loss: 34.9456901550293, test loss: 31.284900665283203\n",
      "h: 14 | epoch: 10, train loss: 34.4569091796875, test loss: 30.83016014099121\n",
      "h: 14 | epoch: 11, train loss: 33.97450637817383, test loss: 30.381113052368164\n",
      "h: 14 | epoch: 12, train loss: 33.49833679199219, test loss: 29.9376277923584\n",
      "h: 14 | epoch: 13, train loss: 33.02824401855469, test loss: 29.499576568603516\n",
      "h: 14 | epoch: 14, train loss: 32.5640983581543, test loss: 29.066843032836914\n",
      "h: 14 | epoch: 15, train loss: 32.105777740478516, test loss: 28.639318466186523\n",
      "h: 14 | epoch: 16, train loss: 31.653156280517578, test loss: 28.216909408569336\n",
      "h: 14 | epoch: 17, train loss: 31.20614242553711, test loss: 27.799530029296875\n",
      "h: 14 | epoch: 18, train loss: 30.7646427154541, test loss: 27.3870849609375\n",
      "h: 14 | epoch: 19, train loss: 30.32855796813965, test loss: 26.979522705078125\n",
      "h: 14 | epoch: 20, train loss: 29.897823333740234, test loss: 26.576763153076172\n",
      "h: 14 | epoch: 21, train loss: 29.472366333007812, test loss: 26.178752899169922\n",
      "h: 14 | epoch: 22, train loss: 29.052114486694336, test loss: 25.785430908203125\n",
      "h: 14 | epoch: 23, train loss: 28.637020111083984, test loss: 25.39675521850586\n",
      "h: 14 | epoch: 24, train loss: 28.227025985717773, test loss: 25.01268768310547\n",
      "h: 14 | epoch: 25, train loss: 27.82208824157715, test loss: 24.633182525634766\n",
      "h: 14 | epoch: 26, train loss: 27.42217445373535, test loss: 24.258214950561523\n",
      "h: 14 | epoch: 27, train loss: 27.02724266052246, test loss: 23.887746810913086\n",
      "h: 14 | epoch: 28, train loss: 26.637264251708984, test loss: 23.52176284790039\n",
      "h: 14 | epoch: 29, train loss: 26.252216339111328, test loss: 23.160236358642578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 14 | epoch: 30, train loss: 25.8720703125, test loss: 22.803157806396484\n",
      "h: 14 | epoch: 31, train loss: 25.496816635131836, test loss: 22.45050811767578\n",
      "h: 14 | epoch: 32, train loss: 25.126432418823242, test loss: 22.102270126342773\n",
      "h: 14 | epoch: 33, train loss: 24.76091194152832, test loss: 21.75844383239746\n",
      "h: 14 | epoch: 34, train loss: 24.400236129760742, test loss: 21.419017791748047\n",
      "h: 14 | epoch: 35, train loss: 24.04440689086914, test loss: 21.0839900970459\n",
      "h: 14 | epoch: 36, train loss: 23.69341468811035, test loss: 20.75334930419922\n",
      "h: 14 | epoch: 37, train loss: 23.34724998474121, test loss: 20.427104949951172\n",
      "h: 14 | epoch: 38, train loss: 23.005922317504883, test loss: 20.10525131225586\n",
      "h: 14 | epoch: 39, train loss: 22.669418334960938, test loss: 19.787784576416016\n",
      "h: 14 | epoch: 40, train loss: 22.337743759155273, test loss: 19.474706649780273\n",
      "h: 14 | epoch: 41, train loss: 22.010896682739258, test loss: 19.166027069091797\n",
      "h: 14 | epoch: 42, train loss: 21.68887710571289, test loss: 18.86173439025879\n",
      "h: 14 | epoch: 43, train loss: 21.371688842773438, test loss: 18.56184196472168\n",
      "h: 14 | epoch: 44, train loss: 21.059329986572266, test loss: 18.266345977783203\n",
      "h: 14 | epoch: 45, train loss: 20.751802444458008, test loss: 17.975252151489258\n",
      "h: 14 | epoch: 46, train loss: 20.44910430908203, test loss: 17.688552856445312\n",
      "h: 14 | epoch: 47, train loss: 20.151235580444336, test loss: 17.406253814697266\n",
      "h: 14 | epoch: 48, train loss: 19.858200073242188, test loss: 17.12836456298828\n",
      "h: 14 | epoch: 49, train loss: 19.569988250732422, test loss: 16.854867935180664\n",
      "h: 14 | epoch: 50, train loss: 19.286602020263672, test loss: 16.585771560668945\n",
      "h: 14 | epoch: 51, train loss: 19.008037567138672, test loss: 16.321067810058594\n",
      "h: 14 | epoch: 52, train loss: 18.734283447265625, test loss: 16.060754776000977\n",
      "h: 14 | epoch: 53, train loss: 18.465341567993164, test loss: 15.804827690124512\n",
      "h: 14 | epoch: 54, train loss: 18.20119857788086, test loss: 15.553281784057617\n",
      "h: 14 | epoch: 55, train loss: 17.941844940185547, test loss: 15.306106567382812\n",
      "h: 14 | epoch: 56, train loss: 17.68726921081543, test loss: 15.063291549682617\n",
      "h: 14 | epoch: 57, train loss: 17.43745994567871, test loss: 14.824823379516602\n",
      "h: 14 | epoch: 58, train loss: 17.192398071289062, test loss: 14.590693473815918\n",
      "h: 14 | epoch: 59, train loss: 16.952072143554688, test loss: 14.360885620117188\n",
      "h: 14 | epoch: 60, train loss: 16.716461181640625, test loss: 14.13538646697998\n",
      "h: 14 | epoch: 61, train loss: 16.48554229736328, test loss: 13.914172172546387\n",
      "h: 14 | epoch: 62, train loss: 16.259292602539062, test loss: 13.697229385375977\n",
      "h: 14 | epoch: 63, train loss: 16.037689208984375, test loss: 13.484527587890625\n",
      "h: 14 | epoch: 64, train loss: 15.820706367492676, test loss: 13.276052474975586\n",
      "h: 14 | epoch: 65, train loss: 15.608312606811523, test loss: 13.07177448272705\n",
      "h: 14 | epoch: 66, train loss: 15.400479316711426, test loss: 12.871665954589844\n",
      "h: 14 | epoch: 67, train loss: 15.197172164916992, test loss: 12.675698280334473\n",
      "h: 14 | epoch: 68, train loss: 14.998357772827148, test loss: 12.483846664428711\n",
      "h: 14 | epoch: 69, train loss: 14.803998947143555, test loss: 12.296070098876953\n",
      "h: 14 | epoch: 70, train loss: 14.614056587219238, test loss: 12.112339973449707\n",
      "h: 14 | epoch: 71, train loss: 14.428489685058594, test loss: 11.9326171875\n",
      "h: 14 | epoch: 72, train loss: 14.2472562789917, test loss: 11.756869316101074\n",
      "h: 14 | epoch: 73, train loss: 14.07031536102295, test loss: 11.585054397583008\n",
      "h: 14 | epoch: 74, train loss: 13.897616386413574, test loss: 11.417131423950195\n",
      "h: 14 | epoch: 75, train loss: 13.72911548614502, test loss: 11.25306224822998\n",
      "h: 14 | epoch: 76, train loss: 13.564760208129883, test loss: 11.092801094055176\n",
      "h: 14 | epoch: 77, train loss: 13.404502868652344, test loss: 10.936300277709961\n",
      "h: 14 | epoch: 78, train loss: 13.248289108276367, test loss: 10.78351879119873\n",
      "h: 14 | epoch: 79, train loss: 13.0960693359375, test loss: 10.634406089782715\n",
      "h: 14 | epoch: 80, train loss: 12.947784423828125, test loss: 10.48891830444336\n",
      "h: 14 | epoch: 81, train loss: 12.803382873535156, test loss: 10.347001075744629\n",
      "h: 14 | epoch: 82, train loss: 12.662801742553711, test loss: 10.208605766296387\n",
      "h: 14 | epoch: 83, train loss: 12.525986671447754, test loss: 10.073680877685547\n",
      "h: 14 | epoch: 84, train loss: 12.392879486083984, test loss: 9.942177772521973\n",
      "h: 14 | epoch: 85, train loss: 12.26341724395752, test loss: 9.814038276672363\n",
      "h: 14 | epoch: 86, train loss: 12.137540817260742, test loss: 9.68920612335205\n",
      "h: 14 | epoch: 87, train loss: 12.015185356140137, test loss: 9.56763744354248\n",
      "h: 14 | epoch: 88, train loss: 11.896292686462402, test loss: 9.44926929473877\n",
      "h: 14 | epoch: 89, train loss: 11.780800819396973, test loss: 9.334047317504883\n",
      "h: 14 | epoch: 90, train loss: 11.668641090393066, test loss: 9.221915245056152\n",
      "h: 14 | epoch: 91, train loss: 11.55975341796875, test loss: 9.112817764282227\n",
      "h: 14 | epoch: 92, train loss: 11.454071998596191, test loss: 9.00670051574707\n",
      "h: 14 | epoch: 93, train loss: 11.351534843444824, test loss: 8.9035005569458\n",
      "h: 14 | epoch: 94, train loss: 11.252073287963867, test loss: 8.803166389465332\n",
      "h: 14 | epoch: 95, train loss: 11.155628204345703, test loss: 8.705638885498047\n",
      "h: 14 | epoch: 96, train loss: 11.062134742736816, test loss: 8.610857963562012\n",
      "h: 14 | epoch: 97, train loss: 10.971521377563477, test loss: 8.518771171569824\n",
      "h: 14 | epoch: 98, train loss: 10.883729934692383, test loss: 8.429319381713867\n",
      "h: 14 | epoch: 99, train loss: 10.798694610595703, test loss: 8.342449188232422\n",
      "h: 15 | epoch: 0, train loss: 40.925132751464844, test loss: 36.54204559326172\n",
      "h: 15 | epoch: 1, train loss: 40.19839096069336, test loss: 35.878421783447266\n",
      "h: 15 | epoch: 2, train loss: 39.48700714111328, test loss: 35.228267669677734\n",
      "h: 15 | epoch: 3, train loss: 38.79037857055664, test loss: 34.591041564941406\n",
      "h: 15 | epoch: 4, train loss: 38.107948303222656, test loss: 33.96628952026367\n",
      "h: 15 | epoch: 5, train loss: 37.43921661376953, test loss: 33.35354995727539\n",
      "h: 15 | epoch: 6, train loss: 36.783687591552734, test loss: 32.752437591552734\n",
      "h: 15 | epoch: 7, train loss: 36.14091110229492, test loss: 32.16255569458008\n",
      "h: 15 | epoch: 8, train loss: 35.5104866027832, test loss: 31.58355712890625\n",
      "h: 15 | epoch: 9, train loss: 34.892005920410156, test loss: 31.015106201171875\n",
      "h: 15 | epoch: 10, train loss: 34.285133361816406, test loss: 30.4569034576416\n",
      "h: 15 | epoch: 11, train loss: 33.68951416015625, test loss: 29.90865707397461\n",
      "h: 15 | epoch: 12, train loss: 33.10485076904297, test loss: 29.370107650756836\n",
      "h: 15 | epoch: 13, train loss: 32.530845642089844, test loss: 28.841012954711914\n",
      "h: 15 | epoch: 14, train loss: 31.967233657836914, test loss: 28.321136474609375\n",
      "h: 15 | epoch: 15, train loss: 31.41376304626465, test loss: 27.810266494750977\n",
      "h: 15 | epoch: 16, train loss: 30.870203018188477, test loss: 27.308197021484375\n",
      "h: 15 | epoch: 17, train loss: 30.33633804321289, test loss: 26.814767837524414\n",
      "h: 15 | epoch: 18, train loss: 29.811962127685547, test loss: 26.329788208007812\n",
      "h: 15 | epoch: 19, train loss: 29.296899795532227, test loss: 25.853107452392578\n",
      "h: 15 | epoch: 20, train loss: 28.790964126586914, test loss: 25.38457489013672\n",
      "h: 15 | epoch: 21, train loss: 28.29400062561035, test loss: 24.92405891418457\n",
      "h: 15 | epoch: 22, train loss: 27.805858612060547, test loss: 24.471420288085938\n",
      "h: 15 | epoch: 23, train loss: 27.326396942138672, test loss: 24.026554107666016\n",
      "h: 15 | epoch: 24, train loss: 26.855487823486328, test loss: 23.589332580566406\n",
      "h: 15 | epoch: 25, train loss: 26.393001556396484, test loss: 23.159664154052734\n",
      "h: 15 | epoch: 26, train loss: 25.938833236694336, test loss: 22.73744773864746\n",
      "h: 15 | epoch: 27, train loss: 25.492860794067383, test loss: 22.32258415222168\n",
      "h: 15 | epoch: 28, train loss: 25.05499839782715, test loss: 21.91500473022461\n",
      "h: 15 | epoch: 29, train loss: 24.625141143798828, test loss: 21.51461410522461\n",
      "h: 15 | epoch: 30, train loss: 24.203205108642578, test loss: 21.121337890625\n",
      "h: 15 | epoch: 31, train loss: 23.78910255432129, test loss: 20.735107421875\n",
      "h: 15 | epoch: 32, train loss: 23.382747650146484, test loss: 20.35584831237793\n",
      "h: 15 | epoch: 33, train loss: 22.984071731567383, test loss: 19.98349952697754\n",
      "h: 15 | epoch: 34, train loss: 22.592992782592773, test loss: 19.617992401123047\n",
      "h: 15 | epoch: 35, train loss: 22.20943832397461, test loss: 19.25926399230957\n",
      "h: 15 | epoch: 36, train loss: 21.833343505859375, test loss: 18.90726089477539\n",
      "h: 15 | epoch: 37, train loss: 21.464641571044922, test loss: 18.561920166015625\n",
      "h: 15 | epoch: 38, train loss: 21.10325813293457, test loss: 18.223186492919922\n",
      "h: 15 | epoch: 39, train loss: 20.749134063720703, test loss: 17.891000747680664\n",
      "h: 15 | epoch: 40, train loss: 20.402206420898438, test loss: 17.565311431884766\n",
      "h: 15 | epoch: 41, train loss: 20.062408447265625, test loss: 17.24605941772461\n",
      "h: 15 | epoch: 42, train loss: 19.72967529296875, test loss: 16.93319320678711\n",
      "h: 15 | epoch: 43, train loss: 19.403949737548828, test loss: 16.62665367126465\n",
      "h: 15 | epoch: 44, train loss: 19.085161209106445, test loss: 16.326391220092773\n",
      "h: 15 | epoch: 45, train loss: 18.773250579833984, test loss: 16.032344818115234\n",
      "h: 15 | epoch: 46, train loss: 18.46815299987793, test loss: 15.744463920593262\n",
      "h: 15 | epoch: 47, train loss: 18.169801712036133, test loss: 15.462684631347656\n",
      "h: 15 | epoch: 48, train loss: 17.878131866455078, test loss: 15.1869535446167\n",
      "h: 15 | epoch: 49, train loss: 17.593074798583984, test loss: 14.917213439941406\n",
      "h: 15 | epoch: 50, train loss: 17.314563751220703, test loss: 14.653396606445312\n",
      "h: 15 | epoch: 51, train loss: 17.042530059814453, test loss: 14.395452499389648\n",
      "h: 15 | epoch: 52, train loss: 16.77690315246582, test loss: 14.143310546875\n",
      "h: 15 | epoch: 53, train loss: 16.51761245727539, test loss: 13.896917343139648\n",
      "h: 15 | epoch: 54, train loss: 16.26458168029785, test loss: 13.65620231628418\n",
      "h: 15 | epoch: 55, train loss: 16.017736434936523, test loss: 13.421101570129395\n",
      "h: 15 | epoch: 56, train loss: 15.777005195617676, test loss: 13.191543579101562\n",
      "h: 15 | epoch: 57, train loss: 15.54230785369873, test loss: 12.967466354370117\n",
      "h: 15 | epoch: 58, train loss: 15.313565254211426, test loss: 12.748800277709961\n",
      "h: 15 | epoch: 59, train loss: 15.090696334838867, test loss: 12.535469055175781\n",
      "h: 15 | epoch: 60, train loss: 14.873621940612793, test loss: 12.327406883239746\n",
      "h: 15 | epoch: 61, train loss: 14.662259101867676, test loss: 12.12453842163086\n",
      "h: 15 | epoch: 62, train loss: 14.456524848937988, test loss: 11.92679214477539\n",
      "h: 15 | epoch: 63, train loss: 14.256330490112305, test loss: 11.734088897705078\n",
      "h: 15 | epoch: 64, train loss: 14.061590194702148, test loss: 11.546356201171875\n",
      "h: 15 | epoch: 65, train loss: 13.872220993041992, test loss: 11.36351203918457\n",
      "h: 15 | epoch: 66, train loss: 13.688128471374512, test loss: 11.185483932495117\n",
      "h: 15 | epoch: 67, train loss: 13.509225845336914, test loss: 11.012190818786621\n",
      "h: 15 | epoch: 68, train loss: 13.335424423217773, test loss: 10.843549728393555\n",
      "h: 15 | epoch: 69, train loss: 13.16662883758545, test loss: 10.679489135742188\n",
      "h: 15 | epoch: 70, train loss: 13.002751350402832, test loss: 10.519917488098145\n",
      "h: 15 | epoch: 71, train loss: 12.843698501586914, test loss: 10.364762306213379\n",
      "h: 15 | epoch: 72, train loss: 12.689374923706055, test loss: 10.213933944702148\n",
      "h: 15 | epoch: 73, train loss: 12.539690971374512, test loss: 10.06735610961914\n",
      "h: 15 | epoch: 74, train loss: 12.394550323486328, test loss: 9.924943923950195\n",
      "h: 15 | epoch: 75, train loss: 12.253859519958496, test loss: 9.786615371704102\n",
      "h: 15 | epoch: 76, train loss: 12.117526054382324, test loss: 9.652284622192383\n",
      "h: 15 | epoch: 77, train loss: 11.985455513000488, test loss: 9.521871566772461\n",
      "h: 15 | epoch: 78, train loss: 11.857553482055664, test loss: 9.395296096801758\n",
      "h: 15 | epoch: 79, train loss: 11.733723640441895, test loss: 9.272473335266113\n",
      "h: 15 | epoch: 80, train loss: 11.613879203796387, test loss: 9.153316497802734\n",
      "h: 15 | epoch: 81, train loss: 11.497919082641602, test loss: 9.037748336791992\n",
      "h: 15 | epoch: 82, train loss: 11.385758399963379, test loss: 8.92568588256836\n",
      "h: 15 | epoch: 83, train loss: 11.277299880981445, test loss: 8.817049026489258\n",
      "h: 15 | epoch: 84, train loss: 11.172451972961426, test loss: 8.711752891540527\n",
      "h: 15 | epoch: 85, train loss: 11.071125984191895, test loss: 8.609723091125488\n",
      "h: 15 | epoch: 86, train loss: 10.973230361938477, test loss: 8.510873794555664\n",
      "h: 15 | epoch: 87, train loss: 10.87867546081543, test loss: 8.415132522583008\n",
      "h: 15 | epoch: 88, train loss: 10.787376403808594, test loss: 8.322415351867676\n",
      "h: 15 | epoch: 89, train loss: 10.699241638183594, test loss: 8.232647895812988\n",
      "h: 15 | epoch: 90, train loss: 10.614188194274902, test loss: 8.14575481414795\n",
      "h: 15 | epoch: 91, train loss: 10.532126426696777, test loss: 8.061657905578613\n",
      "h: 15 | epoch: 92, train loss: 10.452978134155273, test loss: 7.980284214019775\n",
      "h: 15 | epoch: 93, train loss: 10.376653671264648, test loss: 7.901561737060547\n",
      "h: 15 | epoch: 94, train loss: 10.303075790405273, test loss: 7.825416564941406\n",
      "h: 15 | epoch: 95, train loss: 10.23216438293457, test loss: 7.751776218414307\n",
      "h: 15 | epoch: 96, train loss: 10.163837432861328, test loss: 7.68057107925415\n",
      "h: 15 | epoch: 97, train loss: 10.09801959991455, test loss: 7.611732482910156\n",
      "h: 15 | epoch: 98, train loss: 10.03463363647461, test loss: 7.54519510269165\n",
      "h: 15 | epoch: 99, train loss: 9.973605155944824, test loss: 7.480888366699219\n",
      "h: 16 | epoch: 0, train loss: 40.09037780761719, test loss: 36.53619384765625\n",
      "h: 16 | epoch: 1, train loss: 39.27568435668945, test loss: 35.76609802246094\n",
      "h: 16 | epoch: 2, train loss: 38.48140335083008, test loss: 35.014869689941406\n",
      "h: 16 | epoch: 3, train loss: 37.70670700073242, test loss: 34.28175735473633\n",
      "h: 16 | epoch: 4, train loss: 36.950828552246094, test loss: 33.56605911254883\n",
      "h: 16 | epoch: 5, train loss: 36.21306610107422, test loss: 32.86714553833008\n",
      "h: 16 | epoch: 6, train loss: 35.49274826049805, test loss: 32.184410095214844\n",
      "h: 16 | epoch: 7, train loss: 34.78925704956055, test loss: 31.51729965209961\n",
      "h: 16 | epoch: 8, train loss: 34.102020263671875, test loss: 30.865283966064453\n",
      "h: 16 | epoch: 9, train loss: 33.43050765991211, test loss: 30.227880477905273\n",
      "h: 16 | epoch: 10, train loss: 32.77421188354492, test loss: 29.604639053344727\n",
      "h: 16 | epoch: 11, train loss: 32.13267135620117, test loss: 28.995126724243164\n",
      "h: 16 | epoch: 12, train loss: 31.505451202392578, test loss: 28.398956298828125\n",
      "h: 16 | epoch: 13, train loss: 30.892139434814453, test loss: 27.815750122070312\n",
      "h: 16 | epoch: 14, train loss: 30.292362213134766, test loss: 27.245162963867188\n",
      "h: 16 | epoch: 15, train loss: 29.705760955810547, test loss: 26.68686866760254\n",
      "h: 16 | epoch: 16, train loss: 29.132003784179688, test loss: 26.140567779541016\n",
      "h: 16 | epoch: 17, train loss: 28.570775985717773, test loss: 25.605960845947266\n",
      "h: 16 | epoch: 18, train loss: 28.021785736083984, test loss: 25.082782745361328\n",
      "h: 16 | epoch: 19, train loss: 27.48476219177246, test loss: 24.570781707763672\n",
      "h: 16 | epoch: 20, train loss: 26.959436416625977, test loss: 24.069717407226562\n",
      "h: 16 | epoch: 21, train loss: 26.445566177368164, test loss: 23.57935905456543\n",
      "h: 16 | epoch: 22, train loss: 25.942920684814453, test loss: 23.099496841430664\n",
      "h: 16 | epoch: 23, train loss: 25.45128059387207, test loss: 22.629926681518555\n",
      "h: 16 | epoch: 24, train loss: 24.970439910888672, test loss: 22.170452117919922\n",
      "h: 16 | epoch: 25, train loss: 24.500194549560547, test loss: 21.720890045166016\n",
      "h: 16 | epoch: 26, train loss: 24.040363311767578, test loss: 21.28106689453125\n",
      "h: 16 | epoch: 27, train loss: 23.590763092041016, test loss: 20.850814819335938\n",
      "h: 16 | epoch: 28, train loss: 23.151222229003906, test loss: 20.429969787597656\n",
      "h: 16 | epoch: 29, train loss: 22.721572875976562, test loss: 20.018373489379883\n",
      "h: 16 | epoch: 30, train loss: 22.301658630371094, test loss: 19.615888595581055\n",
      "h: 16 | epoch: 31, train loss: 21.891324996948242, test loss: 19.22235870361328\n",
      "h: 16 | epoch: 32, train loss: 21.490421295166016, test loss: 18.837650299072266\n",
      "h: 16 | epoch: 33, train loss: 21.098804473876953, test loss: 18.461627960205078\n",
      "h: 16 | epoch: 34, train loss: 20.71633529663086, test loss: 18.094154357910156\n",
      "h: 16 | epoch: 35, train loss: 20.342870712280273, test loss: 17.735103607177734\n",
      "h: 16 | epoch: 36, train loss: 19.978282928466797, test loss: 17.384347915649414\n",
      "h: 16 | epoch: 37, train loss: 19.6224365234375, test loss: 17.041767120361328\n",
      "h: 16 | epoch: 38, train loss: 19.27520179748535, test loss: 16.707229614257812\n",
      "h: 16 | epoch: 39, train loss: 18.936450958251953, test loss: 16.3806209564209\n",
      "h: 16 | epoch: 40, train loss: 18.60605239868164, test loss: 16.061824798583984\n",
      "h: 16 | epoch: 41, train loss: 18.28388786315918, test loss: 15.75071907043457\n",
      "h: 16 | epoch: 42, train loss: 17.969831466674805, test loss: 15.447186470031738\n",
      "h: 16 | epoch: 43, train loss: 17.66375732421875, test loss: 15.15110969543457\n",
      "h: 16 | epoch: 44, train loss: 17.365541458129883, test loss: 14.86237907409668\n",
      "h: 16 | epoch: 45, train loss: 17.075063705444336, test loss: 14.580876350402832\n",
      "h: 16 | epoch: 46, train loss: 16.792203903198242, test loss: 14.306486129760742\n",
      "h: 16 | epoch: 47, train loss: 16.516834259033203, test loss: 14.039095878601074\n",
      "h: 16 | epoch: 48, train loss: 16.248836517333984, test loss: 13.778590202331543\n",
      "h: 16 | epoch: 49, train loss: 15.988085746765137, test loss: 13.524858474731445\n",
      "h: 16 | epoch: 50, train loss: 15.734464645385742, test loss: 13.27778148651123\n",
      "h: 16 | epoch: 51, train loss: 15.487846374511719, test loss: 13.037251472473145\n",
      "h: 16 | epoch: 52, train loss: 15.2481107711792, test loss: 12.803146362304688\n",
      "h: 16 | epoch: 53, train loss: 15.015134811401367, test loss: 12.575361251831055\n",
      "h: 16 | epoch: 54, train loss: 14.788797378540039, test loss: 12.353778839111328\n",
      "h: 16 | epoch: 55, train loss: 14.5689697265625, test loss: 12.138285636901855\n",
      "h: 16 | epoch: 56, train loss: 14.355535507202148, test loss: 11.928770065307617\n",
      "h: 16 | epoch: 57, train loss: 14.148371696472168, test loss: 11.725112915039062\n",
      "h: 16 | epoch: 58, train loss: 13.947352409362793, test loss: 11.527207374572754\n",
      "h: 16 | epoch: 59, train loss: 13.752357482910156, test loss: 11.334936141967773\n",
      "h: 16 | epoch: 60, train loss: 13.563262939453125, test loss: 11.14819049835205\n",
      "h: 16 | epoch: 61, train loss: 13.3799467086792, test loss: 10.966851234436035\n",
      "h: 16 | epoch: 62, train loss: 13.202282905578613, test loss: 10.790807723999023\n",
      "h: 16 | epoch: 63, train loss: 13.03015422821045, test loss: 10.619952201843262\n",
      "h: 16 | epoch: 64, train loss: 12.863436698913574, test loss: 10.454171180725098\n",
      "h: 16 | epoch: 65, train loss: 12.702009201049805, test loss: 10.293350219726562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 16 | epoch: 66, train loss: 12.545751571655273, test loss: 10.137381553649902\n",
      "h: 16 | epoch: 67, train loss: 12.39454460144043, test loss: 9.986156463623047\n",
      "h: 16 | epoch: 68, train loss: 12.248266220092773, test loss: 9.83956241607666\n",
      "h: 16 | epoch: 69, train loss: 12.106799125671387, test loss: 9.697491645812988\n",
      "h: 16 | epoch: 70, train loss: 11.970026969909668, test loss: 9.559839248657227\n",
      "h: 16 | epoch: 71, train loss: 11.837830543518066, test loss: 9.426494598388672\n",
      "h: 16 | epoch: 72, train loss: 11.710094451904297, test loss: 9.297353744506836\n",
      "h: 16 | epoch: 73, train loss: 11.586705207824707, test loss: 9.17231273651123\n",
      "h: 16 | epoch: 74, train loss: 11.467547416687012, test loss: 9.05126667022705\n",
      "h: 16 | epoch: 75, train loss: 11.352508544921875, test loss: 8.934114456176758\n",
      "h: 16 | epoch: 76, train loss: 11.241479873657227, test loss: 8.82075309753418\n",
      "h: 16 | epoch: 77, train loss: 11.134347915649414, test loss: 8.71108341217041\n",
      "h: 16 | epoch: 78, train loss: 11.031007766723633, test loss: 8.605008125305176\n",
      "h: 16 | epoch: 79, train loss: 10.931351661682129, test loss: 8.502426147460938\n",
      "h: 16 | epoch: 80, train loss: 10.835274696350098, test loss: 8.403244972229004\n",
      "h: 16 | epoch: 81, train loss: 10.742670059204102, test loss: 8.307369232177734\n",
      "h: 16 | epoch: 82, train loss: 10.653440475463867, test loss: 8.21470832824707\n",
      "h: 16 | epoch: 83, train loss: 10.567482948303223, test loss: 8.125165939331055\n",
      "h: 16 | epoch: 84, train loss: 10.484697341918945, test loss: 8.038657188415527\n",
      "h: 16 | epoch: 85, train loss: 10.404989242553711, test loss: 7.955089569091797\n",
      "h: 16 | epoch: 86, train loss: 10.328263282775879, test loss: 7.874379634857178\n",
      "h: 16 | epoch: 87, train loss: 10.254425048828125, test loss: 7.796441555023193\n",
      "h: 16 | epoch: 88, train loss: 10.183385848999023, test loss: 7.721190452575684\n",
      "h: 16 | epoch: 89, train loss: 10.115053176879883, test loss: 7.648547172546387\n",
      "h: 16 | epoch: 90, train loss: 10.049341201782227, test loss: 7.578432559967041\n",
      "h: 16 | epoch: 91, train loss: 9.986162185668945, test loss: 7.510765075683594\n",
      "h: 16 | epoch: 92, train loss: 9.925436019897461, test loss: 7.445471286773682\n",
      "h: 16 | epoch: 93, train loss: 9.86707878112793, test loss: 7.38247537612915\n",
      "h: 16 | epoch: 94, train loss: 9.811010360717773, test loss: 7.3217034339904785\n",
      "h: 16 | epoch: 95, train loss: 9.757152557373047, test loss: 7.263086795806885\n",
      "h: 16 | epoch: 96, train loss: 9.705429077148438, test loss: 7.206554412841797\n",
      "h: 16 | epoch: 97, train loss: 9.655768394470215, test loss: 7.15203857421875\n",
      "h: 16 | epoch: 98, train loss: 9.608098030090332, test loss: 7.0994696617126465\n",
      "h: 16 | epoch: 99, train loss: 9.562344551086426, test loss: 7.04879093170166\n",
      "h: 17 | epoch: 0, train loss: 39.75965118408203, test loss: 35.13945007324219\n",
      "h: 17 | epoch: 1, train loss: 38.77342987060547, test loss: 34.24769973754883\n",
      "h: 17 | epoch: 2, train loss: 37.8160285949707, test loss: 33.38105773925781\n",
      "h: 17 | epoch: 3, train loss: 36.88630294799805, test loss: 32.53858184814453\n",
      "h: 17 | epoch: 4, train loss: 35.983219146728516, test loss: 31.719390869140625\n",
      "h: 17 | epoch: 5, train loss: 35.10578155517578, test loss: 30.922653198242188\n",
      "h: 17 | epoch: 6, train loss: 34.253108978271484, test loss: 30.147624969482422\n",
      "h: 17 | epoch: 7, train loss: 33.424346923828125, test loss: 29.39358901977539\n",
      "h: 17 | epoch: 8, train loss: 32.61870574951172, test loss: 28.65987777709961\n",
      "h: 17 | epoch: 9, train loss: 31.835458755493164, test loss: 27.94586181640625\n",
      "h: 17 | epoch: 10, train loss: 31.07390785217285, test loss: 27.250986099243164\n",
      "h: 17 | epoch: 11, train loss: 30.333410263061523, test loss: 26.574676513671875\n",
      "h: 17 | epoch: 12, train loss: 29.613367080688477, test loss: 25.916427612304688\n",
      "h: 17 | epoch: 13, train loss: 28.913204193115234, test loss: 25.275758743286133\n",
      "h: 17 | epoch: 14, train loss: 28.232379913330078, test loss: 24.65221405029297\n",
      "h: 17 | epoch: 15, train loss: 27.570388793945312, test loss: 24.045352935791016\n",
      "h: 17 | epoch: 16, train loss: 26.926748275756836, test loss: 23.45476722717285\n",
      "h: 17 | epoch: 17, train loss: 26.30099868774414, test loss: 22.88006019592285\n",
      "h: 17 | epoch: 18, train loss: 25.692707061767578, test loss: 22.320863723754883\n",
      "h: 17 | epoch: 19, train loss: 25.101451873779297, test loss: 21.77682113647461\n",
      "h: 17 | epoch: 20, train loss: 24.526840209960938, test loss: 21.247589111328125\n",
      "h: 17 | epoch: 21, train loss: 23.968486785888672, test loss: 20.73283576965332\n",
      "h: 17 | epoch: 22, train loss: 23.426029205322266, test loss: 20.23224639892578\n",
      "h: 17 | epoch: 23, train loss: 22.899105072021484, test loss: 19.745513916015625\n",
      "h: 17 | epoch: 24, train loss: 22.387374877929688, test loss: 19.272342681884766\n",
      "h: 17 | epoch: 25, train loss: 21.8905086517334, test loss: 18.812440872192383\n",
      "h: 17 | epoch: 26, train loss: 21.408184051513672, test loss: 18.365535736083984\n",
      "h: 17 | epoch: 27, train loss: 20.940082550048828, test loss: 17.931352615356445\n",
      "h: 17 | epoch: 28, train loss: 20.485898971557617, test loss: 17.50961685180664\n",
      "h: 17 | epoch: 29, train loss: 20.04533576965332, test loss: 17.100072860717773\n",
      "h: 17 | epoch: 30, train loss: 19.618099212646484, test loss: 16.702463150024414\n",
      "h: 17 | epoch: 31, train loss: 19.203899383544922, test loss: 16.31653594970703\n",
      "h: 17 | epoch: 32, train loss: 18.80245018005371, test loss: 15.942049026489258\n",
      "h: 17 | epoch: 33, train loss: 18.41347885131836, test loss: 15.578752517700195\n",
      "h: 17 | epoch: 34, train loss: 18.03670883178711, test loss: 15.226404190063477\n",
      "h: 17 | epoch: 35, train loss: 17.6718692779541, test loss: 14.884773254394531\n",
      "h: 17 | epoch: 36, train loss: 17.318687438964844, test loss: 14.553616523742676\n",
      "h: 17 | epoch: 37, train loss: 16.97690773010254, test loss: 14.232708930969238\n",
      "h: 17 | epoch: 38, train loss: 16.64626121520996, test loss: 13.921818733215332\n",
      "h: 17 | epoch: 39, train loss: 16.326494216918945, test loss: 13.62071418762207\n",
      "h: 17 | epoch: 40, train loss: 16.017345428466797, test loss: 13.329174995422363\n",
      "h: 17 | epoch: 41, train loss: 15.71856689453125, test loss: 13.046978950500488\n",
      "h: 17 | epoch: 42, train loss: 15.429903984069824, test loss: 12.77390193939209\n",
      "h: 17 | epoch: 43, train loss: 15.151110649108887, test loss: 12.509725570678711\n",
      "h: 17 | epoch: 44, train loss: 14.881937026977539, test loss: 12.254233360290527\n",
      "h: 17 | epoch: 45, train loss: 14.622143745422363, test loss: 12.00721549987793\n",
      "h: 17 | epoch: 46, train loss: 14.371482849121094, test loss: 11.768453598022461\n",
      "h: 17 | epoch: 47, train loss: 14.129724502563477, test loss: 11.537740707397461\n",
      "h: 17 | epoch: 48, train loss: 13.896623611450195, test loss: 11.314865112304688\n",
      "h: 17 | epoch: 49, train loss: 13.671951293945312, test loss: 11.09962272644043\n",
      "h: 17 | epoch: 50, train loss: 13.455476760864258, test loss: 10.891810417175293\n",
      "h: 17 | epoch: 51, train loss: 13.246971130371094, test loss: 10.691231727600098\n",
      "h: 17 | epoch: 52, train loss: 13.0462064743042, test loss: 10.497678756713867\n",
      "h: 17 | epoch: 53, train loss: 12.8529634475708, test loss: 10.310961723327637\n",
      "h: 17 | epoch: 54, train loss: 12.667022705078125, test loss: 10.130888938903809\n",
      "h: 17 | epoch: 55, train loss: 12.488165855407715, test loss: 9.957268714904785\n",
      "h: 17 | epoch: 56, train loss: 12.316183090209961, test loss: 9.789908409118652\n",
      "h: 17 | epoch: 57, train loss: 12.150861740112305, test loss: 9.628629684448242\n",
      "h: 17 | epoch: 58, train loss: 11.991997718811035, test loss: 9.473248481750488\n",
      "h: 17 | epoch: 59, train loss: 11.839385986328125, test loss: 9.32358455657959\n",
      "h: 17 | epoch: 60, train loss: 11.692830085754395, test loss: 9.179464340209961\n",
      "h: 17 | epoch: 61, train loss: 11.552133560180664, test loss: 9.040715217590332\n",
      "h: 17 | epoch: 62, train loss: 11.417102813720703, test loss: 8.907169342041016\n",
      "h: 17 | epoch: 63, train loss: 11.287550926208496, test loss: 8.778657913208008\n",
      "h: 17 | epoch: 64, train loss: 11.163297653198242, test loss: 8.655019760131836\n",
      "h: 17 | epoch: 65, train loss: 11.044156074523926, test loss: 8.536096572875977\n",
      "h: 17 | epoch: 66, train loss: 10.929954528808594, test loss: 8.421733856201172\n",
      "h: 17 | epoch: 67, train loss: 10.820520401000977, test loss: 8.311775207519531\n",
      "h: 17 | epoch: 68, train loss: 10.71568489074707, test loss: 8.206077575683594\n",
      "h: 17 | epoch: 69, train loss: 10.615281105041504, test loss: 8.104490280151367\n",
      "h: 17 | epoch: 70, train loss: 10.519152641296387, test loss: 8.006875991821289\n",
      "h: 17 | epoch: 71, train loss: 10.427141189575195, test loss: 7.913093566894531\n",
      "h: 17 | epoch: 72, train loss: 10.339097023010254, test loss: 7.8230109214782715\n",
      "h: 17 | epoch: 73, train loss: 10.254867553710938, test loss: 7.736496925354004\n",
      "h: 17 | epoch: 74, train loss: 10.17431354522705, test loss: 7.653419494628906\n",
      "h: 17 | epoch: 75, train loss: 10.09729290008545, test loss: 7.573660373687744\n",
      "h: 17 | epoch: 76, train loss: 10.023669242858887, test loss: 7.497092247009277\n",
      "h: 17 | epoch: 77, train loss: 9.953312873840332, test loss: 7.4236040115356445\n",
      "h: 17 | epoch: 78, train loss: 9.886093139648438, test loss: 7.353079319000244\n",
      "h: 17 | epoch: 79, train loss: 9.821887016296387, test loss: 7.285408020019531\n",
      "h: 17 | epoch: 80, train loss: 9.760574340820312, test loss: 7.220481872558594\n",
      "h: 17 | epoch: 81, train loss: 9.702037811279297, test loss: 7.158198356628418\n",
      "h: 17 | epoch: 82, train loss: 9.646164894104004, test loss: 7.098456382751465\n",
      "h: 17 | epoch: 83, train loss: 9.592845916748047, test loss: 7.0411577224731445\n",
      "h: 17 | epoch: 84, train loss: 9.541977882385254, test loss: 6.986207485198975\n",
      "h: 17 | epoch: 85, train loss: 9.49345588684082, test loss: 6.9335174560546875\n",
      "h: 17 | epoch: 86, train loss: 9.447184562683105, test loss: 6.882996559143066\n",
      "h: 17 | epoch: 87, train loss: 9.403064727783203, test loss: 6.834558963775635\n",
      "h: 17 | epoch: 88, train loss: 9.36100959777832, test loss: 6.788125038146973\n",
      "h: 17 | epoch: 89, train loss: 9.32092571258545, test loss: 6.743612766265869\n",
      "h: 17 | epoch: 90, train loss: 9.282732009887695, test loss: 6.700945854187012\n",
      "h: 17 | epoch: 91, train loss: 9.246344566345215, test loss: 6.6600518226623535\n",
      "h: 17 | epoch: 92, train loss: 9.211687088012695, test loss: 6.620857238769531\n",
      "h: 17 | epoch: 93, train loss: 9.178678512573242, test loss: 6.583296298980713\n",
      "h: 17 | epoch: 94, train loss: 9.147250175476074, test loss: 6.54729700088501\n",
      "h: 17 | epoch: 95, train loss: 9.117330551147461, test loss: 6.5127997398376465\n",
      "h: 17 | epoch: 96, train loss: 9.088850021362305, test loss: 6.47974157333374\n",
      "h: 17 | epoch: 97, train loss: 9.061748504638672, test loss: 6.448063850402832\n",
      "h: 17 | epoch: 98, train loss: 9.035957336425781, test loss: 6.4177093505859375\n",
      "h: 17 | epoch: 99, train loss: 9.011423110961914, test loss: 6.388623237609863\n",
      "h: 18 | epoch: 0, train loss: 48.72686767578125, test loss: 43.579559326171875\n",
      "h: 18 | epoch: 1, train loss: 47.811920166015625, test loss: 42.74601364135742\n",
      "h: 18 | epoch: 2, train loss: 46.920379638671875, test loss: 41.932960510253906\n",
      "h: 18 | epoch: 3, train loss: 46.05111312866211, test loss: 41.13941192626953\n",
      "h: 18 | epoch: 4, train loss: 45.20303726196289, test loss: 40.364479064941406\n",
      "h: 18 | epoch: 5, train loss: 44.37517166137695, test loss: 39.607295989990234\n",
      "h: 18 | epoch: 6, train loss: 43.56659698486328, test loss: 38.86709976196289\n",
      "h: 18 | epoch: 7, train loss: 42.776451110839844, test loss: 38.14313507080078\n",
      "h: 18 | epoch: 8, train loss: 42.003944396972656, test loss: 37.43473815917969\n",
      "h: 18 | epoch: 9, train loss: 41.24833297729492, test loss: 36.741268157958984\n",
      "h: 18 | epoch: 10, train loss: 40.508941650390625, test loss: 36.0621452331543\n",
      "h: 18 | epoch: 11, train loss: 39.78511428833008, test loss: 35.396827697753906\n",
      "h: 18 | epoch: 12, train loss: 39.07626724243164, test loss: 34.744789123535156\n",
      "h: 18 | epoch: 13, train loss: 38.38184356689453, test loss: 34.105560302734375\n",
      "h: 18 | epoch: 14, train loss: 37.701324462890625, test loss: 33.47871017456055\n",
      "h: 18 | epoch: 15, train loss: 37.03424072265625, test loss: 32.86381912231445\n",
      "h: 18 | epoch: 16, train loss: 36.38014602661133, test loss: 32.2605094909668\n",
      "h: 18 | epoch: 17, train loss: 35.738624572753906, test loss: 31.668432235717773\n",
      "h: 18 | epoch: 18, train loss: 35.109283447265625, test loss: 31.08725357055664\n",
      "h: 18 | epoch: 19, train loss: 34.49177932739258, test loss: 30.51665687561035\n",
      "h: 18 | epoch: 20, train loss: 33.885772705078125, test loss: 29.95636558532715\n",
      "h: 18 | epoch: 21, train loss: 33.29096221923828, test loss: 29.406118392944336\n",
      "h: 18 | epoch: 22, train loss: 32.70704650878906, test loss: 28.865665435791016\n",
      "h: 18 | epoch: 23, train loss: 32.13376998901367, test loss: 28.33477210998535\n",
      "h: 18 | epoch: 24, train loss: 31.57088851928711, test loss: 27.813228607177734\n",
      "h: 18 | epoch: 25, train loss: 31.018157958984375, test loss: 27.300846099853516\n",
      "h: 18 | epoch: 26, train loss: 30.475378036499023, test loss: 26.797433853149414\n",
      "h: 18 | epoch: 27, train loss: 29.94234275817871, test loss: 26.302806854248047\n",
      "h: 18 | epoch: 28, train loss: 29.418865203857422, test loss: 25.81682777404785\n",
      "h: 18 | epoch: 29, train loss: 28.904775619506836, test loss: 25.33933448791504\n",
      "h: 18 | epoch: 30, train loss: 28.399917602539062, test loss: 24.87019157409668\n",
      "h: 18 | epoch: 31, train loss: 27.904132843017578, test loss: 24.409269332885742\n",
      "h: 18 | epoch: 32, train loss: 27.417287826538086, test loss: 23.95644760131836\n",
      "h: 18 | epoch: 33, train loss: 26.939250946044922, test loss: 23.511619567871094\n",
      "h: 18 | epoch: 34, train loss: 26.469898223876953, test loss: 23.074661254882812\n",
      "h: 18 | epoch: 35, train loss: 26.00912094116211, test loss: 22.64548110961914\n",
      "h: 18 | epoch: 36, train loss: 25.556800842285156, test loss: 22.224000930786133\n",
      "h: 18 | epoch: 37, train loss: 25.112842559814453, test loss: 21.810108184814453\n",
      "h: 18 | epoch: 38, train loss: 24.677146911621094, test loss: 21.403728485107422\n",
      "h: 18 | epoch: 39, train loss: 24.249622344970703, test loss: 21.00478172302246\n",
      "h: 18 | epoch: 40, train loss: 23.83018684387207, test loss: 20.61319351196289\n",
      "h: 18 | epoch: 41, train loss: 23.418750762939453, test loss: 20.2288875579834\n",
      "h: 18 | epoch: 42, train loss: 23.01523780822754, test loss: 19.851789474487305\n",
      "h: 18 | epoch: 43, train loss: 22.61956787109375, test loss: 19.48183822631836\n",
      "h: 18 | epoch: 44, train loss: 22.231670379638672, test loss: 19.11896324157715\n",
      "h: 18 | epoch: 45, train loss: 21.851469039916992, test loss: 18.763099670410156\n",
      "h: 18 | epoch: 46, train loss: 21.478893280029297, test loss: 18.4141845703125\n",
      "h: 18 | epoch: 47, train loss: 21.113872528076172, test loss: 18.072154998779297\n",
      "h: 18 | epoch: 48, train loss: 20.756338119506836, test loss: 17.73694610595703\n",
      "h: 18 | epoch: 49, train loss: 20.40622329711914, test loss: 17.40850067138672\n",
      "h: 18 | epoch: 50, train loss: 20.063457489013672, test loss: 17.086753845214844\n",
      "h: 18 | epoch: 51, train loss: 19.72797393798828, test loss: 16.77164649963379\n",
      "h: 18 | epoch: 52, train loss: 19.399703979492188, test loss: 16.463117599487305\n",
      "h: 18 | epoch: 53, train loss: 19.078577041625977, test loss: 16.161104202270508\n",
      "h: 18 | epoch: 54, train loss: 18.764530181884766, test loss: 15.865537643432617\n",
      "h: 18 | epoch: 55, train loss: 18.45748519897461, test loss: 15.57636833190918\n",
      "h: 18 | epoch: 56, train loss: 18.15737533569336, test loss: 15.29351806640625\n",
      "h: 18 | epoch: 57, train loss: 17.8641300201416, test loss: 15.016931533813477\n",
      "h: 18 | epoch: 58, train loss: 17.57767677307129, test loss: 14.746536254882812\n",
      "h: 18 | epoch: 59, train loss: 17.297937393188477, test loss: 14.482267379760742\n",
      "h: 18 | epoch: 60, train loss: 17.024837493896484, test loss: 14.2240571975708\n",
      "h: 18 | epoch: 61, train loss: 16.758304595947266, test loss: 13.971837043762207\n",
      "h: 18 | epoch: 62, train loss: 16.498258590698242, test loss: 13.725537300109863\n",
      "h: 18 | epoch: 63, train loss: 16.244617462158203, test loss: 13.485078811645508\n",
      "h: 18 | epoch: 64, train loss: 15.997302055358887, test loss: 13.250396728515625\n",
      "h: 18 | epoch: 65, train loss: 15.756230354309082, test loss: 13.021410942077637\n",
      "h: 18 | epoch: 66, train loss: 15.52131462097168, test loss: 12.798050880432129\n",
      "h: 18 | epoch: 67, train loss: 15.29247760772705, test loss: 12.580235481262207\n",
      "h: 18 | epoch: 68, train loss: 15.069625854492188, test loss: 12.367887496948242\n",
      "h: 18 | epoch: 69, train loss: 14.85267448425293, test loss: 12.160932540893555\n",
      "h: 18 | epoch: 70, train loss: 14.641535758972168, test loss: 11.959283828735352\n",
      "h: 18 | epoch: 71, train loss: 14.436116218566895, test loss: 11.762860298156738\n",
      "h: 18 | epoch: 72, train loss: 14.23632526397705, test loss: 11.571584701538086\n",
      "h: 18 | epoch: 73, train loss: 14.042073249816895, test loss: 11.385371208190918\n",
      "h: 18 | epoch: 74, train loss: 13.853261947631836, test loss: 11.204134941101074\n",
      "h: 18 | epoch: 75, train loss: 13.6697998046875, test loss: 11.027793884277344\n",
      "h: 18 | epoch: 76, train loss: 13.49159049987793, test loss: 10.85626220703125\n",
      "h: 18 | epoch: 77, train loss: 13.318540573120117, test loss: 10.689453125\n",
      "h: 18 | epoch: 78, train loss: 13.150552749633789, test loss: 10.527278900146484\n",
      "h: 18 | epoch: 79, train loss: 12.987525939941406, test loss: 10.36965274810791\n",
      "h: 18 | epoch: 80, train loss: 12.829365730285645, test loss: 10.216489791870117\n",
      "h: 18 | epoch: 81, train loss: 12.67597484588623, test loss: 10.067700386047363\n",
      "h: 18 | epoch: 82, train loss: 12.527255058288574, test loss: 9.923198699951172\n",
      "h: 18 | epoch: 83, train loss: 12.383108139038086, test loss: 9.782894134521484\n",
      "h: 18 | epoch: 84, train loss: 12.243432998657227, test loss: 9.64670181274414\n",
      "h: 18 | epoch: 85, train loss: 12.108133316040039, test loss: 9.514533996582031\n",
      "h: 18 | epoch: 86, train loss: 11.977113723754883, test loss: 9.386302947998047\n",
      "h: 18 | epoch: 87, train loss: 11.850271224975586, test loss: 9.261919021606445\n",
      "h: 18 | epoch: 88, train loss: 11.72751235961914, test loss: 9.141298294067383\n",
      "h: 18 | epoch: 89, train loss: 11.608739852905273, test loss: 9.024352073669434\n",
      "h: 18 | epoch: 90, train loss: 11.493854522705078, test loss: 8.91099739074707\n",
      "h: 18 | epoch: 91, train loss: 11.38276195526123, test loss: 8.80114459991455\n",
      "h: 18 | epoch: 92, train loss: 11.275367736816406, test loss: 8.69471549987793\n",
      "h: 18 | epoch: 93, train loss: 11.171579360961914, test loss: 8.591619491577148\n",
      "h: 18 | epoch: 94, train loss: 11.071298599243164, test loss: 8.491777420043945\n",
      "h: 18 | epoch: 95, train loss: 10.974435806274414, test loss: 8.39510440826416\n",
      "h: 18 | epoch: 96, train loss: 10.880900382995605, test loss: 8.301520347595215\n",
      "h: 18 | epoch: 97, train loss: 10.79060173034668, test loss: 8.210945129394531\n",
      "h: 18 | epoch: 98, train loss: 10.703448295593262, test loss: 8.123300552368164\n",
      "h: 18 | epoch: 99, train loss: 10.619352340698242, test loss: 8.038503646850586\n",
      "h: 19 | epoch: 0, train loss: 46.84877014160156, test loss: 41.64649200439453\n",
      "h: 19 | epoch: 1, train loss: 45.32918167114258, test loss: 40.28977584838867\n",
      "h: 19 | epoch: 2, train loss: 43.87060546875, test loss: 38.98539352416992\n",
      "h: 19 | epoch: 3, train loss: 42.46979904174805, test loss: 37.73065185546875\n",
      "h: 19 | epoch: 4, train loss: 41.123779296875, test loss: 36.523067474365234\n",
      "h: 19 | epoch: 5, train loss: 39.82978820800781, test loss: 35.36037826538086\n",
      "h: 19 | epoch: 6, train loss: 38.58530807495117, test loss: 34.240455627441406\n",
      "h: 19 | epoch: 7, train loss: 37.388004302978516, test loss: 33.161373138427734\n",
      "h: 19 | epoch: 8, train loss: 36.235721588134766, test loss: 32.121315002441406\n",
      "h: 19 | epoch: 9, train loss: 35.12645721435547, test loss: 31.11862564086914\n",
      "h: 19 | epoch: 10, train loss: 34.0583610534668, test loss: 30.1517391204834\n",
      "h: 19 | epoch: 11, train loss: 33.02970504760742, test loss: 29.219219207763672\n",
      "h: 19 | epoch: 12, train loss: 32.03889465332031, test loss: 28.319698333740234\n",
      "h: 19 | epoch: 13, train loss: 31.08441162109375, test loss: 27.45192527770996\n",
      "h: 19 | epoch: 14, train loss: 30.16485023498535, test loss: 26.61470603942871\n",
      "h: 19 | epoch: 15, train loss: 29.278894424438477, test loss: 25.806921005249023\n",
      "h: 19 | epoch: 16, train loss: 28.4252986907959, test loss: 25.02752113342285\n",
      "h: 19 | epoch: 17, train loss: 27.602893829345703, test loss: 24.275508880615234\n",
      "h: 19 | epoch: 18, train loss: 26.810562133789062, test loss: 23.549945831298828\n",
      "h: 19 | epoch: 19, train loss: 26.047260284423828, test loss: 22.84994125366211\n",
      "h: 19 | epoch: 20, train loss: 25.311992645263672, test loss: 22.174633026123047\n",
      "h: 19 | epoch: 21, train loss: 24.603803634643555, test loss: 21.523225784301758\n",
      "h: 19 | epoch: 22, train loss: 23.921794891357422, test loss: 20.89493751525879\n",
      "h: 19 | epoch: 23, train loss: 23.265094757080078, test loss: 20.28902244567871\n",
      "h: 19 | epoch: 24, train loss: 22.632875442504883, test loss: 19.704776763916016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 19 | epoch: 25, train loss: 22.024341583251953, test loss: 19.141517639160156\n",
      "h: 19 | epoch: 26, train loss: 21.43872833251953, test loss: 18.598581314086914\n",
      "h: 19 | epoch: 27, train loss: 20.875293731689453, test loss: 18.075332641601562\n",
      "h: 19 | epoch: 28, train loss: 20.333332061767578, test loss: 17.571163177490234\n",
      "h: 19 | epoch: 29, train loss: 19.812152862548828, test loss: 17.085477828979492\n",
      "h: 19 | epoch: 30, train loss: 19.311092376708984, test loss: 16.617694854736328\n",
      "h: 19 | epoch: 31, train loss: 18.829500198364258, test loss: 16.167268753051758\n",
      "h: 19 | epoch: 32, train loss: 18.36675453186035, test loss: 15.733647346496582\n",
      "h: 19 | epoch: 33, train loss: 17.922245025634766, test loss: 15.316307067871094\n",
      "h: 19 | epoch: 34, train loss: 17.49538230895996, test loss: 14.914735794067383\n",
      "h: 19 | epoch: 35, train loss: 17.085594177246094, test loss: 14.528432846069336\n",
      "h: 19 | epoch: 36, train loss: 16.69231414794922, test loss: 14.156919479370117\n",
      "h: 19 | epoch: 37, train loss: 16.315004348754883, test loss: 13.799710273742676\n",
      "h: 19 | epoch: 38, train loss: 15.953129768371582, test loss: 13.456350326538086\n",
      "h: 19 | epoch: 39, train loss: 15.606170654296875, test loss: 13.12639045715332\n",
      "h: 19 | epoch: 40, train loss: 15.273626327514648, test loss: 12.80938720703125\n",
      "h: 19 | epoch: 41, train loss: 14.954999923706055, test loss: 12.504911422729492\n",
      "h: 19 | epoch: 42, train loss: 14.649816513061523, test loss: 12.21254825592041\n",
      "h: 19 | epoch: 43, train loss: 14.357603073120117, test loss: 11.931886672973633\n",
      "h: 19 | epoch: 44, train loss: 14.077908515930176, test loss: 11.662528991699219\n",
      "h: 19 | epoch: 45, train loss: 13.810282707214355, test loss: 11.404088020324707\n",
      "h: 19 | epoch: 46, train loss: 13.554292678833008, test loss: 11.156180381774902\n",
      "h: 19 | epoch: 47, train loss: 13.309514999389648, test loss: 10.918441772460938\n",
      "h: 19 | epoch: 48, train loss: 13.075541496276855, test loss: 10.690511703491211\n",
      "h: 19 | epoch: 49, train loss: 12.851966857910156, test loss: 10.472034454345703\n",
      "h: 19 | epoch: 50, train loss: 12.63840389251709, test loss: 10.262674331665039\n",
      "h: 19 | epoch: 51, train loss: 12.43447208404541, test loss: 10.062094688415527\n",
      "h: 19 | epoch: 52, train loss: 12.239799499511719, test loss: 9.869974136352539\n",
      "h: 19 | epoch: 53, train loss: 12.05402946472168, test loss: 9.685996055603027\n",
      "h: 19 | epoch: 54, train loss: 11.876811981201172, test loss: 9.509854316711426\n",
      "h: 19 | epoch: 55, train loss: 11.707807540893555, test loss: 9.341253280639648\n",
      "h: 19 | epoch: 56, train loss: 11.5466890335083, test loss: 9.179903030395508\n",
      "h: 19 | epoch: 57, train loss: 11.393136978149414, test loss: 9.02552318572998\n",
      "h: 19 | epoch: 58, train loss: 11.24683952331543, test loss: 8.877842903137207\n",
      "h: 19 | epoch: 59, train loss: 11.107501029968262, test loss: 8.736598014831543\n",
      "h: 19 | epoch: 60, train loss: 10.97482967376709, test loss: 8.601532936096191\n",
      "h: 19 | epoch: 61, train loss: 10.848544120788574, test loss: 8.472399711608887\n",
      "h: 19 | epoch: 62, train loss: 10.728372573852539, test loss: 8.348958969116211\n",
      "h: 19 | epoch: 63, train loss: 10.614056587219238, test loss: 8.230979919433594\n",
      "h: 19 | epoch: 64, train loss: 10.505338668823242, test loss: 8.11823844909668\n",
      "h: 19 | epoch: 65, train loss: 10.401973724365234, test loss: 8.010518074035645\n",
      "h: 19 | epoch: 66, train loss: 10.303728103637695, test loss: 7.9076080322265625\n",
      "h: 19 | epoch: 67, train loss: 10.210373878479004, test loss: 7.809309482574463\n",
      "h: 19 | epoch: 68, train loss: 10.121689796447754, test loss: 7.715424537658691\n",
      "h: 19 | epoch: 69, train loss: 10.037466049194336, test loss: 7.62576961517334\n",
      "h: 19 | epoch: 70, train loss: 9.957499504089355, test loss: 7.540160179138184\n",
      "h: 19 | epoch: 71, train loss: 9.881593704223633, test loss: 7.4584221839904785\n",
      "h: 19 | epoch: 72, train loss: 9.809562683105469, test loss: 7.3803911209106445\n",
      "h: 19 | epoch: 73, train loss: 9.741223335266113, test loss: 7.305902004241943\n",
      "h: 19 | epoch: 74, train loss: 9.676401138305664, test loss: 7.234800815582275\n",
      "h: 19 | epoch: 75, train loss: 9.614934921264648, test loss: 7.166938781738281\n",
      "h: 19 | epoch: 76, train loss: 9.556659698486328, test loss: 7.102175235748291\n",
      "h: 19 | epoch: 77, train loss: 9.501424789428711, test loss: 7.040366172790527\n",
      "h: 19 | epoch: 78, train loss: 9.449081420898438, test loss: 6.981383323669434\n",
      "h: 19 | epoch: 79, train loss: 9.399490356445312, test loss: 6.925099849700928\n",
      "h: 19 | epoch: 80, train loss: 9.352518081665039, test loss: 6.871392250061035\n",
      "h: 19 | epoch: 81, train loss: 9.30803394317627, test loss: 6.8201470375061035\n",
      "h: 19 | epoch: 82, train loss: 9.265914916992188, test loss: 6.771248817443848\n",
      "h: 19 | epoch: 83, train loss: 9.226042747497559, test loss: 6.7245917320251465\n",
      "h: 19 | epoch: 84, train loss: 9.18830680847168, test loss: 6.680074214935303\n",
      "h: 19 | epoch: 85, train loss: 9.15259838104248, test loss: 6.637597560882568\n",
      "h: 19 | epoch: 86, train loss: 9.118815422058105, test loss: 6.597066402435303\n",
      "h: 19 | epoch: 87, train loss: 9.086857795715332, test loss: 6.5583930015563965\n",
      "h: 19 | epoch: 88, train loss: 9.056635856628418, test loss: 6.521490573883057\n",
      "h: 19 | epoch: 89, train loss: 9.028058052062988, test loss: 6.486273765563965\n",
      "h: 19 | epoch: 90, train loss: 9.001039505004883, test loss: 6.452670097351074\n",
      "h: 19 | epoch: 91, train loss: 8.97550106048584, test loss: 6.420599460601807\n",
      "h: 19 | epoch: 92, train loss: 8.951362609863281, test loss: 6.3899922370910645\n",
      "h: 19 | epoch: 93, train loss: 8.928552627563477, test loss: 6.360779285430908\n",
      "h: 19 | epoch: 94, train loss: 8.907003402709961, test loss: 6.332894802093506\n",
      "h: 19 | epoch: 95, train loss: 8.88664436340332, test loss: 6.306279182434082\n",
      "h: 19 | epoch: 96, train loss: 8.867415428161621, test loss: 6.280869007110596\n",
      "h: 19 | epoch: 97, train loss: 8.849255561828613, test loss: 6.25661039352417\n",
      "h: 19 | epoch: 98, train loss: 8.83210563659668, test loss: 6.233447551727295\n",
      "h: 19 | epoch: 99, train loss: 8.81591510772705, test loss: 6.211330413818359\n",
      "h: 20 | epoch: 0, train loss: 41.435882568359375, test loss: 36.98111343383789\n",
      "h: 20 | epoch: 1, train loss: 40.45691680908203, test loss: 36.09412384033203\n",
      "h: 20 | epoch: 2, train loss: 39.506141662597656, test loss: 35.231651306152344\n",
      "h: 20 | epoch: 3, train loss: 38.58238983154297, test loss: 34.3927001953125\n",
      "h: 20 | epoch: 4, train loss: 37.684532165527344, test loss: 33.576324462890625\n",
      "h: 20 | epoch: 5, train loss: 36.811553955078125, test loss: 32.78166961669922\n",
      "h: 20 | epoch: 6, train loss: 35.962493896484375, test loss: 32.007930755615234\n",
      "h: 20 | epoch: 7, train loss: 35.136474609375, test loss: 31.25436782836914\n",
      "h: 20 | epoch: 8, train loss: 34.33266830444336, test loss: 30.5202579498291\n",
      "h: 20 | epoch: 9, train loss: 33.55031204223633, test loss: 29.80499267578125\n",
      "h: 20 | epoch: 10, train loss: 32.78868103027344, test loss: 29.10793113708496\n",
      "h: 20 | epoch: 11, train loss: 32.047122955322266, test loss: 28.42852210998535\n",
      "h: 20 | epoch: 12, train loss: 31.325002670288086, test loss: 27.766244888305664\n",
      "h: 20 | epoch: 13, train loss: 30.621734619140625, test loss: 27.120590209960938\n",
      "h: 20 | epoch: 14, train loss: 29.936779022216797, test loss: 26.491098403930664\n",
      "h: 20 | epoch: 15, train loss: 29.269617080688477, test loss: 25.87734031677246\n",
      "h: 20 | epoch: 16, train loss: 28.619770050048828, test loss: 25.27889060974121\n",
      "h: 20 | epoch: 17, train loss: 27.986774444580078, test loss: 24.69537353515625\n",
      "h: 20 | epoch: 18, train loss: 27.370203018188477, test loss: 24.126419067382812\n",
      "h: 20 | epoch: 19, train loss: 26.769657135009766, test loss: 23.571674346923828\n",
      "h: 20 | epoch: 20, train loss: 26.184741973876953, test loss: 23.03082275390625\n",
      "h: 20 | epoch: 21, train loss: 25.61509132385254, test loss: 22.5035343170166\n",
      "h: 20 | epoch: 22, train loss: 25.060361862182617, test loss: 21.989524841308594\n",
      "h: 20 | epoch: 23, train loss: 24.52021598815918, test loss: 21.488496780395508\n",
      "h: 20 | epoch: 24, train loss: 23.99433708190918, test loss: 21.000185012817383\n",
      "h: 20 | epoch: 25, train loss: 23.4824161529541, test loss: 20.524324417114258\n",
      "h: 20 | epoch: 26, train loss: 22.98416519165039, test loss: 20.060659408569336\n",
      "h: 20 | epoch: 27, train loss: 22.49929428100586, test loss: 19.608951568603516\n",
      "h: 20 | epoch: 28, train loss: 22.027530670166016, test loss: 19.168962478637695\n",
      "h: 20 | epoch: 29, train loss: 21.568613052368164, test loss: 18.74045753479004\n",
      "h: 20 | epoch: 30, train loss: 21.12228012084961, test loss: 18.323225021362305\n",
      "h: 20 | epoch: 31, train loss: 20.688282012939453, test loss: 17.917034149169922\n",
      "h: 20 | epoch: 32, train loss: 20.266368865966797, test loss: 17.521684646606445\n",
      "h: 20 | epoch: 33, train loss: 19.856307983398438, test loss: 17.136959075927734\n",
      "h: 20 | epoch: 34, train loss: 19.45786476135254, test loss: 16.76266098022461\n",
      "h: 20 | epoch: 35, train loss: 19.070804595947266, test loss: 16.39858627319336\n",
      "h: 20 | epoch: 36, train loss: 18.694902420043945, test loss: 16.044538497924805\n",
      "h: 20 | epoch: 37, train loss: 18.32993507385254, test loss: 15.70032787322998\n",
      "h: 20 | epoch: 38, train loss: 17.975683212280273, test loss: 15.365757942199707\n",
      "h: 20 | epoch: 39, train loss: 17.63193130493164, test loss: 15.040641784667969\n",
      "h: 20 | epoch: 40, train loss: 17.298463821411133, test loss: 14.724787712097168\n",
      "h: 20 | epoch: 41, train loss: 16.975067138671875, test loss: 14.41801929473877\n",
      "h: 20 | epoch: 42, train loss: 16.66153335571289, test loss: 14.120145797729492\n",
      "h: 20 | epoch: 43, train loss: 16.357650756835938, test loss: 13.830986022949219\n",
      "h: 20 | epoch: 44, train loss: 16.063217163085938, test loss: 13.550366401672363\n",
      "h: 20 | epoch: 45, train loss: 15.77802562713623, test loss: 13.278093338012695\n",
      "h: 20 | epoch: 46, train loss: 15.501873970031738, test loss: 13.013999938964844\n",
      "h: 20 | epoch: 47, train loss: 15.2345552444458, test loss: 12.757909774780273\n",
      "h: 20 | epoch: 48, train loss: 14.975875854492188, test loss: 12.509641647338867\n",
      "h: 20 | epoch: 49, train loss: 14.725637435913086, test loss: 12.269025802612305\n",
      "h: 20 | epoch: 50, train loss: 14.483633041381836, test loss: 12.035887718200684\n",
      "h: 20 | epoch: 51, train loss: 14.249679565429688, test loss: 11.810056686401367\n",
      "h: 20 | epoch: 52, train loss: 14.023576736450195, test loss: 11.591360092163086\n",
      "h: 20 | epoch: 53, train loss: 13.805130004882812, test loss: 11.379631042480469\n",
      "h: 20 | epoch: 54, train loss: 13.594154357910156, test loss: 11.174702644348145\n",
      "h: 20 | epoch: 55, train loss: 13.39045524597168, test loss: 10.976405143737793\n",
      "h: 20 | epoch: 56, train loss: 13.193845748901367, test loss: 10.784579277038574\n",
      "h: 20 | epoch: 57, train loss: 13.004145622253418, test loss: 10.59906005859375\n",
      "h: 20 | epoch: 58, train loss: 12.821168899536133, test loss: 10.419684410095215\n",
      "h: 20 | epoch: 59, train loss: 12.644731521606445, test loss: 10.246294021606445\n",
      "h: 20 | epoch: 60, train loss: 12.474655151367188, test loss: 10.07873249053955\n",
      "h: 20 | epoch: 61, train loss: 12.310765266418457, test loss: 9.916842460632324\n",
      "h: 20 | epoch: 62, train loss: 12.152883529663086, test loss: 9.760469436645508\n",
      "h: 20 | epoch: 63, train loss: 12.000838279724121, test loss: 9.60946273803711\n",
      "h: 20 | epoch: 64, train loss: 11.854463577270508, test loss: 9.463672637939453\n",
      "h: 20 | epoch: 65, train loss: 11.713584899902344, test loss: 9.322953224182129\n",
      "h: 20 | epoch: 66, train loss: 11.578045845031738, test loss: 9.187155723571777\n",
      "h: 20 | epoch: 67, train loss: 11.447675704956055, test loss: 9.056139945983887\n",
      "h: 20 | epoch: 68, train loss: 11.322320938110352, test loss: 8.929763793945312\n",
      "h: 20 | epoch: 69, train loss: 11.201820373535156, test loss: 8.807888984680176\n",
      "h: 20 | epoch: 70, train loss: 11.08602523803711, test loss: 8.69038200378418\n",
      "h: 20 | epoch: 71, train loss: 10.974781036376953, test loss: 8.577105522155762\n",
      "h: 20 | epoch: 72, train loss: 10.867940902709961, test loss: 8.467935562133789\n",
      "h: 20 | epoch: 73, train loss: 10.765359878540039, test loss: 8.362736701965332\n",
      "h: 20 | epoch: 74, train loss: 10.66689682006836, test loss: 8.261385917663574\n",
      "h: 20 | epoch: 75, train loss: 10.572412490844727, test loss: 8.163762092590332\n",
      "h: 20 | epoch: 76, train loss: 10.481770515441895, test loss: 8.069742202758789\n",
      "h: 20 | epoch: 77, train loss: 10.3948392868042, test loss: 7.979211330413818\n",
      "h: 20 | epoch: 78, train loss: 10.311488151550293, test loss: 7.892054080963135\n",
      "h: 20 | epoch: 79, train loss: 10.231593132019043, test loss: 7.808156490325928\n",
      "h: 20 | epoch: 80, train loss: 10.155029296875, test loss: 7.72741174697876\n",
      "h: 20 | epoch: 81, train loss: 10.081676483154297, test loss: 7.649710178375244\n",
      "h: 20 | epoch: 82, train loss: 10.011419296264648, test loss: 7.574948787689209\n",
      "h: 20 | epoch: 83, train loss: 9.94414234161377, test loss: 7.503029823303223\n",
      "h: 20 | epoch: 84, train loss: 9.879735946655273, test loss: 7.433849334716797\n",
      "h: 20 | epoch: 85, train loss: 9.818094253540039, test loss: 7.36731481552124\n",
      "h: 20 | epoch: 86, train loss: 9.759108543395996, test loss: 7.303330421447754\n",
      "h: 20 | epoch: 87, train loss: 9.702680587768555, test loss: 7.241806983947754\n",
      "h: 20 | epoch: 88, train loss: 9.648712158203125, test loss: 7.182656288146973\n",
      "h: 20 | epoch: 89, train loss: 9.597107887268066, test loss: 7.125791072845459\n",
      "h: 20 | epoch: 90, train loss: 9.547773361206055, test loss: 7.071130275726318\n",
      "h: 20 | epoch: 91, train loss: 9.500619888305664, test loss: 7.018592834472656\n",
      "h: 20 | epoch: 92, train loss: 9.455560684204102, test loss: 6.968101501464844\n",
      "h: 20 | epoch: 93, train loss: 9.41251277923584, test loss: 6.919577121734619\n",
      "h: 20 | epoch: 94, train loss: 9.371394157409668, test loss: 6.872950077056885\n",
      "h: 20 | epoch: 95, train loss: 9.332127571105957, test loss: 6.828146457672119\n",
      "h: 20 | epoch: 96, train loss: 9.294633865356445, test loss: 6.785099983215332\n",
      "h: 20 | epoch: 97, train loss: 9.258844375610352, test loss: 6.7437424659729\n",
      "h: 20 | epoch: 98, train loss: 9.224685668945312, test loss: 6.704010963439941\n",
      "h: 20 | epoch: 99, train loss: 9.192089080810547, test loss: 6.665842533111572\n",
      "h: 21 | epoch: 0, train loss: 46.04362487792969, test loss: 41.82545852661133\n",
      "h: 21 | epoch: 1, train loss: 44.71281814575195, test loss: 40.604270935058594\n",
      "h: 21 | epoch: 2, train loss: 43.430606842041016, test loss: 39.42605209350586\n",
      "h: 21 | epoch: 3, train loss: 42.19448471069336, test loss: 38.288665771484375\n",
      "h: 21 | epoch: 4, train loss: 41.00216293334961, test loss: 37.190120697021484\n",
      "h: 21 | epoch: 5, train loss: 39.851524353027344, test loss: 36.12861251831055\n",
      "h: 21 | epoch: 6, train loss: 38.74061584472656, test loss: 35.10245132446289\n",
      "h: 21 | epoch: 7, train loss: 37.66763687133789, test loss: 34.110084533691406\n",
      "h: 21 | epoch: 8, train loss: 36.630916595458984, test loss: 33.15005874633789\n",
      "h: 21 | epoch: 9, train loss: 35.62891387939453, test loss: 32.221046447753906\n",
      "h: 21 | epoch: 10, train loss: 34.660179138183594, test loss: 31.321802139282227\n",
      "h: 21 | epoch: 11, train loss: 33.72339630126953, test loss: 30.451156616210938\n",
      "h: 21 | epoch: 12, train loss: 32.81730270385742, test loss: 29.608041763305664\n",
      "h: 21 | epoch: 13, train loss: 31.940750122070312, test loss: 28.79143714904785\n",
      "h: 21 | epoch: 14, train loss: 31.0926513671875, test loss: 28.000396728515625\n",
      "h: 21 | epoch: 15, train loss: 30.271984100341797, test loss: 27.23403549194336\n",
      "h: 21 | epoch: 16, train loss: 29.477807998657227, test loss: 26.491525650024414\n",
      "h: 21 | epoch: 17, train loss: 28.709209442138672, test loss: 25.77207374572754\n",
      "h: 21 | epoch: 18, train loss: 27.965347290039062, test loss: 25.074939727783203\n",
      "h: 21 | epoch: 19, train loss: 27.24542808532715, test loss: 24.399425506591797\n",
      "h: 21 | epoch: 20, train loss: 26.548694610595703, test loss: 23.744861602783203\n",
      "h: 21 | epoch: 21, train loss: 25.874425888061523, test loss: 23.11062240600586\n",
      "h: 21 | epoch: 22, train loss: 25.221940994262695, test loss: 22.49610710144043\n",
      "h: 21 | epoch: 23, train loss: 24.590595245361328, test loss: 21.90074348449707\n",
      "h: 21 | epoch: 24, train loss: 23.979761123657227, test loss: 21.323978424072266\n",
      "h: 21 | epoch: 25, train loss: 23.388851165771484, test loss: 20.76529884338379\n",
      "h: 21 | epoch: 26, train loss: 22.817296981811523, test loss: 20.224191665649414\n",
      "h: 21 | epoch: 27, train loss: 22.264551162719727, test loss: 19.700176239013672\n",
      "h: 21 | epoch: 28, train loss: 21.730085372924805, test loss: 19.192787170410156\n",
      "h: 21 | epoch: 29, train loss: 21.213394165039062, test loss: 18.701574325561523\n",
      "h: 21 | epoch: 30, train loss: 20.713985443115234, test loss: 18.226106643676758\n",
      "h: 21 | epoch: 31, train loss: 20.231388092041016, test loss: 17.765949249267578\n",
      "h: 21 | epoch: 32, train loss: 19.76513671875, test loss: 17.320707321166992\n",
      "h: 21 | epoch: 33, train loss: 19.314790725708008, test loss: 16.889972686767578\n",
      "h: 21 | epoch: 34, train loss: 18.879905700683594, test loss: 16.47336196899414\n",
      "h: 21 | epoch: 35, train loss: 18.460063934326172, test loss: 16.070507049560547\n",
      "h: 21 | epoch: 36, train loss: 18.05484962463379, test loss: 15.681022644042969\n",
      "h: 21 | epoch: 37, train loss: 17.663862228393555, test loss: 15.304559707641602\n",
      "h: 21 | epoch: 38, train loss: 17.28670883178711, test loss: 14.940763473510742\n",
      "h: 21 | epoch: 39, train loss: 16.922996520996094, test loss: 14.589288711547852\n",
      "h: 21 | epoch: 40, train loss: 16.572355270385742, test loss: 14.249804496765137\n",
      "h: 21 | epoch: 41, train loss: 16.23441505432129, test loss: 13.921978950500488\n",
      "h: 21 | epoch: 42, train loss: 15.908808708190918, test loss: 13.605484008789062\n",
      "h: 21 | epoch: 43, train loss: 15.595186233520508, test loss: 13.300008773803711\n",
      "h: 21 | epoch: 44, train loss: 15.293195724487305, test loss: 13.005243301391602\n",
      "h: 21 | epoch: 45, train loss: 15.002500534057617, test loss: 12.720874786376953\n",
      "h: 21 | epoch: 46, train loss: 14.72276496887207, test loss: 12.446613311767578\n",
      "h: 21 | epoch: 47, train loss: 14.453659057617188, test loss: 12.182159423828125\n",
      "h: 21 | epoch: 48, train loss: 14.194859504699707, test loss: 11.927228927612305\n",
      "h: 21 | epoch: 49, train loss: 13.94605541229248, test loss: 11.681540489196777\n",
      "h: 21 | epoch: 50, train loss: 13.706933975219727, test loss: 11.44481086730957\n",
      "h: 21 | epoch: 51, train loss: 13.477190971374512, test loss: 11.216776847839355\n",
      "h: 21 | epoch: 52, train loss: 13.25653076171875, test loss: 10.997167587280273\n",
      "h: 21 | epoch: 53, train loss: 13.044660568237305, test loss: 10.785720825195312\n",
      "h: 21 | epoch: 54, train loss: 12.841296195983887, test loss: 10.58218765258789\n",
      "h: 21 | epoch: 55, train loss: 12.646156311035156, test loss: 10.386311531066895\n",
      "h: 21 | epoch: 56, train loss: 12.458969116210938, test loss: 10.197851181030273\n",
      "h: 21 | epoch: 57, train loss: 12.27946662902832, test loss: 10.016562461853027\n",
      "h: 21 | epoch: 58, train loss: 12.107385635375977, test loss: 9.842215538024902\n",
      "h: 21 | epoch: 59, train loss: 11.942472457885742, test loss: 9.674581527709961\n",
      "h: 21 | epoch: 60, train loss: 11.784476280212402, test loss: 9.513429641723633\n",
      "h: 21 | epoch: 61, train loss: 11.633152961730957, test loss: 9.358550071716309\n",
      "h: 21 | epoch: 62, train loss: 11.488264083862305, test loss: 9.209723472595215\n",
      "h: 21 | epoch: 63, train loss: 11.349576950073242, test loss: 9.066744804382324\n",
      "h: 21 | epoch: 64, train loss: 11.216869354248047, test loss: 8.92940902709961\n",
      "h: 21 | epoch: 65, train loss: 11.089917182922363, test loss: 8.797517776489258\n",
      "h: 21 | epoch: 66, train loss: 10.968507766723633, test loss: 8.670881271362305\n",
      "h: 21 | epoch: 67, train loss: 10.852431297302246, test loss: 8.549306869506836\n",
      "h: 21 | epoch: 68, train loss: 10.741485595703125, test loss: 8.432615280151367\n",
      "h: 21 | epoch: 69, train loss: 10.63547420501709, test loss: 8.320629119873047\n",
      "h: 21 | epoch: 70, train loss: 10.534205436706543, test loss: 8.213174819946289\n",
      "h: 21 | epoch: 71, train loss: 10.437492370605469, test loss: 8.110085487365723\n",
      "h: 21 | epoch: 72, train loss: 10.345155715942383, test loss: 8.011194229125977\n",
      "h: 21 | epoch: 73, train loss: 10.257020950317383, test loss: 7.9163498878479\n",
      "h: 21 | epoch: 74, train loss: 10.172918319702148, test loss: 7.8253960609436035\n",
      "h: 21 | epoch: 75, train loss: 10.092683792114258, test loss: 7.73818302154541\n",
      "h: 21 | epoch: 76, train loss: 10.016159057617188, test loss: 7.654568672180176\n",
      "h: 21 | epoch: 77, train loss: 9.943190574645996, test loss: 7.574410438537598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 21 | epoch: 78, train loss: 9.873629570007324, test loss: 7.497579097747803\n",
      "h: 21 | epoch: 79, train loss: 9.807334899902344, test loss: 7.4239397048950195\n",
      "h: 21 | epoch: 80, train loss: 9.744166374206543, test loss: 7.353369235992432\n",
      "h: 21 | epoch: 81, train loss: 9.683991432189941, test loss: 7.28574275970459\n",
      "h: 21 | epoch: 82, train loss: 9.626678466796875, test loss: 7.220945835113525\n",
      "h: 21 | epoch: 83, train loss: 9.572107315063477, test loss: 7.1588640213012695\n",
      "h: 21 | epoch: 84, train loss: 9.520156860351562, test loss: 7.0993852615356445\n",
      "h: 21 | epoch: 85, train loss: 9.470711708068848, test loss: 7.042405605316162\n",
      "h: 21 | epoch: 86, train loss: 9.423660278320312, test loss: 6.987822532653809\n",
      "h: 21 | epoch: 87, train loss: 9.378896713256836, test loss: 6.935538291931152\n",
      "h: 21 | epoch: 88, train loss: 9.33631706237793, test loss: 6.885458469390869\n",
      "h: 21 | epoch: 89, train loss: 9.295825004577637, test loss: 6.837491035461426\n",
      "h: 21 | epoch: 90, train loss: 9.257322311401367, test loss: 6.7915496826171875\n",
      "h: 21 | epoch: 91, train loss: 9.220724105834961, test loss: 6.747546195983887\n",
      "h: 21 | epoch: 92, train loss: 9.185937881469727, test loss: 6.7054033279418945\n",
      "h: 21 | epoch: 93, train loss: 9.15287971496582, test loss: 6.665041923522949\n",
      "h: 21 | epoch: 94, train loss: 9.121472358703613, test loss: 6.626385688781738\n",
      "h: 21 | epoch: 95, train loss: 9.091636657714844, test loss: 6.589364051818848\n",
      "h: 21 | epoch: 96, train loss: 9.063298225402832, test loss: 6.5539069175720215\n",
      "h: 21 | epoch: 97, train loss: 9.036386489868164, test loss: 6.5199480056762695\n",
      "h: 21 | epoch: 98, train loss: 9.010836601257324, test loss: 6.487420558929443\n",
      "h: 21 | epoch: 99, train loss: 8.986580848693848, test loss: 6.4562668800354\n",
      "h: 22 | epoch: 0, train loss: 41.603065490722656, test loss: 37.78274917602539\n",
      "h: 22 | epoch: 1, train loss: 40.76374435424805, test loss: 36.98870086669922\n",
      "h: 22 | epoch: 2, train loss: 39.94453430175781, test loss: 36.21342086791992\n",
      "h: 22 | epoch: 3, train loss: 39.14463424682617, test loss: 35.4561767578125\n",
      "h: 22 | epoch: 4, train loss: 38.36330795288086, test loss: 34.7162971496582\n",
      "h: 22 | epoch: 5, train loss: 37.599876403808594, test loss: 33.99315643310547\n",
      "h: 22 | epoch: 6, train loss: 36.85368347167969, test loss: 33.28615951538086\n",
      "h: 22 | epoch: 7, train loss: 36.1241455078125, test loss: 32.59477996826172\n",
      "h: 22 | epoch: 8, train loss: 35.41071319580078, test loss: 31.918493270874023\n",
      "h: 22 | epoch: 9, train loss: 34.71287155151367, test loss: 31.256845474243164\n",
      "h: 22 | epoch: 10, train loss: 34.0301513671875, test loss: 30.609386444091797\n",
      "h: 22 | epoch: 11, train loss: 33.36210632324219, test loss: 29.975723266601562\n",
      "h: 22 | epoch: 12, train loss: 32.70832061767578, test loss: 29.355459213256836\n",
      "h: 22 | epoch: 13, train loss: 32.068424224853516, test loss: 28.748241424560547\n",
      "h: 22 | epoch: 14, train loss: 31.442047119140625, test loss: 28.153751373291016\n",
      "h: 22 | epoch: 15, train loss: 30.8288631439209, test loss: 27.571659088134766\n",
      "h: 22 | epoch: 16, train loss: 30.22855567932129, test loss: 27.00168228149414\n",
      "h: 22 | epoch: 17, train loss: 29.64084243774414, test loss: 26.44356346130371\n",
      "h: 22 | epoch: 18, train loss: 29.065444946289062, test loss: 25.897029876708984\n",
      "h: 22 | epoch: 19, train loss: 28.502111434936523, test loss: 25.36185073852539\n",
      "h: 22 | epoch: 20, train loss: 27.950603485107422, test loss: 24.837797164916992\n",
      "h: 22 | epoch: 21, train loss: 27.41069984436035, test loss: 24.324661254882812\n",
      "h: 22 | epoch: 22, train loss: 26.88218116760254, test loss: 23.82224464416504\n",
      "h: 22 | epoch: 23, train loss: 26.364856719970703, test loss: 23.330347061157227\n",
      "h: 22 | epoch: 24, train loss: 25.858531951904297, test loss: 22.848798751831055\n",
      "h: 22 | epoch: 25, train loss: 25.36302947998047, test loss: 22.37742042541504\n",
      "h: 22 | epoch: 26, train loss: 24.878177642822266, test loss: 21.916061401367188\n",
      "h: 22 | epoch: 27, train loss: 24.403820037841797, test loss: 21.464555740356445\n",
      "h: 22 | epoch: 28, train loss: 23.939796447753906, test loss: 21.02275848388672\n",
      "h: 22 | epoch: 29, train loss: 23.485958099365234, test loss: 20.590526580810547\n",
      "h: 22 | epoch: 30, train loss: 23.042163848876953, test loss: 20.16771697998047\n",
      "h: 22 | epoch: 31, train loss: 22.608272552490234, test loss: 19.754201889038086\n",
      "h: 22 | epoch: 32, train loss: 22.184144973754883, test loss: 19.349842071533203\n",
      "h: 22 | epoch: 33, train loss: 21.7696590423584, test loss: 18.954517364501953\n",
      "h: 22 | epoch: 34, train loss: 21.36467933654785, test loss: 18.568103790283203\n",
      "h: 22 | epoch: 35, train loss: 20.96908187866211, test loss: 18.19047737121582\n",
      "h: 22 | epoch: 36, train loss: 20.582748413085938, test loss: 17.821521759033203\n",
      "h: 22 | epoch: 37, train loss: 20.205547332763672, test loss: 17.461116790771484\n",
      "h: 22 | epoch: 38, train loss: 19.837369918823242, test loss: 17.10914421081543\n",
      "h: 22 | epoch: 39, train loss: 19.478084564208984, test loss: 16.765487670898438\n",
      "h: 22 | epoch: 40, train loss: 19.127580642700195, test loss: 16.43004035949707\n",
      "h: 22 | epoch: 41, train loss: 18.785736083984375, test loss: 16.10268211364746\n",
      "h: 22 | epoch: 42, train loss: 18.452434539794922, test loss: 15.783297538757324\n",
      "h: 22 | epoch: 43, train loss: 18.127553939819336, test loss: 15.471776008605957\n",
      "h: 22 | epoch: 44, train loss: 17.810977935791016, test loss: 15.167997360229492\n",
      "h: 22 | epoch: 45, train loss: 17.502586364746094, test loss: 14.871859550476074\n",
      "h: 22 | epoch: 46, train loss: 17.2022647857666, test loss: 14.583242416381836\n",
      "h: 22 | epoch: 47, train loss: 16.90988540649414, test loss: 14.302034378051758\n",
      "h: 22 | epoch: 48, train loss: 16.62533187866211, test loss: 14.028116226196289\n",
      "h: 22 | epoch: 49, train loss: 16.348480224609375, test loss: 13.761372566223145\n",
      "h: 22 | epoch: 50, train loss: 16.079208374023438, test loss: 13.501690864562988\n",
      "h: 22 | epoch: 51, train loss: 15.81739616394043, test loss: 13.24895191192627\n",
      "h: 22 | epoch: 52, train loss: 15.56291675567627, test loss: 13.003044128417969\n",
      "h: 22 | epoch: 53, train loss: 15.315646171569824, test loss: 12.763845443725586\n",
      "h: 22 | epoch: 54, train loss: 15.075460433959961, test loss: 12.531240463256836\n",
      "h: 22 | epoch: 55, train loss: 14.842228889465332, test loss: 12.30511474609375\n",
      "h: 22 | epoch: 56, train loss: 14.615829467773438, test loss: 12.08534049987793\n",
      "h: 22 | epoch: 57, train loss: 14.396130561828613, test loss: 11.871808052062988\n",
      "h: 22 | epoch: 58, train loss: 14.183006286621094, test loss: 11.66439437866211\n",
      "h: 22 | epoch: 59, train loss: 13.97632884979248, test loss: 11.462980270385742\n",
      "h: 22 | epoch: 60, train loss: 13.775968551635742, test loss: 11.267446517944336\n",
      "h: 22 | epoch: 61, train loss: 13.58179759979248, test loss: 11.077676773071289\n",
      "h: 22 | epoch: 62, train loss: 13.393682479858398, test loss: 10.893548011779785\n",
      "h: 22 | epoch: 63, train loss: 13.211499214172363, test loss: 10.714941024780273\n",
      "h: 22 | epoch: 64, train loss: 13.035115242004395, test loss: 10.541738510131836\n",
      "h: 22 | epoch: 65, train loss: 12.864400863647461, test loss: 10.373824119567871\n",
      "h: 22 | epoch: 66, train loss: 12.699230194091797, test loss: 10.211073875427246\n",
      "h: 22 | epoch: 67, train loss: 12.539472579956055, test loss: 10.053372383117676\n",
      "h: 22 | epoch: 68, train loss: 12.385000228881836, test loss: 9.900603294372559\n",
      "h: 22 | epoch: 69, train loss: 12.235685348510742, test loss: 9.752650260925293\n",
      "h: 22 | epoch: 70, train loss: 12.091403007507324, test loss: 9.609395980834961\n",
      "h: 22 | epoch: 71, train loss: 11.952024459838867, test loss: 9.47072696685791\n",
      "h: 22 | epoch: 72, train loss: 11.817426681518555, test loss: 9.336528778076172\n",
      "h: 22 | epoch: 73, train loss: 11.68748664855957, test loss: 9.20668888092041\n",
      "h: 22 | epoch: 74, train loss: 11.562080383300781, test loss: 9.08109188079834\n",
      "h: 22 | epoch: 75, train loss: 11.441083908081055, test loss: 8.959632873535156\n",
      "h: 22 | epoch: 76, train loss: 11.324380874633789, test loss: 8.842199325561523\n",
      "h: 22 | epoch: 77, train loss: 11.211848258972168, test loss: 8.728682518005371\n",
      "h: 22 | epoch: 78, train loss: 11.103371620178223, test loss: 8.618975639343262\n",
      "h: 22 | epoch: 79, train loss: 10.9988374710083, test loss: 8.512978553771973\n",
      "h: 22 | epoch: 80, train loss: 10.898124694824219, test loss: 8.4105806350708\n",
      "h: 22 | epoch: 81, train loss: 10.801124572753906, test loss: 8.31168270111084\n",
      "h: 22 | epoch: 82, train loss: 10.707725524902344, test loss: 8.216184616088867\n",
      "h: 22 | epoch: 83, train loss: 10.617822647094727, test loss: 8.12398624420166\n",
      "h: 22 | epoch: 84, train loss: 10.531301498413086, test loss: 8.034990310668945\n",
      "h: 22 | epoch: 85, train loss: 10.448060989379883, test loss: 7.949105262756348\n",
      "h: 22 | epoch: 86, train loss: 10.367999076843262, test loss: 7.866232395172119\n",
      "h: 22 | epoch: 87, train loss: 10.291009902954102, test loss: 7.786283016204834\n",
      "h: 22 | epoch: 88, train loss: 10.217000007629395, test loss: 7.709166526794434\n",
      "h: 22 | epoch: 89, train loss: 10.145869255065918, test loss: 7.634793758392334\n",
      "h: 22 | epoch: 90, train loss: 10.077521324157715, test loss: 7.563080787658691\n",
      "h: 22 | epoch: 91, train loss: 10.011866569519043, test loss: 7.493939399719238\n",
      "h: 22 | epoch: 92, train loss: 9.948811531066895, test loss: 7.427292823791504\n",
      "h: 22 | epoch: 93, train loss: 9.888267517089844, test loss: 7.363055229187012\n",
      "h: 22 | epoch: 94, train loss: 9.83014965057373, test loss: 7.301149845123291\n",
      "h: 22 | epoch: 95, train loss: 9.774373054504395, test loss: 7.241502285003662\n",
      "h: 22 | epoch: 96, train loss: 9.720853805541992, test loss: 7.184034824371338\n",
      "h: 22 | epoch: 97, train loss: 9.669513702392578, test loss: 7.1286749839782715\n",
      "h: 22 | epoch: 98, train loss: 9.620274543762207, test loss: 7.075353145599365\n",
      "h: 22 | epoch: 99, train loss: 9.573060035705566, test loss: 7.023998260498047\n",
      "h: 23 | epoch: 0, train loss: 44.78907012939453, test loss: 39.747493743896484\n",
      "h: 23 | epoch: 1, train loss: 43.58567810058594, test loss: 38.65156936645508\n",
      "h: 23 | epoch: 2, train loss: 42.42211151123047, test loss: 37.59101486206055\n",
      "h: 23 | epoch: 3, train loss: 41.2964973449707, test loss: 36.564247131347656\n",
      "h: 23 | epoch: 4, train loss: 40.20710372924805, test loss: 35.56974792480469\n",
      "h: 23 | epoch: 5, train loss: 39.152339935302734, test loss: 34.60614013671875\n",
      "h: 23 | epoch: 6, train loss: 38.13072204589844, test loss: 33.672142028808594\n",
      "h: 23 | epoch: 7, train loss: 37.14088439941406, test loss: 32.766563415527344\n",
      "h: 23 | epoch: 8, train loss: 36.181556701660156, test loss: 31.888309478759766\n",
      "h: 23 | epoch: 9, train loss: 35.25156021118164, test loss: 31.03634262084961\n",
      "h: 23 | epoch: 10, train loss: 34.34980010986328, test loss: 30.2097225189209\n",
      "h: 23 | epoch: 11, train loss: 33.475250244140625, test loss: 29.407527923583984\n",
      "h: 23 | epoch: 12, train loss: 32.62696075439453, test loss: 28.628942489624023\n",
      "h: 23 | epoch: 13, train loss: 31.804031372070312, test loss: 27.873178482055664\n",
      "h: 23 | epoch: 14, train loss: 31.00563621520996, test loss: 27.13949966430664\n",
      "h: 23 | epoch: 15, train loss: 30.230987548828125, test loss: 26.427215576171875\n",
      "h: 23 | epoch: 16, train loss: 29.479345321655273, test loss: 25.735671997070312\n",
      "h: 23 | epoch: 17, train loss: 28.750019073486328, test loss: 25.064258575439453\n",
      "h: 23 | epoch: 18, train loss: 28.042348861694336, test loss: 24.41239356994629\n",
      "h: 23 | epoch: 19, train loss: 27.355716705322266, test loss: 23.779521942138672\n",
      "h: 23 | epoch: 20, train loss: 26.689533233642578, test loss: 23.165119171142578\n",
      "h: 23 | epoch: 21, train loss: 26.043237686157227, test loss: 22.56869125366211\n",
      "h: 23 | epoch: 22, train loss: 25.416296005249023, test loss: 21.989765167236328\n",
      "h: 23 | epoch: 23, train loss: 24.80820083618164, test loss: 21.427875518798828\n",
      "h: 23 | epoch: 24, train loss: 24.218460083007812, test loss: 20.882587432861328\n",
      "h: 23 | epoch: 25, train loss: 23.646610260009766, test loss: 20.353479385375977\n",
      "h: 23 | epoch: 26, train loss: 23.092191696166992, test loss: 19.840158462524414\n",
      "h: 23 | epoch: 27, train loss: 22.554780960083008, test loss: 19.34221839904785\n",
      "h: 23 | epoch: 28, train loss: 22.033954620361328, test loss: 18.85929298400879\n",
      "h: 23 | epoch: 29, train loss: 21.529300689697266, test loss: 18.391010284423828\n",
      "h: 23 | epoch: 30, train loss: 21.040430068969727, test loss: 17.937015533447266\n",
      "h: 23 | epoch: 31, train loss: 20.566959381103516, test loss: 17.496967315673828\n",
      "h: 23 | epoch: 32, train loss: 20.108510971069336, test loss: 17.07051658630371\n",
      "h: 23 | epoch: 33, train loss: 19.664722442626953, test loss: 16.657350540161133\n",
      "h: 23 | epoch: 34, train loss: 19.23523712158203, test loss: 16.257137298583984\n",
      "h: 23 | epoch: 35, train loss: 18.81970977783203, test loss: 15.869565963745117\n",
      "h: 23 | epoch: 36, train loss: 18.417795181274414, test loss: 15.494325637817383\n",
      "h: 23 | epoch: 37, train loss: 18.029159545898438, test loss: 15.131115913391113\n",
      "h: 23 | epoch: 38, train loss: 17.653470993041992, test loss: 14.779635429382324\n",
      "h: 23 | epoch: 39, train loss: 17.2904109954834, test loss: 14.439598083496094\n",
      "h: 23 | epoch: 40, train loss: 16.93965721130371, test loss: 14.11071491241455\n",
      "h: 23 | epoch: 41, train loss: 16.60089683532715, test loss: 13.792704582214355\n",
      "h: 23 | epoch: 42, train loss: 16.273822784423828, test loss: 13.485285758972168\n",
      "h: 23 | epoch: 43, train loss: 15.958131790161133, test loss: 13.188188552856445\n",
      "h: 23 | epoch: 44, train loss: 15.653524398803711, test loss: 12.901140213012695\n",
      "h: 23 | epoch: 45, train loss: 15.359704971313477, test loss: 12.623880386352539\n",
      "h: 23 | epoch: 46, train loss: 15.076382637023926, test loss: 12.356144905090332\n",
      "h: 23 | epoch: 47, train loss: 14.803268432617188, test loss: 12.097673416137695\n",
      "h: 23 | epoch: 48, train loss: 14.540082931518555, test loss: 11.84821605682373\n",
      "h: 23 | epoch: 49, train loss: 14.28654956817627, test loss: 11.607525825500488\n",
      "h: 23 | epoch: 50, train loss: 14.042388916015625, test loss: 11.375349998474121\n",
      "h: 23 | epoch: 51, train loss: 13.807332992553711, test loss: 11.151453018188477\n",
      "h: 23 | epoch: 52, train loss: 13.5811185836792, test loss: 10.935589790344238\n",
      "h: 23 | epoch: 53, train loss: 13.363481521606445, test loss: 10.727535247802734\n",
      "h: 23 | epoch: 54, train loss: 13.15416431427002, test loss: 10.527050971984863\n",
      "h: 23 | epoch: 55, train loss: 12.952914237976074, test loss: 10.333914756774902\n",
      "h: 23 | epoch: 56, train loss: 12.759480476379395, test loss: 10.147905349731445\n",
      "h: 23 | epoch: 57, train loss: 12.57362174987793, test loss: 9.968799591064453\n",
      "h: 23 | epoch: 58, train loss: 12.395095825195312, test loss: 9.796392440795898\n",
      "h: 23 | epoch: 59, train loss: 12.223669052124023, test loss: 9.630465507507324\n",
      "h: 23 | epoch: 60, train loss: 12.059106826782227, test loss: 9.470816612243652\n",
      "h: 23 | epoch: 61, train loss: 11.901189804077148, test loss: 9.317244529724121\n",
      "h: 23 | epoch: 62, train loss: 11.749689102172852, test loss: 9.169550895690918\n",
      "h: 23 | epoch: 63, train loss: 11.604392051696777, test loss: 9.027543067932129\n",
      "h: 23 | epoch: 64, train loss: 11.465085983276367, test loss: 8.891031265258789\n",
      "h: 23 | epoch: 65, train loss: 11.331562042236328, test loss: 8.759832382202148\n",
      "h: 23 | epoch: 66, train loss: 11.203618049621582, test loss: 8.633767127990723\n",
      "h: 23 | epoch: 67, train loss: 11.081059455871582, test loss: 8.512657165527344\n",
      "h: 23 | epoch: 68, train loss: 10.9636869430542, test loss: 8.396329879760742\n",
      "h: 23 | epoch: 69, train loss: 10.851317405700684, test loss: 8.284623146057129\n",
      "h: 23 | epoch: 70, train loss: 10.743766784667969, test loss: 8.177369117736816\n",
      "h: 23 | epoch: 71, train loss: 10.64085578918457, test loss: 8.074411392211914\n",
      "h: 23 | epoch: 72, train loss: 10.542409896850586, test loss: 7.975592136383057\n",
      "h: 23 | epoch: 73, train loss: 10.448262214660645, test loss: 7.880766868591309\n",
      "h: 23 | epoch: 74, train loss: 10.358247756958008, test loss: 7.789783477783203\n",
      "h: 23 | epoch: 75, train loss: 10.272205352783203, test loss: 7.702505588531494\n",
      "h: 23 | epoch: 76, train loss: 10.189983367919922, test loss: 7.618793487548828\n",
      "h: 23 | epoch: 77, train loss: 10.111430168151855, test loss: 7.53851318359375\n",
      "h: 23 | epoch: 78, train loss: 10.036401748657227, test loss: 7.4615325927734375\n",
      "h: 23 | epoch: 79, train loss: 9.964754104614258, test loss: 7.387729644775391\n",
      "h: 23 | epoch: 80, train loss: 9.896352767944336, test loss: 7.316979885101318\n",
      "h: 23 | epoch: 81, train loss: 9.83106803894043, test loss: 7.24916934967041\n",
      "h: 23 | epoch: 82, train loss: 9.768766403198242, test loss: 7.184176445007324\n",
      "h: 23 | epoch: 83, train loss: 9.709328651428223, test loss: 7.121899604797363\n",
      "h: 23 | epoch: 84, train loss: 9.652634620666504, test loss: 7.062228202819824\n",
      "h: 23 | epoch: 85, train loss: 9.5985689163208, test loss: 7.005055904388428\n",
      "h: 23 | epoch: 86, train loss: 9.547021865844727, test loss: 6.950289249420166\n",
      "h: 23 | epoch: 87, train loss: 9.497881889343262, test loss: 6.897826194763184\n",
      "h: 23 | epoch: 88, train loss: 9.451050758361816, test loss: 6.8475775718688965\n",
      "h: 23 | epoch: 89, train loss: 9.406425476074219, test loss: 6.799452304840088\n",
      "h: 23 | epoch: 90, train loss: 9.363912582397461, test loss: 6.753365516662598\n",
      "h: 23 | epoch: 91, train loss: 9.323417663574219, test loss: 6.709230899810791\n",
      "h: 23 | epoch: 92, train loss: 9.28485107421875, test loss: 6.6669721603393555\n",
      "h: 23 | epoch: 93, train loss: 9.248129844665527, test loss: 6.6265082359313965\n",
      "h: 23 | epoch: 94, train loss: 9.213171005249023, test loss: 6.587767601013184\n",
      "h: 23 | epoch: 95, train loss: 9.17989730834961, test loss: 6.550677299499512\n",
      "h: 23 | epoch: 96, train loss: 9.14822769165039, test loss: 6.5151686668396\n",
      "h: 23 | epoch: 97, train loss: 9.118095397949219, test loss: 6.481175899505615\n",
      "h: 23 | epoch: 98, train loss: 9.089426040649414, test loss: 6.448632717132568\n",
      "h: 23 | epoch: 99, train loss: 9.062154769897461, test loss: 6.417481422424316\n",
      "h: 24 | epoch: 0, train loss: 39.76564025878906, test loss: 34.9760856628418\n",
      "h: 24 | epoch: 1, train loss: 38.83905029296875, test loss: 34.11824035644531\n",
      "h: 24 | epoch: 2, train loss: 37.940269470214844, test loss: 33.28614807128906\n",
      "h: 24 | epoch: 3, train loss: 37.06814193725586, test loss: 32.478736877441406\n",
      "h: 24 | epoch: 4, train loss: 36.2215576171875, test loss: 31.69500732421875\n",
      "h: 24 | epoch: 5, train loss: 35.399497985839844, test loss: 30.934009552001953\n",
      "h: 24 | epoch: 6, train loss: 34.60100555419922, test loss: 30.194875717163086\n",
      "h: 24 | epoch: 7, train loss: 33.82520294189453, test loss: 29.4768009185791\n",
      "h: 24 | epoch: 8, train loss: 33.07124710083008, test loss: 28.778995513916016\n",
      "h: 24 | epoch: 9, train loss: 32.33835983276367, test loss: 28.100738525390625\n",
      "h: 24 | epoch: 10, train loss: 31.62581443786621, test loss: 27.44136619567871\n",
      "h: 24 | epoch: 11, train loss: 30.93292808532715, test loss: 26.80023765563965\n",
      "h: 24 | epoch: 12, train loss: 30.259052276611328, test loss: 26.1767578125\n",
      "h: 24 | epoch: 13, train loss: 29.60357666015625, test loss: 25.570350646972656\n",
      "h: 24 | epoch: 14, train loss: 28.965938568115234, test loss: 24.98047637939453\n",
      "h: 24 | epoch: 15, train loss: 28.345596313476562, test loss: 24.406646728515625\n",
      "h: 24 | epoch: 16, train loss: 27.74203109741211, test loss: 23.848373413085938\n",
      "h: 24 | epoch: 17, train loss: 27.154769897460938, test loss: 23.30520248413086\n",
      "h: 24 | epoch: 18, train loss: 26.58334732055664, test loss: 22.77669906616211\n",
      "h: 24 | epoch: 19, train loss: 26.027328491210938, test loss: 22.262454986572266\n",
      "h: 24 | epoch: 20, train loss: 25.48630142211914, test loss: 21.762075424194336\n",
      "h: 24 | epoch: 21, train loss: 24.959867477416992, test loss: 21.275197982788086\n",
      "h: 24 | epoch: 22, train loss: 24.447650909423828, test loss: 20.80144691467285\n",
      "h: 24 | epoch: 23, train loss: 23.949291229248047, test loss: 20.34049415588379\n",
      "h: 24 | epoch: 24, train loss: 23.464441299438477, test loss: 19.892005920410156\n",
      "h: 24 | epoch: 25, train loss: 22.992765426635742, test loss: 19.45566177368164\n",
      "h: 24 | epoch: 26, train loss: 22.533946990966797, test loss: 19.031164169311523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 24 | epoch: 27, train loss: 22.087677001953125, test loss: 18.61821937561035\n",
      "h: 24 | epoch: 28, train loss: 21.65365219116211, test loss: 18.21653938293457\n",
      "h: 24 | epoch: 29, train loss: 21.231592178344727, test loss: 17.825857162475586\n",
      "h: 24 | epoch: 30, train loss: 20.821212768554688, test loss: 17.445903778076172\n",
      "h: 24 | epoch: 31, train loss: 20.422245025634766, test loss: 17.076427459716797\n",
      "h: 24 | epoch: 32, train loss: 20.0344295501709, test loss: 16.717174530029297\n",
      "h: 24 | epoch: 33, train loss: 19.65750503540039, test loss: 16.367908477783203\n",
      "h: 24 | epoch: 34, train loss: 19.291229248046875, test loss: 16.028390884399414\n",
      "h: 24 | epoch: 35, train loss: 18.93535041809082, test loss: 15.698391914367676\n",
      "h: 24 | epoch: 36, train loss: 18.589637756347656, test loss: 15.37768840789795\n",
      "h: 24 | epoch: 37, train loss: 18.253856658935547, test loss: 15.066061019897461\n",
      "h: 24 | epoch: 38, train loss: 17.927780151367188, test loss: 14.763299942016602\n",
      "h: 24 | epoch: 39, train loss: 17.61118507385254, test loss: 14.469192504882812\n",
      "h: 24 | epoch: 40, train loss: 17.303855895996094, test loss: 14.183538436889648\n",
      "h: 24 | epoch: 41, train loss: 17.005578994750977, test loss: 13.906132698059082\n",
      "h: 24 | epoch: 42, train loss: 16.716136932373047, test loss: 13.636784553527832\n",
      "h: 24 | epoch: 43, train loss: 16.435333251953125, test loss: 13.3753023147583\n",
      "h: 24 | epoch: 44, train loss: 16.162960052490234, test loss: 13.121498107910156\n",
      "h: 24 | epoch: 45, train loss: 15.898818969726562, test loss: 12.8751802444458\n",
      "h: 24 | epoch: 46, train loss: 15.64271068572998, test loss: 12.636175155639648\n",
      "h: 24 | epoch: 47, train loss: 15.394444465637207, test loss: 12.404302597045898\n",
      "h: 24 | epoch: 48, train loss: 15.153831481933594, test loss: 12.179388046264648\n",
      "h: 24 | epoch: 49, train loss: 14.920682907104492, test loss: 11.961256980895996\n",
      "h: 24 | epoch: 50, train loss: 14.694816589355469, test loss: 11.74974250793457\n",
      "h: 24 | epoch: 51, train loss: 14.476048469543457, test loss: 11.544679641723633\n",
      "h: 24 | epoch: 52, train loss: 14.264201164245605, test loss: 11.345906257629395\n",
      "h: 24 | epoch: 53, train loss: 14.059100151062012, test loss: 11.153258323669434\n",
      "h: 24 | epoch: 54, train loss: 13.860569953918457, test loss: 10.966582298278809\n",
      "h: 24 | epoch: 55, train loss: 13.66844367980957, test loss: 10.785721778869629\n",
      "h: 24 | epoch: 56, train loss: 13.482548713684082, test loss: 10.610528945922852\n",
      "h: 24 | epoch: 57, train loss: 13.302724838256836, test loss: 10.440851211547852\n",
      "h: 24 | epoch: 58, train loss: 13.128809928894043, test loss: 10.27653694152832\n",
      "h: 24 | epoch: 59, train loss: 12.960641860961914, test loss: 10.117453575134277\n",
      "h: 24 | epoch: 60, train loss: 12.798065185546875, test loss: 9.96345329284668\n",
      "h: 24 | epoch: 61, train loss: 12.640926361083984, test loss: 9.814401626586914\n",
      "h: 24 | epoch: 62, train loss: 12.48907470703125, test loss: 9.670156478881836\n",
      "h: 24 | epoch: 63, train loss: 12.342358589172363, test loss: 9.530592918395996\n",
      "h: 24 | epoch: 64, train loss: 12.200637817382812, test loss: 9.395573616027832\n",
      "h: 24 | epoch: 65, train loss: 12.063764572143555, test loss: 9.264976501464844\n",
      "h: 24 | epoch: 66, train loss: 11.931599617004395, test loss: 9.138669967651367\n",
      "h: 24 | epoch: 67, train loss: 11.80400562286377, test loss: 9.016536712646484\n",
      "h: 24 | epoch: 68, train loss: 11.680849075317383, test loss: 8.89845085144043\n",
      "h: 24 | epoch: 69, train loss: 11.561994552612305, test loss: 8.784303665161133\n",
      "h: 24 | epoch: 70, train loss: 11.447318077087402, test loss: 8.673970222473145\n",
      "h: 24 | epoch: 71, train loss: 11.336688995361328, test loss: 8.567342758178711\n",
      "h: 24 | epoch: 72, train loss: 11.229982376098633, test loss: 8.464312553405762\n",
      "h: 24 | epoch: 73, train loss: 11.127081871032715, test loss: 8.364768028259277\n",
      "h: 24 | epoch: 74, train loss: 11.027867317199707, test loss: 8.268606185913086\n",
      "h: 24 | epoch: 75, train loss: 10.932220458984375, test loss: 8.175724029541016\n",
      "h: 24 | epoch: 76, train loss: 10.840031623840332, test loss: 8.086021423339844\n",
      "h: 24 | epoch: 77, train loss: 10.75118637084961, test loss: 7.9994001388549805\n",
      "h: 24 | epoch: 78, train loss: 10.665582656860352, test loss: 7.91576623916626\n",
      "h: 24 | epoch: 79, train loss: 10.583111763000488, test loss: 7.835022926330566\n",
      "h: 24 | epoch: 80, train loss: 10.503671646118164, test loss: 7.757083892822266\n",
      "h: 24 | epoch: 81, train loss: 10.427164077758789, test loss: 7.681858062744141\n",
      "h: 24 | epoch: 82, train loss: 10.35349178314209, test loss: 7.60925817489624\n",
      "h: 24 | epoch: 83, train loss: 10.282559394836426, test loss: 7.5392022132873535\n",
      "h: 24 | epoch: 84, train loss: 10.214273452758789, test loss: 7.471608638763428\n",
      "h: 24 | epoch: 85, train loss: 10.14854621887207, test loss: 7.406394004821777\n",
      "h: 24 | epoch: 86, train loss: 10.085290908813477, test loss: 7.343485355377197\n",
      "h: 24 | epoch: 87, train loss: 10.024420738220215, test loss: 7.282806396484375\n",
      "h: 24 | epoch: 88, train loss: 9.965856552124023, test loss: 7.224282741546631\n",
      "h: 24 | epoch: 89, train loss: 9.909516334533691, test loss: 7.167841911315918\n",
      "h: 24 | epoch: 90, train loss: 9.855321884155273, test loss: 7.113419532775879\n",
      "h: 24 | epoch: 91, train loss: 9.80319881439209, test loss: 7.060943603515625\n",
      "h: 24 | epoch: 92, train loss: 9.753076553344727, test loss: 7.010350704193115\n",
      "h: 24 | epoch: 93, train loss: 9.704879760742188, test loss: 6.961575984954834\n",
      "h: 24 | epoch: 94, train loss: 9.658539772033691, test loss: 6.914559841156006\n",
      "h: 24 | epoch: 95, train loss: 9.613991737365723, test loss: 6.869242191314697\n",
      "h: 24 | epoch: 96, train loss: 9.571172714233398, test loss: 6.825565338134766\n",
      "h: 24 | epoch: 97, train loss: 9.53001880645752, test loss: 6.783471584320068\n",
      "h: 24 | epoch: 98, train loss: 9.49046516418457, test loss: 6.742907524108887\n",
      "h: 24 | epoch: 99, train loss: 9.452459335327148, test loss: 6.703820705413818\n",
      "h: 25 | epoch: 0, train loss: 45.909820556640625, test loss: 40.42593002319336\n",
      "h: 25 | epoch: 1, train loss: 43.98291778564453, test loss: 38.71709060668945\n",
      "h: 25 | epoch: 2, train loss: 42.15477752685547, test loss: 37.09285354614258\n",
      "h: 25 | epoch: 3, train loss: 40.419471740722656, test loss: 35.54829788208008\n",
      "h: 25 | epoch: 4, train loss: 38.77156066894531, test loss: 34.078922271728516\n",
      "h: 25 | epoch: 5, train loss: 37.2060661315918, test loss: 32.680580139160156\n",
      "h: 25 | epoch: 6, train loss: 35.718406677246094, test loss: 31.34946060180664\n",
      "h: 25 | epoch: 7, train loss: 34.304359436035156, test loss: 30.0820255279541\n",
      "h: 25 | epoch: 8, train loss: 32.96002960205078, test loss: 28.875009536743164\n",
      "h: 25 | epoch: 9, train loss: 31.68178367614746, test loss: 27.72536277770996\n",
      "h: 25 | epoch: 10, train loss: 30.466266632080078, test loss: 26.6302433013916\n",
      "h: 25 | epoch: 11, train loss: 29.310338973999023, test loss: 25.587024688720703\n",
      "h: 25 | epoch: 12, train loss: 28.2110595703125, test loss: 24.593198776245117\n",
      "h: 25 | epoch: 13, train loss: 27.165695190429688, test loss: 23.64645004272461\n",
      "h: 25 | epoch: 14, train loss: 26.17165184020996, test loss: 22.744586944580078\n",
      "h: 25 | epoch: 15, train loss: 25.226505279541016, test loss: 21.885526657104492\n",
      "h: 25 | epoch: 16, train loss: 24.32796287536621, test loss: 21.067325592041016\n",
      "h: 25 | epoch: 17, train loss: 23.473857879638672, test loss: 20.288122177124023\n",
      "h: 25 | epoch: 18, train loss: 22.662137985229492, test loss: 19.54616928100586\n",
      "h: 25 | epoch: 19, train loss: 21.890853881835938, test loss: 18.839786529541016\n",
      "h: 25 | epoch: 20, train loss: 21.158151626586914, test loss: 18.167390823364258\n",
      "h: 25 | epoch: 21, train loss: 20.4622745513916, test loss: 17.527462005615234\n",
      "h: 25 | epoch: 22, train loss: 19.801538467407227, test loss: 16.918550491333008\n",
      "h: 25 | epoch: 23, train loss: 19.174341201782227, test loss: 16.33928108215332\n",
      "h: 25 | epoch: 24, train loss: 18.579151153564453, test loss: 15.788330078125\n",
      "h: 25 | epoch: 25, train loss: 18.014507293701172, test loss: 15.264432907104492\n",
      "h: 25 | epoch: 26, train loss: 17.47900390625, test loss: 14.766377449035645\n",
      "h: 25 | epoch: 27, train loss: 16.971302032470703, test loss: 14.293004989624023\n",
      "h: 25 | epoch: 28, train loss: 16.4901123046875, test loss: 13.843195915222168\n",
      "h: 25 | epoch: 29, train loss: 16.03420639038086, test loss: 13.415885925292969\n",
      "h: 25 | epoch: 30, train loss: 15.60240364074707, test loss: 13.010050773620605\n",
      "h: 25 | epoch: 31, train loss: 15.193567276000977, test loss: 12.624707221984863\n",
      "h: 25 | epoch: 32, train loss: 14.806610107421875, test loss: 12.258909225463867\n",
      "h: 25 | epoch: 33, train loss: 14.4404935836792, test loss: 11.91175365447998\n",
      "h: 25 | epoch: 34, train loss: 14.094215393066406, test loss: 11.582366943359375\n",
      "h: 25 | epoch: 35, train loss: 13.766815185546875, test loss: 11.269918441772461\n",
      "h: 25 | epoch: 36, train loss: 13.457374572753906, test loss: 10.973607063293457\n",
      "h: 25 | epoch: 37, train loss: 13.165011405944824, test loss: 10.692663192749023\n",
      "h: 25 | epoch: 38, train loss: 12.888882637023926, test loss: 10.426350593566895\n",
      "h: 25 | epoch: 39, train loss: 12.62817668914795, test loss: 10.173965454101562\n",
      "h: 25 | epoch: 40, train loss: 12.382115364074707, test loss: 9.93482780456543\n",
      "h: 25 | epoch: 41, train loss: 12.149959564208984, test loss: 9.708292007446289\n",
      "h: 25 | epoch: 42, train loss: 11.930999755859375, test loss: 9.493739128112793\n",
      "h: 25 | epoch: 43, train loss: 11.724555015563965, test loss: 9.290570259094238\n",
      "h: 25 | epoch: 44, train loss: 11.529973983764648, test loss: 9.098220825195312\n",
      "h: 25 | epoch: 45, train loss: 11.346635818481445, test loss: 8.916146278381348\n",
      "h: 25 | epoch: 46, train loss: 11.1739501953125, test loss: 8.743824005126953\n",
      "h: 25 | epoch: 47, train loss: 11.01134967803955, test loss: 8.580761909484863\n",
      "h: 25 | epoch: 48, train loss: 10.858293533325195, test loss: 8.426485061645508\n",
      "h: 25 | epoch: 49, train loss: 10.714264869689941, test loss: 8.280538558959961\n",
      "h: 25 | epoch: 50, train loss: 10.578775405883789, test loss: 8.142492294311523\n",
      "h: 25 | epoch: 51, train loss: 10.4513578414917, test loss: 8.011931419372559\n",
      "h: 25 | epoch: 52, train loss: 10.331563949584961, test loss: 7.888469696044922\n",
      "h: 25 | epoch: 53, train loss: 10.218974113464355, test loss: 7.771729946136475\n",
      "h: 25 | epoch: 54, train loss: 10.11318302154541, test loss: 7.661356449127197\n",
      "h: 25 | epoch: 55, train loss: 10.013808250427246, test loss: 7.557010650634766\n",
      "h: 25 | epoch: 56, train loss: 9.920488357543945, test loss: 7.458370208740234\n",
      "h: 25 | epoch: 57, train loss: 9.83287239074707, test loss: 7.3651275634765625\n",
      "h: 25 | epoch: 58, train loss: 9.750638961791992, test loss: 7.276994228363037\n",
      "h: 25 | epoch: 59, train loss: 9.67347526550293, test loss: 7.193694114685059\n",
      "h: 25 | epoch: 60, train loss: 9.60108757019043, test loss: 7.114962100982666\n",
      "h: 25 | epoch: 61, train loss: 9.533195495605469, test loss: 7.040549278259277\n",
      "h: 25 | epoch: 62, train loss: 9.469535827636719, test loss: 6.97022008895874\n",
      "h: 25 | epoch: 63, train loss: 9.409857749938965, test loss: 6.903750419616699\n",
      "h: 25 | epoch: 64, train loss: 9.353927612304688, test loss: 6.840928554534912\n",
      "h: 25 | epoch: 65, train loss: 9.301517486572266, test loss: 6.781552314758301\n",
      "h: 25 | epoch: 66, train loss: 9.252419471740723, test loss: 6.725430965423584\n",
      "h: 25 | epoch: 67, train loss: 9.206433296203613, test loss: 6.672385215759277\n",
      "h: 25 | epoch: 68, train loss: 9.163370132446289, test loss: 6.622239589691162\n",
      "h: 25 | epoch: 69, train loss: 9.123054504394531, test loss: 6.574837684631348\n",
      "h: 25 | epoch: 70, train loss: 9.08531379699707, test loss: 6.530023097991943\n",
      "h: 25 | epoch: 71, train loss: 9.049995422363281, test loss: 6.487654209136963\n",
      "h: 25 | epoch: 72, train loss: 9.016945838928223, test loss: 6.4475908279418945\n",
      "h: 25 | epoch: 73, train loss: 8.9860258102417, test loss: 6.409704685211182\n",
      "h: 25 | epoch: 74, train loss: 8.95710563659668, test loss: 6.373875617980957\n",
      "h: 25 | epoch: 75, train loss: 8.930055618286133, test loss: 6.339985370635986\n",
      "h: 25 | epoch: 76, train loss: 8.904765129089355, test loss: 6.307927131652832\n",
      "h: 25 | epoch: 77, train loss: 8.881119728088379, test loss: 6.277596950531006\n",
      "h: 25 | epoch: 78, train loss: 8.859015464782715, test loss: 6.248896598815918\n",
      "h: 25 | epoch: 79, train loss: 8.838356018066406, test loss: 6.221737861633301\n",
      "h: 25 | epoch: 80, train loss: 8.819050788879395, test loss: 6.1960296630859375\n",
      "h: 25 | epoch: 81, train loss: 8.801011085510254, test loss: 6.171693801879883\n",
      "h: 25 | epoch: 82, train loss: 8.784158706665039, test loss: 6.148652076721191\n",
      "h: 25 | epoch: 83, train loss: 8.768415451049805, test loss: 6.126832008361816\n",
      "h: 25 | epoch: 84, train loss: 8.753711700439453, test loss: 6.10616397857666\n",
      "h: 25 | epoch: 85, train loss: 8.73997974395752, test loss: 6.086585521697998\n",
      "h: 25 | epoch: 86, train loss: 8.727157592773438, test loss: 6.068034648895264\n",
      "h: 25 | epoch: 87, train loss: 8.715185165405273, test loss: 6.0504536628723145\n",
      "h: 25 | epoch: 88, train loss: 8.704010009765625, test loss: 6.033787727355957\n",
      "h: 25 | epoch: 89, train loss: 8.693577766418457, test loss: 6.017987251281738\n",
      "h: 25 | epoch: 90, train loss: 8.683839797973633, test loss: 6.003005027770996\n",
      "h: 25 | epoch: 91, train loss: 8.674753189086914, test loss: 5.9887919425964355\n",
      "h: 25 | epoch: 92, train loss: 8.66627311706543, test loss: 5.975307941436768\n",
      "h: 25 | epoch: 93, train loss: 8.658360481262207, test loss: 5.962512493133545\n",
      "h: 25 | epoch: 94, train loss: 8.65097713470459, test loss: 5.950368404388428\n",
      "h: 25 | epoch: 95, train loss: 8.64409065246582, test loss: 5.938838481903076\n",
      "h: 25 | epoch: 96, train loss: 8.637666702270508, test loss: 5.927889823913574\n",
      "h: 25 | epoch: 97, train loss: 8.631674766540527, test loss: 5.9174909591674805\n",
      "h: 25 | epoch: 98, train loss: 8.62608528137207, test loss: 5.9076104164123535\n",
      "h: 25 | epoch: 99, train loss: 8.62087345123291, test loss: 5.898221015930176\n",
      "h: 26 | epoch: 0, train loss: 47.2999267578125, test loss: 42.58097839355469\n",
      "h: 26 | epoch: 1, train loss: 46.11736297607422, test loss: 41.48329162597656\n",
      "h: 26 | epoch: 2, train loss: 44.97383117675781, test loss: 40.42119216918945\n",
      "h: 26 | epoch: 3, train loss: 43.86730194091797, test loss: 39.39290237426758\n",
      "h: 26 | epoch: 4, train loss: 42.795936584472656, test loss: 38.3967399597168\n",
      "h: 26 | epoch: 5, train loss: 41.75800704956055, test loss: 37.43120193481445\n",
      "h: 26 | epoch: 6, train loss: 40.75193786621094, test loss: 36.494850158691406\n",
      "h: 26 | epoch: 7, train loss: 39.77626037597656, test loss: 35.586387634277344\n",
      "h: 26 | epoch: 8, train loss: 38.829627990722656, test loss: 34.704593658447266\n",
      "h: 26 | epoch: 9, train loss: 37.910789489746094, test loss: 33.84834671020508\n",
      "h: 26 | epoch: 10, train loss: 37.01857376098633, test loss: 33.01659393310547\n",
      "h: 26 | epoch: 11, train loss: 36.15191650390625, test loss: 32.208370208740234\n",
      "h: 26 | epoch: 12, train loss: 35.30980682373047, test loss: 31.422765731811523\n",
      "h: 26 | epoch: 13, train loss: 34.4913215637207, test loss: 30.658931732177734\n",
      "h: 26 | epoch: 14, train loss: 33.69558334350586, test loss: 29.91609764099121\n",
      "h: 26 | epoch: 15, train loss: 32.921791076660156, test loss: 29.19351577758789\n",
      "h: 26 | epoch: 16, train loss: 32.16919708251953, test loss: 28.49050521850586\n",
      "h: 26 | epoch: 17, train loss: 31.437084197998047, test loss: 27.80642318725586\n",
      "h: 26 | epoch: 18, train loss: 30.724802017211914, test loss: 27.14066505432129\n",
      "h: 26 | epoch: 19, train loss: 30.031728744506836, test loss: 26.492656707763672\n",
      "h: 26 | epoch: 20, train loss: 29.357290267944336, test loss: 25.86188316345215\n",
      "h: 26 | epoch: 21, train loss: 28.700937271118164, test loss: 25.247833251953125\n",
      "h: 26 | epoch: 22, train loss: 28.062152862548828, test loss: 24.650041580200195\n",
      "h: 26 | epoch: 23, train loss: 27.440460205078125, test loss: 24.068050384521484\n",
      "h: 26 | epoch: 24, train loss: 26.835397720336914, test loss: 23.501441955566406\n",
      "h: 26 | epoch: 25, train loss: 26.246532440185547, test loss: 22.94982147216797\n",
      "h: 26 | epoch: 26, train loss: 25.6734561920166, test loss: 22.412799835205078\n",
      "h: 26 | epoch: 27, train loss: 25.11577796936035, test loss: 21.8900203704834\n",
      "h: 26 | epoch: 28, train loss: 24.573116302490234, test loss: 21.38113021850586\n",
      "h: 26 | epoch: 29, train loss: 24.045124053955078, test loss: 20.88579750061035\n",
      "h: 26 | epoch: 30, train loss: 23.531461715698242, test loss: 20.403711318969727\n",
      "h: 26 | epoch: 31, train loss: 23.031795501708984, test loss: 19.9345645904541\n",
      "h: 26 | epoch: 32, train loss: 22.545818328857422, test loss: 19.47806167602539\n",
      "h: 26 | epoch: 33, train loss: 22.07322120666504, test loss: 19.033924102783203\n",
      "h: 26 | epoch: 34, train loss: 21.61371612548828, test loss: 18.601871490478516\n",
      "h: 26 | epoch: 35, train loss: 21.167015075683594, test loss: 18.181642532348633\n",
      "h: 26 | epoch: 36, train loss: 20.73284912109375, test loss: 17.772979736328125\n",
      "h: 26 | epoch: 37, train loss: 20.31094741821289, test loss: 17.37563705444336\n",
      "h: 26 | epoch: 38, train loss: 19.901046752929688, test loss: 16.989362716674805\n",
      "h: 26 | epoch: 39, train loss: 19.502901077270508, test loss: 16.61392593383789\n",
      "h: 26 | epoch: 40, train loss: 19.116252899169922, test loss: 16.24909019470215\n",
      "h: 26 | epoch: 41, train loss: 18.740861892700195, test loss: 15.894635200500488\n",
      "h: 26 | epoch: 42, train loss: 18.376495361328125, test loss: 15.550329208374023\n",
      "h: 26 | epoch: 43, train loss: 18.022911071777344, test loss: 15.215960502624512\n",
      "h: 26 | epoch: 44, train loss: 17.679882049560547, test loss: 14.891311645507812\n",
      "h: 26 | epoch: 45, train loss: 17.347179412841797, test loss: 14.576167106628418\n",
      "h: 26 | epoch: 46, train loss: 17.024578094482422, test loss: 14.270326614379883\n",
      "h: 26 | epoch: 47, train loss: 16.71185874938965, test loss: 13.973579406738281\n",
      "h: 26 | epoch: 48, train loss: 16.4088077545166, test loss: 13.6857271194458\n",
      "h: 26 | epoch: 49, train loss: 16.115203857421875, test loss: 13.406562805175781\n",
      "h: 26 | epoch: 50, train loss: 15.830833435058594, test loss: 13.135900497436523\n",
      "h: 26 | epoch: 51, train loss: 15.555490493774414, test loss: 12.87353801727295\n",
      "h: 26 | epoch: 52, train loss: 15.288961410522461, test loss: 12.619285583496094\n",
      "h: 26 | epoch: 53, train loss: 15.031045913696289, test loss: 12.372952461242676\n",
      "h: 26 | epoch: 54, train loss: 14.781535148620605, test loss: 12.134350776672363\n",
      "h: 26 | epoch: 55, train loss: 14.540227890014648, test loss: 11.90329647064209\n",
      "h: 26 | epoch: 56, train loss: 14.306928634643555, test loss: 11.679604530334473\n",
      "h: 26 | epoch: 57, train loss: 14.08143424987793, test loss: 11.463096618652344\n",
      "h: 26 | epoch: 58, train loss: 13.863551139831543, test loss: 11.253591537475586\n",
      "h: 26 | epoch: 59, train loss: 13.653085708618164, test loss: 11.05091381072998\n",
      "h: 26 | epoch: 60, train loss: 13.449847221374512, test loss: 10.854888916015625\n",
      "h: 26 | epoch: 61, train loss: 13.253644943237305, test loss: 10.665342330932617\n",
      "h: 26 | epoch: 62, train loss: 13.064295768737793, test loss: 10.482108116149902\n",
      "h: 26 | epoch: 63, train loss: 12.881611824035645, test loss: 10.305017471313477\n",
      "h: 26 | epoch: 64, train loss: 12.705414772033691, test loss: 10.133901596069336\n",
      "h: 26 | epoch: 65, train loss: 12.535520553588867, test loss: 9.96860122680664\n",
      "h: 26 | epoch: 66, train loss: 12.37175464630127, test loss: 9.808955192565918\n",
      "h: 26 | epoch: 67, train loss: 12.213942527770996, test loss: 9.654802322387695\n",
      "h: 26 | epoch: 68, train loss: 12.061911582946777, test loss: 9.50599479675293\n",
      "h: 26 | epoch: 69, train loss: 11.915493965148926, test loss: 9.362369537353516\n",
      "h: 26 | epoch: 70, train loss: 11.774519920349121, test loss: 9.223782539367676\n",
      "h: 26 | epoch: 71, train loss: 11.638829231262207, test loss: 9.090085983276367\n",
      "h: 26 | epoch: 72, train loss: 11.508259773254395, test loss: 8.961133003234863\n",
      "h: 26 | epoch: 73, train loss: 11.382650375366211, test loss: 8.836780548095703\n",
      "h: 26 | epoch: 74, train loss: 11.261852264404297, test loss: 8.716891288757324\n",
      "h: 26 | epoch: 75, train loss: 11.145708084106445, test loss: 8.601325988769531\n",
      "h: 26 | epoch: 76, train loss: 11.034070014953613, test loss: 8.489952087402344\n",
      "h: 26 | epoch: 77, train loss: 10.92679500579834, test loss: 8.382635116577148\n",
      "h: 26 | epoch: 78, train loss: 10.823736190795898, test loss: 8.279251098632812\n",
      "h: 26 | epoch: 79, train loss: 10.724756240844727, test loss: 8.179673194885254\n",
      "h: 26 | epoch: 80, train loss: 10.629717826843262, test loss: 8.083772659301758\n",
      "h: 26 | epoch: 81, train loss: 10.538485527038574, test loss: 7.991436958312988\n",
      "h: 26 | epoch: 82, train loss: 10.450931549072266, test loss: 7.9025444984436035\n",
      "h: 26 | epoch: 83, train loss: 10.366928100585938, test loss: 7.816982269287109\n",
      "h: 26 | epoch: 84, train loss: 10.286347389221191, test loss: 7.734638214111328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 26 | epoch: 85, train loss: 10.209071159362793, test loss: 7.655404567718506\n",
      "h: 26 | epoch: 86, train loss: 10.134984016418457, test loss: 7.579171180725098\n",
      "h: 26 | epoch: 87, train loss: 10.063966751098633, test loss: 7.5058393478393555\n",
      "h: 26 | epoch: 88, train loss: 9.995909690856934, test loss: 7.43530797958374\n",
      "h: 26 | epoch: 89, train loss: 9.930705070495605, test loss: 7.367476463317871\n",
      "h: 26 | epoch: 90, train loss: 9.868246078491211, test loss: 7.302252292633057\n",
      "h: 26 | epoch: 91, train loss: 9.808429718017578, test loss: 7.2395429611206055\n",
      "h: 26 | epoch: 92, train loss: 9.751157760620117, test loss: 7.179257392883301\n",
      "h: 26 | epoch: 93, train loss: 9.696331024169922, test loss: 7.121310234069824\n",
      "h: 26 | epoch: 94, train loss: 9.643857955932617, test loss: 7.065614223480225\n",
      "h: 26 | epoch: 95, train loss: 9.593647956848145, test loss: 7.012089729309082\n",
      "h: 26 | epoch: 96, train loss: 9.545612335205078, test loss: 6.96065616607666\n",
      "h: 26 | epoch: 97, train loss: 9.499665260314941, test loss: 6.911236763000488\n",
      "h: 26 | epoch: 98, train loss: 9.455726623535156, test loss: 6.863755702972412\n",
      "h: 26 | epoch: 99, train loss: 9.413713455200195, test loss: 6.818141937255859\n",
      "h: 27 | epoch: 0, train loss: 43.716583251953125, test loss: 39.02708435058594\n",
      "h: 27 | epoch: 1, train loss: 42.59648895263672, test loss: 38.00096130371094\n",
      "h: 27 | epoch: 2, train loss: 41.51088333129883, test loss: 37.00567626953125\n",
      "h: 27 | epoch: 3, train loss: 40.45823287963867, test loss: 36.03992462158203\n",
      "h: 27 | epoch: 4, train loss: 39.43712615966797, test loss: 35.10246658325195\n",
      "h: 27 | epoch: 5, train loss: 38.446266174316406, test loss: 34.19215774536133\n",
      "h: 27 | epoch: 6, train loss: 37.48445129394531, test loss: 33.30794906616211\n",
      "h: 27 | epoch: 7, train loss: 36.550567626953125, test loss: 32.448875427246094\n",
      "h: 27 | epoch: 8, train loss: 35.6435661315918, test loss: 31.614009857177734\n",
      "h: 27 | epoch: 9, train loss: 34.76250457763672, test loss: 30.802515029907227\n",
      "h: 27 | epoch: 10, train loss: 33.906463623046875, test loss: 30.013601303100586\n",
      "h: 27 | epoch: 11, train loss: 33.074623107910156, test loss: 29.246517181396484\n",
      "h: 27 | epoch: 12, train loss: 32.26620101928711, test loss: 28.500595092773438\n",
      "h: 27 | epoch: 13, train loss: 31.48046875, test loss: 27.77517318725586\n",
      "h: 27 | epoch: 14, train loss: 30.71674156188965, test loss: 27.069650650024414\n",
      "h: 27 | epoch: 15, train loss: 29.974374771118164, test loss: 26.38346290588379\n",
      "h: 27 | epoch: 16, train loss: 29.252771377563477, test loss: 25.716060638427734\n",
      "h: 27 | epoch: 17, train loss: 28.55135726928711, test loss: 25.06694984436035\n",
      "h: 27 | epoch: 18, train loss: 27.869598388671875, test loss: 24.435649871826172\n",
      "h: 27 | epoch: 19, train loss: 27.206981658935547, test loss: 23.82168960571289\n",
      "h: 27 | epoch: 20, train loss: 26.563030242919922, test loss: 23.224655151367188\n",
      "h: 27 | epoch: 21, train loss: 25.937280654907227, test loss: 22.644123077392578\n",
      "h: 27 | epoch: 22, train loss: 25.329294204711914, test loss: 22.079700469970703\n",
      "h: 27 | epoch: 23, train loss: 24.738649368286133, test loss: 21.5310115814209\n",
      "h: 27 | epoch: 24, train loss: 24.16494369506836, test loss: 20.997697830200195\n",
      "h: 27 | epoch: 25, train loss: 23.60779571533203, test loss: 20.479400634765625\n",
      "h: 27 | epoch: 26, train loss: 23.066823959350586, test loss: 19.97579002380371\n",
      "h: 27 | epoch: 27, train loss: 22.541671752929688, test loss: 19.48653793334961\n",
      "h: 27 | epoch: 28, train loss: 22.03199005126953, test loss: 19.01132583618164\n",
      "h: 27 | epoch: 29, train loss: 21.537433624267578, test loss: 18.549846649169922\n",
      "h: 27 | epoch: 30, train loss: 21.057674407958984, test loss: 18.10179901123047\n",
      "h: 27 | epoch: 31, train loss: 20.59239387512207, test loss: 17.66689682006836\n",
      "h: 27 | epoch: 32, train loss: 20.141267776489258, test loss: 17.24484634399414\n",
      "h: 27 | epoch: 33, train loss: 19.703990936279297, test loss: 16.835376739501953\n",
      "h: 27 | epoch: 34, train loss: 19.280258178710938, test loss: 16.438201904296875\n",
      "h: 27 | epoch: 35, train loss: 18.869773864746094, test loss: 16.05306053161621\n",
      "h: 27 | epoch: 36, train loss: 18.472240447998047, test loss: 15.679682731628418\n",
      "h: 27 | epoch: 37, train loss: 18.08737564086914, test loss: 15.317804336547852\n",
      "h: 27 | epoch: 38, train loss: 17.71487808227539, test loss: 14.96717357635498\n",
      "h: 27 | epoch: 39, train loss: 17.3544864654541, test loss: 14.627530097961426\n",
      "h: 27 | epoch: 40, train loss: 17.005903244018555, test loss: 14.298620223999023\n",
      "h: 27 | epoch: 41, train loss: 16.66886329650879, test loss: 13.980198860168457\n",
      "h: 27 | epoch: 42, train loss: 16.343090057373047, test loss: 13.672019958496094\n",
      "h: 27 | epoch: 43, train loss: 16.0283145904541, test loss: 13.373838424682617\n",
      "h: 27 | epoch: 44, train loss: 15.724268913269043, test loss: 13.085416793823242\n",
      "h: 27 | epoch: 45, train loss: 15.430689811706543, test loss: 12.80650806427002\n",
      "h: 27 | epoch: 46, train loss: 15.147311210632324, test loss: 12.536885261535645\n",
      "h: 27 | epoch: 47, train loss: 14.873880386352539, test loss: 12.276312828063965\n",
      "h: 27 | epoch: 48, train loss: 14.610132217407227, test loss: 12.024559020996094\n",
      "h: 27 | epoch: 49, train loss: 14.355819702148438, test loss: 11.78139591217041\n",
      "h: 27 | epoch: 50, train loss: 14.110687255859375, test loss: 11.546598434448242\n",
      "h: 27 | epoch: 51, train loss: 13.874486923217773, test loss: 11.319944381713867\n",
      "h: 27 | epoch: 52, train loss: 13.646974563598633, test loss: 11.101210594177246\n",
      "h: 27 | epoch: 53, train loss: 13.42790412902832, test loss: 10.890180587768555\n",
      "h: 27 | epoch: 54, train loss: 13.217036247253418, test loss: 10.686643600463867\n",
      "h: 27 | epoch: 55, train loss: 13.014139175415039, test loss: 10.490385055541992\n",
      "h: 27 | epoch: 56, train loss: 12.81897258758545, test loss: 10.30119514465332\n",
      "h: 27 | epoch: 57, train loss: 12.631311416625977, test loss: 10.118871688842773\n",
      "h: 27 | epoch: 58, train loss: 12.450925827026367, test loss: 9.943209648132324\n",
      "h: 27 | epoch: 59, train loss: 12.277593612670898, test loss: 9.77401065826416\n",
      "h: 27 | epoch: 60, train loss: 12.11109447479248, test loss: 9.611079216003418\n",
      "h: 27 | epoch: 61, train loss: 11.951212882995605, test loss: 9.454224586486816\n",
      "h: 27 | epoch: 62, train loss: 11.797739028930664, test loss: 9.303251266479492\n",
      "h: 27 | epoch: 63, train loss: 11.650460243225098, test loss: 9.157980918884277\n",
      "h: 27 | epoch: 64, train loss: 11.509175300598145, test loss: 9.018229484558105\n",
      "h: 27 | epoch: 65, train loss: 11.373682975769043, test loss: 8.883817672729492\n",
      "h: 27 | epoch: 66, train loss: 11.24378776550293, test loss: 8.754571914672852\n",
      "h: 27 | epoch: 67, train loss: 11.119296073913574, test loss: 8.63032054901123\n",
      "h: 27 | epoch: 68, train loss: 11.000021934509277, test loss: 8.510896682739258\n",
      "h: 27 | epoch: 69, train loss: 10.885782241821289, test loss: 8.396139144897461\n",
      "h: 27 | epoch: 70, train loss: 10.776395797729492, test loss: 8.285884857177734\n",
      "h: 27 | epoch: 71, train loss: 10.6716890335083, test loss: 8.179980278015137\n",
      "h: 27 | epoch: 72, train loss: 10.571491241455078, test loss: 8.078271865844727\n",
      "h: 27 | epoch: 73, train loss: 10.47563362121582, test loss: 7.980613708496094\n",
      "h: 27 | epoch: 74, train loss: 10.38395881652832, test loss: 7.886859893798828\n",
      "h: 27 | epoch: 75, train loss: 10.296308517456055, test loss: 7.796872615814209\n",
      "h: 27 | epoch: 76, train loss: 10.212526321411133, test loss: 7.71051025390625\n",
      "h: 27 | epoch: 77, train loss: 10.132464408874512, test loss: 7.6276445388793945\n",
      "h: 27 | epoch: 78, train loss: 10.05597972869873, test loss: 7.5481462478637695\n",
      "h: 27 | epoch: 79, train loss: 9.982931137084961, test loss: 7.47188663482666\n",
      "h: 27 | epoch: 80, train loss: 9.91318130493164, test loss: 7.398745536804199\n",
      "h: 27 | epoch: 81, train loss: 9.846597671508789, test loss: 7.32860803604126\n",
      "h: 27 | epoch: 82, train loss: 9.783056259155273, test loss: 7.261354923248291\n",
      "h: 27 | epoch: 83, train loss: 9.722430229187012, test loss: 7.1968793869018555\n",
      "h: 27 | epoch: 84, train loss: 9.664600372314453, test loss: 7.13507080078125\n",
      "h: 27 | epoch: 85, train loss: 9.60944938659668, test loss: 7.075828552246094\n",
      "h: 27 | epoch: 86, train loss: 9.556869506835938, test loss: 7.01904821395874\n",
      "h: 27 | epoch: 87, train loss: 9.506747245788574, test loss: 6.964636325836182\n",
      "h: 27 | epoch: 88, train loss: 9.458982467651367, test loss: 6.912497520446777\n",
      "h: 27 | epoch: 89, train loss: 9.413473129272461, test loss: 6.862540245056152\n",
      "h: 27 | epoch: 90, train loss: 9.37012004852295, test loss: 6.8146772384643555\n",
      "h: 27 | epoch: 91, train loss: 9.32883358001709, test loss: 6.768825531005859\n",
      "h: 27 | epoch: 92, train loss: 9.289520263671875, test loss: 6.724902153015137\n",
      "h: 27 | epoch: 93, train loss: 9.252096176147461, test loss: 6.682828426361084\n",
      "h: 27 | epoch: 94, train loss: 9.216473579406738, test loss: 6.642528533935547\n",
      "h: 27 | epoch: 95, train loss: 9.182576179504395, test loss: 6.603930473327637\n",
      "h: 27 | epoch: 96, train loss: 9.150323867797852, test loss: 6.566963195800781\n",
      "h: 27 | epoch: 97, train loss: 9.119646072387695, test loss: 6.53156042098999\n",
      "h: 27 | epoch: 98, train loss: 9.090465545654297, test loss: 6.497655391693115\n",
      "h: 27 | epoch: 99, train loss: 9.062719345092773, test loss: 6.465184211730957\n",
      "h: 28 | epoch: 0, train loss: 46.453582763671875, test loss: 41.92892074584961\n",
      "h: 28 | epoch: 1, train loss: 44.77857208251953, test loss: 40.37336349487305\n",
      "h: 28 | epoch: 2, train loss: 43.18122100830078, test loss: 38.88887405395508\n",
      "h: 28 | epoch: 3, train loss: 41.6568603515625, test loss: 37.47126007080078\n",
      "h: 28 | epoch: 4, train loss: 40.20122528076172, test loss: 36.11669158935547\n",
      "h: 28 | epoch: 5, train loss: 38.8104133605957, test loss: 34.82164001464844\n",
      "h: 28 | epoch: 6, train loss: 37.4808464050293, test loss: 33.582855224609375\n",
      "h: 28 | epoch: 7, train loss: 36.2092399597168, test loss: 32.397377014160156\n",
      "h: 28 | epoch: 8, train loss: 34.99256896972656, test loss: 31.262447357177734\n",
      "h: 28 | epoch: 9, train loss: 33.828025817871094, test loss: 30.175527572631836\n",
      "h: 28 | epoch: 10, train loss: 32.71303176879883, test loss: 29.13425636291504\n",
      "h: 28 | epoch: 11, train loss: 31.645191192626953, test loss: 28.136444091796875\n",
      "h: 28 | epoch: 12, train loss: 30.622272491455078, test loss: 27.1800479888916\n",
      "h: 28 | epoch: 13, train loss: 29.642196655273438, test loss: 26.263164520263672\n",
      "h: 28 | epoch: 14, train loss: 28.70302963256836, test loss: 25.384023666381836\n",
      "h: 28 | epoch: 15, train loss: 27.802963256835938, test loss: 24.540950775146484\n",
      "h: 28 | epoch: 16, train loss: 26.940288543701172, test loss: 23.732393264770508\n",
      "h: 28 | epoch: 17, train loss: 26.113418579101562, test loss: 22.95687484741211\n",
      "h: 28 | epoch: 18, train loss: 25.32084846496582, test loss: 22.21300506591797\n",
      "h: 28 | epoch: 19, train loss: 24.561153411865234, test loss: 21.49948501586914\n",
      "h: 28 | epoch: 20, train loss: 23.832996368408203, test loss: 20.815067291259766\n",
      "h: 28 | epoch: 21, train loss: 23.135112762451172, test loss: 20.158580780029297\n",
      "h: 28 | epoch: 22, train loss: 22.466285705566406, test loss: 19.528919219970703\n",
      "h: 28 | epoch: 23, train loss: 21.825376510620117, test loss: 18.92501449584961\n",
      "h: 28 | epoch: 24, train loss: 21.21129608154297, test loss: 18.34585189819336\n",
      "h: 28 | epoch: 25, train loss: 20.622997283935547, test loss: 17.790483474731445\n",
      "h: 28 | epoch: 26, train loss: 20.059486389160156, test loss: 17.257978439331055\n",
      "h: 28 | epoch: 27, train loss: 19.51980972290039, test loss: 16.747453689575195\n",
      "h: 28 | epoch: 28, train loss: 19.003055572509766, test loss: 16.258073806762695\n",
      "h: 28 | epoch: 29, train loss: 18.508350372314453, test loss: 15.789024353027344\n",
      "h: 28 | epoch: 30, train loss: 18.034847259521484, test loss: 15.33952808380127\n",
      "h: 28 | epoch: 31, train loss: 17.581745147705078, test loss: 14.908843994140625\n",
      "h: 28 | epoch: 32, train loss: 17.14826011657715, test loss: 14.496249198913574\n",
      "h: 28 | epoch: 33, train loss: 16.733638763427734, test loss: 14.101053237915039\n",
      "h: 28 | epoch: 34, train loss: 16.337167739868164, test loss: 13.722589492797852\n",
      "h: 28 | epoch: 35, train loss: 15.958139419555664, test loss: 13.36021900177002\n",
      "h: 28 | epoch: 36, train loss: 15.595888137817383, test loss: 13.013318061828613\n",
      "h: 28 | epoch: 37, train loss: 15.24975872039795, test loss: 12.681295394897461\n",
      "h: 28 | epoch: 38, train loss: 14.919126510620117, test loss: 12.363571166992188\n",
      "h: 28 | epoch: 39, train loss: 14.603381156921387, test loss: 12.059587478637695\n",
      "h: 28 | epoch: 40, train loss: 14.30194091796875, test loss: 11.768810272216797\n",
      "h: 28 | epoch: 41, train loss: 14.01423454284668, test loss: 11.49071979522705\n",
      "h: 28 | epoch: 42, train loss: 13.739717483520508, test loss: 11.224814414978027\n",
      "h: 28 | epoch: 43, train loss: 13.477856636047363, test loss: 10.970611572265625\n",
      "h: 28 | epoch: 44, train loss: 13.228139877319336, test loss: 10.727641105651855\n",
      "h: 28 | epoch: 45, train loss: 12.990076065063477, test loss: 10.495452880859375\n",
      "h: 28 | epoch: 46, train loss: 12.763181686401367, test loss: 10.273611068725586\n",
      "h: 28 | epoch: 47, train loss: 12.546998977661133, test loss: 10.061694145202637\n",
      "h: 28 | epoch: 48, train loss: 12.341078758239746, test loss: 9.859296798706055\n",
      "h: 28 | epoch: 49, train loss: 12.144990921020508, test loss: 9.666025161743164\n",
      "h: 28 | epoch: 50, train loss: 11.9583158493042, test loss: 9.481502532958984\n",
      "h: 28 | epoch: 51, train loss: 11.780655860900879, test loss: 9.3053617477417\n",
      "h: 28 | epoch: 52, train loss: 11.611620903015137, test loss: 9.137250900268555\n",
      "h: 28 | epoch: 53, train loss: 11.450837135314941, test loss: 8.976831436157227\n",
      "h: 28 | epoch: 54, train loss: 11.297942161560059, test loss: 8.823779106140137\n",
      "h: 28 | epoch: 55, train loss: 11.152592658996582, test loss: 8.677775382995605\n",
      "h: 28 | epoch: 56, train loss: 11.014450073242188, test loss: 8.538514137268066\n",
      "h: 28 | epoch: 57, train loss: 10.883191108703613, test loss: 8.405708312988281\n",
      "h: 28 | epoch: 58, train loss: 10.758508682250977, test loss: 8.279069900512695\n",
      "h: 28 | epoch: 59, train loss: 10.640103340148926, test loss: 8.158337593078613\n",
      "h: 28 | epoch: 60, train loss: 10.527685165405273, test loss: 8.043241500854492\n",
      "h: 28 | epoch: 61, train loss: 10.420980453491211, test loss: 7.93353796005249\n",
      "h: 28 | epoch: 62, train loss: 10.319723129272461, test loss: 7.828985691070557\n",
      "h: 28 | epoch: 63, train loss: 10.22365951538086, test loss: 7.7293500900268555\n",
      "h: 28 | epoch: 64, train loss: 10.13254451751709, test loss: 7.634413719177246\n",
      "h: 28 | epoch: 65, train loss: 10.046144485473633, test loss: 7.543962001800537\n",
      "h: 28 | epoch: 66, train loss: 9.964231491088867, test loss: 7.457791328430176\n",
      "h: 28 | epoch: 67, train loss: 9.886594772338867, test loss: 7.375704765319824\n",
      "h: 28 | epoch: 68, train loss: 9.81302261352539, test loss: 7.297515869140625\n",
      "h: 28 | epoch: 69, train loss: 9.743321418762207, test loss: 7.223045349121094\n",
      "h: 28 | epoch: 70, train loss: 9.677299499511719, test loss: 7.152120113372803\n",
      "h: 28 | epoch: 71, train loss: 9.614776611328125, test loss: 7.084573268890381\n",
      "h: 28 | epoch: 72, train loss: 9.555580139160156, test loss: 7.020251274108887\n",
      "h: 28 | epoch: 73, train loss: 9.499543190002441, test loss: 6.958998680114746\n",
      "h: 28 | epoch: 74, train loss: 9.446507453918457, test loss: 6.900675296783447\n",
      "h: 28 | epoch: 75, train loss: 9.396322250366211, test loss: 6.8451385498046875\n",
      "h: 28 | epoch: 76, train loss: 9.348844528198242, test loss: 6.792259216308594\n",
      "h: 28 | epoch: 77, train loss: 9.303936004638672, test loss: 6.741910457611084\n",
      "h: 28 | epoch: 78, train loss: 9.26146411895752, test loss: 6.693970680236816\n",
      "h: 28 | epoch: 79, train loss: 9.221303939819336, test loss: 6.648325443267822\n",
      "h: 28 | epoch: 80, train loss: 9.183337211608887, test loss: 6.604864597320557\n",
      "h: 28 | epoch: 81, train loss: 9.147451400756836, test loss: 6.563485145568848\n",
      "h: 28 | epoch: 82, train loss: 9.113533973693848, test loss: 6.524083614349365\n",
      "h: 28 | epoch: 83, train loss: 9.081483840942383, test loss: 6.486566066741943\n",
      "h: 28 | epoch: 84, train loss: 9.051206588745117, test loss: 6.450842380523682\n",
      "h: 28 | epoch: 85, train loss: 9.022603034973145, test loss: 6.4168243408203125\n",
      "h: 28 | epoch: 86, train loss: 8.995588302612305, test loss: 6.384428977966309\n",
      "h: 28 | epoch: 87, train loss: 8.970075607299805, test loss: 6.353579998016357\n",
      "h: 28 | epoch: 88, train loss: 8.945985794067383, test loss: 6.324200630187988\n",
      "h: 28 | epoch: 89, train loss: 8.923242568969727, test loss: 6.296217441558838\n",
      "h: 28 | epoch: 90, train loss: 8.901775360107422, test loss: 6.269564628601074\n",
      "h: 28 | epoch: 91, train loss: 8.881511688232422, test loss: 6.244176387786865\n",
      "h: 28 | epoch: 92, train loss: 8.862388610839844, test loss: 6.219990253448486\n",
      "h: 28 | epoch: 93, train loss: 8.844343185424805, test loss: 6.196950912475586\n",
      "h: 28 | epoch: 94, train loss: 8.82731819152832, test loss: 6.174997806549072\n",
      "h: 28 | epoch: 95, train loss: 8.811256408691406, test loss: 6.154080390930176\n",
      "h: 28 | epoch: 96, train loss: 8.79610538482666, test loss: 6.134146690368652\n",
      "h: 28 | epoch: 97, train loss: 8.781815528869629, test loss: 6.115149021148682\n",
      "h: 28 | epoch: 98, train loss: 8.768338203430176, test loss: 6.097043037414551\n",
      "h: 28 | epoch: 99, train loss: 8.75562858581543, test loss: 6.079782009124756\n",
      "h: 29 | epoch: 0, train loss: 48.49951934814453, test loss: 42.38576889038086\n",
      "h: 29 | epoch: 1, train loss: 46.72850799560547, test loss: 40.806785583496094\n",
      "h: 29 | epoch: 2, train loss: 45.0396842956543, test loss: 39.299312591552734\n",
      "h: 29 | epoch: 3, train loss: 43.42814254760742, test loss: 37.85921859741211\n",
      "h: 29 | epoch: 4, train loss: 41.88941192626953, test loss: 36.48271179199219\n",
      "h: 29 | epoch: 5, train loss: 40.41938400268555, test loss: 35.16631317138672\n",
      "h: 29 | epoch: 6, train loss: 39.0142936706543, test loss: 33.90681076049805\n",
      "h: 29 | epoch: 7, train loss: 37.670692443847656, test loss: 32.70125961303711\n",
      "h: 29 | epoch: 8, train loss: 36.3853759765625, test loss: 31.54693603515625\n",
      "h: 29 | epoch: 9, train loss: 35.15540313720703, test loss: 30.441299438476562\n",
      "h: 29 | epoch: 10, train loss: 33.978050231933594, test loss: 29.38201332092285\n",
      "h: 29 | epoch: 11, train loss: 32.85078430175781, test loss: 28.366901397705078\n",
      "h: 29 | epoch: 12, train loss: 31.7712459564209, test loss: 27.3939208984375\n",
      "h: 29 | epoch: 13, train loss: 30.73723793029785, test loss: 26.4611759185791\n",
      "h: 29 | epoch: 14, train loss: 29.746707916259766, test loss: 25.566883087158203\n",
      "h: 29 | epoch: 15, train loss: 28.797739028930664, test loss: 24.709379196166992\n",
      "h: 29 | epoch: 16, train loss: 27.888519287109375, test loss: 23.8870849609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 29 | epoch: 17, train loss: 27.017358779907227, test loss: 23.098526000976562\n",
      "h: 29 | epoch: 18, train loss: 26.18265151977539, test loss: 22.342302322387695\n",
      "h: 29 | epoch: 19, train loss: 25.382892608642578, test loss: 21.617090225219727\n",
      "h: 29 | epoch: 20, train loss: 24.61664581298828, test loss: 20.92163848876953\n",
      "h: 29 | epoch: 21, train loss: 23.882556915283203, test loss: 20.2547550201416\n",
      "h: 29 | epoch: 22, train loss: 23.179336547851562, test loss: 19.61530113220215\n",
      "h: 29 | epoch: 23, train loss: 22.50575828552246, test loss: 19.002208709716797\n",
      "h: 29 | epoch: 24, train loss: 21.86065101623535, test loss: 18.41443634033203\n",
      "h: 29 | epoch: 25, train loss: 21.242897033691406, test loss: 17.851009368896484\n",
      "h: 29 | epoch: 26, train loss: 20.65143394470215, test loss: 17.310977935791016\n",
      "h: 29 | epoch: 27, train loss: 20.085237503051758, test loss: 16.79344367980957\n",
      "h: 29 | epoch: 28, train loss: 19.543315887451172, test loss: 16.29754066467285\n",
      "h: 29 | epoch: 29, train loss: 19.024744033813477, test loss: 15.822433471679688\n",
      "h: 29 | epoch: 30, train loss: 18.528610229492188, test loss: 15.367327690124512\n",
      "h: 29 | epoch: 31, train loss: 18.054044723510742, test loss: 14.931451797485352\n",
      "h: 29 | epoch: 32, train loss: 17.600208282470703, test loss: 14.514065742492676\n",
      "h: 29 | epoch: 33, train loss: 17.166296005249023, test loss: 14.11445426940918\n",
      "h: 29 | epoch: 34, train loss: 16.75153350830078, test loss: 13.73193073272705\n",
      "h: 29 | epoch: 35, train loss: 16.355167388916016, test loss: 13.365835189819336\n",
      "h: 29 | epoch: 36, train loss: 15.976476669311523, test loss: 13.015528678894043\n",
      "h: 29 | epoch: 37, train loss: 15.614758491516113, test loss: 12.680387496948242\n",
      "h: 29 | epoch: 38, train loss: 15.2693452835083, test loss: 12.359823226928711\n",
      "h: 29 | epoch: 39, train loss: 14.939579963684082, test loss: 12.053256034851074\n",
      "h: 29 | epoch: 40, train loss: 14.624836921691895, test loss: 11.760132789611816\n",
      "h: 29 | epoch: 41, train loss: 14.32451057434082, test loss: 11.4799165725708\n",
      "h: 29 | epoch: 42, train loss: 14.038007736206055, test loss: 11.212093353271484\n",
      "h: 29 | epoch: 43, train loss: 13.764772415161133, test loss: 10.956159591674805\n",
      "h: 29 | epoch: 44, train loss: 13.504247665405273, test loss: 10.71163272857666\n",
      "h: 29 | epoch: 45, train loss: 13.25590991973877, test loss: 10.478047370910645\n",
      "h: 29 | epoch: 46, train loss: 13.019250869750977, test loss: 10.254953384399414\n",
      "h: 29 | epoch: 47, train loss: 12.79377555847168, test loss: 10.041923522949219\n",
      "h: 29 | epoch: 48, train loss: 12.579010009765625, test loss: 9.838531494140625\n",
      "h: 29 | epoch: 49, train loss: 12.374494552612305, test loss: 9.644375801086426\n",
      "h: 29 | epoch: 50, train loss: 12.179792404174805, test loss: 9.459070205688477\n",
      "h: 29 | epoch: 51, train loss: 11.994473457336426, test loss: 9.282236099243164\n",
      "h: 29 | epoch: 52, train loss: 11.818126678466797, test loss: 9.113519668579102\n",
      "h: 29 | epoch: 53, train loss: 11.650362014770508, test loss: 8.95256233215332\n",
      "h: 29 | epoch: 54, train loss: 11.490793228149414, test loss: 8.799036026000977\n",
      "h: 29 | epoch: 55, train loss: 11.339055061340332, test loss: 8.652620315551758\n",
      "h: 29 | epoch: 56, train loss: 11.194799423217773, test loss: 8.512998580932617\n",
      "h: 29 | epoch: 57, train loss: 11.057683944702148, test loss: 8.379878044128418\n",
      "h: 29 | epoch: 58, train loss: 10.927383422851562, test loss: 8.252967834472656\n",
      "h: 29 | epoch: 59, train loss: 10.803584098815918, test loss: 8.131997108459473\n",
      "h: 29 | epoch: 60, train loss: 10.68598747253418, test loss: 8.016698837280273\n",
      "h: 29 | epoch: 61, train loss: 10.574304580688477, test loss: 7.906815528869629\n",
      "h: 29 | epoch: 62, train loss: 10.468255996704102, test loss: 7.802107334136963\n",
      "h: 29 | epoch: 63, train loss: 10.367581367492676, test loss: 7.702339172363281\n",
      "h: 29 | epoch: 64, train loss: 10.27202033996582, test loss: 7.607287406921387\n",
      "h: 29 | epoch: 65, train loss: 10.181333541870117, test loss: 7.5167365074157715\n",
      "h: 29 | epoch: 66, train loss: 10.095285415649414, test loss: 7.430477142333984\n",
      "h: 29 | epoch: 67, train loss: 10.013651847839355, test loss: 7.3483147621154785\n",
      "h: 29 | epoch: 68, train loss: 9.936219215393066, test loss: 7.270060062408447\n",
      "h: 29 | epoch: 69, train loss: 9.862783432006836, test loss: 7.195530891418457\n",
      "h: 29 | epoch: 70, train loss: 9.793149948120117, test loss: 7.124554634094238\n",
      "h: 29 | epoch: 71, train loss: 9.727129936218262, test loss: 7.056962490081787\n",
      "h: 29 | epoch: 72, train loss: 9.664543151855469, test loss: 6.992600917816162\n",
      "h: 29 | epoch: 73, train loss: 9.605222702026367, test loss: 6.931312561035156\n",
      "h: 29 | epoch: 74, train loss: 9.549001693725586, test loss: 6.872956275939941\n",
      "h: 29 | epoch: 75, train loss: 9.495726585388184, test loss: 6.817391872406006\n",
      "h: 29 | epoch: 76, train loss: 9.44525146484375, test loss: 6.764487266540527\n",
      "h: 29 | epoch: 77, train loss: 9.397430419921875, test loss: 6.714117527008057\n",
      "h: 29 | epoch: 78, train loss: 9.352131843566895, test loss: 6.6661577224731445\n",
      "h: 29 | epoch: 79, train loss: 9.309224128723145, test loss: 6.620497703552246\n",
      "h: 29 | epoch: 80, train loss: 9.268588066101074, test loss: 6.577025413513184\n",
      "h: 29 | epoch: 81, train loss: 9.230107307434082, test loss: 6.535634517669678\n",
      "h: 29 | epoch: 82, train loss: 9.1936674118042, test loss: 6.496228218078613\n",
      "h: 29 | epoch: 83, train loss: 9.159167289733887, test loss: 6.458707332611084\n",
      "h: 29 | epoch: 84, train loss: 9.12650203704834, test loss: 6.4229841232299805\n",
      "h: 29 | epoch: 85, train loss: 9.095580101013184, test loss: 6.388970851898193\n",
      "h: 29 | epoch: 86, train loss: 9.066308975219727, test loss: 6.35658597946167\n",
      "h: 29 | epoch: 87, train loss: 9.03860092163086, test loss: 6.325748920440674\n",
      "h: 29 | epoch: 88, train loss: 9.01237678527832, test loss: 6.296386241912842\n",
      "h: 29 | epoch: 89, train loss: 8.987557411193848, test loss: 6.268425464630127\n",
      "h: 29 | epoch: 90, train loss: 8.964068412780762, test loss: 6.241799831390381\n",
      "h: 29 | epoch: 91, train loss: 8.941839218139648, test loss: 6.216443061828613\n",
      "h: 29 | epoch: 92, train loss: 8.920804023742676, test loss: 6.192296028137207\n",
      "h: 29 | epoch: 93, train loss: 8.900898933410645, test loss: 6.1692986488342285\n",
      "h: 29 | epoch: 94, train loss: 8.88206672668457, test loss: 6.147395133972168\n",
      "h: 29 | epoch: 95, train loss: 8.86424446105957, test loss: 6.126532077789307\n",
      "h: 29 | epoch: 96, train loss: 8.847383499145508, test loss: 6.10666036605835\n",
      "h: 29 | epoch: 97, train loss: 8.831430435180664, test loss: 6.087729454040527\n",
      "h: 29 | epoch: 98, train loss: 8.816336631774902, test loss: 6.069697380065918\n",
      "h: 29 | epoch: 99, train loss: 8.802057266235352, test loss: 6.052516937255859\n",
      "h: 30 | epoch: 0, train loss: 39.34679412841797, test loss: 35.33754348754883\n",
      "h: 30 | epoch: 1, train loss: 38.242431640625, test loss: 34.31008529663086\n",
      "h: 30 | epoch: 2, train loss: 37.17558288574219, test loss: 33.31684494018555\n",
      "h: 30 | epoch: 3, train loss: 36.1446533203125, test loss: 32.35637664794922\n",
      "h: 30 | epoch: 4, train loss: 35.148155212402344, test loss: 31.4273738861084\n",
      "h: 30 | epoch: 5, train loss: 34.1847038269043, test loss: 30.528589248657227\n",
      "h: 30 | epoch: 6, train loss: 33.253021240234375, test loss: 29.658884048461914\n",
      "h: 30 | epoch: 7, train loss: 32.351905822753906, test loss: 28.817180633544922\n",
      "h: 30 | epoch: 8, train loss: 31.480234146118164, test loss: 28.002466201782227\n",
      "h: 30 | epoch: 9, train loss: 30.6369571685791, test loss: 27.213796615600586\n",
      "h: 30 | epoch: 10, train loss: 29.821086883544922, test loss: 26.450286865234375\n",
      "h: 30 | epoch: 11, train loss: 29.03170394897461, test loss: 25.71109962463379\n",
      "h: 30 | epoch: 12, train loss: 28.267934799194336, test loss: 24.995433807373047\n",
      "h: 30 | epoch: 13, train loss: 27.528945922851562, test loss: 24.302549362182617\n",
      "h: 30 | epoch: 14, train loss: 26.81396484375, test loss: 23.631742477416992\n",
      "h: 30 | epoch: 15, train loss: 26.122249603271484, test loss: 22.982330322265625\n",
      "h: 30 | epoch: 16, train loss: 25.453094482421875, test loss: 22.353673934936523\n",
      "h: 30 | epoch: 17, train loss: 24.805830001831055, test loss: 21.745161056518555\n",
      "h: 30 | epoch: 18, train loss: 24.1798095703125, test loss: 21.156208038330078\n",
      "h: 30 | epoch: 19, train loss: 23.57442855834961, test loss: 20.586246490478516\n",
      "h: 30 | epoch: 20, train loss: 22.98908805847168, test loss: 20.03474235534668\n",
      "h: 30 | epoch: 21, train loss: 22.42323112487793, test loss: 19.501176834106445\n",
      "h: 30 | epoch: 22, train loss: 21.87630271911621, test loss: 18.985044479370117\n",
      "h: 30 | epoch: 23, train loss: 21.3477840423584, test loss: 18.48586654663086\n",
      "h: 30 | epoch: 24, train loss: 20.837162017822266, test loss: 18.003170013427734\n",
      "h: 30 | epoch: 25, train loss: 20.34394073486328, test loss: 17.536500930786133\n",
      "h: 30 | epoch: 26, train loss: 19.86764144897461, test loss: 17.085426330566406\n",
      "h: 30 | epoch: 27, train loss: 19.40780258178711, test loss: 16.649507522583008\n",
      "h: 30 | epoch: 28, train loss: 18.963970184326172, test loss: 16.22833824157715\n",
      "h: 30 | epoch: 29, train loss: 18.535696029663086, test loss: 15.821504592895508\n",
      "h: 30 | epoch: 30, train loss: 18.122554779052734, test loss: 15.428613662719727\n",
      "h: 30 | epoch: 31, train loss: 17.72412109375, test loss: 15.049275398254395\n",
      "h: 30 | epoch: 32, train loss: 17.33998680114746, test loss: 14.683111190795898\n",
      "h: 30 | epoch: 33, train loss: 16.969745635986328, test loss: 14.329755783081055\n",
      "h: 30 | epoch: 34, train loss: 16.61300277709961, test loss: 13.98884391784668\n",
      "h: 30 | epoch: 35, train loss: 16.26937484741211, test loss: 13.660017013549805\n",
      "h: 30 | epoch: 36, train loss: 15.9384765625, test loss: 13.342933654785156\n",
      "h: 30 | epoch: 37, train loss: 15.619943618774414, test loss: 13.037249565124512\n",
      "h: 30 | epoch: 38, train loss: 15.31340503692627, test loss: 12.742631912231445\n",
      "h: 30 | epoch: 39, train loss: 15.018506050109863, test loss: 12.458757400512695\n",
      "h: 30 | epoch: 40, train loss: 14.734895706176758, test loss: 12.185297966003418\n",
      "h: 30 | epoch: 41, train loss: 14.46222972869873, test loss: 11.921945571899414\n",
      "h: 30 | epoch: 42, train loss: 14.200172424316406, test loss: 11.668390274047852\n",
      "h: 30 | epoch: 43, train loss: 13.948391914367676, test loss: 11.42432975769043\n",
      "h: 30 | epoch: 44, train loss: 13.706561088562012, test loss: 11.189468383789062\n",
      "h: 30 | epoch: 45, train loss: 13.474363327026367, test loss: 10.963518142700195\n",
      "h: 30 | epoch: 46, train loss: 13.251489639282227, test loss: 10.746194839477539\n",
      "h: 30 | epoch: 47, train loss: 13.037631034851074, test loss: 10.537219047546387\n",
      "h: 30 | epoch: 48, train loss: 12.832490921020508, test loss: 10.336321830749512\n",
      "h: 30 | epoch: 49, train loss: 12.635778427124023, test loss: 10.143237113952637\n",
      "h: 30 | epoch: 50, train loss: 12.44720458984375, test loss: 9.95770263671875\n",
      "h: 30 | epoch: 51, train loss: 12.266489028930664, test loss: 9.77946662902832\n",
      "h: 30 | epoch: 52, train loss: 12.093361854553223, test loss: 9.608283042907715\n",
      "h: 30 | epoch: 53, train loss: 11.927553176879883, test loss: 9.443906784057617\n",
      "h: 30 | epoch: 54, train loss: 11.768804550170898, test loss: 9.286104202270508\n",
      "h: 30 | epoch: 55, train loss: 11.61685848236084, test loss: 9.134644508361816\n",
      "h: 30 | epoch: 56, train loss: 11.47146987915039, test loss: 8.989300727844238\n",
      "h: 30 | epoch: 57, train loss: 11.3323974609375, test loss: 8.849859237670898\n",
      "h: 30 | epoch: 58, train loss: 11.199403762817383, test loss: 8.716104507446289\n",
      "h: 30 | epoch: 59, train loss: 11.072265625, test loss: 8.587827682495117\n",
      "h: 30 | epoch: 60, train loss: 10.950753211975098, test loss: 8.46483039855957\n",
      "h: 30 | epoch: 61, train loss: 10.834653854370117, test loss: 8.346917152404785\n",
      "h: 30 | epoch: 62, train loss: 10.72375774383545, test loss: 8.233896255493164\n",
      "h: 30 | epoch: 63, train loss: 10.61785888671875, test loss: 8.125585556030273\n",
      "h: 30 | epoch: 64, train loss: 10.516762733459473, test loss: 8.021800994873047\n",
      "h: 30 | epoch: 65, train loss: 10.420273780822754, test loss: 7.9223761558532715\n",
      "h: 30 | epoch: 66, train loss: 10.328207015991211, test loss: 7.827136993408203\n",
      "h: 30 | epoch: 67, train loss: 10.240385055541992, test loss: 7.7359209060668945\n",
      "h: 30 | epoch: 68, train loss: 10.156628608703613, test loss: 7.648573875427246\n",
      "h: 30 | epoch: 69, train loss: 10.076772689819336, test loss: 7.564940452575684\n",
      "h: 30 | epoch: 70, train loss: 10.000652313232422, test loss: 7.484871864318848\n",
      "h: 30 | epoch: 71, train loss: 9.92811107635498, test loss: 7.408227443695068\n",
      "h: 30 | epoch: 72, train loss: 9.858999252319336, test loss: 7.334869384765625\n",
      "h: 30 | epoch: 73, train loss: 9.79316520690918, test loss: 7.264664649963379\n",
      "h: 30 | epoch: 74, train loss: 9.730472564697266, test loss: 7.19748067855835\n",
      "h: 30 | epoch: 75, train loss: 9.67077922821045, test loss: 7.1331987380981445\n",
      "h: 30 | epoch: 76, train loss: 9.613956451416016, test loss: 7.0716962814331055\n",
      "h: 30 | epoch: 77, train loss: 9.559880256652832, test loss: 7.012856483459473\n",
      "h: 30 | epoch: 78, train loss: 9.508423805236816, test loss: 6.956572532653809\n",
      "h: 30 | epoch: 79, train loss: 9.459473609924316, test loss: 6.902734279632568\n",
      "h: 30 | epoch: 80, train loss: 9.412915229797363, test loss: 6.851238250732422\n",
      "h: 30 | epoch: 81, train loss: 9.36863899230957, test loss: 6.801987648010254\n",
      "h: 30 | epoch: 82, train loss: 9.326543807983398, test loss: 6.754885196685791\n",
      "h: 30 | epoch: 83, train loss: 9.286528587341309, test loss: 6.709840297698975\n",
      "h: 30 | epoch: 84, train loss: 9.248496055603027, test loss: 6.666764736175537\n",
      "h: 30 | epoch: 85, train loss: 9.21235466003418, test loss: 6.625572204589844\n",
      "h: 30 | epoch: 86, train loss: 9.178018569946289, test loss: 6.586184501647949\n",
      "h: 30 | epoch: 87, train loss: 9.14539909362793, test loss: 6.548521995544434\n",
      "h: 30 | epoch: 88, train loss: 9.114419937133789, test loss: 6.512507438659668\n",
      "h: 30 | epoch: 89, train loss: 9.085000991821289, test loss: 6.4780731201171875\n",
      "h: 30 | epoch: 90, train loss: 9.057065963745117, test loss: 6.445147514343262\n",
      "h: 30 | epoch: 91, train loss: 9.030546188354492, test loss: 6.4136643409729\n",
      "h: 30 | epoch: 92, train loss: 9.005373001098633, test loss: 6.383561134338379\n",
      "h: 30 | epoch: 93, train loss: 8.98148250579834, test loss: 6.354776382446289\n",
      "h: 30 | epoch: 94, train loss: 8.958810806274414, test loss: 6.327253818511963\n",
      "h: 30 | epoch: 95, train loss: 8.937299728393555, test loss: 6.300933837890625\n",
      "h: 30 | epoch: 96, train loss: 8.916891098022461, test loss: 6.275766849517822\n",
      "h: 30 | epoch: 97, train loss: 8.897533416748047, test loss: 6.251700401306152\n",
      "h: 30 | epoch: 98, train loss: 8.879172325134277, test loss: 6.22868537902832\n",
      "h: 30 | epoch: 99, train loss: 8.861760139465332, test loss: 6.206676006317139\n",
      "h: 31 | epoch: 0, train loss: 52.8721809387207, test loss: 46.12419891357422\n",
      "h: 31 | epoch: 1, train loss: 50.49139404296875, test loss: 44.05105972290039\n",
      "h: 31 | epoch: 2, train loss: 48.242897033691406, test loss: 42.08897399902344\n",
      "h: 31 | epoch: 3, train loss: 46.117652893066406, test loss: 40.230594635009766\n",
      "h: 31 | epoch: 4, train loss: 44.107479095458984, test loss: 38.46929931640625\n",
      "h: 31 | epoch: 5, train loss: 42.204952239990234, test loss: 36.799049377441406\n",
      "h: 31 | epoch: 6, train loss: 40.403343200683594, test loss: 35.21432876586914\n",
      "h: 31 | epoch: 7, train loss: 38.69648742675781, test loss: 33.710105895996094\n",
      "h: 31 | epoch: 8, train loss: 37.07876205444336, test loss: 32.281761169433594\n",
      "h: 31 | epoch: 9, train loss: 35.54499816894531, test loss: 30.925039291381836\n",
      "h: 31 | epoch: 10, train loss: 34.0904426574707, test loss: 29.63600730895996\n",
      "h: 31 | epoch: 11, train loss: 32.71070861816406, test loss: 28.411056518554688\n",
      "h: 31 | epoch: 12, train loss: 31.4017391204834, test loss: 27.24678611755371\n",
      "h: 31 | epoch: 13, train loss: 30.15976333618164, test loss: 26.14008140563965\n",
      "h: 31 | epoch: 14, train loss: 28.98126792907715, test loss: 25.087997436523438\n",
      "h: 31 | epoch: 15, train loss: 27.862991333007812, test loss: 24.087818145751953\n",
      "h: 31 | epoch: 16, train loss: 26.801864624023438, test loss: 23.136972427368164\n",
      "h: 31 | epoch: 17, train loss: 25.79502296447754, test loss: 22.23303985595703\n",
      "h: 31 | epoch: 18, train loss: 24.839763641357422, test loss: 21.373767852783203\n",
      "h: 31 | epoch: 19, train loss: 23.93355941772461, test loss: 20.557008743286133\n",
      "h: 31 | epoch: 20, train loss: 23.074005126953125, test loss: 19.780736923217773\n",
      "h: 31 | epoch: 21, train loss: 22.25884246826172, test loss: 19.043041229248047\n",
      "h: 31 | epoch: 22, train loss: 21.485925674438477, test loss: 18.342105865478516\n",
      "h: 31 | epoch: 23, train loss: 20.753219604492188, test loss: 17.676204681396484\n",
      "h: 31 | epoch: 24, train loss: 20.058799743652344, test loss: 17.04369354248047\n",
      "h: 31 | epoch: 25, train loss: 19.40082359313965, test loss: 16.443016052246094\n",
      "h: 31 | epoch: 26, train loss: 18.77754783630371, test loss: 15.872674942016602\n",
      "h: 31 | epoch: 27, train loss: 18.18730926513672, test loss: 15.331258773803711\n",
      "h: 31 | epoch: 28, train loss: 17.628515243530273, test loss: 14.817408561706543\n",
      "h: 31 | epoch: 29, train loss: 17.099651336669922, test loss: 14.329826354980469\n",
      "h: 31 | epoch: 30, train loss: 16.599271774291992, test loss: 13.867274284362793\n",
      "h: 31 | epoch: 31, train loss: 16.12598991394043, test loss: 13.428568840026855\n",
      "h: 31 | epoch: 32, train loss: 15.678487777709961, test loss: 13.01257610321045\n",
      "h: 31 | epoch: 33, train loss: 15.255497932434082, test loss: 12.61821460723877\n",
      "h: 31 | epoch: 34, train loss: 14.855812072753906, test loss: 12.2444429397583\n",
      "h: 31 | epoch: 35, train loss: 14.478277206420898, test loss: 11.890268325805664\n",
      "h: 31 | epoch: 36, train loss: 14.121782302856445, test loss: 11.554742813110352\n",
      "h: 31 | epoch: 37, train loss: 13.785273551940918, test loss: 11.236955642700195\n",
      "h: 31 | epoch: 38, train loss: 13.467740058898926, test loss: 10.93603515625\n",
      "h: 31 | epoch: 39, train loss: 13.16821002960205, test loss: 10.651148796081543\n",
      "h: 31 | epoch: 40, train loss: 12.885766983032227, test loss: 10.381502151489258\n",
      "h: 31 | epoch: 41, train loss: 12.619519233703613, test loss: 10.126331329345703\n",
      "h: 31 | epoch: 42, train loss: 12.368627548217773, test loss: 9.884909629821777\n",
      "h: 31 | epoch: 43, train loss: 12.132287979125977, test loss: 9.656538963317871\n",
      "h: 31 | epoch: 44, train loss: 11.909725189208984, test loss: 9.440553665161133\n",
      "h: 31 | epoch: 45, train loss: 11.700210571289062, test loss: 9.236322402954102\n",
      "h: 31 | epoch: 46, train loss: 11.503042221069336, test loss: 9.043237686157227\n",
      "h: 31 | epoch: 47, train loss: 11.317553520202637, test loss: 8.860718727111816\n",
      "h: 31 | epoch: 48, train loss: 11.143105506896973, test loss: 8.688215255737305\n",
      "h: 31 | epoch: 49, train loss: 10.979097366333008, test loss: 8.525203704833984\n",
      "h: 31 | epoch: 50, train loss: 10.824947357177734, test loss: 8.371180534362793\n",
      "h: 31 | epoch: 51, train loss: 10.680109977722168, test loss: 8.225671768188477\n",
      "h: 31 | epoch: 52, train loss: 10.544063568115234, test loss: 8.088220596313477\n",
      "h: 31 | epoch: 53, train loss: 10.416311264038086, test loss: 7.958390712738037\n",
      "h: 31 | epoch: 54, train loss: 10.296382904052734, test loss: 7.835780143737793\n",
      "h: 31 | epoch: 55, train loss: 10.183831214904785, test loss: 7.719989776611328\n",
      "h: 31 | epoch: 56, train loss: 10.078231811523438, test loss: 7.610653877258301\n",
      "h: 31 | epoch: 57, train loss: 9.979181289672852, test loss: 7.507415771484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 31 | epoch: 58, train loss: 9.886299133300781, test loss: 7.409945011138916\n",
      "h: 31 | epoch: 59, train loss: 9.799226760864258, test loss: 7.3179168701171875\n",
      "h: 31 | epoch: 60, train loss: 9.717616081237793, test loss: 7.2310380935668945\n",
      "h: 31 | epoch: 61, train loss: 9.64114761352539, test loss: 7.1490159034729\n",
      "h: 31 | epoch: 62, train loss: 9.569513320922852, test loss: 7.071585178375244\n",
      "h: 31 | epoch: 63, train loss: 9.502424240112305, test loss: 6.998485565185547\n",
      "h: 31 | epoch: 64, train loss: 9.439604759216309, test loss: 6.929476261138916\n",
      "h: 31 | epoch: 65, train loss: 9.380800247192383, test loss: 6.86432409286499\n",
      "h: 31 | epoch: 66, train loss: 9.325761795043945, test loss: 6.802814483642578\n",
      "h: 31 | epoch: 67, train loss: 9.274262428283691, test loss: 6.744741916656494\n",
      "h: 31 | epoch: 68, train loss: 9.226083755493164, test loss: 6.689910888671875\n",
      "h: 31 | epoch: 69, train loss: 9.181020736694336, test loss: 6.638134002685547\n",
      "h: 31 | epoch: 70, train loss: 9.13887882232666, test loss: 6.589242458343506\n",
      "h: 31 | epoch: 71, train loss: 9.099477767944336, test loss: 6.543070316314697\n",
      "h: 31 | epoch: 72, train loss: 9.062646865844727, test loss: 6.4994611740112305\n",
      "h: 31 | epoch: 73, train loss: 9.028223991394043, test loss: 6.4582695960998535\n",
      "h: 31 | epoch: 74, train loss: 8.99605655670166, test loss: 6.419356346130371\n",
      "h: 31 | epoch: 75, train loss: 8.96600341796875, test loss: 6.382594585418701\n",
      "h: 31 | epoch: 76, train loss: 8.937929153442383, test loss: 6.3478569984436035\n",
      "h: 31 | epoch: 77, train loss: 8.911707878112793, test loss: 6.315028190612793\n",
      "h: 31 | epoch: 78, train loss: 8.887222290039062, test loss: 6.283999919891357\n",
      "h: 31 | epoch: 79, train loss: 8.864359855651855, test loss: 6.254668712615967\n",
      "h: 31 | epoch: 80, train loss: 8.843016624450684, test loss: 6.226938247680664\n",
      "h: 31 | epoch: 81, train loss: 8.823092460632324, test loss: 6.200715065002441\n",
      "h: 31 | epoch: 82, train loss: 8.804499626159668, test loss: 6.1759138107299805\n",
      "h: 31 | epoch: 83, train loss: 8.787149429321289, test loss: 6.15245246887207\n",
      "h: 31 | epoch: 84, train loss: 8.770959854125977, test loss: 6.130254745483398\n",
      "h: 31 | epoch: 85, train loss: 8.755857467651367, test loss: 6.109249114990234\n",
      "h: 31 | epoch: 86, train loss: 8.741767883300781, test loss: 6.089366912841797\n",
      "h: 31 | epoch: 87, train loss: 8.728628158569336, test loss: 6.070542335510254\n",
      "h: 31 | epoch: 88, train loss: 8.716373443603516, test loss: 6.052718639373779\n",
      "h: 31 | epoch: 89, train loss: 8.70494556427002, test loss: 6.035836219787598\n",
      "h: 31 | epoch: 90, train loss: 8.694291114807129, test loss: 6.019843578338623\n",
      "h: 31 | epoch: 91, train loss: 8.684358596801758, test loss: 6.0046892166137695\n",
      "h: 31 | epoch: 92, train loss: 8.675098419189453, test loss: 5.990324974060059\n",
      "h: 31 | epoch: 93, train loss: 8.666467666625977, test loss: 5.97670841217041\n",
      "h: 31 | epoch: 94, train loss: 8.658422470092773, test loss: 5.963796615600586\n",
      "h: 31 | epoch: 95, train loss: 8.650925636291504, test loss: 5.951549530029297\n",
      "h: 31 | epoch: 96, train loss: 8.643939018249512, test loss: 5.939929008483887\n",
      "h: 31 | epoch: 97, train loss: 8.637430191040039, test loss: 5.928903579711914\n",
      "h: 31 | epoch: 98, train loss: 8.631364822387695, test loss: 5.9184370040893555\n",
      "h: 31 | epoch: 99, train loss: 8.625715255737305, test loss: 5.908498764038086\n",
      "h: 32 | epoch: 0, train loss: 41.18802261352539, test loss: 36.1136474609375\n",
      "h: 32 | epoch: 1, train loss: 39.81721878051758, test loss: 34.86149215698242\n",
      "h: 32 | epoch: 2, train loss: 38.50639343261719, test loss: 33.663448333740234\n",
      "h: 32 | epoch: 3, train loss: 37.252342224121094, test loss: 32.51670837402344\n",
      "h: 32 | epoch: 4, train loss: 36.0521125793457, test loss: 31.418655395507812\n",
      "h: 32 | epoch: 5, train loss: 34.90296936035156, test loss: 30.366840362548828\n",
      "h: 32 | epoch: 6, train loss: 33.80236053466797, test loss: 29.359004974365234\n",
      "h: 32 | epoch: 7, train loss: 32.747947692871094, test loss: 28.393062591552734\n",
      "h: 32 | epoch: 8, train loss: 31.73752212524414, test loss: 27.467025756835938\n",
      "h: 32 | epoch: 9, train loss: 30.769027709960938, test loss: 26.579065322875977\n",
      "h: 32 | epoch: 10, train loss: 29.840557098388672, test loss: 25.72745704650879\n",
      "h: 32 | epoch: 11, train loss: 28.950305938720703, test loss: 24.910572052001953\n",
      "h: 32 | epoch: 12, train loss: 28.096582412719727, test loss: 24.12689781188965\n",
      "h: 32 | epoch: 13, train loss: 27.277801513671875, test loss: 23.374998092651367\n",
      "h: 32 | epoch: 14, train loss: 26.49247169494629, test loss: 22.65351104736328\n",
      "h: 32 | epoch: 15, train loss: 25.739171981811523, test loss: 21.961162567138672\n",
      "h: 32 | epoch: 16, train loss: 25.016572952270508, test loss: 21.296735763549805\n",
      "h: 32 | epoch: 17, train loss: 24.32340431213379, test loss: 20.659076690673828\n",
      "h: 32 | epoch: 18, train loss: 23.65846061706543, test loss: 20.047100067138672\n",
      "h: 32 | epoch: 19, train loss: 23.020606994628906, test loss: 19.459754943847656\n",
      "h: 32 | epoch: 20, train loss: 22.408754348754883, test loss: 18.896059036254883\n",
      "h: 32 | epoch: 21, train loss: 21.821863174438477, test loss: 18.35506248474121\n",
      "h: 32 | epoch: 22, train loss: 21.25894546508789, test loss: 17.835859298706055\n",
      "h: 32 | epoch: 23, train loss: 20.71905517578125, test loss: 17.337596893310547\n",
      "h: 32 | epoch: 24, train loss: 20.201292037963867, test loss: 16.859439849853516\n",
      "h: 32 | epoch: 25, train loss: 19.704792022705078, test loss: 16.400602340698242\n",
      "h: 32 | epoch: 26, train loss: 19.228715896606445, test loss: 15.960325241088867\n",
      "h: 32 | epoch: 27, train loss: 18.772274017333984, test loss: 15.537878036499023\n",
      "h: 32 | epoch: 28, train loss: 18.334701538085938, test loss: 15.132573127746582\n",
      "h: 32 | epoch: 29, train loss: 17.915264129638672, test loss: 14.743730545043945\n",
      "h: 32 | epoch: 30, train loss: 17.51325225830078, test loss: 14.37071418762207\n",
      "h: 32 | epoch: 31, train loss: 17.127992630004883, test loss: 14.012895584106445\n",
      "h: 32 | epoch: 32, train loss: 16.758825302124023, test loss: 13.669682502746582\n",
      "h: 32 | epoch: 33, train loss: 16.405122756958008, test loss: 13.340509414672852\n",
      "h: 32 | epoch: 34, train loss: 16.066280364990234, test loss: 13.024815559387207\n",
      "h: 32 | epoch: 35, train loss: 15.741711616516113, test loss: 12.722073554992676\n",
      "h: 32 | epoch: 36, train loss: 15.430856704711914, test loss: 12.43177604675293\n",
      "h: 32 | epoch: 37, train loss: 15.133166313171387, test loss: 12.153423309326172\n",
      "h: 32 | epoch: 38, train loss: 14.848126411437988, test loss: 11.886548042297363\n",
      "h: 32 | epoch: 39, train loss: 14.575225830078125, test loss: 11.630690574645996\n",
      "h: 32 | epoch: 40, train loss: 14.313982963562012, test loss: 11.38541030883789\n",
      "h: 32 | epoch: 41, train loss: 14.063923835754395, test loss: 11.150289535522461\n",
      "h: 32 | epoch: 42, train loss: 13.824602127075195, test loss: 10.924911499023438\n",
      "h: 32 | epoch: 43, train loss: 13.595579147338867, test loss: 10.708890914916992\n",
      "h: 32 | epoch: 44, train loss: 13.376437187194824, test loss: 10.501848220825195\n",
      "h: 32 | epoch: 45, train loss: 13.166772842407227, test loss: 10.30341625213623\n",
      "h: 32 | epoch: 46, train loss: 12.966192245483398, test loss: 10.113248825073242\n",
      "h: 32 | epoch: 47, train loss: 12.774324417114258, test loss: 9.931007385253906\n",
      "h: 32 | epoch: 48, train loss: 12.590805053710938, test loss: 9.756367683410645\n",
      "h: 32 | epoch: 49, train loss: 12.415287017822266, test loss: 9.589017868041992\n",
      "h: 32 | epoch: 50, train loss: 12.247438430786133, test loss: 9.428656578063965\n",
      "h: 32 | epoch: 51, train loss: 12.086931228637695, test loss: 9.274996757507324\n",
      "h: 32 | epoch: 52, train loss: 11.933460235595703, test loss: 9.12775993347168\n",
      "h: 32 | epoch: 53, train loss: 11.786725997924805, test loss: 8.986680030822754\n",
      "h: 32 | epoch: 54, train loss: 11.646440505981445, test loss: 8.851500511169434\n",
      "h: 32 | epoch: 55, train loss: 11.51232624053955, test loss: 8.721973419189453\n",
      "h: 32 | epoch: 56, train loss: 11.384121894836426, test loss: 8.597865104675293\n",
      "h: 32 | epoch: 57, train loss: 11.26156997680664, test loss: 8.478947639465332\n",
      "h: 32 | epoch: 58, train loss: 11.144428253173828, test loss: 8.36500072479248\n",
      "h: 32 | epoch: 59, train loss: 11.032463073730469, test loss: 8.255818367004395\n",
      "h: 32 | epoch: 60, train loss: 10.925444602966309, test loss: 8.151196479797363\n",
      "h: 32 | epoch: 61, train loss: 10.823160171508789, test loss: 8.050943374633789\n",
      "h: 32 | epoch: 62, train loss: 10.725400924682617, test loss: 7.9548773765563965\n",
      "h: 32 | epoch: 63, train loss: 10.63196849822998, test loss: 7.86281681060791\n",
      "h: 32 | epoch: 64, train loss: 10.542670249938965, test loss: 7.774593353271484\n",
      "h: 32 | epoch: 65, train loss: 10.457327842712402, test loss: 7.6900434494018555\n",
      "h: 32 | epoch: 66, train loss: 10.375761032104492, test loss: 7.609012603759766\n",
      "h: 32 | epoch: 67, train loss: 10.297807693481445, test loss: 7.531350612640381\n",
      "h: 32 | epoch: 68, train loss: 10.22330093383789, test loss: 7.456911563873291\n",
      "h: 32 | epoch: 69, train loss: 10.15208911895752, test loss: 7.385561466217041\n",
      "h: 32 | epoch: 70, train loss: 10.084025382995605, test loss: 7.31716775894165\n",
      "h: 32 | epoch: 71, train loss: 10.018972396850586, test loss: 7.2516045570373535\n",
      "h: 32 | epoch: 72, train loss: 9.956789016723633, test loss: 7.188747406005859\n",
      "h: 32 | epoch: 73, train loss: 9.897348403930664, test loss: 7.128487586975098\n",
      "h: 32 | epoch: 74, train loss: 9.840529441833496, test loss: 7.070710182189941\n",
      "h: 32 | epoch: 75, train loss: 9.786211967468262, test loss: 7.015310764312744\n",
      "h: 32 | epoch: 76, train loss: 9.734284400939941, test loss: 6.962187767028809\n",
      "h: 32 | epoch: 77, train loss: 9.684637069702148, test loss: 6.911242485046387\n",
      "h: 32 | epoch: 78, train loss: 9.637168884277344, test loss: 6.862386226654053\n",
      "h: 32 | epoch: 79, train loss: 9.591781616210938, test loss: 6.815528869628906\n",
      "h: 32 | epoch: 80, train loss: 9.548379898071289, test loss: 6.770585060119629\n",
      "h: 32 | epoch: 81, train loss: 9.506875991821289, test loss: 6.727471828460693\n",
      "h: 32 | epoch: 82, train loss: 9.467182159423828, test loss: 6.686115264892578\n",
      "h: 32 | epoch: 83, train loss: 9.429216384887695, test loss: 6.6464385986328125\n",
      "h: 32 | epoch: 84, train loss: 9.392902374267578, test loss: 6.608370780944824\n",
      "h: 32 | epoch: 85, train loss: 9.358163833618164, test loss: 6.571845054626465\n",
      "h: 32 | epoch: 86, train loss: 9.324930191040039, test loss: 6.536798000335693\n",
      "h: 32 | epoch: 87, train loss: 9.293134689331055, test loss: 6.503163814544678\n",
      "h: 32 | epoch: 88, train loss: 9.262710571289062, test loss: 6.47088623046875\n",
      "h: 32 | epoch: 89, train loss: 9.233597755432129, test loss: 6.439904689788818\n",
      "h: 32 | epoch: 90, train loss: 9.20573616027832, test loss: 6.4101715087890625\n",
      "h: 32 | epoch: 91, train loss: 9.179068565368652, test loss: 6.381629467010498\n",
      "h: 32 | epoch: 92, train loss: 9.153542518615723, test loss: 6.354228973388672\n",
      "h: 32 | epoch: 93, train loss: 9.129106521606445, test loss: 6.3279242515563965\n",
      "h: 32 | epoch: 94, train loss: 9.105712890625, test loss: 6.302669048309326\n",
      "h: 32 | epoch: 95, train loss: 9.083312034606934, test loss: 6.2784199714660645\n",
      "h: 32 | epoch: 96, train loss: 9.061861991882324, test loss: 6.255136489868164\n",
      "h: 32 | epoch: 97, train loss: 9.041319847106934, test loss: 6.2327775955200195\n",
      "h: 32 | epoch: 98, train loss: 9.02164363861084, test loss: 6.211306095123291\n",
      "h: 32 | epoch: 99, train loss: 9.002798080444336, test loss: 6.190684795379639\n",
      "h: 33 | epoch: 0, train loss: 41.749427795410156, test loss: 36.931434631347656\n",
      "h: 33 | epoch: 1, train loss: 39.93442916870117, test loss: 35.28913879394531\n",
      "h: 33 | epoch: 2, train loss: 38.21772766113281, test loss: 33.733585357666016\n",
      "h: 33 | epoch: 3, train loss: 36.593326568603516, test loss: 32.25961685180664\n",
      "h: 33 | epoch: 4, train loss: 35.05572509765625, test loss: 30.862497329711914\n",
      "h: 33 | epoch: 5, train loss: 33.5998649597168, test loss: 29.537853240966797\n",
      "h: 33 | epoch: 6, train loss: 32.221092224121094, test loss: 28.28163719177246\n",
      "h: 33 | epoch: 7, train loss: 30.915081024169922, test loss: 27.09011459350586\n",
      "h: 33 | epoch: 8, train loss: 29.677841186523438, test loss: 25.959789276123047\n",
      "h: 33 | epoch: 9, train loss: 28.505651473999023, test loss: 24.887428283691406\n",
      "h: 33 | epoch: 10, train loss: 27.39504623413086, test loss: 23.86998748779297\n",
      "h: 33 | epoch: 11, train loss: 26.342784881591797, test loss: 22.904634475708008\n",
      "h: 33 | epoch: 12, train loss: 25.345829010009766, test loss: 21.988706588745117\n",
      "h: 33 | epoch: 13, train loss: 24.401342391967773, test loss: 21.119699478149414\n",
      "h: 33 | epoch: 14, train loss: 23.506641387939453, test loss: 20.29526138305664\n",
      "h: 33 | epoch: 15, train loss: 22.659208297729492, test loss: 19.513160705566406\n",
      "h: 33 | epoch: 16, train loss: 21.85666275024414, test loss: 18.771299362182617\n",
      "h: 33 | epoch: 17, train loss: 21.09675407409668, test loss: 18.06769371032715\n",
      "h: 33 | epoch: 18, train loss: 20.37735366821289, test loss: 17.400455474853516\n",
      "h: 33 | epoch: 19, train loss: 19.696447372436523, test loss: 16.767803192138672\n",
      "h: 33 | epoch: 20, train loss: 19.05211639404297, test loss: 16.168033599853516\n",
      "h: 33 | epoch: 21, train loss: 18.44253921508789, test loss: 15.599533081054688\n",
      "h: 33 | epoch: 22, train loss: 17.865989685058594, test loss: 15.060775756835938\n",
      "h: 33 | epoch: 23, train loss: 17.32082176208496, test loss: 14.55029010772705\n",
      "h: 33 | epoch: 24, train loss: 16.805465698242188, test loss: 14.066686630249023\n",
      "h: 33 | epoch: 25, train loss: 16.31842803955078, test loss: 13.60864543914795\n",
      "h: 33 | epoch: 26, train loss: 15.85828685760498, test loss: 13.174896240234375\n",
      "h: 33 | epoch: 27, train loss: 15.423683166503906, test loss: 12.764230728149414\n",
      "h: 33 | epoch: 28, train loss: 15.013320922851562, test loss: 12.37550163269043\n",
      "h: 33 | epoch: 29, train loss: 14.625967025756836, test loss: 12.007609367370605\n",
      "h: 33 | epoch: 30, train loss: 14.260442733764648, test loss: 11.659507751464844\n",
      "h: 33 | epoch: 31, train loss: 13.915621757507324, test loss: 11.330195426940918\n",
      "h: 33 | epoch: 32, train loss: 13.590429306030273, test loss: 11.018717765808105\n",
      "h: 33 | epoch: 33, train loss: 13.283846855163574, test loss: 10.72416877746582\n",
      "h: 33 | epoch: 34, train loss: 12.994897842407227, test loss: 10.4456787109375\n",
      "h: 33 | epoch: 35, train loss: 12.722646713256836, test loss: 10.182417869567871\n",
      "h: 33 | epoch: 36, train loss: 12.466209411621094, test loss: 9.93359661102295\n",
      "h: 33 | epoch: 37, train loss: 12.224740982055664, test loss: 9.698466300964355\n",
      "h: 33 | epoch: 38, train loss: 11.997434616088867, test loss: 9.476306915283203\n",
      "h: 33 | epoch: 39, train loss: 11.783520698547363, test loss: 9.266437530517578\n",
      "h: 33 | epoch: 40, train loss: 11.58227252960205, test loss: 9.068205833435059\n",
      "h: 33 | epoch: 41, train loss: 11.392990112304688, test loss: 8.880995750427246\n",
      "h: 33 | epoch: 42, train loss: 11.215017318725586, test loss: 8.704214096069336\n",
      "h: 33 | epoch: 43, train loss: 11.04771900177002, test loss: 8.537304878234863\n",
      "h: 33 | epoch: 44, train loss: 10.890503883361816, test loss: 8.37973403930664\n",
      "h: 33 | epoch: 45, train loss: 10.742799758911133, test loss: 8.230992317199707\n",
      "h: 33 | epoch: 46, train loss: 10.6040678024292, test loss: 8.090600967407227\n",
      "h: 33 | epoch: 47, train loss: 10.473798751831055, test loss: 7.95810079574585\n",
      "h: 33 | epoch: 48, train loss: 10.351505279541016, test loss: 7.833062648773193\n",
      "h: 33 | epoch: 49, train loss: 10.236726760864258, test loss: 7.7150702476501465\n",
      "h: 33 | epoch: 50, train loss: 10.1290283203125, test loss: 7.603737831115723\n",
      "h: 33 | epoch: 51, train loss: 10.027996063232422, test loss: 7.498688697814941\n",
      "h: 33 | epoch: 52, train loss: 9.933237075805664, test loss: 7.399579048156738\n",
      "h: 33 | epoch: 53, train loss: 9.844385147094727, test loss: 7.306072235107422\n",
      "h: 33 | epoch: 54, train loss: 9.761086463928223, test loss: 7.2178544998168945\n",
      "h: 33 | epoch: 55, train loss: 9.683012008666992, test loss: 7.1346282958984375\n",
      "h: 33 | epoch: 56, train loss: 9.609848022460938, test loss: 7.056110382080078\n",
      "h: 33 | epoch: 57, train loss: 9.541297912597656, test loss: 6.982035160064697\n",
      "h: 33 | epoch: 58, train loss: 9.477084159851074, test loss: 6.912149906158447\n",
      "h: 33 | epoch: 59, train loss: 9.41694450378418, test loss: 6.846217155456543\n",
      "h: 33 | epoch: 60, train loss: 9.360628128051758, test loss: 6.7840094566345215\n",
      "h: 33 | epoch: 61, train loss: 9.307902336120605, test loss: 6.725314140319824\n",
      "h: 33 | epoch: 62, train loss: 9.258544921875, test loss: 6.669933319091797\n",
      "h: 33 | epoch: 63, train loss: 9.212347030639648, test loss: 6.617671012878418\n",
      "h: 33 | epoch: 64, train loss: 9.169116020202637, test loss: 6.568354606628418\n",
      "h: 33 | epoch: 65, train loss: 9.128665924072266, test loss: 6.521811008453369\n",
      "h: 33 | epoch: 66, train loss: 9.090822219848633, test loss: 6.47788143157959\n",
      "h: 33 | epoch: 67, train loss: 9.055421829223633, test loss: 6.4364142417907715\n",
      "h: 33 | epoch: 68, train loss: 9.022310256958008, test loss: 6.397271156311035\n",
      "h: 33 | epoch: 69, train loss: 8.99134635925293, test loss: 6.360313415527344\n",
      "h: 33 | epoch: 70, train loss: 8.962390899658203, test loss: 6.325417518615723\n",
      "h: 33 | epoch: 71, train loss: 8.935317993164062, test loss: 6.292466163635254\n",
      "h: 33 | epoch: 72, train loss: 8.910008430480957, test loss: 6.261343955993652\n",
      "h: 33 | epoch: 73, train loss: 8.886347770690918, test loss: 6.231944561004639\n",
      "h: 33 | epoch: 74, train loss: 8.864233016967773, test loss: 6.2041730880737305\n",
      "h: 33 | epoch: 75, train loss: 8.843563079833984, test loss: 6.177933692932129\n",
      "h: 33 | epoch: 76, train loss: 8.82424545288086, test loss: 6.153136253356934\n",
      "h: 33 | epoch: 77, train loss: 8.806193351745605, test loss: 6.12969970703125\n",
      "h: 33 | epoch: 78, train loss: 8.789325714111328, test loss: 6.107546329498291\n",
      "h: 33 | epoch: 79, train loss: 8.773564338684082, test loss: 6.086601734161377\n",
      "h: 33 | epoch: 80, train loss: 8.758840560913086, test loss: 6.06679630279541\n",
      "h: 33 | epoch: 81, train loss: 8.745081901550293, test loss: 6.048065662384033\n",
      "h: 33 | epoch: 82, train loss: 8.732230186462402, test loss: 6.0303473472595215\n",
      "h: 33 | epoch: 83, train loss: 8.720224380493164, test loss: 6.013583183288574\n",
      "h: 33 | epoch: 84, train loss: 8.709009170532227, test loss: 5.9977216720581055\n",
      "h: 33 | epoch: 85, train loss: 8.69853401184082, test loss: 5.9827070236206055\n",
      "h: 33 | epoch: 86, train loss: 8.688749313354492, test loss: 5.968494892120361\n",
      "h: 33 | epoch: 87, train loss: 8.679609298706055, test loss: 5.955039024353027\n",
      "h: 33 | epoch: 88, train loss: 8.671072959899902, test loss: 5.942294120788574\n",
      "h: 33 | epoch: 89, train loss: 8.663101196289062, test loss: 5.930224418640137\n",
      "h: 33 | epoch: 90, train loss: 8.655655860900879, test loss: 5.918789863586426\n",
      "h: 33 | epoch: 91, train loss: 8.648700714111328, test loss: 5.907954692840576\n",
      "h: 33 | epoch: 92, train loss: 8.642206192016602, test loss: 5.8976850509643555\n",
      "h: 33 | epoch: 93, train loss: 8.636140823364258, test loss: 5.8879499435424805\n",
      "h: 33 | epoch: 94, train loss: 8.630475997924805, test loss: 5.878719806671143\n",
      "h: 33 | epoch: 95, train loss: 8.625185012817383, test loss: 5.869966506958008\n",
      "h: 33 | epoch: 96, train loss: 8.620244026184082, test loss: 5.861663341522217\n",
      "h: 33 | epoch: 97, train loss: 8.615628242492676, test loss: 5.853786945343018\n",
      "h: 33 | epoch: 98, train loss: 8.61131763458252, test loss: 5.846311092376709\n",
      "h: 33 | epoch: 99, train loss: 8.607293128967285, test loss: 5.839217185974121\n",
      "h: 34 | epoch: 0, train loss: 36.373069763183594, test loss: 32.00074005126953\n",
      "h: 34 | epoch: 1, train loss: 34.945068359375, test loss: 30.720144271850586\n",
      "h: 34 | epoch: 2, train loss: 33.586700439453125, test loss: 29.499893188476562\n",
      "h: 34 | epoch: 3, train loss: 32.294349670410156, test loss: 28.336933135986328\n",
      "h: 34 | epoch: 4, train loss: 31.06465721130371, test loss: 27.228439331054688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 34 | epoch: 5, train loss: 29.894489288330078, test loss: 26.171772003173828\n",
      "h: 34 | epoch: 6, train loss: 28.78092384338379, test loss: 25.164459228515625\n",
      "h: 34 | epoch: 7, train loss: 27.72122573852539, test loss: 24.204181671142578\n",
      "h: 34 | epoch: 8, train loss: 26.712825775146484, test loss: 23.288761138916016\n",
      "h: 34 | epoch: 9, train loss: 25.753314971923828, test loss: 22.41614532470703\n",
      "h: 34 | epoch: 10, train loss: 24.840412139892578, test loss: 21.58438491821289\n",
      "h: 34 | epoch: 11, train loss: 23.971973419189453, test loss: 20.791648864746094\n",
      "h: 34 | epoch: 12, train loss: 23.14595603942871, test loss: 20.036205291748047\n",
      "h: 34 | epoch: 13, train loss: 22.360441207885742, test loss: 19.316387176513672\n",
      "h: 34 | epoch: 14, train loss: 21.613590240478516, test loss: 18.63062858581543\n",
      "h: 34 | epoch: 15, train loss: 20.903656005859375, test loss: 17.977428436279297\n",
      "h: 34 | epoch: 16, train loss: 20.22898292541504, test loss: 17.355358123779297\n",
      "h: 34 | epoch: 17, train loss: 19.58797836303711, test loss: 16.7630558013916\n",
      "h: 34 | epoch: 18, train loss: 18.979141235351562, test loss: 16.199207305908203\n",
      "h: 34 | epoch: 19, train loss: 18.401010513305664, test loss: 15.662567138671875\n",
      "h: 34 | epoch: 20, train loss: 17.85220718383789, test loss: 15.151939392089844\n",
      "h: 34 | epoch: 21, train loss: 17.331405639648438, test loss: 14.666172981262207\n",
      "h: 34 | epoch: 22, train loss: 16.837331771850586, test loss: 14.204172134399414\n",
      "h: 34 | epoch: 23, train loss: 16.368762969970703, test loss: 13.764869689941406\n",
      "h: 34 | epoch: 24, train loss: 15.924539566040039, test loss: 13.347262382507324\n",
      "h: 34 | epoch: 25, train loss: 15.50352668762207, test loss: 12.950373649597168\n",
      "h: 34 | epoch: 26, train loss: 15.104655265808105, test loss: 12.573262214660645\n",
      "h: 34 | epoch: 27, train loss: 14.726888656616211, test loss: 12.215036392211914\n",
      "h: 34 | epoch: 28, train loss: 14.369236946105957, test loss: 11.874828338623047\n",
      "h: 34 | epoch: 29, train loss: 14.030743598937988, test loss: 11.551813125610352\n",
      "h: 34 | epoch: 30, train loss: 13.71049690246582, test loss: 11.245194435119629\n",
      "h: 34 | epoch: 31, train loss: 13.407617568969727, test loss: 10.954204559326172\n",
      "h: 34 | epoch: 32, train loss: 13.12126636505127, test loss: 10.67811393737793\n",
      "h: 34 | epoch: 33, train loss: 12.850634574890137, test loss: 10.416215896606445\n",
      "h: 34 | epoch: 34, train loss: 12.59494686126709, test loss: 10.167838096618652\n",
      "h: 34 | epoch: 35, train loss: 12.35346508026123, test loss: 9.932330131530762\n",
      "h: 34 | epoch: 36, train loss: 12.12547492980957, test loss: 9.70907211303711\n",
      "h: 34 | epoch: 37, train loss: 11.910294532775879, test loss: 9.497468948364258\n",
      "h: 34 | epoch: 38, train loss: 11.707276344299316, test loss: 9.296952247619629\n",
      "h: 34 | epoch: 39, train loss: 11.51579475402832, test loss: 9.106971740722656\n",
      "h: 34 | epoch: 40, train loss: 11.33525276184082, test loss: 8.927008628845215\n",
      "h: 34 | epoch: 41, train loss: 11.165082931518555, test loss: 8.756562232971191\n",
      "h: 34 | epoch: 42, train loss: 11.004738807678223, test loss: 8.595155715942383\n",
      "h: 34 | epoch: 43, train loss: 10.8537015914917, test loss: 8.442331314086914\n",
      "h: 34 | epoch: 44, train loss: 10.711474418640137, test loss: 8.297653198242188\n",
      "h: 34 | epoch: 45, train loss: 10.577584266662598, test loss: 8.160703659057617\n",
      "h: 34 | epoch: 46, train loss: 10.451581001281738, test loss: 8.031085968017578\n",
      "h: 34 | epoch: 47, train loss: 10.333035469055176, test loss: 7.908424377441406\n",
      "h: 34 | epoch: 48, train loss: 10.221536636352539, test loss: 7.792354583740234\n",
      "h: 34 | epoch: 49, train loss: 10.116697311401367, test loss: 7.682531833648682\n",
      "h: 34 | epoch: 50, train loss: 10.018144607543945, test loss: 7.578629493713379\n",
      "h: 34 | epoch: 51, train loss: 9.925531387329102, test loss: 7.480332374572754\n",
      "h: 34 | epoch: 52, train loss: 9.838516235351562, test loss: 7.387349605560303\n",
      "h: 34 | epoch: 53, train loss: 9.75678825378418, test loss: 7.2993927001953125\n",
      "h: 34 | epoch: 54, train loss: 9.68004035949707, test loss: 7.216196537017822\n",
      "h: 34 | epoch: 55, train loss: 9.607992172241211, test loss: 7.137503623962402\n",
      "h: 34 | epoch: 56, train loss: 9.540369033813477, test loss: 7.06307315826416\n",
      "h: 34 | epoch: 57, train loss: 9.47691535949707, test loss: 6.992674350738525\n",
      "h: 34 | epoch: 58, train loss: 9.417387008666992, test loss: 6.926088809967041\n",
      "h: 34 | epoch: 59, train loss: 9.361556053161621, test loss: 6.863109588623047\n",
      "h: 34 | epoch: 60, train loss: 9.309202194213867, test loss: 6.8035407066345215\n",
      "h: 34 | epoch: 61, train loss: 9.260119438171387, test loss: 6.7471923828125\n",
      "h: 34 | epoch: 62, train loss: 9.21411418914795, test loss: 6.693892002105713\n",
      "h: 34 | epoch: 63, train loss: 9.171001434326172, test loss: 6.643473148345947\n",
      "h: 34 | epoch: 64, train loss: 9.130606651306152, test loss: 6.595772743225098\n",
      "h: 34 | epoch: 65, train loss: 9.092767715454102, test loss: 6.550645351409912\n",
      "h: 34 | epoch: 66, train loss: 9.057329177856445, test loss: 6.507943630218506\n",
      "h: 34 | epoch: 67, train loss: 9.02414321899414, test loss: 6.467539310455322\n",
      "h: 34 | epoch: 68, train loss: 8.993074417114258, test loss: 6.4293012619018555\n",
      "h: 34 | epoch: 69, train loss: 8.963991165161133, test loss: 6.393112659454346\n",
      "h: 34 | epoch: 70, train loss: 8.93677043914795, test loss: 6.358857154846191\n",
      "h: 34 | epoch: 71, train loss: 8.911298751831055, test loss: 6.326427936553955\n",
      "h: 34 | epoch: 72, train loss: 8.887466430664062, test loss: 6.2957258224487305\n",
      "h: 34 | epoch: 73, train loss: 8.86517333984375, test loss: 6.2666521072387695\n",
      "h: 34 | epoch: 74, train loss: 8.844320297241211, test loss: 6.239119052886963\n",
      "h: 34 | epoch: 75, train loss: 8.824819564819336, test loss: 6.213037014007568\n",
      "h: 34 | epoch: 76, train loss: 8.806585311889648, test loss: 6.188328742980957\n",
      "h: 34 | epoch: 77, train loss: 8.789535522460938, test loss: 6.164918422698975\n",
      "h: 34 | epoch: 78, train loss: 8.773597717285156, test loss: 6.142730712890625\n",
      "h: 34 | epoch: 79, train loss: 8.758702278137207, test loss: 6.121700763702393\n",
      "h: 34 | epoch: 80, train loss: 8.744779586791992, test loss: 6.10176420211792\n",
      "h: 34 | epoch: 81, train loss: 8.731769561767578, test loss: 6.082859516143799\n",
      "h: 34 | epoch: 82, train loss: 8.719614028930664, test loss: 6.0649309158325195\n",
      "h: 34 | epoch: 83, train loss: 8.708257675170898, test loss: 6.0479230880737305\n",
      "h: 34 | epoch: 84, train loss: 8.697649002075195, test loss: 6.031785488128662\n",
      "h: 34 | epoch: 85, train loss: 8.687738418579102, test loss: 6.0164713859558105\n",
      "h: 34 | epoch: 86, train loss: 8.678483963012695, test loss: 6.001935005187988\n",
      "h: 34 | epoch: 87, train loss: 8.669840812683105, test loss: 5.988133907318115\n",
      "h: 34 | epoch: 88, train loss: 8.661770820617676, test loss: 5.975028038024902\n",
      "h: 34 | epoch: 89, train loss: 8.654236793518066, test loss: 5.962579727172852\n",
      "h: 34 | epoch: 90, train loss: 8.647201538085938, test loss: 5.9507527351379395\n",
      "h: 34 | epoch: 91, train loss: 8.640634536743164, test loss: 5.939512729644775\n",
      "h: 34 | epoch: 92, train loss: 8.634504318237305, test loss: 5.928830146789551\n",
      "h: 34 | epoch: 93, train loss: 8.628782272338867, test loss: 5.918671607971191\n",
      "h: 34 | epoch: 94, train loss: 8.623441696166992, test loss: 5.909011363983154\n",
      "h: 34 | epoch: 95, train loss: 8.618459701538086, test loss: 5.899822235107422\n",
      "h: 34 | epoch: 96, train loss: 8.613809585571289, test loss: 5.891079425811768\n",
      "h: 34 | epoch: 97, train loss: 8.609468460083008, test loss: 5.882758140563965\n",
      "h: 34 | epoch: 98, train loss: 8.605422019958496, test loss: 5.874837875366211\n",
      "h: 34 | epoch: 99, train loss: 8.601644515991211, test loss: 5.867295265197754\n",
      "h: 35 | epoch: 0, train loss: 48.2349967956543, test loss: 43.01050567626953\n",
      "h: 35 | epoch: 1, train loss: 46.94135284423828, test loss: 41.86057662963867\n",
      "h: 35 | epoch: 2, train loss: 45.69178009033203, test loss: 40.7479133605957\n",
      "h: 35 | epoch: 3, train loss: 44.4840087890625, test loss: 39.67066192626953\n",
      "h: 35 | epoch: 4, train loss: 43.31595993041992, test loss: 38.627113342285156\n",
      "h: 35 | epoch: 5, train loss: 42.18570327758789, test loss: 37.61570358276367\n",
      "h: 35 | epoch: 6, train loss: 41.09148025512695, test loss: 36.63496398925781\n",
      "h: 35 | epoch: 7, train loss: 40.03165054321289, test loss: 35.68355941772461\n",
      "h: 35 | epoch: 8, train loss: 39.00470733642578, test loss: 34.76025390625\n",
      "h: 35 | epoch: 9, train loss: 38.009254455566406, test loss: 33.86389923095703\n",
      "h: 35 | epoch: 10, train loss: 37.04400634765625, test loss: 32.993438720703125\n",
      "h: 35 | epoch: 11, train loss: 36.10776901245117, test loss: 32.14787292480469\n",
      "h: 35 | epoch: 12, train loss: 35.19942092895508, test loss: 31.3262939453125\n",
      "h: 35 | epoch: 13, train loss: 34.317928314208984, test loss: 30.527856826782227\n",
      "h: 35 | epoch: 14, train loss: 33.46234130859375, test loss: 29.751766204833984\n",
      "h: 35 | epoch: 15, train loss: 32.631752014160156, test loss: 28.997268676757812\n",
      "h: 35 | epoch: 16, train loss: 31.825328826904297, test loss: 28.263675689697266\n",
      "h: 35 | epoch: 17, train loss: 31.042285919189453, test loss: 27.550357818603516\n",
      "h: 35 | epoch: 18, train loss: 30.281885147094727, test loss: 26.856678009033203\n",
      "h: 35 | epoch: 19, train loss: 29.543437957763672, test loss: 26.182079315185547\n",
      "h: 35 | epoch: 20, train loss: 28.8262939453125, test loss: 25.526031494140625\n",
      "h: 35 | epoch: 21, train loss: 28.129840850830078, test loss: 24.888011932373047\n",
      "h: 35 | epoch: 22, train loss: 27.4534969329834, test loss: 24.26754379272461\n",
      "h: 35 | epoch: 23, train loss: 26.796716690063477, test loss: 23.664169311523438\n",
      "h: 35 | epoch: 24, train loss: 26.158977508544922, test loss: 23.077457427978516\n",
      "h: 35 | epoch: 25, train loss: 25.539785385131836, test loss: 22.50699806213379\n",
      "h: 35 | epoch: 26, train loss: 24.93865966796875, test loss: 21.952388763427734\n",
      "h: 35 | epoch: 27, train loss: 24.35515785217285, test loss: 21.41325569152832\n",
      "h: 35 | epoch: 28, train loss: 23.78883934020996, test loss: 20.889230728149414\n",
      "h: 35 | epoch: 29, train loss: 23.239290237426758, test loss: 20.379972457885742\n",
      "h: 35 | epoch: 30, train loss: 22.706104278564453, test loss: 19.885143280029297\n",
      "h: 35 | epoch: 31, train loss: 22.18889617919922, test loss: 19.404409408569336\n",
      "h: 35 | epoch: 32, train loss: 21.68729019165039, test loss: 18.93745994567871\n",
      "h: 35 | epoch: 33, train loss: 21.200923919677734, test loss: 18.483989715576172\n",
      "h: 35 | epoch: 34, train loss: 20.72943687438965, test loss: 18.043697357177734\n",
      "h: 35 | epoch: 35, train loss: 20.272493362426758, test loss: 17.616291046142578\n",
      "h: 35 | epoch: 36, train loss: 19.829750061035156, test loss: 17.20149040222168\n",
      "h: 35 | epoch: 37, train loss: 19.400875091552734, test loss: 16.799015045166016\n",
      "h: 35 | epoch: 38, train loss: 18.985557556152344, test loss: 16.408580780029297\n",
      "h: 35 | epoch: 39, train loss: 18.58346939086914, test loss: 16.029935836791992\n",
      "h: 35 | epoch: 40, train loss: 18.19430923461914, test loss: 15.662809371948242\n",
      "h: 35 | epoch: 41, train loss: 17.81777000427246, test loss: 15.306936264038086\n",
      "h: 35 | epoch: 42, train loss: 17.453548431396484, test loss: 14.962071418762207\n",
      "h: 35 | epoch: 43, train loss: 17.10135269165039, test loss: 14.627954483032227\n",
      "h: 35 | epoch: 44, train loss: 16.760887145996094, test loss: 14.304339408874512\n",
      "h: 35 | epoch: 45, train loss: 16.431869506835938, test loss: 13.990979194641113\n",
      "h: 35 | epoch: 46, train loss: 16.114015579223633, test loss: 13.687629699707031\n",
      "h: 35 | epoch: 47, train loss: 15.807043075561523, test loss: 13.394052505493164\n",
      "h: 35 | epoch: 48, train loss: 15.5106782913208, test loss: 13.110013961791992\n",
      "h: 35 | epoch: 49, train loss: 15.224649429321289, test loss: 12.835275650024414\n",
      "h: 35 | epoch: 50, train loss: 14.948684692382812, test loss: 12.569605827331543\n",
      "h: 35 | epoch: 51, train loss: 14.682519912719727, test loss: 12.312777519226074\n",
      "h: 35 | epoch: 52, train loss: 14.42589282989502, test loss: 12.064562797546387\n",
      "h: 35 | epoch: 53, train loss: 14.178543090820312, test loss: 11.824738502502441\n",
      "h: 35 | epoch: 54, train loss: 13.940217971801758, test loss: 11.593084335327148\n",
      "h: 35 | epoch: 55, train loss: 13.710660934448242, test loss: 11.369380950927734\n",
      "h: 35 | epoch: 56, train loss: 13.489628791809082, test loss: 11.153412818908691\n",
      "h: 35 | epoch: 57, train loss: 13.276870727539062, test loss: 10.944969177246094\n",
      "h: 35 | epoch: 58, train loss: 13.072148323059082, test loss: 10.743841171264648\n",
      "h: 35 | epoch: 59, train loss: 12.875221252441406, test loss: 10.54981803894043\n",
      "h: 35 | epoch: 60, train loss: 12.685858726501465, test loss: 10.362702369689941\n",
      "h: 35 | epoch: 61, train loss: 12.503828048706055, test loss: 10.182287216186523\n",
      "h: 35 | epoch: 62, train loss: 12.328901290893555, test loss: 10.008378982543945\n",
      "h: 35 | epoch: 63, train loss: 12.160858154296875, test loss: 9.840784072875977\n",
      "h: 35 | epoch: 64, train loss: 11.999479293823242, test loss: 9.67931079864502\n",
      "h: 35 | epoch: 65, train loss: 11.844549179077148, test loss: 9.523773193359375\n",
      "h: 35 | epoch: 66, train loss: 11.695859909057617, test loss: 9.373983383178711\n",
      "h: 35 | epoch: 67, train loss: 11.553201675415039, test loss: 9.229765892028809\n",
      "h: 35 | epoch: 68, train loss: 11.416376113891602, test loss: 9.090943336486816\n",
      "h: 35 | epoch: 69, train loss: 11.285181045532227, test loss: 8.957340240478516\n",
      "h: 35 | epoch: 70, train loss: 11.159425735473633, test loss: 8.828790664672852\n",
      "h: 35 | epoch: 71, train loss: 11.038920402526855, test loss: 8.705122947692871\n",
      "h: 35 | epoch: 72, train loss: 10.923482894897461, test loss: 8.586180686950684\n",
      "h: 35 | epoch: 73, train loss: 10.812928199768066, test loss: 8.471797943115234\n",
      "h: 35 | epoch: 74, train loss: 10.707082748413086, test loss: 8.36182975769043\n",
      "h: 35 | epoch: 75, train loss: 10.605774879455566, test loss: 8.256118774414062\n",
      "h: 35 | epoch: 76, train loss: 10.508837699890137, test loss: 8.15451717376709\n",
      "h: 35 | epoch: 77, train loss: 10.416109085083008, test loss: 8.056883811950684\n",
      "h: 35 | epoch: 78, train loss: 10.327430725097656, test loss: 7.963080406188965\n",
      "h: 35 | epoch: 79, train loss: 10.242646217346191, test loss: 7.872964382171631\n",
      "h: 35 | epoch: 80, train loss: 10.161608695983887, test loss: 7.786410331726074\n",
      "h: 35 | epoch: 81, train loss: 10.084173202514648, test loss: 7.703286170959473\n",
      "h: 35 | epoch: 82, train loss: 10.010197639465332, test loss: 7.623465061187744\n",
      "h: 35 | epoch: 83, train loss: 9.939546585083008, test loss: 7.546828269958496\n",
      "h: 35 | epoch: 84, train loss: 9.872084617614746, test loss: 7.473258018493652\n",
      "h: 35 | epoch: 85, train loss: 9.807686805725098, test loss: 7.4026360511779785\n",
      "h: 35 | epoch: 86, train loss: 9.746227264404297, test loss: 7.3348565101623535\n",
      "h: 35 | epoch: 87, train loss: 9.687585830688477, test loss: 7.269806861877441\n",
      "h: 35 | epoch: 88, train loss: 9.631647109985352, test loss: 7.2073869705200195\n",
      "h: 35 | epoch: 89, train loss: 9.57829761505127, test loss: 7.147493839263916\n",
      "h: 35 | epoch: 90, train loss: 9.527429580688477, test loss: 7.090029239654541\n",
      "h: 35 | epoch: 91, train loss: 9.478938102722168, test loss: 7.034899711608887\n",
      "h: 35 | epoch: 92, train loss: 9.432723045349121, test loss: 6.982015132904053\n",
      "h: 35 | epoch: 93, train loss: 9.388687133789062, test loss: 6.931286811828613\n",
      "h: 35 | epoch: 94, train loss: 9.346733093261719, test loss: 6.882627964019775\n",
      "h: 35 | epoch: 95, train loss: 9.306774139404297, test loss: 6.835960388183594\n",
      "h: 35 | epoch: 96, train loss: 9.268721580505371, test loss: 6.791199684143066\n",
      "h: 35 | epoch: 97, train loss: 9.232492446899414, test loss: 6.748271942138672\n",
      "h: 35 | epoch: 98, train loss: 9.198004722595215, test loss: 6.707103729248047\n",
      "h: 35 | epoch: 99, train loss: 9.165179252624512, test loss: 6.667623043060303\n",
      "h: 36 | epoch: 0, train loss: 46.93313980102539, test loss: 41.28638458251953\n",
      "h: 36 | epoch: 1, train loss: 44.820743560791016, test loss: 39.398292541503906\n",
      "h: 36 | epoch: 2, train loss: 42.82606506347656, test loss: 37.61267852783203\n",
      "h: 36 | epoch: 3, train loss: 40.941444396972656, test loss: 35.92304229736328\n",
      "h: 36 | epoch: 4, train loss: 39.159881591796875, test loss: 34.32347869873047\n",
      "h: 36 | epoch: 5, train loss: 37.47499465942383, test loss: 32.80855178833008\n",
      "h: 36 | epoch: 6, train loss: 35.880943298339844, test loss: 31.373287200927734\n",
      "h: 36 | epoch: 7, train loss: 34.37235641479492, test loss: 30.013076782226562\n",
      "h: 36 | epoch: 8, train loss: 32.944297790527344, test loss: 28.723709106445312\n",
      "h: 36 | epoch: 9, train loss: 31.59218978881836, test loss: 27.50124168395996\n",
      "h: 36 | epoch: 10, train loss: 30.311813354492188, test loss: 26.342031478881836\n",
      "h: 36 | epoch: 11, train loss: 29.0992374420166, test loss: 25.242694854736328\n",
      "h: 36 | epoch: 12, train loss: 27.9508056640625, test loss: 24.200061798095703\n",
      "h: 36 | epoch: 13, train loss: 26.863109588623047, test loss: 23.211170196533203\n",
      "h: 36 | epoch: 14, train loss: 25.832956314086914, test loss: 22.273250579833984\n",
      "h: 36 | epoch: 15, train loss: 24.857351303100586, test loss: 21.383697509765625\n",
      "h: 36 | epoch: 16, train loss: 23.933486938476562, test loss: 20.54006576538086\n",
      "h: 36 | epoch: 17, train loss: 23.0587158203125, test loss: 19.740032196044922\n",
      "h: 36 | epoch: 18, train loss: 22.230545043945312, test loss: 18.981416702270508\n",
      "h: 36 | epoch: 19, train loss: 21.446617126464844, test loss: 18.26216697692871\n",
      "h: 36 | epoch: 20, train loss: 20.7047061920166, test loss: 17.580326080322266\n",
      "h: 36 | epoch: 21, train loss: 20.002700805664062, test loss: 16.934036254882812\n",
      "h: 36 | epoch: 22, train loss: 19.33859634399414, test loss: 16.3215389251709\n",
      "h: 36 | epoch: 23, train loss: 18.710500717163086, test loss: 15.741172790527344\n",
      "h: 36 | epoch: 24, train loss: 18.11659812927246, test loss: 15.191347122192383\n",
      "h: 36 | epoch: 25, train loss: 17.5551815032959, test loss: 14.670547485351562\n",
      "h: 36 | epoch: 26, train loss: 17.024612426757812, test loss: 14.177332878112793\n",
      "h: 36 | epoch: 27, train loss: 16.523330688476562, test loss: 13.710339546203613\n",
      "h: 36 | epoch: 28, train loss: 16.04986000061035, test loss: 13.268259048461914\n",
      "h: 36 | epoch: 29, train loss: 15.602783203125, test loss: 12.849841117858887\n",
      "h: 36 | epoch: 30, train loss: 15.180758476257324, test loss: 12.45390510559082\n",
      "h: 36 | epoch: 31, train loss: 14.782495498657227, test loss: 12.079314231872559\n",
      "h: 36 | epoch: 32, train loss: 14.406770706176758, test loss: 11.724990844726562\n",
      "h: 36 | epoch: 33, train loss: 14.05241584777832, test loss: 11.38990592956543\n",
      "h: 36 | epoch: 34, train loss: 13.7183198928833, test loss: 11.073070526123047\n",
      "h: 36 | epoch: 35, train loss: 13.403414726257324, test loss: 10.773553848266602\n",
      "h: 36 | epoch: 36, train loss: 13.106694221496582, test loss: 10.490458488464355\n",
      "h: 36 | epoch: 37, train loss: 12.827188491821289, test loss: 10.222936630249023\n",
      "h: 36 | epoch: 38, train loss: 12.56397819519043, test loss: 9.970170974731445\n",
      "h: 36 | epoch: 39, train loss: 12.31618881225586, test loss: 9.731392860412598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 36 | epoch: 40, train loss: 12.082982063293457, test loss: 9.505861282348633\n",
      "h: 36 | epoch: 41, train loss: 11.863567352294922, test loss: 9.292877197265625\n",
      "h: 36 | epoch: 42, train loss: 11.657183647155762, test loss: 9.091772079467773\n",
      "h: 36 | epoch: 43, train loss: 11.463115692138672, test loss: 8.901911735534668\n",
      "h: 36 | epoch: 44, train loss: 11.28067684173584, test loss: 8.722685813903809\n",
      "h: 36 | epoch: 45, train loss: 11.109217643737793, test loss: 8.553525924682617\n",
      "h: 36 | epoch: 46, train loss: 10.948121070861816, test loss: 8.393882751464844\n",
      "h: 36 | epoch: 47, train loss: 10.796797752380371, test loss: 8.243237495422363\n",
      "h: 36 | epoch: 48, train loss: 10.654696464538574, test loss: 8.101096153259277\n",
      "h: 36 | epoch: 49, train loss: 10.521282196044922, test loss: 7.9669904708862305\n",
      "h: 36 | epoch: 50, train loss: 10.39605712890625, test loss: 7.840479373931885\n",
      "h: 36 | epoch: 51, train loss: 10.278547286987305, test loss: 7.721137046813965\n",
      "h: 36 | epoch: 52, train loss: 10.168301582336426, test loss: 7.6085662841796875\n",
      "h: 36 | epoch: 53, train loss: 10.06489372253418, test loss: 7.502390384674072\n",
      "h: 36 | epoch: 54, train loss: 9.967920303344727, test loss: 7.402245998382568\n",
      "h: 36 | epoch: 55, train loss: 9.87700080871582, test loss: 7.30779504776001\n",
      "h: 36 | epoch: 56, train loss: 9.791776657104492, test loss: 7.218716621398926\n",
      "h: 36 | epoch: 57, train loss: 9.711904525756836, test loss: 7.134708404541016\n",
      "h: 36 | epoch: 58, train loss: 9.637062072753906, test loss: 7.055478572845459\n",
      "h: 36 | epoch: 59, train loss: 9.566946029663086, test loss: 6.9807562828063965\n",
      "h: 36 | epoch: 60, train loss: 9.50127124786377, test loss: 6.910286903381348\n",
      "h: 36 | epoch: 61, train loss: 9.439764022827148, test loss: 6.8438215255737305\n",
      "h: 36 | epoch: 62, train loss: 9.382170677185059, test loss: 6.781134605407715\n",
      "h: 36 | epoch: 63, train loss: 9.328250885009766, test loss: 6.722006797790527\n",
      "h: 36 | epoch: 64, train loss: 9.277776718139648, test loss: 6.666236877441406\n",
      "h: 36 | epoch: 65, train loss: 9.230537414550781, test loss: 6.613628387451172\n",
      "h: 36 | epoch: 66, train loss: 9.18632698059082, test loss: 6.5639967918396\n",
      "h: 36 | epoch: 67, train loss: 9.1449613571167, test loss: 6.517173767089844\n",
      "h: 36 | epoch: 68, train loss: 9.106260299682617, test loss: 6.472996711730957\n",
      "h: 36 | epoch: 69, train loss: 9.07005500793457, test loss: 6.431311130523682\n",
      "h: 36 | epoch: 70, train loss: 9.0361909866333, test loss: 6.391973495483398\n",
      "h: 36 | epoch: 71, train loss: 9.004518508911133, test loss: 6.354846000671387\n",
      "h: 36 | epoch: 72, train loss: 8.974897384643555, test loss: 6.3198041915893555\n",
      "h: 36 | epoch: 73, train loss: 8.947198867797852, test loss: 6.2867231369018555\n",
      "h: 36 | epoch: 74, train loss: 8.921302795410156, test loss: 6.255491256713867\n",
      "h: 36 | epoch: 75, train loss: 8.897088050842285, test loss: 6.226001739501953\n",
      "h: 36 | epoch: 76, train loss: 8.874452590942383, test loss: 6.198153018951416\n",
      "h: 36 | epoch: 77, train loss: 8.853292465209961, test loss: 6.171849250793457\n",
      "h: 36 | epoch: 78, train loss: 8.833512306213379, test loss: 6.147003173828125\n",
      "h: 36 | epoch: 79, train loss: 8.815023422241211, test loss: 6.12352991104126\n",
      "h: 36 | epoch: 80, train loss: 8.79774284362793, test loss: 6.101347923278809\n",
      "h: 36 | epoch: 81, train loss: 8.78159236907959, test loss: 6.080386638641357\n",
      "h: 36 | epoch: 82, train loss: 8.766498565673828, test loss: 6.06057071685791\n",
      "h: 36 | epoch: 83, train loss: 8.75239372253418, test loss: 6.041839599609375\n",
      "h: 36 | epoch: 84, train loss: 8.739212036132812, test loss: 6.024129390716553\n",
      "h: 36 | epoch: 85, train loss: 8.726893424987793, test loss: 6.007378578186035\n",
      "h: 36 | epoch: 86, train loss: 8.715381622314453, test loss: 5.991536617279053\n",
      "h: 36 | epoch: 87, train loss: 8.704626083374023, test loss: 5.976547718048096\n",
      "h: 36 | epoch: 88, train loss: 8.694572448730469, test loss: 5.962367057800293\n",
      "h: 36 | epoch: 89, train loss: 8.6851806640625, test loss: 5.948945045471191\n",
      "h: 36 | epoch: 90, train loss: 8.676403999328613, test loss: 5.936241149902344\n",
      "h: 36 | epoch: 91, train loss: 8.66820240020752, test loss: 5.924213886260986\n",
      "h: 36 | epoch: 92, train loss: 8.660537719726562, test loss: 5.912825584411621\n",
      "h: 36 | epoch: 93, train loss: 8.653376579284668, test loss: 5.902039051055908\n",
      "h: 36 | epoch: 94, train loss: 8.646683692932129, test loss: 5.89182186126709\n",
      "h: 36 | epoch: 95, train loss: 8.640429496765137, test loss: 5.882140636444092\n",
      "h: 36 | epoch: 96, train loss: 8.634584426879883, test loss: 5.872967720031738\n",
      "h: 36 | epoch: 97, train loss: 8.629121780395508, test loss: 5.8642730712890625\n",
      "h: 36 | epoch: 98, train loss: 8.624016761779785, test loss: 5.856030464172363\n",
      "h: 36 | epoch: 99, train loss: 8.619245529174805, test loss: 5.8482160568237305\n",
      "h: 37 | epoch: 0, train loss: 44.722312927246094, test loss: 38.45491409301758\n",
      "h: 37 | epoch: 1, train loss: 42.417137145996094, test loss: 36.45896530151367\n",
      "h: 37 | epoch: 2, train loss: 40.258094787597656, test loss: 34.58559799194336\n",
      "h: 37 | epoch: 3, train loss: 38.23495864868164, test loss: 32.82649612426758\n",
      "h: 37 | epoch: 4, train loss: 36.33842086791992, test loss: 31.1740779876709\n",
      "h: 37 | epoch: 5, train loss: 34.55999755859375, test loss: 29.621402740478516\n",
      "h: 37 | epoch: 6, train loss: 32.89191436767578, test loss: 28.162099838256836\n",
      "h: 37 | epoch: 7, train loss: 31.327056884765625, test loss: 26.79030418395996\n",
      "h: 37 | epoch: 8, train loss: 29.85884666442871, test loss: 25.50060272216797\n",
      "h: 37 | epoch: 9, train loss: 28.481237411499023, test loss: 24.28798484802246\n",
      "h: 37 | epoch: 10, train loss: 27.188610076904297, test loss: 23.147794723510742\n",
      "h: 37 | epoch: 11, train loss: 25.975765228271484, test loss: 22.075706481933594\n",
      "h: 37 | epoch: 12, train loss: 24.837862014770508, test loss: 21.06768035888672\n",
      "h: 37 | epoch: 13, train loss: 23.770381927490234, test loss: 20.11995506286621\n",
      "h: 37 | epoch: 14, train loss: 22.76911163330078, test loss: 19.229000091552734\n",
      "h: 37 | epoch: 15, train loss: 21.830106735229492, test loss: 18.39150619506836\n",
      "h: 37 | epoch: 16, train loss: 20.949676513671875, test loss: 17.6043758392334\n",
      "h: 37 | epoch: 17, train loss: 20.12434959411621, test loss: 16.86469078063965\n",
      "h: 37 | epoch: 18, train loss: 19.35086441040039, test loss: 16.169702529907227\n",
      "h: 37 | epoch: 19, train loss: 18.626157760620117, test loss: 15.516828536987305\n",
      "h: 37 | epoch: 20, train loss: 17.947345733642578, test loss: 14.903635025024414\n",
      "h: 37 | epoch: 21, train loss: 17.311702728271484, test loss: 14.32781982421875\n",
      "h: 37 | epoch: 22, train loss: 16.716672897338867, test loss: 13.787214279174805\n",
      "h: 37 | epoch: 23, train loss: 16.159835815429688, test loss: 13.279769897460938\n",
      "h: 37 | epoch: 24, train loss: 15.638906478881836, test loss: 12.803548812866211\n",
      "h: 37 | epoch: 25, train loss: 15.15173053741455, test loss: 12.356727600097656\n",
      "h: 37 | epoch: 26, train loss: 14.69627571105957, test loss: 11.937578201293945\n",
      "h: 37 | epoch: 27, train loss: 14.27061939239502, test loss: 11.544462203979492\n",
      "h: 37 | epoch: 28, train loss: 13.872949600219727, test loss: 11.175841331481934\n",
      "h: 37 | epoch: 29, train loss: 13.501553535461426, test loss: 10.830259323120117\n",
      "h: 37 | epoch: 30, train loss: 13.154813766479492, test loss: 10.506333351135254\n",
      "h: 37 | epoch: 31, train loss: 12.831204414367676, test loss: 10.20276927947998\n",
      "h: 37 | epoch: 32, train loss: 12.529287338256836, test loss: 9.918331146240234\n",
      "h: 37 | epoch: 33, train loss: 12.247705459594727, test loss: 9.651864051818848\n",
      "h: 37 | epoch: 34, train loss: 11.985172271728516, test loss: 9.402270317077637\n",
      "h: 37 | epoch: 35, train loss: 11.740484237670898, test loss: 9.1685209274292\n",
      "h: 37 | epoch: 36, train loss: 11.512502670288086, test loss: 8.949636459350586\n",
      "h: 37 | epoch: 37, train loss: 11.300158500671387, test loss: 8.74470329284668\n",
      "h: 37 | epoch: 38, train loss: 11.102437973022461, test loss: 8.552850723266602\n",
      "h: 37 | epoch: 39, train loss: 10.918395042419434, test loss: 8.373266220092773\n",
      "h: 37 | epoch: 40, train loss: 10.747136116027832, test loss: 8.205182075500488\n",
      "h: 37 | epoch: 41, train loss: 10.587820053100586, test loss: 8.047873497009277\n",
      "h: 37 | epoch: 42, train loss: 10.439659118652344, test loss: 7.9006667137146\n",
      "h: 37 | epoch: 43, train loss: 10.301913261413574, test loss: 7.762912750244141\n",
      "h: 37 | epoch: 44, train loss: 10.173884391784668, test loss: 7.634015083312988\n",
      "h: 37 | epoch: 45, train loss: 10.05492115020752, test loss: 7.513409614562988\n",
      "h: 37 | epoch: 46, train loss: 9.944413185119629, test loss: 7.400566101074219\n",
      "h: 37 | epoch: 47, train loss: 9.841785430908203, test loss: 7.29498291015625\n",
      "h: 37 | epoch: 48, train loss: 9.746498107910156, test loss: 7.196194648742676\n",
      "h: 37 | epoch: 49, train loss: 9.658052444458008, test loss: 7.103762626647949\n",
      "h: 37 | epoch: 50, train loss: 9.575974464416504, test loss: 7.017276763916016\n",
      "h: 37 | epoch: 51, train loss: 9.499823570251465, test loss: 6.936347961425781\n",
      "h: 37 | epoch: 52, train loss: 9.429190635681152, test loss: 6.860617637634277\n",
      "h: 37 | epoch: 53, train loss: 9.363687515258789, test loss: 6.789747714996338\n",
      "h: 37 | epoch: 54, train loss: 9.302956581115723, test loss: 6.72341775894165\n",
      "h: 37 | epoch: 55, train loss: 9.246660232543945, test loss: 6.661336421966553\n",
      "h: 37 | epoch: 56, train loss: 9.194486618041992, test loss: 6.6032209396362305\n",
      "h: 37 | epoch: 57, train loss: 9.146143913269043, test loss: 6.5488152503967285\n",
      "h: 37 | epoch: 58, train loss: 9.101358413696289, test loss: 6.497872829437256\n",
      "h: 37 | epoch: 59, train loss: 9.059876441955566, test loss: 6.450168609619141\n",
      "h: 37 | epoch: 60, train loss: 9.021458625793457, test loss: 6.405489921569824\n",
      "h: 37 | epoch: 61, train loss: 8.98588752746582, test loss: 6.363638401031494\n",
      "h: 37 | epoch: 62, train loss: 8.952957153320312, test loss: 6.324427604675293\n",
      "h: 37 | epoch: 63, train loss: 8.922475814819336, test loss: 6.28768253326416\n",
      "h: 37 | epoch: 64, train loss: 8.894266128540039, test loss: 6.253247261047363\n",
      "h: 37 | epoch: 65, train loss: 8.868162155151367, test loss: 6.220964431762695\n",
      "h: 37 | epoch: 66, train loss: 8.844011306762695, test loss: 6.190695762634277\n",
      "h: 37 | epoch: 67, train loss: 8.82166862487793, test loss: 6.162309646606445\n",
      "h: 37 | epoch: 68, train loss: 8.801003456115723, test loss: 6.135681629180908\n",
      "h: 37 | epoch: 69, train loss: 8.781889915466309, test loss: 6.1106977462768555\n",
      "h: 37 | epoch: 70, train loss: 8.764216423034668, test loss: 6.087249755859375\n",
      "h: 37 | epoch: 71, train loss: 8.747873306274414, test loss: 6.0652384757995605\n",
      "h: 37 | epoch: 72, train loss: 8.732765197753906, test loss: 6.044571876525879\n",
      "h: 37 | epoch: 73, train loss: 8.71879768371582, test loss: 6.025158882141113\n",
      "h: 37 | epoch: 74, train loss: 8.705886840820312, test loss: 6.006923198699951\n",
      "h: 37 | epoch: 75, train loss: 8.693955421447754, test loss: 5.989784240722656\n",
      "h: 37 | epoch: 76, train loss: 8.682928085327148, test loss: 5.97367525100708\n",
      "h: 37 | epoch: 77, train loss: 8.672736167907715, test loss: 5.9585280418396\n",
      "h: 37 | epoch: 78, train loss: 8.663320541381836, test loss: 5.944280624389648\n",
      "h: 37 | epoch: 79, train loss: 8.654622077941895, test loss: 5.930877208709717\n",
      "h: 37 | epoch: 80, train loss: 8.646584510803223, test loss: 5.918262958526611\n",
      "h: 37 | epoch: 81, train loss: 8.639158248901367, test loss: 5.906388282775879\n",
      "h: 37 | epoch: 82, train loss: 8.632301330566406, test loss: 5.895206928253174\n",
      "h: 37 | epoch: 83, train loss: 8.625967025756836, test loss: 5.884672164916992\n",
      "h: 37 | epoch: 84, train loss: 8.620115280151367, test loss: 5.874746799468994\n",
      "h: 37 | epoch: 85, train loss: 8.614709854125977, test loss: 5.865392208099365\n",
      "h: 37 | epoch: 86, train loss: 8.609720230102539, test loss: 5.856571674346924\n",
      "h: 37 | epoch: 87, train loss: 8.605111122131348, test loss: 5.8482537269592285\n",
      "h: 37 | epoch: 88, train loss: 8.600854873657227, test loss: 5.840405464172363\n",
      "h: 37 | epoch: 89, train loss: 8.596925735473633, test loss: 5.832999229431152\n",
      "h: 37 | epoch: 90, train loss: 8.593297958374023, test loss: 5.826007843017578\n",
      "h: 37 | epoch: 91, train loss: 8.589946746826172, test loss: 5.819406032562256\n",
      "h: 37 | epoch: 92, train loss: 8.586853981018066, test loss: 5.813170433044434\n",
      "h: 37 | epoch: 93, train loss: 8.58399772644043, test loss: 5.807276248931885\n",
      "h: 37 | epoch: 94, train loss: 8.581361770629883, test loss: 5.801707744598389\n",
      "h: 37 | epoch: 95, train loss: 8.578927040100098, test loss: 5.796442031860352\n",
      "h: 37 | epoch: 96, train loss: 8.576680183410645, test loss: 5.791462421417236\n",
      "h: 37 | epoch: 97, train loss: 8.574604988098145, test loss: 5.786751747131348\n",
      "h: 37 | epoch: 98, train loss: 8.5726900100708, test loss: 5.782293319702148\n",
      "h: 37 | epoch: 99, train loss: 8.570921897888184, test loss: 5.778075218200684\n",
      "h: 38 | epoch: 0, train loss: 44.36011505126953, test loss: 40.00690460205078\n",
      "h: 38 | epoch: 1, train loss: 42.450950622558594, test loss: 38.257911682128906\n",
      "h: 38 | epoch: 2, train loss: 40.642818450927734, test loss: 36.598793029785156\n",
      "h: 38 | epoch: 3, train loss: 38.9295539855957, test loss: 35.02423858642578\n",
      "h: 38 | epoch: 4, train loss: 37.305519104003906, test loss: 33.52933883666992\n",
      "h: 38 | epoch: 5, train loss: 35.76554489135742, test loss: 32.109615325927734\n",
      "h: 38 | epoch: 6, train loss: 34.304866790771484, test loss: 30.760921478271484\n",
      "h: 38 | epoch: 7, train loss: 32.91909408569336, test loss: 29.47941017150879\n",
      "h: 38 | epoch: 8, train loss: 31.604171752929688, test loss: 28.261524200439453\n",
      "h: 38 | epoch: 9, train loss: 30.3563175201416, test loss: 27.103952407836914\n",
      "h: 38 | epoch: 10, train loss: 29.172027587890625, test loss: 26.003616333007812\n",
      "h: 38 | epoch: 11, train loss: 28.048025131225586, test loss: 24.957622528076172\n",
      "h: 38 | epoch: 12, train loss: 26.981252670288086, test loss: 23.963272094726562\n",
      "h: 38 | epoch: 13, train loss: 25.96884536743164, test loss: 23.01802635192871\n",
      "h: 38 | epoch: 14, train loss: 25.008098602294922, test loss: 22.119504928588867\n",
      "h: 38 | epoch: 15, train loss: 24.096481323242188, test loss: 21.265443801879883\n",
      "h: 38 | epoch: 16, train loss: 23.23160171508789, test loss: 20.453725814819336\n",
      "h: 38 | epoch: 17, train loss: 22.411197662353516, test loss: 19.682334899902344\n",
      "h: 38 | epoch: 18, train loss: 21.63312339782715, test loss: 18.949365615844727\n",
      "h: 38 | epoch: 19, train loss: 20.89535140991211, test loss: 18.253005981445312\n",
      "h: 38 | epoch: 20, train loss: 20.195951461791992, test loss: 17.591537475585938\n",
      "h: 38 | epoch: 21, train loss: 19.533092498779297, test loss: 16.963319778442383\n",
      "h: 38 | epoch: 22, train loss: 18.905033111572266, test loss: 16.36680030822754\n",
      "h: 38 | epoch: 23, train loss: 18.310100555419922, test loss: 15.800480842590332\n",
      "h: 38 | epoch: 24, train loss: 17.746715545654297, test loss: 15.262948989868164\n",
      "h: 38 | epoch: 25, train loss: 17.213361740112305, test loss: 14.752845764160156\n",
      "h: 38 | epoch: 26, train loss: 16.708595275878906, test loss: 14.268880844116211\n",
      "h: 38 | epoch: 27, train loss: 16.231035232543945, test loss: 13.80981731414795\n",
      "h: 38 | epoch: 28, train loss: 15.779359817504883, test loss: 13.37446403503418\n",
      "h: 38 | epoch: 29, train loss: 15.352304458618164, test loss: 12.961694717407227\n",
      "h: 38 | epoch: 30, train loss: 14.94865894317627, test loss: 12.57042407989502\n",
      "h: 38 | epoch: 31, train loss: 14.56727409362793, test loss: 12.199615478515625\n",
      "h: 38 | epoch: 32, train loss: 14.207043647766113, test loss: 11.84827709197998\n",
      "h: 38 | epoch: 33, train loss: 13.866907119750977, test loss: 11.515460968017578\n",
      "h: 38 | epoch: 34, train loss: 13.545854568481445, test loss: 11.200258255004883\n",
      "h: 38 | epoch: 35, train loss: 13.242918014526367, test loss: 10.901800155639648\n",
      "h: 38 | epoch: 36, train loss: 12.957171440124512, test loss: 10.619257926940918\n",
      "h: 38 | epoch: 37, train loss: 12.687735557556152, test loss: 10.351835250854492\n",
      "h: 38 | epoch: 38, train loss: 12.433762550354004, test loss: 10.098773956298828\n",
      "h: 38 | epoch: 39, train loss: 12.19444465637207, test loss: 9.859347343444824\n",
      "h: 38 | epoch: 40, train loss: 11.969012260437012, test loss: 9.63286304473877\n",
      "h: 38 | epoch: 41, train loss: 11.756731033325195, test loss: 9.418660163879395\n",
      "h: 38 | epoch: 42, train loss: 11.556899070739746, test loss: 9.216107368469238\n",
      "h: 38 | epoch: 43, train loss: 11.368844985961914, test loss: 9.0246000289917\n",
      "h: 38 | epoch: 44, train loss: 11.191932678222656, test loss: 8.843564987182617\n",
      "h: 38 | epoch: 45, train loss: 11.025555610656738, test loss: 8.672452926635742\n",
      "h: 38 | epoch: 46, train loss: 10.869132995605469, test loss: 8.5107421875\n",
      "h: 38 | epoch: 47, train loss: 10.722115516662598, test loss: 8.357934951782227\n",
      "h: 38 | epoch: 48, train loss: 10.583976745605469, test loss: 8.213560104370117\n",
      "h: 38 | epoch: 49, train loss: 10.454221725463867, test loss: 8.077162742614746\n",
      "h: 38 | epoch: 50, train loss: 10.332374572753906, test loss: 7.948319435119629\n",
      "h: 38 | epoch: 51, train loss: 10.217988967895508, test loss: 7.826615810394287\n",
      "h: 38 | epoch: 52, train loss: 10.110635757446289, test loss: 7.711667537689209\n",
      "h: 38 | epoch: 53, train loss: 10.009908676147461, test loss: 7.603109836578369\n",
      "h: 38 | epoch: 54, train loss: 9.915427207946777, test loss: 7.5005903244018555\n",
      "h: 38 | epoch: 55, train loss: 9.826826095581055, test loss: 7.403779029846191\n",
      "h: 38 | epoch: 56, train loss: 9.74376106262207, test loss: 7.312357425689697\n",
      "h: 38 | epoch: 57, train loss: 9.665903091430664, test loss: 7.2260332107543945\n",
      "h: 38 | epoch: 58, train loss: 9.592945098876953, test loss: 7.1445207595825195\n",
      "h: 38 | epoch: 59, train loss: 9.524595260620117, test loss: 7.0675506591796875\n",
      "h: 38 | epoch: 60, train loss: 9.460579872131348, test loss: 6.994870662689209\n",
      "h: 38 | epoch: 61, train loss: 9.400633811950684, test loss: 6.92624044418335\n",
      "h: 38 | epoch: 62, train loss: 9.344512939453125, test loss: 6.861433982849121\n",
      "h: 38 | epoch: 63, train loss: 9.291982650756836, test loss: 6.800233364105225\n",
      "h: 38 | epoch: 64, train loss: 9.24282455444336, test loss: 6.742436408996582\n",
      "h: 38 | epoch: 65, train loss: 9.196832656860352, test loss: 6.687849998474121\n",
      "h: 38 | epoch: 66, train loss: 9.153809547424316, test loss: 6.636294364929199\n",
      "h: 38 | epoch: 67, train loss: 9.11357307434082, test loss: 6.587596893310547\n",
      "h: 38 | epoch: 68, train loss: 9.075948715209961, test loss: 6.541592597961426\n",
      "h: 38 | epoch: 69, train loss: 9.040775299072266, test loss: 6.498132228851318\n",
      "h: 38 | epoch: 70, train loss: 9.007896423339844, test loss: 6.457070350646973\n",
      "h: 38 | epoch: 71, train loss: 8.977168083190918, test loss: 6.41826868057251\n",
      "h: 38 | epoch: 72, train loss: 8.948456764221191, test loss: 6.381596565246582\n",
      "h: 38 | epoch: 73, train loss: 8.921631813049316, test loss: 6.3469367027282715\n",
      "h: 38 | epoch: 74, train loss: 8.896574020385742, test loss: 6.31417179107666\n",
      "h: 38 | epoch: 75, train loss: 8.873170852661133, test loss: 6.283194065093994\n",
      "h: 38 | epoch: 76, train loss: 8.851316452026367, test loss: 6.253901958465576\n",
      "h: 38 | epoch: 77, train loss: 8.830911636352539, test loss: 6.226200103759766\n",
      "h: 38 | epoch: 78, train loss: 8.811861038208008, test loss: 6.1999969482421875\n",
      "h: 38 | epoch: 79, train loss: 8.79408073425293, test loss: 6.175207138061523\n",
      "h: 38 | epoch: 80, train loss: 8.777482986450195, test loss: 6.151748180389404\n",
      "h: 38 | epoch: 81, train loss: 8.761996269226074, test loss: 6.129547595977783\n",
      "h: 38 | epoch: 82, train loss: 8.74754524230957, test loss: 6.108532905578613\n",
      "h: 38 | epoch: 83, train loss: 8.734063148498535, test loss: 6.08863639831543\n",
      "h: 38 | epoch: 84, train loss: 8.721485137939453, test loss: 6.069793701171875\n",
      "h: 38 | epoch: 85, train loss: 8.709753036499023, test loss: 6.051947593688965\n",
      "h: 38 | epoch: 86, train loss: 8.698812484741211, test loss: 6.03503942489624\n",
      "h: 38 | epoch: 87, train loss: 8.688608169555664, test loss: 6.019018650054932\n",
      "h: 38 | epoch: 88, train loss: 8.679093360900879, test loss: 6.00383186340332\n",
      "h: 38 | epoch: 89, train loss: 8.670222282409668, test loss: 5.989437103271484\n",
      "h: 38 | epoch: 90, train loss: 8.661951065063477, test loss: 5.975786209106445\n",
      "h: 38 | epoch: 91, train loss: 8.654241561889648, test loss: 5.962838172912598\n",
      "h: 38 | epoch: 92, train loss: 8.647054672241211, test loss: 5.950554847717285\n",
      "h: 38 | epoch: 93, train loss: 8.64035701751709, test loss: 5.938899040222168\n",
      "h: 38 | epoch: 94, train loss: 8.634115219116211, test loss: 5.927836894989014\n",
      "h: 38 | epoch: 95, train loss: 8.628296852111816, test loss: 5.917333602905273\n",
      "h: 38 | epoch: 96, train loss: 8.622876167297363, test loss: 5.907359600067139\n",
      "h: 38 | epoch: 97, train loss: 8.617826461791992, test loss: 5.897885322570801\n",
      "h: 38 | epoch: 98, train loss: 8.613120079040527, test loss: 5.888884544372559\n",
      "h: 38 | epoch: 99, train loss: 8.608736991882324, test loss: 5.880329608917236\n",
      "h: 39 | epoch: 0, train loss: 40.92704772949219, test loss: 35.43019104003906\n",
      "h: 39 | epoch: 1, train loss: 39.32168197631836, test loss: 33.98432159423828\n",
      "h: 39 | epoch: 2, train loss: 37.801788330078125, test loss: 32.614871978759766\n",
      "h: 39 | epoch: 3, train loss: 36.36196517944336, test loss: 31.317052841186523\n",
      "h: 39 | epoch: 4, train loss: 34.99727249145508, test loss: 30.086475372314453\n",
      "h: 39 | epoch: 5, train loss: 33.703147888183594, test loss: 28.919113159179688\n",
      "h: 39 | epoch: 6, train loss: 32.47540283203125, test loss: 27.81122398376465\n",
      "h: 39 | epoch: 7, train loss: 31.31015968322754, test loss: 26.759347915649414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 39 | epoch: 8, train loss: 30.203838348388672, test loss: 25.760284423828125\n",
      "h: 39 | epoch: 9, train loss: 29.153106689453125, test loss: 24.81106185913086\n",
      "h: 39 | epoch: 10, train loss: 28.154882431030273, test loss: 23.90890884399414\n",
      "h: 39 | epoch: 11, train loss: 27.206287384033203, test loss: 23.051244735717773\n",
      "h: 39 | epoch: 12, train loss: 26.30463218688965, test loss: 22.235668182373047\n",
      "h: 39 | epoch: 13, train loss: 25.447423934936523, test loss: 21.45992660522461\n",
      "h: 39 | epoch: 14, train loss: 24.63231658935547, test loss: 20.721904754638672\n",
      "h: 39 | epoch: 15, train loss: 23.857112884521484, test loss: 20.019636154174805\n",
      "h: 39 | epoch: 16, train loss: 23.119747161865234, test loss: 19.351259231567383\n",
      "h: 39 | epoch: 17, train loss: 22.418285369873047, test loss: 18.715038299560547\n",
      "h: 39 | epoch: 18, train loss: 21.75090980529785, test loss: 18.109329223632812\n",
      "h: 39 | epoch: 19, train loss: 21.11590003967285, test loss: 17.53258514404297\n",
      "h: 39 | epoch: 20, train loss: 20.511634826660156, test loss: 16.983356475830078\n",
      "h: 39 | epoch: 21, train loss: 19.936588287353516, test loss: 16.460254669189453\n",
      "h: 39 | epoch: 22, train loss: 19.38931655883789, test loss: 15.961993217468262\n",
      "h: 39 | epoch: 23, train loss: 18.86844825744629, test loss: 15.487342834472656\n",
      "h: 39 | epoch: 24, train loss: 18.372695922851562, test loss: 15.035131454467773\n",
      "h: 39 | epoch: 25, train loss: 17.900829315185547, test loss: 14.604273796081543\n",
      "h: 39 | epoch: 26, train loss: 17.451690673828125, test loss: 14.193720817565918\n",
      "h: 39 | epoch: 27, train loss: 17.0241756439209, test loss: 13.80248737335205\n",
      "h: 39 | epoch: 28, train loss: 16.617237091064453, test loss: 13.429636001586914\n",
      "h: 39 | epoch: 29, train loss: 16.229883193969727, test loss: 13.0742769241333\n",
      "h: 39 | epoch: 30, train loss: 15.861169815063477, test loss: 12.73557186126709\n",
      "h: 39 | epoch: 31, train loss: 15.5101957321167, test loss: 12.41271686553955\n",
      "h: 39 | epoch: 32, train loss: 15.176114082336426, test loss: 12.104952812194824\n",
      "h: 39 | epoch: 33, train loss: 14.858110427856445, test loss: 11.81155776977539\n",
      "h: 39 | epoch: 34, train loss: 14.555412292480469, test loss: 11.531843185424805\n",
      "h: 39 | epoch: 35, train loss: 14.26728630065918, test loss: 11.26516056060791\n",
      "h: 39 | epoch: 36, train loss: 13.993034362792969, test loss: 11.010884284973145\n",
      "h: 39 | epoch: 37, train loss: 13.73198413848877, test loss: 10.768428802490234\n",
      "h: 39 | epoch: 38, train loss: 13.483510971069336, test loss: 10.5372314453125\n",
      "h: 39 | epoch: 39, train loss: 13.247004508972168, test loss: 10.316758155822754\n",
      "h: 39 | epoch: 40, train loss: 13.021894454956055, test loss: 10.106497764587402\n",
      "h: 39 | epoch: 41, train loss: 12.807632446289062, test loss: 9.905973434448242\n",
      "h: 39 | epoch: 42, train loss: 12.603693008422852, test loss: 9.714720726013184\n",
      "h: 39 | epoch: 43, train loss: 12.409585952758789, test loss: 9.53230094909668\n",
      "h: 39 | epoch: 44, train loss: 12.224835395812988, test loss: 9.358301162719727\n",
      "h: 39 | epoch: 45, train loss: 12.048989295959473, test loss: 9.192320823669434\n",
      "h: 39 | epoch: 46, train loss: 11.881622314453125, test loss: 9.033985137939453\n",
      "h: 39 | epoch: 47, train loss: 11.722322463989258, test loss: 8.882933616638184\n",
      "h: 39 | epoch: 48, train loss: 11.570703506469727, test loss: 8.738821983337402\n",
      "h: 39 | epoch: 49, train loss: 11.426392555236816, test loss: 8.601327896118164\n",
      "h: 39 | epoch: 50, train loss: 11.289037704467773, test loss: 8.47014045715332\n",
      "h: 39 | epoch: 51, train loss: 11.158303260803223, test loss: 8.344959259033203\n",
      "h: 39 | epoch: 52, train loss: 11.033868789672852, test loss: 8.225510597229004\n",
      "h: 39 | epoch: 53, train loss: 10.915430068969727, test loss: 8.11152172088623\n",
      "h: 39 | epoch: 54, train loss: 10.80269718170166, test loss: 8.002740859985352\n",
      "h: 39 | epoch: 55, train loss: 10.695390701293945, test loss: 7.89892053604126\n",
      "h: 39 | epoch: 56, train loss: 10.593252182006836, test loss: 7.799835205078125\n",
      "h: 39 | epoch: 57, train loss: 10.496030807495117, test loss: 7.70525598526001\n",
      "h: 39 | epoch: 58, train loss: 10.403486251831055, test loss: 7.614983558654785\n",
      "h: 39 | epoch: 59, train loss: 10.315393447875977, test loss: 7.528811454772949\n",
      "h: 39 | epoch: 60, train loss: 10.231534957885742, test loss: 7.446549892425537\n",
      "h: 39 | epoch: 61, train loss: 10.15170669555664, test loss: 7.368020057678223\n",
      "h: 39 | epoch: 62, train loss: 10.075712203979492, test loss: 7.293047904968262\n",
      "h: 39 | epoch: 63, train loss: 10.00336742401123, test loss: 7.221469879150391\n",
      "h: 39 | epoch: 64, train loss: 9.934494018554688, test loss: 7.1531267166137695\n",
      "h: 39 | epoch: 65, train loss: 9.868924140930176, test loss: 7.0878705978393555\n",
      "h: 39 | epoch: 66, train loss: 9.806497573852539, test loss: 7.025562286376953\n",
      "h: 39 | epoch: 67, train loss: 9.747060775756836, test loss: 6.966062068939209\n",
      "h: 39 | epoch: 68, train loss: 9.690470695495605, test loss: 6.909243106842041\n",
      "h: 39 | epoch: 69, train loss: 9.636589050292969, test loss: 6.854981899261475\n",
      "h: 39 | epoch: 70, train loss: 9.585283279418945, test loss: 6.803159236907959\n",
      "h: 39 | epoch: 71, train loss: 9.536428451538086, test loss: 6.753668308258057\n",
      "h: 39 | epoch: 72, train loss: 9.489909172058105, test loss: 6.706398963928223\n",
      "h: 39 | epoch: 73, train loss: 9.445609092712402, test loss: 6.66124963760376\n",
      "h: 39 | epoch: 74, train loss: 9.403421401977539, test loss: 6.618125915527344\n",
      "h: 39 | epoch: 75, train loss: 9.363245964050293, test loss: 6.57693338394165\n",
      "h: 39 | epoch: 76, train loss: 9.324982643127441, test loss: 6.537586212158203\n",
      "h: 39 | epoch: 77, train loss: 9.288541793823242, test loss: 6.499999046325684\n",
      "h: 39 | epoch: 78, train loss: 9.25383472442627, test loss: 6.464093208312988\n",
      "h: 39 | epoch: 79, train loss: 9.220776557922363, test loss: 6.4297895431518555\n",
      "h: 39 | epoch: 80, train loss: 9.189288139343262, test loss: 6.3970208168029785\n",
      "h: 39 | epoch: 81, train loss: 9.159294128417969, test loss: 6.365714073181152\n",
      "h: 39 | epoch: 82, train loss: 9.130722999572754, test loss: 6.335803031921387\n",
      "h: 39 | epoch: 83, train loss: 9.103506088256836, test loss: 6.307226181030273\n",
      "h: 39 | epoch: 84, train loss: 9.077577590942383, test loss: 6.279923439025879\n",
      "h: 39 | epoch: 85, train loss: 9.052877426147461, test loss: 6.253838539123535\n",
      "h: 39 | epoch: 86, train loss: 9.02934455871582, test loss: 6.228911876678467\n",
      "h: 39 | epoch: 87, train loss: 9.00692367553711, test loss: 6.2050981521606445\n",
      "h: 39 | epoch: 88, train loss: 8.985562324523926, test loss: 6.182343006134033\n",
      "h: 39 | epoch: 89, train loss: 8.965208053588867, test loss: 6.160600185394287\n",
      "h: 39 | epoch: 90, train loss: 8.94581413269043, test loss: 6.139824390411377\n",
      "h: 39 | epoch: 91, train loss: 8.927334785461426, test loss: 6.119973182678223\n",
      "h: 39 | epoch: 92, train loss: 8.909724235534668, test loss: 6.101003646850586\n",
      "h: 39 | epoch: 93, train loss: 8.8929443359375, test loss: 6.082878112792969\n",
      "h: 39 | epoch: 94, train loss: 8.876952171325684, test loss: 6.06555700302124\n",
      "h: 39 | epoch: 95, train loss: 8.861711502075195, test loss: 6.049008369445801\n",
      "h: 39 | epoch: 96, train loss: 8.847187995910645, test loss: 6.0331926345825195\n",
      "h: 39 | epoch: 97, train loss: 8.833345413208008, test loss: 6.018080234527588\n",
      "h: 39 | epoch: 98, train loss: 8.820152282714844, test loss: 6.0036396980285645\n",
      "h: 39 | epoch: 99, train loss: 8.807577133178711, test loss: 5.989840507507324\n",
      "h: 40 | epoch: 0, train loss: 45.060791015625, test loss: 39.041236877441406\n",
      "h: 40 | epoch: 1, train loss: 42.674068450927734, test loss: 37.007484436035156\n",
      "h: 40 | epoch: 2, train loss: 40.44691848754883, test loss: 35.10420608520508\n",
      "h: 40 | epoch: 3, train loss: 38.367454528808594, test loss: 33.321983337402344\n",
      "h: 40 | epoch: 4, train loss: 36.42490768432617, test loss: 31.652252197265625\n",
      "h: 40 | epoch: 5, train loss: 34.6094970703125, test loss: 30.08722496032715\n",
      "h: 40 | epoch: 6, train loss: 32.91231155395508, test loss: 28.619762420654297\n",
      "h: 40 | epoch: 7, train loss: 31.325185775756836, test loss: 27.243343353271484\n",
      "h: 40 | epoch: 8, train loss: 29.84065818786621, test loss: 25.951955795288086\n",
      "h: 40 | epoch: 9, train loss: 28.451847076416016, test loss: 24.740070343017578\n",
      "h: 40 | epoch: 10, train loss: 27.152416229248047, test loss: 23.602575302124023\n",
      "h: 40 | epoch: 11, train loss: 25.9365234375, test loss: 22.534732818603516\n",
      "h: 40 | epoch: 12, train loss: 24.798742294311523, test loss: 21.53215980529785\n",
      "h: 40 | epoch: 13, train loss: 23.734045028686523, test loss: 20.590770721435547\n",
      "h: 40 | epoch: 14, train loss: 22.73776626586914, test loss: 19.70676612854004\n",
      "h: 40 | epoch: 15, train loss: 21.805545806884766, test loss: 18.876602172851562\n",
      "h: 40 | epoch: 16, train loss: 20.933330535888672, test loss: 18.096969604492188\n",
      "h: 40 | epoch: 17, train loss: 20.117341995239258, test loss: 17.364778518676758\n",
      "h: 40 | epoch: 18, train loss: 19.354034423828125, test loss: 16.67713165283203\n",
      "h: 40 | epoch: 19, train loss: 18.64009666442871, test loss: 16.03131675720215\n",
      "h: 40 | epoch: 20, train loss: 17.972431182861328, test loss: 15.424787521362305\n",
      "h: 40 | epoch: 21, train loss: 17.348125457763672, test loss: 14.855157852172852\n",
      "h: 40 | epoch: 22, train loss: 16.764461517333984, test loss: 14.320192337036133\n",
      "h: 40 | epoch: 23, train loss: 16.218883514404297, test loss: 13.817779541015625\n",
      "h: 40 | epoch: 24, train loss: 15.708986282348633, test loss: 13.345947265625\n",
      "h: 40 | epoch: 25, train loss: 15.232519149780273, test loss: 12.902830123901367\n",
      "h: 40 | epoch: 26, train loss: 14.787368774414062, test loss: 12.486685752868652\n",
      "h: 40 | epoch: 27, train loss: 14.371545791625977, test loss: 12.095865249633789\n",
      "h: 40 | epoch: 28, train loss: 13.983182907104492, test loss: 11.728827476501465\n",
      "h: 40 | epoch: 29, train loss: 13.62053108215332, test loss: 11.384115219116211\n",
      "h: 40 | epoch: 30, train loss: 13.281938552856445, test loss: 11.060361862182617\n",
      "h: 40 | epoch: 31, train loss: 12.965861320495605, test loss: 10.756280899047852\n",
      "h: 40 | epoch: 32, train loss: 12.670846939086914, test loss: 10.470663070678711\n",
      "h: 40 | epoch: 33, train loss: 12.395533561706543, test loss: 10.202369689941406\n",
      "h: 40 | epoch: 34, train loss: 12.13863754272461, test loss: 9.950337409973145\n",
      "h: 40 | epoch: 35, train loss: 11.89896297454834, test loss: 9.713558197021484\n",
      "h: 40 | epoch: 36, train loss: 11.675379753112793, test loss: 9.49108600616455\n",
      "h: 40 | epoch: 37, train loss: 11.46683120727539, test loss: 9.282036781311035\n",
      "h: 40 | epoch: 38, train loss: 11.27232837677002, test loss: 9.085572242736816\n",
      "h: 40 | epoch: 39, train loss: 11.0909423828125, test loss: 8.900917053222656\n",
      "h: 40 | epoch: 40, train loss: 10.921801567077637, test loss: 8.727333068847656\n",
      "h: 40 | epoch: 41, train loss: 10.764092445373535, test loss: 8.564129829406738\n",
      "h: 40 | epoch: 42, train loss: 10.61705207824707, test loss: 8.410660743713379\n",
      "h: 40 | epoch: 43, train loss: 10.479965209960938, test loss: 8.266321182250977\n",
      "h: 40 | epoch: 44, train loss: 10.352163314819336, test loss: 8.130535125732422\n",
      "h: 40 | epoch: 45, train loss: 10.23302173614502, test loss: 8.002777099609375\n",
      "h: 40 | epoch: 46, train loss: 10.121956825256348, test loss: 7.88253927230835\n",
      "h: 40 | epoch: 47, train loss: 10.01841926574707, test loss: 7.769354343414307\n",
      "h: 40 | epoch: 48, train loss: 9.921900749206543, test loss: 7.662783145904541\n",
      "h: 40 | epoch: 49, train loss: 9.831925392150879, test loss: 7.56241512298584\n",
      "h: 40 | epoch: 50, train loss: 9.748046875, test loss: 7.467860221862793\n",
      "h: 40 | epoch: 51, train loss: 9.66984748840332, test loss: 7.378757476806641\n",
      "h: 40 | epoch: 52, train loss: 9.596939086914062, test loss: 7.294770240783691\n",
      "h: 40 | epoch: 53, train loss: 9.528963088989258, test loss: 7.215578556060791\n",
      "h: 40 | epoch: 54, train loss: 9.465578079223633, test loss: 7.1408867835998535\n",
      "h: 40 | epoch: 55, train loss: 9.406472206115723, test loss: 7.070417881011963\n",
      "h: 40 | epoch: 56, train loss: 9.351347923278809, test loss: 7.003908634185791\n",
      "h: 40 | epoch: 57, train loss: 9.299934387207031, test loss: 6.941117286682129\n",
      "h: 40 | epoch: 58, train loss: 9.251975059509277, test loss: 6.881815433502197\n",
      "h: 40 | epoch: 59, train loss: 9.207234382629395, test loss: 6.825788974761963\n",
      "h: 40 | epoch: 60, train loss: 9.16549015045166, test loss: 6.7728376388549805\n",
      "h: 40 | epoch: 61, train loss: 9.126533508300781, test loss: 6.722775459289551\n",
      "h: 40 | epoch: 62, train loss: 9.090176582336426, test loss: 6.675427436828613\n",
      "h: 40 | epoch: 63, train loss: 9.056238174438477, test loss: 6.630629539489746\n",
      "h: 40 | epoch: 64, train loss: 9.024551391601562, test loss: 6.58822774887085\n",
      "h: 40 | epoch: 65, train loss: 8.994963645935059, test loss: 6.548077583312988\n",
      "h: 40 | epoch: 66, train loss: 8.967329025268555, test loss: 6.510047912597656\n",
      "h: 40 | epoch: 67, train loss: 8.941515922546387, test loss: 6.4740095138549805\n",
      "h: 40 | epoch: 68, train loss: 8.917396545410156, test loss: 6.439847469329834\n",
      "h: 40 | epoch: 69, train loss: 8.894856452941895, test loss: 6.407448768615723\n",
      "h: 40 | epoch: 70, train loss: 8.873786926269531, test loss: 6.376711845397949\n",
      "h: 40 | epoch: 71, train loss: 8.854087829589844, test loss: 6.347538948059082\n",
      "h: 40 | epoch: 72, train loss: 8.835665702819824, test loss: 6.319841384887695\n",
      "h: 40 | epoch: 73, train loss: 8.81843376159668, test loss: 6.293532371520996\n",
      "h: 40 | epoch: 74, train loss: 8.802311897277832, test loss: 6.268533229827881\n",
      "h: 40 | epoch: 75, train loss: 8.787221908569336, test loss: 6.244769096374512\n",
      "h: 40 | epoch: 76, train loss: 8.773098945617676, test loss: 6.222170829772949\n",
      "h: 40 | epoch: 77, train loss: 8.75987434387207, test loss: 6.200669765472412\n",
      "h: 40 | epoch: 78, train loss: 8.747488021850586, test loss: 6.180209159851074\n",
      "h: 40 | epoch: 79, train loss: 8.735882759094238, test loss: 6.160727500915527\n",
      "h: 40 | epoch: 80, train loss: 8.725008010864258, test loss: 6.142173767089844\n",
      "h: 40 | epoch: 81, train loss: 8.714815139770508, test loss: 6.124495506286621\n",
      "h: 40 | epoch: 82, train loss: 8.7052583694458, test loss: 6.1076436042785645\n",
      "h: 40 | epoch: 83, train loss: 8.696292877197266, test loss: 6.091575622558594\n",
      "h: 40 | epoch: 84, train loss: 8.687883377075195, test loss: 6.0762481689453125\n",
      "h: 40 | epoch: 85, train loss: 8.679991722106934, test loss: 6.06162166595459\n",
      "h: 40 | epoch: 86, train loss: 8.67258358001709, test loss: 6.047660827636719\n",
      "h: 40 | epoch: 87, train loss: 8.665627479553223, test loss: 6.034328460693359\n",
      "h: 40 | epoch: 88, train loss: 8.659093856811523, test loss: 6.021592140197754\n",
      "h: 40 | epoch: 89, train loss: 8.652955055236816, test loss: 6.009421348571777\n",
      "h: 40 | epoch: 90, train loss: 8.647184371948242, test loss: 5.997786521911621\n",
      "h: 40 | epoch: 91, train loss: 8.641761779785156, test loss: 5.986661911010742\n",
      "h: 40 | epoch: 92, train loss: 8.6366605758667, test loss: 5.976019859313965\n",
      "h: 40 | epoch: 93, train loss: 8.631863594055176, test loss: 5.965836048126221\n",
      "h: 40 | epoch: 94, train loss: 8.627349853515625, test loss: 5.956089019775391\n",
      "h: 40 | epoch: 95, train loss: 8.623100280761719, test loss: 5.9467549324035645\n",
      "h: 40 | epoch: 96, train loss: 8.619100570678711, test loss: 5.937814712524414\n",
      "h: 40 | epoch: 97, train loss: 8.615333557128906, test loss: 5.9292497634887695\n",
      "h: 40 | epoch: 98, train loss: 8.611783981323242, test loss: 5.9210405349731445\n",
      "h: 40 | epoch: 99, train loss: 8.608441352844238, test loss: 5.9131693840026855\n",
      "h: 41 | epoch: 0, train loss: 46.113006591796875, test loss: 40.752708435058594\n",
      "h: 41 | epoch: 1, train loss: 44.31182861328125, test loss: 39.15439224243164\n",
      "h: 41 | epoch: 2, train loss: 42.59716033935547, test loss: 37.629966735839844\n",
      "h: 41 | epoch: 3, train loss: 40.96393966674805, test loss: 36.175270080566406\n",
      "h: 41 | epoch: 4, train loss: 39.407569885253906, test loss: 34.78649139404297\n",
      "h: 41 | epoch: 5, train loss: 37.92381286621094, test loss: 33.46013641357422\n",
      "h: 41 | epoch: 6, train loss: 36.508792877197266, test loss: 32.19298553466797\n",
      "h: 41 | epoch: 7, train loss: 35.15892028808594, test loss: 30.982044219970703\n",
      "h: 41 | epoch: 8, train loss: 33.87087631225586, test loss: 29.824554443359375\n",
      "h: 41 | epoch: 9, train loss: 32.64159393310547, test loss: 28.71795082092285\n",
      "h: 41 | epoch: 10, train loss: 31.468212127685547, test loss: 27.659832000732422\n",
      "h: 41 | epoch: 11, train loss: 30.34807777404785, test loss: 26.647974014282227\n",
      "h: 41 | epoch: 12, train loss: 29.278697967529297, test loss: 25.680278778076172\n",
      "h: 41 | epoch: 13, train loss: 28.257755279541016, test loss: 24.75478744506836\n",
      "h: 41 | epoch: 14, train loss: 27.283056259155273, test loss: 23.869651794433594\n",
      "h: 41 | epoch: 15, train loss: 26.3525447845459, test loss: 23.023136138916016\n",
      "h: 41 | epoch: 16, train loss: 25.464292526245117, test loss: 22.213590621948242\n",
      "h: 41 | epoch: 17, train loss: 24.616458892822266, test loss: 21.439462661743164\n",
      "h: 41 | epoch: 18, train loss: 23.807308197021484, test loss: 20.699270248413086\n",
      "h: 41 | epoch: 19, train loss: 23.035198211669922, test loss: 19.99161720275879\n",
      "h: 41 | epoch: 20, train loss: 22.298555374145508, test loss: 19.31515121459961\n",
      "h: 41 | epoch: 21, train loss: 21.59589385986328, test loss: 18.66861343383789\n",
      "h: 41 | epoch: 22, train loss: 20.9257869720459, test loss: 18.050769805908203\n",
      "h: 41 | epoch: 23, train loss: 20.286876678466797, test loss: 17.460460662841797\n",
      "h: 41 | epoch: 24, train loss: 19.6778564453125, test loss: 16.896570205688477\n",
      "h: 41 | epoch: 25, train loss: 19.097484588623047, test loss: 16.358022689819336\n",
      "h: 41 | epoch: 26, train loss: 18.544565200805664, test loss: 15.843791007995605\n",
      "h: 41 | epoch: 27, train loss: 18.017948150634766, test loss: 15.352877616882324\n",
      "h: 41 | epoch: 28, train loss: 17.516530990600586, test loss: 14.884346008300781\n",
      "h: 41 | epoch: 29, train loss: 17.039255142211914, test loss: 14.437268257141113\n",
      "h: 41 | epoch: 30, train loss: 16.58509635925293, test loss: 14.010766983032227\n",
      "h: 41 | epoch: 31, train loss: 16.153076171875, test loss: 13.603991508483887\n",
      "h: 41 | epoch: 32, train loss: 15.7422513961792, test loss: 13.21612548828125\n",
      "h: 41 | epoch: 33, train loss: 15.351701736450195, test loss: 12.846379280090332\n",
      "h: 41 | epoch: 34, train loss: 14.980557441711426, test loss: 12.493993759155273\n",
      "h: 41 | epoch: 35, train loss: 14.627973556518555, test loss: 12.158231735229492\n",
      "h: 41 | epoch: 36, train loss: 14.293131828308105, test loss: 11.838391304016113\n",
      "h: 41 | epoch: 37, train loss: 13.975250244140625, test loss: 11.533785820007324\n",
      "h: 41 | epoch: 38, train loss: 13.673571586608887, test loss: 11.243762016296387\n",
      "h: 41 | epoch: 39, train loss: 13.387369155883789, test loss: 10.96768569946289\n",
      "h: 41 | epoch: 40, train loss: 13.115939140319824, test loss: 10.704943656921387\n",
      "h: 41 | epoch: 41, train loss: 12.858609199523926, test loss: 10.454954147338867\n",
      "h: 41 | epoch: 42, train loss: 12.614726066589355, test loss: 10.217144966125488\n",
      "h: 41 | epoch: 43, train loss: 12.383668899536133, test loss: 9.990972518920898\n",
      "h: 41 | epoch: 44, train loss: 12.164835929870605, test loss: 9.775915145874023\n",
      "h: 41 | epoch: 45, train loss: 11.957647323608398, test loss: 9.571465492248535\n",
      "h: 41 | epoch: 46, train loss: 11.76154899597168, test loss: 9.377138137817383\n",
      "h: 41 | epoch: 47, train loss: 11.576007843017578, test loss: 9.192466735839844\n",
      "h: 41 | epoch: 48, train loss: 11.400510787963867, test loss: 9.017007827758789\n",
      "h: 41 | epoch: 49, train loss: 11.23457145690918, test loss: 8.85032844543457\n",
      "h: 41 | epoch: 50, train loss: 11.077714920043945, test loss: 8.692010879516602\n",
      "h: 41 | epoch: 51, train loss: 10.929488182067871, test loss: 8.541664123535156\n",
      "h: 41 | epoch: 52, train loss: 10.789462089538574, test loss: 8.398908615112305\n",
      "h: 41 | epoch: 53, train loss: 10.657224655151367, test loss: 8.263376235961914\n",
      "h: 41 | epoch: 54, train loss: 10.532374382019043, test loss: 8.134721755981445\n",
      "h: 41 | epoch: 55, train loss: 10.41453742980957, test loss: 8.012609481811523\n",
      "h: 41 | epoch: 56, train loss: 10.303350448608398, test loss: 7.896722316741943\n",
      "h: 41 | epoch: 57, train loss: 10.198465347290039, test loss: 7.786753177642822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 41 | epoch: 58, train loss: 10.099555969238281, test loss: 7.682409763336182\n",
      "h: 41 | epoch: 59, train loss: 10.006303787231445, test loss: 7.583410739898682\n",
      "h: 41 | epoch: 60, train loss: 9.91840934753418, test loss: 7.489494323730469\n",
      "h: 41 | epoch: 61, train loss: 9.835587501525879, test loss: 7.400403022766113\n",
      "h: 41 | epoch: 62, train loss: 9.757563591003418, test loss: 7.315892219543457\n",
      "h: 41 | epoch: 63, train loss: 9.684080123901367, test loss: 7.235731601715088\n",
      "h: 41 | epoch: 64, train loss: 9.614887237548828, test loss: 7.1596999168396\n",
      "h: 41 | epoch: 65, train loss: 9.549752235412598, test loss: 7.08759069442749\n",
      "h: 41 | epoch: 66, train loss: 9.488450050354004, test loss: 7.019195556640625\n",
      "h: 41 | epoch: 67, train loss: 9.430767059326172, test loss: 6.9543304443359375\n",
      "h: 41 | epoch: 68, train loss: 9.376505851745605, test loss: 6.892809867858887\n",
      "h: 41 | epoch: 69, train loss: 9.325469017028809, test loss: 6.834462642669678\n",
      "h: 41 | epoch: 70, train loss: 9.27747917175293, test loss: 6.779122829437256\n",
      "h: 41 | epoch: 71, train loss: 9.23236083984375, test loss: 6.7266364097595215\n",
      "h: 41 | epoch: 72, train loss: 9.189953804016113, test loss: 6.676851749420166\n",
      "h: 41 | epoch: 73, train loss: 9.15009880065918, test loss: 6.629629611968994\n",
      "h: 41 | epoch: 74, train loss: 9.112654685974121, test loss: 6.584836006164551\n",
      "h: 41 | epoch: 75, train loss: 9.077478408813477, test loss: 6.542340278625488\n",
      "h: 41 | epoch: 76, train loss: 9.044439315795898, test loss: 6.5020270347595215\n",
      "h: 41 | epoch: 77, train loss: 9.013413429260254, test loss: 6.4637770652771\n",
      "h: 41 | epoch: 78, train loss: 8.984281539916992, test loss: 6.427481651306152\n",
      "h: 41 | epoch: 79, train loss: 8.956934928894043, test loss: 6.393040657043457\n",
      "h: 41 | epoch: 80, train loss: 8.931265830993652, test loss: 6.360355854034424\n",
      "h: 41 | epoch: 81, train loss: 8.90717887878418, test loss: 6.329331398010254\n",
      "h: 41 | epoch: 82, train loss: 8.884576797485352, test loss: 6.299879550933838\n",
      "h: 41 | epoch: 83, train loss: 8.863371849060059, test loss: 6.2719221115112305\n",
      "h: 41 | epoch: 84, train loss: 8.843481063842773, test loss: 6.245375633239746\n",
      "h: 41 | epoch: 85, train loss: 8.824825286865234, test loss: 6.220166206359863\n",
      "h: 41 | epoch: 86, train loss: 8.807330131530762, test loss: 6.196223258972168\n",
      "h: 41 | epoch: 87, train loss: 8.790925979614258, test loss: 6.173478603363037\n",
      "h: 41 | epoch: 88, train loss: 8.77554702758789, test loss: 6.151869773864746\n",
      "h: 41 | epoch: 89, train loss: 8.761130332946777, test loss: 6.131338596343994\n",
      "h: 41 | epoch: 90, train loss: 8.747617721557617, test loss: 6.111825466156006\n",
      "h: 41 | epoch: 91, train loss: 8.734952926635742, test loss: 6.093278408050537\n",
      "h: 41 | epoch: 92, train loss: 8.723085403442383, test loss: 6.0756449699401855\n",
      "h: 41 | epoch: 93, train loss: 8.711966514587402, test loss: 6.058877944946289\n",
      "h: 41 | epoch: 94, train loss: 8.70154857635498, test loss: 6.042932033538818\n",
      "h: 41 | epoch: 95, train loss: 8.691789627075195, test loss: 6.027763843536377\n",
      "h: 41 | epoch: 96, train loss: 8.682647705078125, test loss: 6.013331413269043\n",
      "h: 41 | epoch: 97, train loss: 8.67408561706543, test loss: 5.999598026275635\n",
      "h: 41 | epoch: 98, train loss: 8.666069030761719, test loss: 5.986526966094971\n",
      "h: 41 | epoch: 99, train loss: 8.658559799194336, test loss: 5.9740824699401855\n",
      "h: 42 | epoch: 0, train loss: 51.93585205078125, test loss: 44.722373962402344\n",
      "h: 42 | epoch: 1, train loss: 49.358123779296875, test loss: 42.50234603881836\n",
      "h: 42 | epoch: 2, train loss: 46.938377380371094, test loss: 40.41381072998047\n",
      "h: 42 | epoch: 3, train loss: 44.66508102416992, test loss: 38.447509765625\n",
      "h: 42 | epoch: 4, train loss: 42.52787780761719, test loss: 36.59510040283203\n",
      "h: 42 | epoch: 5, train loss: 40.517372131347656, test loss: 34.84897994995117\n",
      "h: 42 | epoch: 6, train loss: 38.625064849853516, test loss: 33.20225143432617\n",
      "h: 42 | epoch: 7, train loss: 36.84320068359375, test loss: 31.648601531982422\n",
      "h: 42 | epoch: 8, train loss: 35.164710998535156, test loss: 30.182241439819336\n",
      "h: 42 | epoch: 9, train loss: 33.58310317993164, test loss: 28.797876358032227\n",
      "h: 42 | epoch: 10, train loss: 32.092430114746094, test loss: 27.4906063079834\n",
      "h: 42 | epoch: 11, train loss: 30.687206268310547, test loss: 26.25592041015625\n",
      "h: 42 | epoch: 12, train loss: 29.36236000061035, test loss: 25.08962059020996\n",
      "h: 42 | epoch: 13, train loss: 28.11319351196289, test loss: 23.987817764282227\n",
      "h: 42 | epoch: 14, train loss: 26.93533706665039, test loss: 22.946887969970703\n",
      "h: 42 | epoch: 15, train loss: 25.82473373413086, test loss: 21.96344566345215\n",
      "h: 42 | epoch: 16, train loss: 24.777591705322266, test loss: 21.034345626831055\n",
      "h: 42 | epoch: 17, train loss: 23.790359497070312, test loss: 20.156612396240234\n",
      "h: 42 | epoch: 18, train loss: 22.85972023010254, test loss: 19.327457427978516\n",
      "h: 42 | epoch: 19, train loss: 21.982547760009766, test loss: 18.54427146911621\n",
      "h: 42 | epoch: 20, train loss: 21.15591049194336, test loss: 17.804588317871094\n",
      "h: 42 | epoch: 21, train loss: 20.377044677734375, test loss: 17.106082916259766\n",
      "h: 42 | epoch: 22, train loss: 19.643348693847656, test loss: 16.44655990600586\n",
      "h: 42 | epoch: 23, train loss: 18.95235824584961, test loss: 15.823945999145508\n",
      "h: 42 | epoch: 24, train loss: 18.3017520904541, test loss: 15.2362699508667\n",
      "h: 42 | epoch: 25, train loss: 17.689327239990234, test loss: 14.681673049926758\n",
      "h: 42 | epoch: 26, train loss: 17.113000869750977, test loss: 14.158395767211914\n",
      "h: 42 | epoch: 27, train loss: 16.570796966552734, test loss: 13.664762496948242\n",
      "h: 42 | epoch: 28, train loss: 16.060855865478516, test loss: 13.199190139770508\n",
      "h: 42 | epoch: 29, train loss: 15.581395149230957, test loss: 12.760167121887207\n",
      "h: 42 | epoch: 30, train loss: 15.130735397338867, test loss: 12.346266746520996\n",
      "h: 42 | epoch: 31, train loss: 14.70727825164795, test loss: 11.956130027770996\n",
      "h: 42 | epoch: 32, train loss: 14.309511184692383, test loss: 11.58846664428711\n",
      "h: 42 | epoch: 33, train loss: 13.935989379882812, test loss: 11.24205207824707\n",
      "h: 42 | epoch: 34, train loss: 13.585357666015625, test loss: 10.915725708007812\n",
      "h: 42 | epoch: 35, train loss: 13.256312370300293, test loss: 10.608377456665039\n",
      "h: 42 | epoch: 36, train loss: 12.947629928588867, test loss: 10.318961143493652\n",
      "h: 42 | epoch: 37, train loss: 12.6581449508667, test loss: 10.04648208618164\n",
      "h: 42 | epoch: 38, train loss: 12.386747360229492, test loss: 9.78999137878418\n",
      "h: 42 | epoch: 39, train loss: 12.132390975952148, test loss: 9.54859733581543\n",
      "h: 42 | epoch: 40, train loss: 11.894083023071289, test loss: 9.32144546508789\n",
      "h: 42 | epoch: 41, train loss: 11.670881271362305, test loss: 9.107728004455566\n",
      "h: 42 | epoch: 42, train loss: 11.461891174316406, test loss: 8.9066801071167\n",
      "h: 42 | epoch: 43, train loss: 11.266271591186523, test loss: 8.71757984161377\n",
      "h: 42 | epoch: 44, train loss: 11.083223342895508, test loss: 8.539739608764648\n",
      "h: 42 | epoch: 45, train loss: 10.9119873046875, test loss: 8.372505187988281\n",
      "h: 42 | epoch: 46, train loss: 10.751850128173828, test loss: 8.2152681350708\n",
      "h: 42 | epoch: 47, train loss: 10.602136611938477, test loss: 8.067440032958984\n",
      "h: 42 | epoch: 48, train loss: 10.462207794189453, test loss: 7.928472995758057\n",
      "h: 42 | epoch: 49, train loss: 10.331460952758789, test loss: 7.797844886779785\n",
      "h: 42 | epoch: 50, train loss: 10.209327697753906, test loss: 7.6750640869140625\n",
      "h: 42 | epoch: 51, train loss: 10.095270156860352, test loss: 7.559666633605957\n",
      "h: 42 | epoch: 52, train loss: 9.98878288269043, test loss: 7.451216220855713\n",
      "h: 42 | epoch: 53, train loss: 9.88939094543457, test loss: 7.349292755126953\n",
      "h: 42 | epoch: 54, train loss: 9.796642303466797, test loss: 7.2535080909729\n",
      "h: 42 | epoch: 55, train loss: 9.710115432739258, test loss: 7.163494110107422\n",
      "h: 42 | epoch: 56, train loss: 9.629413604736328, test loss: 7.0789031982421875\n",
      "h: 42 | epoch: 57, train loss: 9.554159164428711, test loss: 6.999408721923828\n",
      "h: 42 | epoch: 58, train loss: 9.484004974365234, test loss: 6.924700736999512\n",
      "h: 42 | epoch: 59, train loss: 9.41861629486084, test loss: 6.854491233825684\n",
      "h: 42 | epoch: 60, train loss: 9.357683181762695, test loss: 6.788501739501953\n",
      "h: 42 | epoch: 61, train loss: 9.300914764404297, test loss: 6.726480960845947\n",
      "h: 42 | epoch: 62, train loss: 9.24803638458252, test loss: 6.668182373046875\n",
      "h: 42 | epoch: 63, train loss: 9.19879150390625, test loss: 6.613381862640381\n",
      "h: 42 | epoch: 64, train loss: 9.15294075012207, test loss: 6.56186056137085\n",
      "h: 42 | epoch: 65, train loss: 9.110258102416992, test loss: 6.51342248916626\n",
      "h: 42 | epoch: 66, train loss: 9.070530891418457, test loss: 6.4678754806518555\n",
      "h: 42 | epoch: 67, train loss: 9.03355884552002, test loss: 6.42504358291626\n",
      "h: 42 | epoch: 68, train loss: 8.999160766601562, test loss: 6.384757995605469\n",
      "h: 42 | epoch: 69, train loss: 8.96716022491455, test loss: 6.346862316131592\n",
      "h: 42 | epoch: 70, train loss: 8.937395095825195, test loss: 6.31121301651001\n",
      "h: 42 | epoch: 71, train loss: 8.90971565246582, test loss: 6.277667045593262\n",
      "h: 42 | epoch: 72, train loss: 8.883977890014648, test loss: 6.246098518371582\n",
      "h: 42 | epoch: 73, train loss: 8.860048294067383, test loss: 6.216383457183838\n",
      "h: 42 | epoch: 74, train loss: 8.83780574798584, test loss: 6.18841028213501\n",
      "h: 42 | epoch: 75, train loss: 8.817131996154785, test loss: 6.162070274353027\n",
      "h: 42 | epoch: 76, train loss: 8.797919273376465, test loss: 6.137263298034668\n",
      "h: 42 | epoch: 77, train loss: 8.780065536499023, test loss: 6.113894462585449\n",
      "h: 42 | epoch: 78, train loss: 8.763479232788086, test loss: 6.091878414154053\n",
      "h: 42 | epoch: 79, train loss: 8.748071670532227, test loss: 6.071131229400635\n",
      "h: 42 | epoch: 80, train loss: 8.733758926391602, test loss: 6.05157470703125\n",
      "h: 42 | epoch: 81, train loss: 8.720464706420898, test loss: 6.033135414123535\n",
      "h: 42 | epoch: 82, train loss: 8.708121299743652, test loss: 6.0157470703125\n",
      "h: 42 | epoch: 83, train loss: 8.69665813446045, test loss: 5.999345302581787\n",
      "h: 42 | epoch: 84, train loss: 8.686015129089355, test loss: 5.983870029449463\n",
      "h: 42 | epoch: 85, train loss: 8.67613410949707, test loss: 5.969265460968018\n",
      "h: 42 | epoch: 86, train loss: 8.666962623596191, test loss: 5.955477714538574\n",
      "h: 42 | epoch: 87, train loss: 8.658448219299316, test loss: 5.9424614906311035\n",
      "h: 42 | epoch: 88, train loss: 8.650547981262207, test loss: 5.9301652908325195\n",
      "h: 42 | epoch: 89, train loss: 8.643213272094727, test loss: 5.91855001449585\n",
      "h: 42 | epoch: 90, train loss: 8.636407852172852, test loss: 5.907570838928223\n",
      "h: 42 | epoch: 91, train loss: 8.630094528198242, test loss: 5.897195339202881\n",
      "h: 42 | epoch: 92, train loss: 8.624235153198242, test loss: 5.887383937835693\n",
      "h: 42 | epoch: 93, train loss: 8.618799209594727, test loss: 5.878105163574219\n",
      "h: 42 | epoch: 94, train loss: 8.61375617980957, test loss: 5.869324684143066\n",
      "h: 42 | epoch: 95, train loss: 8.609077453613281, test loss: 5.861016273498535\n",
      "h: 42 | epoch: 96, train loss: 8.604738235473633, test loss: 5.853152275085449\n",
      "h: 42 | epoch: 97, train loss: 8.600713729858398, test loss: 5.845705986022949\n",
      "h: 42 | epoch: 98, train loss: 8.596982955932617, test loss: 5.838653087615967\n",
      "h: 42 | epoch: 99, train loss: 8.59351921081543, test loss: 5.831969738006592\n",
      "h: 43 | epoch: 0, train loss: 39.27783966064453, test loss: 33.3917236328125\n",
      "h: 43 | epoch: 1, train loss: 37.50720977783203, test loss: 31.870834350585938\n",
      "h: 43 | epoch: 2, train loss: 35.83675765991211, test loss: 30.43314552307129\n",
      "h: 43 | epoch: 3, train loss: 34.26026916503906, test loss: 29.073665618896484\n",
      "h: 43 | epoch: 4, train loss: 32.77205276489258, test loss: 27.787799835205078\n",
      "h: 43 | epoch: 5, train loss: 31.36684226989746, test loss: 26.571304321289062\n",
      "h: 43 | epoch: 6, train loss: 30.039806365966797, test loss: 25.420246124267578\n",
      "h: 43 | epoch: 7, train loss: 28.78643798828125, test loss: 24.33098793029785\n",
      "h: 43 | epoch: 8, train loss: 27.602575302124023, test loss: 23.300121307373047\n",
      "h: 43 | epoch: 9, train loss: 26.48434829711914, test loss: 22.32447624206543\n",
      "h: 43 | epoch: 10, train loss: 25.428117752075195, test loss: 21.40109634399414\n",
      "h: 43 | epoch: 11, train loss: 24.430511474609375, test loss: 20.527202606201172\n",
      "h: 43 | epoch: 12, train loss: 23.488353729248047, test loss: 19.700183868408203\n",
      "h: 43 | epoch: 13, train loss: 22.598670959472656, test loss: 18.91759490966797\n",
      "h: 43 | epoch: 14, train loss: 21.758657455444336, test loss: 18.177120208740234\n",
      "h: 43 | epoch: 15, train loss: 20.965682983398438, test loss: 17.476579666137695\n",
      "h: 43 | epoch: 16, train loss: 20.21725082397461, test loss: 16.81390380859375\n",
      "h: 43 | epoch: 17, train loss: 19.511009216308594, test loss: 16.18715476989746\n",
      "h: 43 | epoch: 18, train loss: 18.84473991394043, test loss: 15.59447193145752\n",
      "h: 43 | epoch: 19, train loss: 18.21633529663086, test loss: 15.034113883972168\n",
      "h: 43 | epoch: 20, train loss: 17.623794555664062, test loss: 14.504411697387695\n",
      "h: 43 | epoch: 21, train loss: 17.065227508544922, test loss: 14.003786087036133\n",
      "h: 43 | epoch: 22, train loss: 16.538833618164062, test loss: 13.530738830566406\n",
      "h: 43 | epoch: 23, train loss: 16.04290771484375, test loss: 13.083841323852539\n",
      "h: 43 | epoch: 24, train loss: 15.575828552246094, test loss: 12.661737442016602\n",
      "h: 43 | epoch: 25, train loss: 15.136053085327148, test loss: 12.263132095336914\n",
      "h: 43 | epoch: 26, train loss: 14.722116470336914, test loss: 11.88680362701416\n",
      "h: 43 | epoch: 27, train loss: 14.332623481750488, test loss: 11.531576156616211\n",
      "h: 43 | epoch: 28, train loss: 13.966249465942383, test loss: 11.196338653564453\n",
      "h: 43 | epoch: 29, train loss: 13.621732711791992, test loss: 10.880033493041992\n",
      "h: 43 | epoch: 30, train loss: 13.297877311706543, test loss: 10.581647872924805\n",
      "h: 43 | epoch: 31, train loss: 12.99354076385498, test loss: 10.300230026245117\n",
      "h: 43 | epoch: 32, train loss: 12.707639694213867, test loss: 10.034858703613281\n",
      "h: 43 | epoch: 33, train loss: 12.439146041870117, test loss: 9.784673690795898\n",
      "h: 43 | epoch: 34, train loss: 12.187082290649414, test loss: 9.54884147644043\n",
      "h: 43 | epoch: 35, train loss: 11.950516700744629, test loss: 9.326583862304688\n",
      "h: 43 | epoch: 36, train loss: 11.728568077087402, test loss: 9.11715316772461\n",
      "h: 43 | epoch: 37, train loss: 11.520400047302246, test loss: 8.919837951660156\n",
      "h: 43 | epoch: 38, train loss: 11.325217247009277, test loss: 8.733969688415527\n",
      "h: 43 | epoch: 39, train loss: 11.142265319824219, test loss: 8.558905601501465\n",
      "h: 43 | epoch: 40, train loss: 10.970830917358398, test loss: 8.394043922424316\n",
      "h: 43 | epoch: 41, train loss: 10.8102388381958, test loss: 8.238805770874023\n",
      "h: 43 | epoch: 42, train loss: 10.659844398498535, test loss: 8.092647552490234\n",
      "h: 43 | epoch: 43, train loss: 10.519041061401367, test loss: 7.955052375793457\n",
      "h: 43 | epoch: 44, train loss: 10.387258529663086, test loss: 7.82553243637085\n",
      "h: 43 | epoch: 45, train loss: 10.263949394226074, test loss: 7.703621864318848\n",
      "h: 43 | epoch: 46, train loss: 10.148603439331055, test loss: 7.588881492614746\n",
      "h: 43 | epoch: 47, train loss: 10.040731430053711, test loss: 7.480898857116699\n",
      "h: 43 | epoch: 48, train loss: 9.939879417419434, test loss: 7.379281520843506\n",
      "h: 43 | epoch: 49, train loss: 9.84561538696289, test loss: 7.283657073974609\n",
      "h: 43 | epoch: 50, train loss: 9.757527351379395, test loss: 7.193672180175781\n",
      "h: 43 | epoch: 51, train loss: 9.675232887268066, test loss: 7.108998775482178\n",
      "h: 43 | epoch: 52, train loss: 9.598368644714355, test loss: 7.029324531555176\n",
      "h: 43 | epoch: 53, train loss: 9.526596069335938, test loss: 6.954352378845215\n",
      "h: 43 | epoch: 54, train loss: 9.459589004516602, test loss: 6.883805751800537\n",
      "h: 43 | epoch: 55, train loss: 9.397048950195312, test loss: 6.817419528961182\n",
      "h: 43 | epoch: 56, train loss: 9.3386869430542, test loss: 6.754948616027832\n",
      "h: 43 | epoch: 57, train loss: 9.284238815307617, test loss: 6.696155548095703\n",
      "h: 43 | epoch: 58, train loss: 9.233449935913086, test loss: 6.640824317932129\n",
      "h: 43 | epoch: 59, train loss: 9.18608570098877, test loss: 6.5887451171875\n",
      "h: 43 | epoch: 60, train loss: 9.141923904418945, test loss: 6.539726257324219\n",
      "h: 43 | epoch: 61, train loss: 9.100752830505371, test loss: 6.493579864501953\n",
      "h: 43 | epoch: 62, train loss: 9.06238079071045, test loss: 6.450135231018066\n",
      "h: 43 | epoch: 63, train loss: 9.026620864868164, test loss: 6.409229278564453\n",
      "h: 43 | epoch: 64, train loss: 8.993303298950195, test loss: 6.370710849761963\n",
      "h: 43 | epoch: 65, train loss: 8.962265968322754, test loss: 6.334433078765869\n",
      "h: 43 | epoch: 66, train loss: 8.933355331420898, test loss: 6.300262451171875\n",
      "h: 43 | epoch: 67, train loss: 8.906434059143066, test loss: 6.26807165145874\n",
      "h: 43 | epoch: 68, train loss: 8.881364822387695, test loss: 6.237740993499756\n",
      "h: 43 | epoch: 69, train loss: 8.858027458190918, test loss: 6.209157466888428\n",
      "h: 43 | epoch: 70, train loss: 8.836301803588867, test loss: 6.1822190284729\n",
      "h: 43 | epoch: 71, train loss: 8.816083908081055, test loss: 6.156821250915527\n",
      "h: 43 | epoch: 72, train loss: 8.797266006469727, test loss: 6.132876396179199\n",
      "h: 43 | epoch: 73, train loss: 8.779759407043457, test loss: 6.110293388366699\n",
      "h: 43 | epoch: 74, train loss: 8.763469696044922, test loss: 6.088991165161133\n",
      "h: 43 | epoch: 75, train loss: 8.74831771850586, test loss: 6.068894386291504\n",
      "h: 43 | epoch: 76, train loss: 8.734224319458008, test loss: 6.049928665161133\n",
      "h: 43 | epoch: 77, train loss: 8.721116065979004, test loss: 6.032027721405029\n",
      "h: 43 | epoch: 78, train loss: 8.708927154541016, test loss: 6.0151262283325195\n",
      "h: 43 | epoch: 79, train loss: 8.697593688964844, test loss: 5.999166011810303\n",
      "h: 43 | epoch: 80, train loss: 8.687056541442871, test loss: 5.9840898513793945\n",
      "h: 43 | epoch: 81, train loss: 8.677260398864746, test loss: 5.969846725463867\n",
      "h: 43 | epoch: 82, train loss: 8.668155670166016, test loss: 5.956385612487793\n",
      "h: 43 | epoch: 83, train loss: 8.659692764282227, test loss: 5.943661689758301\n",
      "h: 43 | epoch: 84, train loss: 8.651826858520508, test loss: 5.931628227233887\n",
      "h: 43 | epoch: 85, train loss: 8.644518852233887, test loss: 5.920249938964844\n",
      "h: 43 | epoch: 86, train loss: 8.637726783752441, test loss: 5.909483909606934\n",
      "h: 43 | epoch: 87, train loss: 8.631417274475098, test loss: 5.899295330047607\n",
      "h: 43 | epoch: 88, train loss: 8.625554084777832, test loss: 5.889651298522949\n",
      "h: 43 | epoch: 89, train loss: 8.620107650756836, test loss: 5.880520820617676\n",
      "h: 43 | epoch: 90, train loss: 8.615049362182617, test loss: 5.871870994567871\n",
      "h: 43 | epoch: 91, train loss: 8.610349655151367, test loss: 5.863677024841309\n",
      "h: 43 | epoch: 92, train loss: 8.605984687805176, test loss: 5.855912208557129\n",
      "h: 43 | epoch: 93, train loss: 8.60193157196045, test loss: 5.8485517501831055\n",
      "h: 43 | epoch: 94, train loss: 8.59816837310791, test loss: 5.841571807861328\n",
      "h: 43 | epoch: 95, train loss: 8.594671249389648, test loss: 5.834952354431152\n",
      "h: 43 | epoch: 96, train loss: 8.591426849365234, test loss: 5.828670501708984\n",
      "h: 43 | epoch: 97, train loss: 8.58841323852539, test loss: 5.822709083557129\n",
      "h: 43 | epoch: 98, train loss: 8.585615158081055, test loss: 5.817049026489258\n",
      "h: 43 | epoch: 99, train loss: 8.583017349243164, test loss: 5.811675548553467\n",
      "h: 44 | epoch: 0, train loss: 38.04100036621094, test loss: 32.95470428466797\n",
      "h: 44 | epoch: 1, train loss: 36.226837158203125, test loss: 31.350744247436523\n",
      "h: 44 | epoch: 2, train loss: 34.52161407470703, test loss: 29.840484619140625\n",
      "h: 44 | epoch: 3, train loss: 32.91837692260742, test loss: 28.4180965423584\n",
      "h: 44 | epoch: 4, train loss: 31.41072654724121, test loss: 27.078205108642578\n",
      "h: 44 | epoch: 5, train loss: 29.992767333984375, test loss: 25.815832138061523\n",
      "h: 44 | epoch: 6, train loss: 28.659042358398438, test loss: 24.62636947631836\n",
      "h: 44 | epoch: 7, train loss: 27.40450668334961, test loss: 23.505542755126953\n",
      "h: 44 | epoch: 8, train loss: 26.224456787109375, test loss: 22.449359893798828\n",
      "h: 44 | epoch: 9, train loss: 25.114530563354492, test loss: 21.454099655151367\n",
      "h: 44 | epoch: 10, train loss: 24.07064437866211, test loss: 20.51629638671875\n",
      "h: 44 | epoch: 11, train loss: 23.08898162841797, test loss: 19.632686614990234\n",
      "h: 44 | epoch: 12, train loss: 22.165973663330078, test loss: 18.800212860107422\n",
      "h: 44 | epoch: 13, train loss: 21.298259735107422, test loss: 18.01600456237793\n",
      "h: 44 | epoch: 14, train loss: 20.482685089111328, test loss: 17.277359008789062\n",
      "h: 44 | epoch: 15, train loss: 19.716297149658203, test loss: 16.58173179626465\n",
      "h: 44 | epoch: 16, train loss: 18.996288299560547, test loss: 15.926722526550293\n",
      "h: 44 | epoch: 17, train loss: 18.320032119750977, test loss: 15.310060501098633\n",
      "h: 44 | epoch: 18, train loss: 17.685033798217773, test loss: 14.729604721069336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 44 | epoch: 19, train loss: 17.088953018188477, test loss: 14.183337211608887\n",
      "h: 44 | epoch: 20, train loss: 16.529560089111328, test loss: 13.669337272644043\n",
      "h: 44 | epoch: 21, train loss: 16.0047550201416, test loss: 13.18579387664795\n",
      "h: 44 | epoch: 22, train loss: 15.512557983398438, test loss: 12.730993270874023\n",
      "h: 44 | epoch: 23, train loss: 15.051080703735352, test loss: 12.303311347961426\n",
      "h: 44 | epoch: 24, train loss: 14.618547439575195, test loss: 11.901212692260742\n",
      "h: 44 | epoch: 25, train loss: 14.213272094726562, test loss: 11.523239135742188\n",
      "h: 44 | epoch: 26, train loss: 13.833662033081055, test loss: 11.168013572692871\n",
      "h: 44 | epoch: 27, train loss: 13.478208541870117, test loss: 10.834227561950684\n",
      "h: 44 | epoch: 28, train loss: 13.145482063293457, test loss: 10.520647048950195\n",
      "h: 44 | epoch: 29, train loss: 12.834129333496094, test loss: 10.22610092163086\n",
      "h: 44 | epoch: 30, train loss: 12.542874336242676, test loss: 9.949485778808594\n",
      "h: 44 | epoch: 31, train loss: 12.2705078125, test loss: 9.689746856689453\n",
      "h: 44 | epoch: 32, train loss: 12.015883445739746, test loss: 9.44589614868164\n",
      "h: 44 | epoch: 33, train loss: 11.777925491333008, test loss: 9.216996192932129\n",
      "h: 44 | epoch: 34, train loss: 11.555609703063965, test loss: 9.002163887023926\n",
      "h: 44 | epoch: 35, train loss: 11.347973823547363, test loss: 8.800555229187012\n",
      "h: 44 | epoch: 36, train loss: 11.154104232788086, test loss: 8.611383438110352\n",
      "h: 44 | epoch: 37, train loss: 10.973146438598633, test loss: 8.43389892578125\n",
      "h: 44 | epoch: 38, train loss: 10.804288864135742, test loss: 8.267398834228516\n",
      "h: 44 | epoch: 39, train loss: 10.646768569946289, test loss: 8.111217498779297\n",
      "h: 44 | epoch: 40, train loss: 10.499865531921387, test loss: 7.96472692489624\n",
      "h: 44 | epoch: 41, train loss: 10.362900733947754, test loss: 7.827332496643066\n",
      "h: 44 | epoch: 42, train loss: 10.235239028930664, test loss: 7.698483467102051\n",
      "h: 44 | epoch: 43, train loss: 10.116277694702148, test loss: 7.5776472091674805\n",
      "h: 44 | epoch: 44, train loss: 10.005454063415527, test loss: 7.464334011077881\n",
      "h: 44 | epoch: 45, train loss: 9.902238845825195, test loss: 7.3580756187438965\n",
      "h: 44 | epoch: 46, train loss: 9.806129455566406, test loss: 7.258435249328613\n",
      "h: 44 | epoch: 47, train loss: 9.71666145324707, test loss: 7.165000915527344\n",
      "h: 44 | epoch: 48, train loss: 9.633394241333008, test loss: 7.077385902404785\n",
      "h: 44 | epoch: 49, train loss: 9.555915832519531, test loss: 6.9952239990234375\n",
      "h: 44 | epoch: 50, train loss: 9.483839988708496, test loss: 6.91817569732666\n",
      "h: 44 | epoch: 51, train loss: 9.416803359985352, test loss: 6.845915794372559\n",
      "h: 44 | epoch: 52, train loss: 9.354467391967773, test loss: 6.7781476974487305\n",
      "h: 44 | epoch: 53, train loss: 9.296514511108398, test loss: 6.7145843505859375\n",
      "h: 44 | epoch: 54, train loss: 9.242646217346191, test loss: 6.654963493347168\n",
      "h: 44 | epoch: 55, train loss: 9.192584991455078, test loss: 6.5990309715271\n",
      "h: 44 | epoch: 56, train loss: 9.14606761932373, test loss: 6.546557426452637\n",
      "h: 44 | epoch: 57, train loss: 9.10285472869873, test loss: 6.497323513031006\n",
      "h: 44 | epoch: 58, train loss: 9.062715530395508, test loss: 6.451120853424072\n",
      "h: 44 | epoch: 59, train loss: 9.02543830871582, test loss: 6.407759189605713\n",
      "h: 44 | epoch: 60, train loss: 8.990826606750488, test loss: 6.367056846618652\n",
      "h: 44 | epoch: 61, train loss: 8.958691596984863, test loss: 6.328847408294678\n",
      "h: 44 | epoch: 62, train loss: 8.928862571716309, test loss: 6.292970657348633\n",
      "h: 44 | epoch: 63, train loss: 8.901175498962402, test loss: 6.259278297424316\n",
      "h: 44 | epoch: 64, train loss: 8.875483512878418, test loss: 6.227632999420166\n",
      "h: 44 | epoch: 65, train loss: 8.851644515991211, test loss: 6.197903633117676\n",
      "h: 44 | epoch: 66, train loss: 8.829526901245117, test loss: 6.1699700355529785\n",
      "h: 44 | epoch: 67, train loss: 8.809009552001953, test loss: 6.1437177658081055\n",
      "h: 44 | epoch: 68, train loss: 8.789979934692383, test loss: 6.119040012359619\n",
      "h: 44 | epoch: 69, train loss: 8.772329330444336, test loss: 6.095839977264404\n",
      "h: 44 | epoch: 70, train loss: 8.755963325500488, test loss: 6.074019908905029\n",
      "h: 44 | epoch: 71, train loss: 8.7407865524292, test loss: 6.053496360778809\n",
      "h: 44 | epoch: 72, train loss: 8.726716995239258, test loss: 6.034186840057373\n",
      "h: 44 | epoch: 73, train loss: 8.713671684265137, test loss: 6.01601505279541\n",
      "h: 44 | epoch: 74, train loss: 8.701580047607422, test loss: 5.998909950256348\n",
      "h: 44 | epoch: 75, train loss: 8.690372467041016, test loss: 5.982804298400879\n",
      "h: 44 | epoch: 76, train loss: 8.679985046386719, test loss: 5.967636585235596\n",
      "h: 44 | epoch: 77, train loss: 8.670358657836914, test loss: 5.953347206115723\n",
      "h: 44 | epoch: 78, train loss: 8.661438941955566, test loss: 5.939883708953857\n",
      "h: 44 | epoch: 79, train loss: 8.653172492980957, test loss: 5.927192211151123\n",
      "h: 44 | epoch: 80, train loss: 8.645513534545898, test loss: 5.915227890014648\n",
      "h: 44 | epoch: 81, train loss: 8.638418197631836, test loss: 5.903944492340088\n",
      "h: 44 | epoch: 82, train loss: 8.631845474243164, test loss: 5.893301963806152\n",
      "h: 44 | epoch: 83, train loss: 8.62575626373291, test loss: 5.883258819580078\n",
      "h: 44 | epoch: 84, train loss: 8.620115280151367, test loss: 5.873778343200684\n",
      "h: 44 | epoch: 85, train loss: 8.614891052246094, test loss: 5.864829063415527\n",
      "h: 44 | epoch: 86, train loss: 8.610052108764648, test loss: 5.856377601623535\n",
      "h: 44 | epoch: 87, train loss: 8.605569839477539, test loss: 5.84839391708374\n",
      "h: 44 | epoch: 88, train loss: 8.601419448852539, test loss: 5.840848922729492\n",
      "h: 44 | epoch: 89, train loss: 8.597575187683105, test loss: 5.833718299865723\n",
      "h: 44 | epoch: 90, train loss: 8.594015121459961, test loss: 5.8269758224487305\n",
      "h: 44 | epoch: 91, train loss: 8.590719223022461, test loss: 5.8206000328063965\n",
      "h: 44 | epoch: 92, train loss: 8.587667465209961, test loss: 5.814567565917969\n",
      "h: 44 | epoch: 93, train loss: 8.584840774536133, test loss: 5.808859348297119\n",
      "h: 44 | epoch: 94, train loss: 8.582223892211914, test loss: 5.803455829620361\n",
      "h: 44 | epoch: 95, train loss: 8.579801559448242, test loss: 5.79833984375\n",
      "h: 44 | epoch: 96, train loss: 8.577556610107422, test loss: 5.79349422454834\n",
      "h: 44 | epoch: 97, train loss: 8.575479507446289, test loss: 5.788905143737793\n",
      "h: 44 | epoch: 98, train loss: 8.573556900024414, test loss: 5.784554481506348\n",
      "h: 44 | epoch: 99, train loss: 8.571776390075684, test loss: 5.780431270599365\n",
      "h: 45 | epoch: 0, train loss: 46.7572021484375, test loss: 40.303672790527344\n",
      "h: 45 | epoch: 1, train loss: 43.97430419921875, test loss: 37.90947341918945\n",
      "h: 45 | epoch: 2, train loss: 41.39460754394531, test loss: 35.68404769897461\n",
      "h: 45 | epoch: 3, train loss: 39.00192642211914, test loss: 33.614402770996094\n",
      "h: 45 | epoch: 4, train loss: 36.781681060791016, test loss: 31.68878173828125\n",
      "h: 45 | epoch: 5, train loss: 34.7206916809082, test loss: 29.896503448486328\n",
      "h: 45 | epoch: 6, train loss: 32.80699920654297, test loss: 28.227859497070312\n",
      "h: 45 | epoch: 7, train loss: 31.029712677001953, test loss: 26.67397689819336\n",
      "h: 45 | epoch: 8, train loss: 29.378894805908203, test loss: 25.226713180541992\n",
      "h: 45 | epoch: 9, train loss: 27.845434188842773, test loss: 23.878612518310547\n",
      "h: 45 | epoch: 10, train loss: 26.420970916748047, test loss: 22.622793197631836\n",
      "h: 45 | epoch: 11, train loss: 25.097808837890625, test loss: 21.45290756225586\n",
      "h: 45 | epoch: 12, train loss: 23.86884307861328, test loss: 20.363094329833984\n",
      "h: 45 | epoch: 13, train loss: 22.727502822875977, test loss: 19.347902297973633\n",
      "h: 45 | epoch: 14, train loss: 21.667713165283203, test loss: 18.402294158935547\n",
      "h: 45 | epoch: 15, train loss: 20.6838321685791, test loss: 17.521574020385742\n",
      "h: 45 | epoch: 16, train loss: 19.770618438720703, test loss: 16.701379776000977\n",
      "h: 45 | epoch: 17, train loss: 18.923202514648438, test loss: 15.937644958496094\n",
      "h: 45 | epoch: 18, train loss: 18.137042999267578, test loss: 15.226582527160645\n",
      "h: 45 | epoch: 19, train loss: 17.407922744750977, test loss: 14.564648628234863\n",
      "h: 45 | epoch: 20, train loss: 16.73189353942871, test loss: 13.948539733886719\n",
      "h: 45 | epoch: 21, train loss: 16.105283737182617, test loss: 13.375173568725586\n",
      "h: 45 | epoch: 22, train loss: 15.52466106414795, test loss: 12.841670036315918\n",
      "h: 45 | epoch: 23, train loss: 14.986828804016113, test loss: 12.345331192016602\n",
      "h: 45 | epoch: 24, train loss: 14.488790512084961, test loss: 11.883637428283691\n",
      "h: 45 | epoch: 25, train loss: 14.02775764465332, test loss: 11.454231262207031\n",
      "h: 45 | epoch: 26, train loss: 13.60112190246582, test loss: 11.054916381835938\n",
      "h: 45 | epoch: 27, train loss: 13.206453323364258, test loss: 10.683629989624023\n",
      "h: 45 | epoch: 28, train loss: 12.841474533081055, test loss: 10.338447570800781\n",
      "h: 45 | epoch: 29, train loss: 12.504068374633789, test loss: 10.017573356628418\n",
      "h: 45 | epoch: 30, train loss: 12.192255973815918, test loss: 9.719325065612793\n",
      "h: 45 | epoch: 31, train loss: 11.90418815612793, test loss: 9.442136764526367\n",
      "h: 45 | epoch: 32, train loss: 11.638145446777344, test loss: 9.18453598022461\n",
      "h: 45 | epoch: 33, train loss: 11.392521858215332, test loss: 8.945161819458008\n",
      "h: 45 | epoch: 34, train loss: 11.165823936462402, test loss: 8.722731590270996\n",
      "h: 45 | epoch: 35, train loss: 10.956655502319336, test loss: 8.516057014465332\n",
      "h: 45 | epoch: 36, train loss: 10.76372241973877, test loss: 8.32402229309082\n",
      "h: 45 | epoch: 37, train loss: 10.585817337036133, test loss: 8.145601272583008\n",
      "h: 45 | epoch: 38, train loss: 10.4218168258667, test loss: 7.979820251464844\n",
      "h: 45 | epoch: 39, train loss: 10.270678520202637, test loss: 7.825781345367432\n",
      "h: 45 | epoch: 40, train loss: 10.131430625915527, test loss: 7.682652473449707\n",
      "h: 45 | epoch: 41, train loss: 10.003173828125, test loss: 7.549649715423584\n",
      "h: 45 | epoch: 42, train loss: 9.8850736618042, test loss: 7.426048278808594\n",
      "h: 45 | epoch: 43, train loss: 9.776350021362305, test loss: 7.311175346374512\n",
      "h: 45 | epoch: 44, train loss: 9.676285743713379, test loss: 7.20440149307251\n",
      "h: 45 | epoch: 45, train loss: 9.584211349487305, test loss: 7.1051459312438965\n",
      "h: 45 | epoch: 46, train loss: 9.499512672424316, test loss: 7.012866973876953\n",
      "h: 45 | epoch: 47, train loss: 9.421612739562988, test loss: 6.927059173583984\n",
      "h: 45 | epoch: 48, train loss: 9.349983215332031, test loss: 6.847257137298584\n",
      "h: 45 | epoch: 49, train loss: 9.28413200378418, test loss: 6.773026466369629\n",
      "h: 45 | epoch: 50, train loss: 9.223608016967773, test loss: 6.703965187072754\n",
      "h: 45 | epoch: 51, train loss: 9.167989730834961, test loss: 6.639697074890137\n",
      "h: 45 | epoch: 52, train loss: 9.116888046264648, test loss: 6.579878330230713\n",
      "h: 45 | epoch: 53, train loss: 9.069947242736816, test loss: 6.524186611175537\n",
      "h: 45 | epoch: 54, train loss: 9.026834487915039, test loss: 6.472323417663574\n",
      "h: 45 | epoch: 55, train loss: 8.987245559692383, test loss: 6.424012660980225\n",
      "h: 45 | epoch: 56, train loss: 8.950897216796875, test loss: 6.378997802734375\n",
      "h: 45 | epoch: 57, train loss: 8.917529106140137, test loss: 6.337041854858398\n",
      "h: 45 | epoch: 58, train loss: 8.886903762817383, test loss: 6.297924518585205\n",
      "h: 45 | epoch: 59, train loss: 8.858798027038574, test loss: 6.261441707611084\n",
      "h: 45 | epoch: 60, train loss: 8.833009719848633, test loss: 6.227404594421387\n",
      "h: 45 | epoch: 61, train loss: 8.809349060058594, test loss: 6.195639133453369\n",
      "h: 45 | epoch: 62, train loss: 8.78764533996582, test loss: 6.165980339050293\n",
      "h: 45 | epoch: 63, train loss: 8.767736434936523, test loss: 6.13828182220459\n",
      "h: 45 | epoch: 64, train loss: 8.749479293823242, test loss: 6.112401962280273\n",
      "h: 45 | epoch: 65, train loss: 8.732735633850098, test loss: 6.088212013244629\n",
      "h: 45 | epoch: 66, train loss: 8.717384338378906, test loss: 6.0655927658081055\n",
      "h: 45 | epoch: 67, train loss: 8.703309059143066, test loss: 6.044436454772949\n",
      "h: 45 | epoch: 68, train loss: 8.690404891967773, test loss: 6.024636745452881\n",
      "h: 45 | epoch: 69, train loss: 8.678579330444336, test loss: 6.006100177764893\n",
      "h: 45 | epoch: 70, train loss: 8.66773796081543, test loss: 5.98874044418335\n",
      "h: 45 | epoch: 71, train loss: 8.657803535461426, test loss: 5.972472667694092\n",
      "h: 45 | epoch: 72, train loss: 8.648698806762695, test loss: 5.9572224617004395\n",
      "h: 45 | epoch: 73, train loss: 8.640356063842773, test loss: 5.942922115325928\n",
      "h: 45 | epoch: 74, train loss: 8.632712364196777, test loss: 5.929505348205566\n",
      "h: 45 | epoch: 75, train loss: 8.62570858001709, test loss: 5.9169111251831055\n",
      "h: 45 | epoch: 76, train loss: 8.619292259216309, test loss: 5.905084133148193\n",
      "h: 45 | epoch: 77, train loss: 8.613414764404297, test loss: 5.893973350524902\n",
      "h: 45 | epoch: 78, train loss: 8.608030319213867, test loss: 5.883530616760254\n",
      "h: 45 | epoch: 79, train loss: 8.603097915649414, test loss: 5.873711109161377\n",
      "h: 45 | epoch: 80, train loss: 8.598580360412598, test loss: 5.864474296569824\n",
      "h: 45 | epoch: 81, train loss: 8.594444274902344, test loss: 5.855780601501465\n",
      "h: 45 | epoch: 82, train loss: 8.590654373168945, test loss: 5.84759521484375\n",
      "h: 45 | epoch: 83, train loss: 8.58718490600586, test loss: 5.8398847579956055\n",
      "h: 45 | epoch: 84, train loss: 8.584007263183594, test loss: 5.832618236541748\n",
      "h: 45 | epoch: 85, train loss: 8.581097602844238, test loss: 5.825767517089844\n",
      "h: 45 | epoch: 86, train loss: 8.578433990478516, test loss: 5.819305896759033\n",
      "h: 45 | epoch: 87, train loss: 8.575993537902832, test loss: 5.813209533691406\n",
      "h: 45 | epoch: 88, train loss: 8.573760032653809, test loss: 5.807453632354736\n",
      "h: 45 | epoch: 89, train loss: 8.571714401245117, test loss: 5.802019119262695\n",
      "h: 45 | epoch: 90, train loss: 8.569842338562012, test loss: 5.796883583068848\n",
      "h: 45 | epoch: 91, train loss: 8.56812858581543, test loss: 5.7920308113098145\n",
      "h: 45 | epoch: 92, train loss: 8.566557884216309, test loss: 5.787440776824951\n",
      "h: 45 | epoch: 93, train loss: 8.5651216506958, test loss: 5.783100128173828\n",
      "h: 45 | epoch: 94, train loss: 8.56380558013916, test loss: 5.778993129730225\n",
      "h: 45 | epoch: 95, train loss: 8.562602043151855, test loss: 5.775104522705078\n",
      "h: 45 | epoch: 96, train loss: 8.56149959564209, test loss: 5.771422863006592\n",
      "h: 45 | epoch: 97, train loss: 8.560490608215332, test loss: 5.7679338455200195\n",
      "h: 45 | epoch: 98, train loss: 8.55956745147705, test loss: 5.764629364013672\n",
      "h: 45 | epoch: 99, train loss: 8.558721542358398, test loss: 5.761496067047119\n",
      "h: 46 | epoch: 0, train loss: 43.59712219238281, test loss: 37.93144989013672\n",
      "h: 46 | epoch: 1, train loss: 41.220767974853516, test loss: 35.8533821105957\n",
      "h: 46 | epoch: 2, train loss: 39.004371643066406, test loss: 33.910587310791016\n",
      "h: 46 | epoch: 3, train loss: 36.93626022338867, test loss: 32.09349060058594\n",
      "h: 46 | epoch: 4, train loss: 35.00582504272461, test loss: 30.393375396728516\n",
      "h: 46 | epoch: 5, train loss: 33.203407287597656, test loss: 28.802257537841797\n",
      "h: 46 | epoch: 6, train loss: 31.52016258239746, test loss: 27.312835693359375\n",
      "h: 46 | epoch: 7, train loss: 29.948001861572266, test loss: 25.91839027404785\n",
      "h: 46 | epoch: 8, train loss: 28.479473114013672, test loss: 24.612712860107422\n",
      "h: 46 | epoch: 9, train loss: 27.107715606689453, test loss: 23.39008140563965\n",
      "h: 46 | epoch: 10, train loss: 25.826385498046875, test loss: 22.245182037353516\n",
      "h: 46 | epoch: 11, train loss: 24.629596710205078, test loss: 21.17308807373047\n",
      "h: 46 | epoch: 12, train loss: 23.511892318725586, test loss: 20.169214248657227\n",
      "h: 46 | epoch: 13, train loss: 22.468202590942383, test loss: 19.229284286499023\n",
      "h: 46 | epoch: 14, train loss: 21.493791580200195, test loss: 18.349308013916016\n",
      "h: 46 | epoch: 15, train loss: 20.58424186706543, test loss: 17.525558471679688\n",
      "h: 46 | epoch: 16, train loss: 19.735437393188477, test loss: 16.754539489746094\n",
      "h: 46 | epoch: 17, train loss: 18.943511962890625, test loss: 16.032991409301758\n",
      "h: 46 | epoch: 18, train loss: 18.204853057861328, test loss: 15.357831001281738\n",
      "h: 46 | epoch: 19, train loss: 17.516071319580078, test loss: 14.726194381713867\n",
      "h: 46 | epoch: 20, train loss: 16.873994827270508, test loss: 14.135366439819336\n",
      "h: 46 | epoch: 21, train loss: 16.275638580322266, test loss: 13.582819938659668\n",
      "h: 46 | epoch: 22, train loss: 15.71820068359375, test loss: 13.066160202026367\n",
      "h: 46 | epoch: 23, train loss: 15.199056625366211, test loss: 12.583144187927246\n",
      "h: 46 | epoch: 24, train loss: 14.715734481811523, test loss: 12.131661415100098\n",
      "h: 46 | epoch: 25, train loss: 14.265913009643555, test loss: 11.709728240966797\n",
      "h: 46 | epoch: 26, train loss: 13.847412109375, test loss: 11.315478324890137\n",
      "h: 46 | epoch: 27, train loss: 13.458185195922852, test loss: 10.947160720825195\n",
      "h: 46 | epoch: 28, train loss: 13.096307754516602, test loss: 10.603116989135742\n",
      "h: 46 | epoch: 29, train loss: 12.759971618652344, test loss: 10.281803131103516\n",
      "h: 46 | epoch: 30, train loss: 12.447484016418457, test loss: 9.981754302978516\n",
      "h: 46 | epoch: 31, train loss: 12.157247543334961, test loss: 9.701608657836914\n",
      "h: 46 | epoch: 32, train loss: 11.88776969909668, test loss: 9.440071105957031\n",
      "h: 46 | epoch: 33, train loss: 11.637651443481445, test loss: 9.195939064025879\n",
      "h: 46 | epoch: 34, train loss: 11.405572891235352, test loss: 8.968072891235352\n",
      "h: 46 | epoch: 35, train loss: 11.190305709838867, test loss: 8.755412101745605\n",
      "h: 46 | epoch: 36, train loss: 10.990694046020508, test loss: 8.556955337524414\n",
      "h: 46 | epoch: 37, train loss: 10.805659294128418, test loss: 8.371764183044434\n",
      "h: 46 | epoch: 38, train loss: 10.634187698364258, test loss: 8.198965072631836\n",
      "h: 46 | epoch: 39, train loss: 10.475333213806152, test loss: 8.037731170654297\n",
      "h: 46 | epoch: 40, train loss: 10.32820987701416, test loss: 7.887292385101318\n",
      "h: 46 | epoch: 41, train loss: 10.191991806030273, test loss: 7.7469282150268555\n",
      "h: 46 | epoch: 42, train loss: 10.06590461730957, test loss: 7.61596155166626\n",
      "h: 46 | epoch: 43, train loss: 9.949228286743164, test loss: 7.4937639236450195\n",
      "h: 46 | epoch: 44, train loss: 9.841287612915039, test loss: 7.379741668701172\n",
      "h: 46 | epoch: 45, train loss: 9.741456985473633, test loss: 7.27334451675415\n",
      "h: 46 | epoch: 46, train loss: 9.649145126342773, test loss: 7.174053192138672\n",
      "h: 46 | epoch: 47, train loss: 9.563813209533691, test loss: 7.081389427185059\n",
      "h: 46 | epoch: 48, train loss: 9.484949111938477, test loss: 6.994900703430176\n",
      "h: 46 | epoch: 49, train loss: 9.412080764770508, test loss: 6.914167881011963\n",
      "h: 46 | epoch: 50, train loss: 9.344765663146973, test loss: 6.838798522949219\n",
      "h: 46 | epoch: 51, train loss: 9.282594680786133, test loss: 6.768426418304443\n",
      "h: 46 | epoch: 52, train loss: 9.225187301635742, test loss: 6.702710151672363\n",
      "h: 46 | epoch: 53, train loss: 9.172189712524414, test loss: 6.641332149505615\n",
      "h: 46 | epoch: 54, train loss: 9.123272895812988, test loss: 6.5839972496032715\n",
      "h: 46 | epoch: 55, train loss: 9.078130722045898, test loss: 6.530425071716309\n",
      "h: 46 | epoch: 56, train loss: 9.036478996276855, test loss: 6.480363368988037\n",
      "h: 46 | epoch: 57, train loss: 8.998056411743164, test loss: 6.433568000793457\n",
      "h: 46 | epoch: 58, train loss: 8.962617874145508, test loss: 6.389817714691162\n",
      "h: 46 | epoch: 59, train loss: 8.929937362670898, test loss: 6.348903179168701\n",
      "h: 46 | epoch: 60, train loss: 8.899805068969727, test loss: 6.310633659362793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 46 | epoch: 61, train loss: 8.872027397155762, test loss: 6.274825096130371\n",
      "h: 46 | epoch: 62, train loss: 8.84642219543457, test loss: 6.241312503814697\n",
      "h: 46 | epoch: 63, train loss: 8.822824478149414, test loss: 6.209939002990723\n",
      "h: 46 | epoch: 64, train loss: 8.801078796386719, test loss: 6.180559158325195\n",
      "h: 46 | epoch: 65, train loss: 8.78104305267334, test loss: 6.153038501739502\n",
      "h: 46 | epoch: 66, train loss: 8.76258659362793, test loss: 6.127251148223877\n",
      "h: 46 | epoch: 67, train loss: 8.745585441589355, test loss: 6.103078365325928\n",
      "h: 46 | epoch: 68, train loss: 8.729927062988281, test loss: 6.080412864685059\n",
      "h: 46 | epoch: 69, train loss: 8.715506553649902, test loss: 6.059155464172363\n",
      "h: 46 | epoch: 70, train loss: 8.702228546142578, test loss: 6.039207935333252\n",
      "h: 46 | epoch: 71, train loss: 8.690004348754883, test loss: 6.020485877990723\n",
      "h: 46 | epoch: 72, train loss: 8.678749084472656, test loss: 6.002906799316406\n",
      "h: 46 | epoch: 73, train loss: 8.668388366699219, test loss: 5.98639440536499\n",
      "h: 46 | epoch: 74, train loss: 8.658852577209473, test loss: 5.970879077911377\n",
      "h: 46 | epoch: 75, train loss: 8.650075912475586, test loss: 5.956292629241943\n",
      "h: 46 | epoch: 76, train loss: 8.642000198364258, test loss: 5.942578315734863\n",
      "h: 46 | epoch: 77, train loss: 8.634567260742188, test loss: 5.9296770095825195\n",
      "h: 46 | epoch: 78, train loss: 8.627729415893555, test loss: 5.917534828186035\n",
      "h: 46 | epoch: 79, train loss: 8.621438026428223, test loss: 5.906103610992432\n",
      "h: 46 | epoch: 80, train loss: 8.615649223327637, test loss: 5.895339012145996\n",
      "h: 46 | epoch: 81, train loss: 8.61032485961914, test loss: 5.885196208953857\n",
      "h: 46 | epoch: 82, train loss: 8.605426788330078, test loss: 5.875636100769043\n",
      "h: 46 | epoch: 83, train loss: 8.600921630859375, test loss: 5.866621971130371\n",
      "h: 46 | epoch: 84, train loss: 8.596776962280273, test loss: 5.858119487762451\n",
      "h: 46 | epoch: 85, train loss: 8.592966079711914, test loss: 5.850094795227051\n",
      "h: 46 | epoch: 86, train loss: 8.589463233947754, test loss: 5.842520236968994\n",
      "h: 46 | epoch: 87, train loss: 8.586240768432617, test loss: 5.835366725921631\n",
      "h: 46 | epoch: 88, train loss: 8.58327579498291, test loss: 5.828608989715576\n",
      "h: 46 | epoch: 89, train loss: 8.580551147460938, test loss: 5.822221755981445\n",
      "h: 46 | epoch: 90, train loss: 8.578046798706055, test loss: 5.8161821365356445\n",
      "h: 46 | epoch: 91, train loss: 8.575742721557617, test loss: 5.810470104217529\n",
      "h: 46 | epoch: 92, train loss: 8.573625564575195, test loss: 5.8050665855407715\n",
      "h: 46 | epoch: 93, train loss: 8.57167911529541, test loss: 5.79995059967041\n",
      "h: 46 | epoch: 94, train loss: 8.569889068603516, test loss: 5.795108318328857\n",
      "h: 46 | epoch: 95, train loss: 8.568243026733398, test loss: 5.790518760681152\n",
      "h: 46 | epoch: 96, train loss: 8.566732406616211, test loss: 5.786171913146973\n",
      "h: 46 | epoch: 97, train loss: 8.56534194946289, test loss: 5.7820515632629395\n",
      "h: 46 | epoch: 98, train loss: 8.564064979553223, test loss: 5.778143405914307\n",
      "h: 46 | epoch: 99, train loss: 8.562889099121094, test loss: 5.7744364738464355\n",
      "h: 47 | epoch: 0, train loss: 44.1745491027832, test loss: 38.0329475402832\n",
      "h: 47 | epoch: 1, train loss: 41.66314697265625, test loss: 35.83053970336914\n",
      "h: 47 | epoch: 2, train loss: 39.33052062988281, test loss: 33.781612396240234\n",
      "h: 47 | epoch: 3, train loss: 37.16276931762695, test loss: 31.874496459960938\n",
      "h: 47 | epoch: 4, train loss: 35.14735794067383, test loss: 30.098613739013672\n",
      "h: 47 | epoch: 5, train loss: 33.27288818359375, test loss: 28.444351196289062\n",
      "h: 47 | epoch: 6, train loss: 31.529001235961914, test loss: 26.902908325195312\n",
      "h: 47 | epoch: 7, train loss: 29.906240463256836, test loss: 25.466262817382812\n",
      "h: 47 | epoch: 8, train loss: 28.395954132080078, test loss: 24.127025604248047\n",
      "h: 47 | epoch: 9, train loss: 26.990184783935547, test loss: 22.878419876098633\n",
      "h: 47 | epoch: 10, train loss: 25.6816349029541, test loss: 21.714191436767578\n",
      "h: 47 | epoch: 11, train loss: 24.463552474975586, test loss: 20.628564834594727\n",
      "h: 47 | epoch: 12, train loss: 23.32971954345703, test loss: 19.616199493408203\n",
      "h: 47 | epoch: 13, train loss: 22.274368286132812, test loss: 18.67214012145996\n",
      "h: 47 | epoch: 14, train loss: 21.292152404785156, test loss: 17.791790008544922\n",
      "h: 47 | epoch: 15, train loss: 20.378110885620117, test loss: 16.970876693725586\n",
      "h: 47 | epoch: 16, train loss: 19.527631759643555, test loss: 16.205425262451172\n",
      "h: 47 | epoch: 17, train loss: 18.736417770385742, test loss: 15.491737365722656\n",
      "h: 47 | epoch: 18, train loss: 18.000476837158203, test loss: 14.82635498046875\n",
      "h: 47 | epoch: 19, train loss: 17.316068649291992, test loss: 14.206062316894531\n",
      "h: 47 | epoch: 20, train loss: 16.679718017578125, test loss: 13.627856254577637\n",
      "h: 47 | epoch: 21, train loss: 16.088180541992188, test loss: 13.08892822265625\n",
      "h: 47 | epoch: 22, train loss: 15.538416862487793, test loss: 12.586655616760254\n",
      "h: 47 | epoch: 23, train loss: 15.027593612670898, test loss: 12.118595123291016\n",
      "h: 47 | epoch: 24, train loss: 14.553064346313477, test loss: 11.682450294494629\n",
      "h: 47 | epoch: 25, train loss: 14.112350463867188, test loss: 11.276089668273926\n",
      "h: 47 | epoch: 26, train loss: 13.703140258789062, test loss: 10.897504806518555\n",
      "h: 47 | epoch: 27, train loss: 13.323269844055176, test loss: 10.544828414916992\n",
      "h: 47 | epoch: 28, train loss: 12.970718383789062, test loss: 10.216313362121582\n",
      "h: 47 | epoch: 29, train loss: 12.643599510192871, test loss: 9.910326957702637\n",
      "h: 47 | epoch: 30, train loss: 12.340143203735352, test loss: 9.625338554382324\n",
      "h: 47 | epoch: 31, train loss: 12.05870532989502, test loss: 9.359923362731934\n",
      "h: 47 | epoch: 32, train loss: 11.797741889953613, test loss: 9.112748146057129\n",
      "h: 47 | epoch: 33, train loss: 11.555821418762207, test loss: 8.882568359375\n",
      "h: 47 | epoch: 34, train loss: 11.331595420837402, test loss: 8.668217658996582\n",
      "h: 47 | epoch: 35, train loss: 11.123815536499023, test loss: 8.46861457824707\n",
      "h: 47 | epoch: 36, train loss: 10.931310653686523, test loss: 8.282743453979492\n",
      "h: 47 | epoch: 37, train loss: 10.7529935836792, test loss: 8.109660148620605\n",
      "h: 47 | epoch: 38, train loss: 10.587849617004395, test loss: 7.948481559753418\n",
      "h: 47 | epoch: 39, train loss: 10.434929847717285, test loss: 7.798384189605713\n",
      "h: 47 | epoch: 40, train loss: 10.293355941772461, test loss: 7.658600807189941\n",
      "h: 47 | epoch: 41, train loss: 10.162304878234863, test loss: 7.52841854095459\n",
      "h: 47 | epoch: 42, train loss: 10.041013717651367, test loss: 7.407170295715332\n",
      "h: 47 | epoch: 43, train loss: 9.928773880004883, test loss: 7.294233798980713\n",
      "h: 47 | epoch: 44, train loss: 9.824920654296875, test loss: 7.189032077789307\n",
      "h: 47 | epoch: 45, train loss: 9.728841781616211, test loss: 7.091023921966553\n",
      "h: 47 | epoch: 46, train loss: 9.639965057373047, test loss: 6.9997124671936035\n",
      "h: 47 | epoch: 47, train loss: 9.557759284973145, test loss: 6.914628028869629\n",
      "h: 47 | epoch: 48, train loss: 9.481733322143555, test loss: 6.835335731506348\n",
      "h: 47 | epoch: 49, train loss: 9.411428451538086, test loss: 6.761432647705078\n",
      "h: 47 | epoch: 50, train loss: 9.346418380737305, test loss: 6.692544460296631\n",
      "h: 47 | epoch: 51, train loss: 9.2863130569458, test loss: 6.628320217132568\n",
      "h: 47 | epoch: 52, train loss: 9.230741500854492, test loss: 6.5684332847595215\n",
      "h: 47 | epoch: 53, train loss: 9.17936897277832, test loss: 6.512583255767822\n",
      "h: 47 | epoch: 54, train loss: 9.131879806518555, test loss: 6.460488319396973\n",
      "h: 47 | epoch: 55, train loss: 9.087983131408691, test loss: 6.411887168884277\n",
      "h: 47 | epoch: 56, train loss: 9.047406196594238, test loss: 6.366538047790527\n",
      "h: 47 | epoch: 57, train loss: 9.009902954101562, test loss: 6.32421350479126\n",
      "h: 47 | epoch: 58, train loss: 8.975239753723145, test loss: 6.28470516204834\n",
      "h: 47 | epoch: 59, train loss: 8.943201065063477, test loss: 6.24781608581543\n",
      "h: 47 | epoch: 60, train loss: 8.913591384887695, test loss: 6.2133684158325195\n",
      "h: 47 | epoch: 61, train loss: 8.886224746704102, test loss: 6.181190013885498\n",
      "h: 47 | epoch: 62, train loss: 8.860932350158691, test loss: 6.151127338409424\n",
      "h: 47 | epoch: 63, train loss: 8.837556838989258, test loss: 6.1230340003967285\n",
      "h: 47 | epoch: 64, train loss: 8.815950393676758, test loss: 6.096774101257324\n",
      "h: 47 | epoch: 65, train loss: 8.79598331451416, test loss: 6.072223663330078\n",
      "h: 47 | epoch: 66, train loss: 8.777528762817383, test loss: 6.049266338348389\n",
      "h: 47 | epoch: 67, train loss: 8.760469436645508, test loss: 6.0277910232543945\n",
      "h: 47 | epoch: 68, train loss: 8.74470329284668, test loss: 6.007698059082031\n",
      "h: 47 | epoch: 69, train loss: 8.730128288269043, test loss: 5.988895416259766\n",
      "h: 47 | epoch: 70, train loss: 8.716657638549805, test loss: 5.971295356750488\n",
      "h: 47 | epoch: 71, train loss: 8.704202651977539, test loss: 5.9548163414001465\n",
      "h: 47 | epoch: 72, train loss: 8.692689895629883, test loss: 5.939382076263428\n",
      "h: 47 | epoch: 73, train loss: 8.682044982910156, test loss: 5.924925327301025\n",
      "h: 47 | epoch: 74, train loss: 8.67220401763916, test loss: 5.911377906799316\n",
      "h: 47 | epoch: 75, train loss: 8.663104057312012, test loss: 5.898681163787842\n",
      "h: 47 | epoch: 76, train loss: 8.65468978881836, test loss: 5.886778831481934\n",
      "h: 47 | epoch: 77, train loss: 8.646909713745117, test loss: 5.875617980957031\n",
      "h: 47 | epoch: 78, train loss: 8.6397123336792, test loss: 5.865150451660156\n",
      "h: 47 | epoch: 79, train loss: 8.633058547973633, test loss: 5.85532808303833\n",
      "h: 47 | epoch: 80, train loss: 8.626901626586914, test loss: 5.846113681793213\n",
      "h: 47 | epoch: 81, train loss: 8.621206283569336, test loss: 5.837462902069092\n",
      "h: 47 | epoch: 82, train loss: 8.615938186645508, test loss: 5.829341888427734\n",
      "h: 47 | epoch: 83, train loss: 8.611063957214355, test loss: 5.821715354919434\n",
      "h: 47 | epoch: 84, train loss: 8.60655403137207, test loss: 5.81455135345459\n",
      "h: 47 | epoch: 85, train loss: 8.602380752563477, test loss: 5.807822227478027\n",
      "h: 47 | epoch: 86, train loss: 8.598518371582031, test loss: 5.801497936248779\n",
      "h: 47 | epoch: 87, train loss: 8.59494400024414, test loss: 5.795554161071777\n",
      "h: 47 | epoch: 88, train loss: 8.591634750366211, test loss: 5.7899651527404785\n",
      "h: 47 | epoch: 89, train loss: 8.588571548461914, test loss: 5.78471040725708\n",
      "h: 47 | epoch: 90, train loss: 8.585734367370605, test loss: 5.779767036437988\n",
      "h: 47 | epoch: 91, train loss: 8.583109855651855, test loss: 5.77511739730835\n",
      "h: 47 | epoch: 92, train loss: 8.58067798614502, test loss: 5.770743370056152\n",
      "h: 47 | epoch: 93, train loss: 8.578425407409668, test loss: 5.76662540435791\n",
      "h: 47 | epoch: 94, train loss: 8.576340675354004, test loss: 5.762750148773193\n",
      "h: 47 | epoch: 95, train loss: 8.574407577514648, test loss: 5.759100914001465\n",
      "h: 47 | epoch: 96, train loss: 8.57261848449707, test loss: 5.7556657791137695\n",
      "h: 47 | epoch: 97, train loss: 8.570959091186523, test loss: 5.752429008483887\n",
      "h: 47 | epoch: 98, train loss: 8.569421768188477, test loss: 5.749381065368652\n",
      "h: 47 | epoch: 99, train loss: 8.567997932434082, test loss: 5.746508598327637\n",
      "h: 48 | epoch: 0, train loss: 42.07799530029297, test loss: 36.36440658569336\n",
      "h: 48 | epoch: 1, train loss: 40.12778854370117, test loss: 34.65700912475586\n",
      "h: 48 | epoch: 2, train loss: 38.289894104003906, test loss: 33.04505920410156\n",
      "h: 48 | epoch: 3, train loss: 36.557106018066406, test loss: 31.522628784179688\n",
      "h: 48 | epoch: 4, train loss: 34.922855377197266, test loss: 30.084264755249023\n",
      "h: 48 | epoch: 5, train loss: 33.38108444213867, test loss: 28.724960327148438\n",
      "h: 48 | epoch: 6, train loss: 31.926244735717773, test loss: 27.440082550048828\n",
      "h: 48 | epoch: 7, train loss: 30.553203582763672, test loss: 26.225351333618164\n",
      "h: 48 | epoch: 8, train loss: 29.257221221923828, test loss: 25.076801300048828\n",
      "h: 48 | epoch: 9, train loss: 28.033870697021484, test loss: 23.990734100341797\n",
      "h: 48 | epoch: 10, train loss: 26.879077911376953, test loss: 22.96370506286621\n",
      "h: 48 | epoch: 11, train loss: 25.78900146484375, test loss: 21.99249839782715\n",
      "h: 48 | epoch: 12, train loss: 24.760082244873047, test loss: 21.0740966796875\n",
      "h: 48 | epoch: 13, train loss: 23.788970947265625, test loss: 20.205677032470703\n",
      "h: 48 | epoch: 14, train loss: 22.87253189086914, test loss: 19.384578704833984\n",
      "h: 48 | epoch: 15, train loss: 22.007808685302734, test loss: 18.60830307006836\n",
      "h: 48 | epoch: 16, train loss: 21.192031860351562, test loss: 17.874481201171875\n",
      "h: 48 | epoch: 17, train loss: 20.422576904296875, test loss: 17.180891036987305\n",
      "h: 48 | epoch: 18, train loss: 19.696969985961914, test loss: 16.525432586669922\n",
      "h: 48 | epoch: 19, train loss: 19.01287269592285, test loss: 15.906097412109375\n",
      "h: 48 | epoch: 20, train loss: 18.368072509765625, test loss: 15.321006774902344\n",
      "h: 48 | epoch: 21, train loss: 17.760473251342773, test loss: 14.7683687210083\n",
      "h: 48 | epoch: 22, train loss: 17.188087463378906, test loss: 14.246479988098145\n",
      "h: 48 | epoch: 23, train loss: 16.649028778076172, test loss: 13.7537260055542\n",
      "h: 48 | epoch: 24, train loss: 16.14150619506836, test loss: 13.288581848144531\n",
      "h: 48 | epoch: 25, train loss: 15.663824081420898, test loss: 12.849584579467773\n",
      "h: 48 | epoch: 26, train loss: 15.214365005493164, test loss: 12.435354232788086\n",
      "h: 48 | epoch: 27, train loss: 14.791595458984375, test loss: 12.044571876525879\n",
      "h: 48 | epoch: 28, train loss: 14.39405632019043, test loss: 11.675985336303711\n",
      "h: 48 | epoch: 29, train loss: 14.020367622375488, test loss: 11.328409194946289\n",
      "h: 48 | epoch: 30, train loss: 13.669209480285645, test loss: 11.000711441040039\n",
      "h: 48 | epoch: 31, train loss: 13.339332580566406, test loss: 10.691816329956055\n",
      "h: 48 | epoch: 32, train loss: 13.029545783996582, test loss: 10.400702476501465\n",
      "h: 48 | epoch: 33, train loss: 12.738723754882812, test loss: 10.126396179199219\n",
      "h: 48 | epoch: 34, train loss: 12.465792655944824, test loss: 9.867975234985352\n",
      "h: 48 | epoch: 35, train loss: 12.209734916687012, test loss: 9.624567031860352\n",
      "h: 48 | epoch: 36, train loss: 11.969585418701172, test loss: 9.395332336425781\n",
      "h: 48 | epoch: 37, train loss: 11.744427680969238, test loss: 9.179486274719238\n",
      "h: 48 | epoch: 38, train loss: 11.533390045166016, test loss: 8.976274490356445\n",
      "h: 48 | epoch: 39, train loss: 11.335650444030762, test loss: 8.784988403320312\n",
      "h: 48 | epoch: 40, train loss: 11.150428771972656, test loss: 8.604951858520508\n",
      "h: 48 | epoch: 41, train loss: 10.976981163024902, test loss: 8.435523986816406\n",
      "h: 48 | epoch: 42, train loss: 10.814614295959473, test loss: 8.276100158691406\n",
      "h: 48 | epoch: 43, train loss: 10.66265869140625, test loss: 8.126105308532715\n",
      "h: 48 | epoch: 44, train loss: 10.520489692687988, test loss: 7.984996795654297\n",
      "h: 48 | epoch: 45, train loss: 10.3875150680542, test loss: 7.852258205413818\n",
      "h: 48 | epoch: 46, train loss: 10.263174057006836, test loss: 7.727404594421387\n",
      "h: 48 | epoch: 47, train loss: 10.146939277648926, test loss: 7.609976291656494\n",
      "h: 48 | epoch: 48, train loss: 10.038310050964355, test loss: 7.499533176422119\n",
      "h: 48 | epoch: 49, train loss: 9.936816215515137, test loss: 7.395671844482422\n",
      "h: 48 | epoch: 50, train loss: 9.842011451721191, test loss: 7.297999382019043\n",
      "h: 48 | epoch: 51, train loss: 9.753477096557617, test loss: 7.206148624420166\n",
      "h: 48 | epoch: 52, train loss: 9.670819282531738, test loss: 7.11977481842041\n",
      "h: 48 | epoch: 53, train loss: 9.593667030334473, test loss: 7.038552284240723\n",
      "h: 48 | epoch: 54, train loss: 9.52166748046875, test loss: 6.962172508239746\n",
      "h: 48 | epoch: 55, train loss: 9.454495429992676, test loss: 6.890345573425293\n",
      "h: 48 | epoch: 56, train loss: 9.391838073730469, test loss: 6.822798252105713\n",
      "h: 48 | epoch: 57, train loss: 9.333405494689941, test loss: 6.759273529052734\n",
      "h: 48 | epoch: 58, train loss: 9.278923034667969, test loss: 6.699526309967041\n",
      "h: 48 | epoch: 59, train loss: 9.228134155273438, test loss: 6.643332004547119\n",
      "h: 48 | epoch: 60, train loss: 9.18079948425293, test loss: 6.590473175048828\n",
      "h: 48 | epoch: 61, train loss: 9.136691093444824, test loss: 6.540750026702881\n",
      "h: 48 | epoch: 62, train loss: 9.095598220825195, test loss: 6.493969917297363\n",
      "h: 48 | epoch: 63, train loss: 9.057319641113281, test loss: 6.449954986572266\n",
      "h: 48 | epoch: 64, train loss: 9.021668434143066, test loss: 6.4085373878479\n",
      "h: 48 | epoch: 65, train loss: 8.988470077514648, test loss: 6.369558334350586\n",
      "h: 48 | epoch: 66, train loss: 8.957563400268555, test loss: 6.332870960235596\n",
      "h: 48 | epoch: 67, train loss: 8.928792953491211, test loss: 6.29833459854126\n",
      "h: 48 | epoch: 68, train loss: 8.902013778686523, test loss: 6.265818119049072\n",
      "h: 48 | epoch: 69, train loss: 8.877093315124512, test loss: 6.235198974609375\n",
      "h: 48 | epoch: 70, train loss: 8.853907585144043, test loss: 6.206362247467041\n",
      "h: 48 | epoch: 71, train loss: 8.832337379455566, test loss: 6.179198265075684\n",
      "h: 48 | epoch: 72, train loss: 8.812270164489746, test loss: 6.1536054611206055\n",
      "h: 48 | epoch: 73, train loss: 8.793607711791992, test loss: 6.129489421844482\n",
      "h: 48 | epoch: 74, train loss: 8.776250839233398, test loss: 6.106760501861572\n",
      "h: 48 | epoch: 75, train loss: 8.760113716125488, test loss: 6.085333347320557\n",
      "h: 48 | epoch: 76, train loss: 8.745107650756836, test loss: 6.065129280090332\n",
      "h: 48 | epoch: 77, train loss: 8.731159210205078, test loss: 6.046074867248535\n",
      "h: 48 | epoch: 78, train loss: 8.718193054199219, test loss: 6.02810001373291\n",
      "h: 48 | epoch: 79, train loss: 8.706143379211426, test loss: 6.011139869689941\n",
      "h: 48 | epoch: 80, train loss: 8.694942474365234, test loss: 5.995135307312012\n",
      "h: 48 | epoch: 81, train loss: 8.68453598022461, test loss: 5.980024814605713\n",
      "h: 48 | epoch: 82, train loss: 8.67486572265625, test loss: 5.96575927734375\n",
      "h: 48 | epoch: 83, train loss: 8.665881156921387, test loss: 5.952284812927246\n",
      "h: 48 | epoch: 84, train loss: 8.657536506652832, test loss: 5.939555644989014\n",
      "h: 48 | epoch: 85, train loss: 8.649782180786133, test loss: 5.927526473999023\n",
      "h: 48 | epoch: 86, train loss: 8.642581939697266, test loss: 5.9161577224731445\n",
      "h: 48 | epoch: 87, train loss: 8.635892868041992, test loss: 5.905409336090088\n",
      "h: 48 | epoch: 88, train loss: 8.629682540893555, test loss: 5.8952436447143555\n",
      "h: 48 | epoch: 89, train loss: 8.62391471862793, test loss: 5.885627269744873\n",
      "h: 48 | epoch: 90, train loss: 8.618558883666992, test loss: 5.876528739929199\n",
      "h: 48 | epoch: 91, train loss: 8.61358642578125, test loss: 5.867916107177734\n",
      "h: 48 | epoch: 92, train loss: 8.608968734741211, test loss: 5.859762191772461\n",
      "h: 48 | epoch: 93, train loss: 8.604683876037598, test loss: 5.8520402908325195\n",
      "h: 48 | epoch: 94, train loss: 8.600704193115234, test loss: 5.844726085662842\n",
      "h: 48 | epoch: 95, train loss: 8.597010612487793, test loss: 5.837794303894043\n",
      "h: 48 | epoch: 96, train loss: 8.593584060668945, test loss: 5.831225395202637\n",
      "h: 48 | epoch: 97, train loss: 8.590401649475098, test loss: 5.8249969482421875\n",
      "h: 48 | epoch: 98, train loss: 8.587448120117188, test loss: 5.819090843200684\n",
      "h: 48 | epoch: 99, train loss: 8.584707260131836, test loss: 5.813486576080322\n",
      "h: 49 | epoch: 0, train loss: 45.061668395996094, test loss: 39.41189193725586\n",
      "h: 49 | epoch: 1, train loss: 42.875038146972656, test loss: 37.47757339477539\n",
      "h: 49 | epoch: 2, train loss: 40.82001495361328, test loss: 35.656394958496094\n",
      "h: 49 | epoch: 3, train loss: 38.887611389160156, test loss: 33.94083786010742\n",
      "h: 49 | epoch: 4, train loss: 37.06965255737305, test loss: 32.324073791503906\n",
      "h: 49 | epoch: 5, train loss: 35.35870361328125, test loss: 30.799819946289062\n",
      "h: 49 | epoch: 6, train loss: 33.747928619384766, test loss: 29.362340927124023\n",
      "h: 49 | epoch: 7, train loss: 32.23107147216797, test loss: 28.006351470947266\n",
      "h: 49 | epoch: 8, train loss: 30.802383422851562, test loss: 26.72695541381836\n",
      "h: 49 | epoch: 9, train loss: 29.456527709960938, test loss: 25.519651412963867\n",
      "h: 49 | epoch: 10, train loss: 28.1885986328125, test loss: 24.38023567199707\n",
      "h: 49 | epoch: 11, train loss: 26.9940185546875, test loss: 23.304821014404297\n",
      "h: 49 | epoch: 12, train loss: 25.86855125427246, test loss: 22.289770126342773\n",
      "h: 49 | epoch: 13, train loss: 24.8082275390625, test loss: 21.33169174194336\n",
      "h: 49 | epoch: 14, train loss: 23.809345245361328, test loss: 20.427412033081055\n",
      "h: 49 | epoch: 15, train loss: 22.868444442749023, test loss: 19.573955535888672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 49 | epoch: 16, train loss: 21.982280731201172, test loss: 18.768522262573242\n",
      "h: 49 | epoch: 17, train loss: 21.147790908813477, test loss: 18.008487701416016\n",
      "h: 49 | epoch: 18, train loss: 20.36211585998535, test loss: 17.291378021240234\n",
      "h: 49 | epoch: 19, train loss: 19.622547149658203, test loss: 16.614849090576172\n",
      "h: 49 | epoch: 20, train loss: 18.92653465270996, test loss: 15.976696968078613\n",
      "h: 49 | epoch: 21, train loss: 18.271669387817383, test loss: 15.374844551086426\n",
      "h: 49 | epoch: 22, train loss: 17.65567398071289, test loss: 14.807319641113281\n",
      "h: 49 | epoch: 23, train loss: 17.076400756835938, test loss: 14.272252082824707\n",
      "h: 49 | epoch: 24, train loss: 16.531810760498047, test loss: 13.767885208129883\n",
      "h: 49 | epoch: 25, train loss: 16.019975662231445, test loss: 13.292535781860352\n",
      "h: 49 | epoch: 26, train loss: 15.53906536102295, test loss: 12.844622611999512\n",
      "h: 49 | epoch: 27, train loss: 15.087350845336914, test loss: 12.42264175415039\n",
      "h: 49 | epoch: 28, train loss: 14.663189888000488, test loss: 12.025166511535645\n",
      "h: 49 | epoch: 29, train loss: 14.26502513885498, test loss: 11.650843620300293\n",
      "h: 49 | epoch: 30, train loss: 13.891385078430176, test loss: 11.298389434814453\n",
      "h: 49 | epoch: 31, train loss: 13.540864944458008, test loss: 10.966591835021973\n",
      "h: 49 | epoch: 32, train loss: 13.21214485168457, test loss: 10.654288291931152\n",
      "h: 49 | epoch: 33, train loss: 12.903963088989258, test loss: 10.360391616821289\n",
      "h: 49 | epoch: 34, train loss: 12.615131378173828, test loss: 10.083864212036133\n",
      "h: 49 | epoch: 35, train loss: 12.344518661499023, test loss: 9.82371711730957\n",
      "h: 49 | epoch: 36, train loss: 12.091058731079102, test loss: 9.579020500183105\n",
      "h: 49 | epoch: 37, train loss: 11.853734970092773, test loss: 9.348893165588379\n",
      "h: 49 | epoch: 38, train loss: 11.63159465789795, test loss: 9.132497787475586\n",
      "h: 49 | epoch: 39, train loss: 11.423727035522461, test loss: 8.929040908813477\n",
      "h: 49 | epoch: 40, train loss: 11.229272842407227, test loss: 8.737771987915039\n",
      "h: 49 | epoch: 41, train loss: 11.047423362731934, test loss: 8.557984352111816\n",
      "h: 49 | epoch: 42, train loss: 10.877411842346191, test loss: 8.38900375366211\n",
      "h: 49 | epoch: 43, train loss: 10.718513488769531, test loss: 8.230199813842773\n",
      "h: 49 | epoch: 44, train loss: 10.57004451751709, test loss: 8.08096694946289\n",
      "h: 49 | epoch: 45, train loss: 10.431358337402344, test loss: 7.940741539001465\n",
      "h: 49 | epoch: 46, train loss: 10.301847457885742, test loss: 7.808991432189941\n",
      "h: 49 | epoch: 47, train loss: 10.180936813354492, test loss: 7.685206413269043\n",
      "h: 49 | epoch: 48, train loss: 10.068082809448242, test loss: 7.568910121917725\n",
      "h: 49 | epoch: 49, train loss: 9.96278190612793, test loss: 7.459657192230225\n",
      "h: 49 | epoch: 50, train loss: 9.86454963684082, test loss: 7.357019901275635\n",
      "h: 49 | epoch: 51, train loss: 9.772932052612305, test loss: 7.260601997375488\n",
      "h: 49 | epoch: 52, train loss: 9.687509536743164, test loss: 7.170022010803223\n",
      "h: 49 | epoch: 53, train loss: 9.607876777648926, test loss: 7.084927558898926\n",
      "h: 49 | epoch: 54, train loss: 9.533660888671875, test loss: 7.004984378814697\n",
      "h: 49 | epoch: 55, train loss: 9.464509963989258, test loss: 6.929877281188965\n",
      "h: 49 | epoch: 56, train loss: 9.400091171264648, test loss: 6.859313011169434\n",
      "h: 49 | epoch: 57, train loss: 9.340092658996582, test loss: 6.793011665344238\n",
      "h: 49 | epoch: 58, train loss: 9.284223556518555, test loss: 6.730709075927734\n",
      "h: 49 | epoch: 59, train loss: 9.232208251953125, test loss: 6.6721625328063965\n",
      "h: 49 | epoch: 60, train loss: 9.183794021606445, test loss: 6.617140293121338\n",
      "h: 49 | epoch: 61, train loss: 9.1387357711792, test loss: 6.56542444229126\n",
      "h: 49 | epoch: 62, train loss: 9.09681224822998, test loss: 6.516809940338135\n",
      "h: 49 | epoch: 63, train loss: 9.057809829711914, test loss: 6.471109867095947\n",
      "h: 49 | epoch: 64, train loss: 9.02153491973877, test loss: 6.428136348724365\n",
      "h: 49 | epoch: 65, train loss: 8.987797737121582, test loss: 6.387728214263916\n",
      "h: 49 | epoch: 66, train loss: 8.956427574157715, test loss: 6.3497209548950195\n",
      "h: 49 | epoch: 67, train loss: 8.927264213562012, test loss: 6.313971519470215\n",
      "h: 49 | epoch: 68, train loss: 8.900156021118164, test loss: 6.280335903167725\n",
      "h: 49 | epoch: 69, train loss: 8.874959945678711, test loss: 6.24868631362915\n",
      "h: 49 | epoch: 70, train loss: 8.851546287536621, test loss: 6.218898773193359\n",
      "h: 49 | epoch: 71, train loss: 8.829792976379395, test loss: 6.190857887268066\n",
      "h: 49 | epoch: 72, train loss: 8.809582710266113, test loss: 6.164456844329834\n",
      "h: 49 | epoch: 73, train loss: 8.790807723999023, test loss: 6.139594078063965\n",
      "h: 49 | epoch: 74, train loss: 8.773372650146484, test loss: 6.116174221038818\n",
      "h: 49 | epoch: 75, train loss: 8.757180213928223, test loss: 6.094109535217285\n",
      "h: 49 | epoch: 76, train loss: 8.742143630981445, test loss: 6.073317527770996\n",
      "h: 49 | epoch: 77, train loss: 8.72818374633789, test loss: 6.053718566894531\n",
      "h: 49 | epoch: 78, train loss: 8.715224266052246, test loss: 6.035240173339844\n",
      "h: 49 | epoch: 79, train loss: 8.703194618225098, test loss: 6.017814636230469\n",
      "h: 49 | epoch: 80, train loss: 8.692028999328613, test loss: 6.001377105712891\n",
      "h: 49 | epoch: 81, train loss: 8.681665420532227, test loss: 5.985867023468018\n",
      "h: 49 | epoch: 82, train loss: 8.672048568725586, test loss: 5.971229076385498\n",
      "h: 49 | epoch: 83, train loss: 8.663125991821289, test loss: 5.957409858703613\n",
      "h: 49 | epoch: 84, train loss: 8.654847145080566, test loss: 5.944360256195068\n",
      "h: 49 | epoch: 85, train loss: 8.647165298461914, test loss: 5.932034492492676\n",
      "h: 49 | epoch: 86, train loss: 8.640040397644043, test loss: 5.920388698577881\n",
      "h: 49 | epoch: 87, train loss: 8.633430480957031, test loss: 5.9093828201293945\n",
      "h: 49 | epoch: 88, train loss: 8.627300262451172, test loss: 5.898977756500244\n",
      "h: 49 | epoch: 89, train loss: 8.621615409851074, test loss: 5.889136791229248\n",
      "h: 49 | epoch: 90, train loss: 8.616342544555664, test loss: 5.879831314086914\n",
      "h: 49 | epoch: 91, train loss: 8.611452102661133, test loss: 5.871023654937744\n",
      "h: 49 | epoch: 92, train loss: 8.606919288635254, test loss: 5.862687110900879\n",
      "h: 49 | epoch: 93, train loss: 8.602714538574219, test loss: 5.854796409606934\n",
      "h: 49 | epoch: 94, train loss: 8.598817825317383, test loss: 5.847322463989258\n",
      "h: 49 | epoch: 95, train loss: 8.59520435333252, test loss: 5.840242862701416\n",
      "h: 49 | epoch: 96, train loss: 8.5918550491333, test loss: 5.833534240722656\n",
      "h: 49 | epoch: 97, train loss: 8.588748931884766, test loss: 5.827175617218018\n",
      "h: 49 | epoch: 98, train loss: 8.585869789123535, test loss: 5.821145057678223\n",
      "h: 49 | epoch: 99, train loss: 8.583202362060547, test loss: 5.815426826477051\n",
      "h: 50 | epoch: 0, train loss: 38.547767639160156, test loss: 33.23716354370117\n",
      "h: 50 | epoch: 1, train loss: 36.03040313720703, test loss: 31.040634155273438\n",
      "h: 50 | epoch: 2, train loss: 33.72122573852539, test loss: 29.020334243774414\n",
      "h: 50 | epoch: 3, train loss: 31.602502822875977, test loss: 27.161602020263672\n",
      "h: 50 | epoch: 4, train loss: 29.658222198486328, test loss: 25.45117950439453\n",
      "h: 50 | epoch: 5, train loss: 27.873889923095703, test loss: 23.877012252807617\n",
      "h: 50 | epoch: 6, train loss: 26.2363224029541, test loss: 22.42812728881836\n",
      "h: 50 | epoch: 7, train loss: 24.733518600463867, test loss: 21.094497680664062\n",
      "h: 50 | epoch: 8, train loss: 23.35452651977539, test loss: 19.86695098876953\n",
      "h: 50 | epoch: 9, train loss: 22.089326858520508, test loss: 18.73708724975586\n",
      "h: 50 | epoch: 10, train loss: 20.928741455078125, test loss: 17.697185516357422\n",
      "h: 50 | epoch: 11, train loss: 19.864356994628906, test loss: 16.740163803100586\n",
      "h: 50 | epoch: 12, train loss: 18.88843536376953, test loss: 15.859498977661133\n",
      "h: 50 | epoch: 13, train loss: 17.993867874145508, test loss: 15.049181938171387\n",
      "h: 50 | epoch: 14, train loss: 17.17410659790039, test loss: 14.303680419921875\n",
      "h: 50 | epoch: 15, train loss: 16.42313003540039, test loss: 13.617894172668457\n",
      "h: 50 | epoch: 16, train loss: 15.735384941101074, test loss: 12.9871187210083\n",
      "h: 50 | epoch: 17, train loss: 15.105758666992188, test loss: 12.407007217407227\n",
      "h: 50 | epoch: 18, train loss: 14.529528617858887, test loss: 11.873555183410645\n",
      "h: 50 | epoch: 19, train loss: 14.002351760864258, test loss: 11.383064270019531\n",
      "h: 50 | epoch: 20, train loss: 13.52021598815918, test loss: 10.932121276855469\n",
      "h: 50 | epoch: 21, train loss: 13.079429626464844, test loss: 10.517573356628418\n",
      "h: 50 | epoch: 22, train loss: 12.67658519744873, test loss: 10.136518478393555\n",
      "h: 50 | epoch: 23, train loss: 12.30854320526123, test loss: 9.786272048950195\n",
      "h: 50 | epoch: 24, train loss: 11.972414016723633, test loss: 9.464362144470215\n",
      "h: 50 | epoch: 25, train loss: 11.665533065795898, test loss: 9.168508529663086\n",
      "h: 50 | epoch: 26, train loss: 11.385452270507812, test loss: 8.896600723266602\n",
      "h: 50 | epoch: 27, train loss: 11.129912376403809, test loss: 8.646714210510254\n",
      "h: 50 | epoch: 28, train loss: 10.896839141845703, test loss: 8.41705322265625\n",
      "h: 50 | epoch: 29, train loss: 10.684326171875, test loss: 8.20598030090332\n",
      "h: 50 | epoch: 30, train loss: 10.490616798400879, test loss: 8.011975288391113\n",
      "h: 50 | epoch: 31, train loss: 10.314101219177246, test loss: 7.833647727966309\n",
      "h: 50 | epoch: 32, train loss: 10.153303146362305, test loss: 7.6697187423706055\n",
      "h: 50 | epoch: 33, train loss: 10.006861686706543, test loss: 7.51900577545166\n",
      "h: 50 | epoch: 34, train loss: 9.873533248901367, test loss: 7.380425930023193\n",
      "h: 50 | epoch: 35, train loss: 9.752176284790039, test loss: 7.252984523773193\n",
      "h: 50 | epoch: 36, train loss: 9.641745567321777, test loss: 7.135767936706543\n",
      "h: 50 | epoch: 37, train loss: 9.541280746459961, test loss: 7.02793025970459\n",
      "h: 50 | epoch: 38, train loss: 9.449906349182129, test loss: 6.928703308105469\n",
      "h: 50 | epoch: 39, train loss: 9.366819381713867, test loss: 6.837378025054932\n",
      "h: 50 | epoch: 40, train loss: 9.291284561157227, test loss: 6.753303527832031\n",
      "h: 50 | epoch: 41, train loss: 9.222630500793457, test loss: 6.675882816314697\n",
      "h: 50 | epoch: 42, train loss: 9.160242080688477, test loss: 6.604569911956787\n",
      "h: 50 | epoch: 43, train loss: 9.103561401367188, test loss: 6.538861274719238\n",
      "h: 50 | epoch: 44, train loss: 9.05207633972168, test loss: 6.478298187255859\n",
      "h: 50 | epoch: 45, train loss: 9.005318641662598, test loss: 6.42245626449585\n",
      "h: 50 | epoch: 46, train loss: 8.962861061096191, test loss: 6.3709516525268555\n",
      "h: 50 | epoch: 47, train loss: 8.924314498901367, test loss: 6.323427200317383\n",
      "h: 50 | epoch: 48, train loss: 8.889328002929688, test loss: 6.27955961227417\n",
      "h: 50 | epoch: 49, train loss: 8.857573509216309, test loss: 6.239049911499023\n",
      "h: 50 | epoch: 50, train loss: 8.82875919342041, test loss: 6.201624870300293\n",
      "h: 50 | epoch: 51, train loss: 8.802616119384766, test loss: 6.167035102844238\n",
      "h: 50 | epoch: 52, train loss: 8.778900146484375, test loss: 6.135051727294922\n",
      "h: 50 | epoch: 53, train loss: 8.7573881149292, test loss: 6.105462074279785\n",
      "h: 50 | epoch: 54, train loss: 8.737878799438477, test loss: 6.078078746795654\n",
      "h: 50 | epoch: 55, train loss: 8.720186233520508, test loss: 6.05272102355957\n",
      "h: 50 | epoch: 56, train loss: 8.704145431518555, test loss: 6.029229640960693\n",
      "h: 50 | epoch: 57, train loss: 8.68960189819336, test loss: 6.007452964782715\n",
      "h: 50 | epoch: 58, train loss: 8.67641830444336, test loss: 5.987260341644287\n",
      "h: 50 | epoch: 59, train loss: 8.664468765258789, test loss: 5.968523025512695\n",
      "h: 50 | epoch: 60, train loss: 8.653636932373047, test loss: 5.951128959655762\n",
      "h: 50 | epoch: 61, train loss: 8.643821716308594, test loss: 5.934971809387207\n",
      "h: 50 | epoch: 62, train loss: 8.634927749633789, test loss: 5.919957637786865\n",
      "h: 50 | epoch: 63, train loss: 8.626867294311523, test loss: 5.9059953689575195\n",
      "h: 50 | epoch: 64, train loss: 8.6195650100708, test loss: 5.893006324768066\n",
      "h: 50 | epoch: 65, train loss: 8.612950325012207, test loss: 5.880916118621826\n",
      "h: 50 | epoch: 66, train loss: 8.606958389282227, test loss: 5.869656562805176\n",
      "h: 50 | epoch: 67, train loss: 8.601530075073242, test loss: 5.859162330627441\n",
      "h: 50 | epoch: 68, train loss: 8.596612930297852, test loss: 5.849380970001221\n",
      "h: 50 | epoch: 69, train loss: 8.59216022491455, test loss: 5.840254783630371\n",
      "h: 50 | epoch: 70, train loss: 8.588127136230469, test loss: 5.831737518310547\n",
      "h: 50 | epoch: 71, train loss: 8.584473609924316, test loss: 5.823784828186035\n",
      "h: 50 | epoch: 72, train loss: 8.58116626739502, test loss: 5.816352844238281\n",
      "h: 50 | epoch: 73, train loss: 8.578171730041504, test loss: 5.80940580368042\n",
      "h: 50 | epoch: 74, train loss: 8.575460433959961, test loss: 5.802907943725586\n",
      "h: 50 | epoch: 75, train loss: 8.573004722595215, test loss: 5.796828746795654\n",
      "h: 50 | epoch: 76, train loss: 8.570781707763672, test loss: 5.791136741638184\n",
      "h: 50 | epoch: 77, train loss: 8.568769454956055, test loss: 5.785804748535156\n",
      "h: 50 | epoch: 78, train loss: 8.566946029663086, test loss: 5.780807971954346\n",
      "h: 50 | epoch: 79, train loss: 8.565298080444336, test loss: 5.776124954223633\n",
      "h: 50 | epoch: 80, train loss: 8.563804626464844, test loss: 5.771729469299316\n",
      "h: 50 | epoch: 81, train loss: 8.562453269958496, test loss: 5.767606258392334\n",
      "h: 50 | epoch: 82, train loss: 8.561229705810547, test loss: 5.763734817504883\n",
      "h: 50 | epoch: 83, train loss: 8.560121536254883, test loss: 5.760097503662109\n",
      "h: 50 | epoch: 84, train loss: 8.559120178222656, test loss: 5.756680965423584\n",
      "h: 50 | epoch: 85, train loss: 8.558211326599121, test loss: 5.7534685134887695\n",
      "h: 50 | epoch: 86, train loss: 8.557392120361328, test loss: 5.750448703765869\n",
      "h: 50 | epoch: 87, train loss: 8.556647300720215, test loss: 5.747605323791504\n",
      "h: 50 | epoch: 88, train loss: 8.555974960327148, test loss: 5.744930744171143\n",
      "h: 50 | epoch: 89, train loss: 8.555367469787598, test loss: 5.742412090301514\n",
      "h: 50 | epoch: 90, train loss: 8.554816246032715, test loss: 5.740040302276611\n",
      "h: 50 | epoch: 91, train loss: 8.554316520690918, test loss: 5.737804889678955\n",
      "h: 50 | epoch: 92, train loss: 8.553865432739258, test loss: 5.7356977462768555\n",
      "h: 50 | epoch: 93, train loss: 8.553457260131836, test loss: 5.733712196350098\n",
      "h: 50 | epoch: 94, train loss: 8.553088188171387, test loss: 5.731839179992676\n",
      "h: 50 | epoch: 95, train loss: 8.552753448486328, test loss: 5.730071544647217\n",
      "h: 50 | epoch: 96, train loss: 8.552450180053711, test loss: 5.728403091430664\n",
      "h: 50 | epoch: 97, train loss: 8.552177429199219, test loss: 5.726830005645752\n",
      "h: 50 | epoch: 98, train loss: 8.55193042755127, test loss: 5.725343227386475\n",
      "h: 50 | epoch: 99, train loss: 8.551704406738281, test loss: 5.723938941955566\n",
      "h: 51 | epoch: 0, train loss: 48.90958023071289, test loss: 42.42277145385742\n",
      "h: 51 | epoch: 1, train loss: 45.818382263183594, test loss: 39.79526138305664\n",
      "h: 51 | epoch: 2, train loss: 42.96989822387695, test loss: 37.36543655395508\n",
      "h: 51 | epoch: 3, train loss: 40.34318542480469, test loss: 35.11682891845703\n",
      "h: 51 | epoch: 4, train loss: 37.919532775878906, test loss: 33.034629821777344\n",
      "h: 51 | epoch: 5, train loss: 35.68211364746094, test loss: 31.105493545532227\n",
      "h: 51 | epoch: 6, train loss: 33.61578369140625, test loss: 29.317367553710938\n",
      "h: 51 | epoch: 7, train loss: 31.706829071044922, test loss: 27.6592960357666\n",
      "h: 51 | epoch: 8, train loss: 29.942813873291016, test loss: 26.121318817138672\n",
      "h: 51 | epoch: 9, train loss: 28.31241798400879, test loss: 24.6943416595459\n",
      "h: 51 | epoch: 10, train loss: 26.805313110351562, test loss: 23.37004852294922\n",
      "h: 51 | epoch: 11, train loss: 25.412059783935547, test loss: 22.140811920166016\n",
      "h: 51 | epoch: 12, train loss: 24.12399673461914, test loss: 20.999631881713867\n",
      "h: 51 | epoch: 13, train loss: 22.933183670043945, test loss: 19.9400577545166\n",
      "h: 51 | epoch: 14, train loss: 21.83230209350586, test loss: 18.95614242553711\n",
      "h: 51 | epoch: 15, train loss: 20.814619064331055, test loss: 18.042407989501953\n",
      "h: 51 | epoch: 16, train loss: 19.87392234802246, test loss: 17.193782806396484\n",
      "h: 51 | epoch: 17, train loss: 19.004478454589844, test loss: 16.405582427978516\n",
      "h: 51 | epoch: 18, train loss: 18.200977325439453, test loss: 15.67344856262207\n",
      "h: 51 | epoch: 19, train loss: 17.458515167236328, test loss: 14.993368148803711\n",
      "h: 51 | epoch: 20, train loss: 16.77254867553711, test loss: 14.361607551574707\n",
      "h: 51 | epoch: 21, train loss: 16.138872146606445, test loss: 13.774696350097656\n",
      "h: 51 | epoch: 22, train loss: 15.553586959838867, test loss: 13.2294282913208\n",
      "h: 51 | epoch: 23, train loss: 15.013082504272461, test loss: 12.72281265258789\n",
      "h: 51 | epoch: 24, train loss: 14.514009475708008, test loss: 12.252080917358398\n",
      "h: 51 | epoch: 25, train loss: 14.053257942199707, test loss: 11.814658164978027\n",
      "h: 51 | epoch: 26, train loss: 13.627950668334961, test loss: 11.408151626586914\n",
      "h: 51 | epoch: 27, train loss: 13.235420227050781, test loss: 11.030342102050781\n",
      "h: 51 | epoch: 28, train loss: 12.873189926147461, test loss: 10.679166793823242\n",
      "h: 51 | epoch: 29, train loss: 12.53896713256836, test loss: 10.352710723876953\n",
      "h: 51 | epoch: 30, train loss: 12.230622291564941, test loss: 10.049188613891602\n",
      "h: 51 | epoch: 31, train loss: 11.946187973022461, test loss: 9.766950607299805\n",
      "h: 51 | epoch: 32, train loss: 11.683836936950684, test loss: 9.504464149475098\n",
      "h: 51 | epoch: 33, train loss: 11.441877365112305, test loss: 9.260297775268555\n",
      "h: 51 | epoch: 34, train loss: 11.218748092651367, test loss: 9.033132553100586\n",
      "h: 51 | epoch: 35, train loss: 11.012995719909668, test loss: 8.821741104125977\n",
      "h: 51 | epoch: 36, train loss: 10.823282241821289, test loss: 8.62498664855957\n",
      "h: 51 | epoch: 37, train loss: 10.648364067077637, test loss: 8.441805839538574\n",
      "h: 51 | epoch: 38, train loss: 10.487096786499023, test loss: 8.271225929260254\n",
      "h: 51 | epoch: 39, train loss: 10.338417053222656, test loss: 8.112329483032227\n",
      "h: 51 | epoch: 40, train loss: 10.201349258422852, test loss: 7.9642815589904785\n",
      "h: 51 | epoch: 41, train loss: 10.074983596801758, test loss: 7.826297760009766\n",
      "h: 51 | epoch: 42, train loss: 9.95848274230957, test loss: 7.697652339935303\n",
      "h: 51 | epoch: 43, train loss: 9.851078987121582, test loss: 7.577673435211182\n",
      "h: 51 | epoch: 44, train loss: 9.752055168151855, test loss: 7.4657416343688965\n",
      "h: 51 | epoch: 45, train loss: 9.66075611114502, test loss: 7.361277103424072\n",
      "h: 51 | epoch: 46, train loss: 9.576571464538574, test loss: 7.263747215270996\n",
      "h: 51 | epoch: 47, train loss: 9.498945236206055, test loss: 7.172658443450928\n",
      "h: 51 | epoch: 48, train loss: 9.42735767364502, test loss: 7.087550163269043\n",
      "h: 51 | epoch: 49, train loss: 9.361333847045898, test loss: 7.007997989654541\n",
      "h: 51 | epoch: 50, train loss: 9.300436973571777, test loss: 6.9336090087890625\n",
      "h: 51 | epoch: 51, train loss: 9.244260787963867, test loss: 6.86401891708374\n",
      "h: 51 | epoch: 52, train loss: 9.192434310913086, test loss: 6.79888916015625\n",
      "h: 51 | epoch: 53, train loss: 9.144613265991211, test loss: 6.737908363342285\n",
      "h: 51 | epoch: 54, train loss: 9.100479125976562, test loss: 6.680785179138184\n",
      "h: 51 | epoch: 55, train loss: 9.059747695922852, test loss: 6.627251625061035\n",
      "h: 51 | epoch: 56, train loss: 9.02214241027832, test loss: 6.577059745788574\n",
      "h: 51 | epoch: 57, train loss: 8.98742389678955, test loss: 6.529979705810547\n",
      "h: 51 | epoch: 58, train loss: 8.955361366271973, test loss: 6.485795021057129\n",
      "h: 51 | epoch: 59, train loss: 8.92574691772461, test loss: 6.444310188293457\n",
      "h: 51 | epoch: 60, train loss: 8.898386001586914, test loss: 6.405341148376465\n",
      "h: 51 | epoch: 61, train loss: 8.873103141784668, test loss: 6.368717670440674\n",
      "h: 51 | epoch: 62, train loss: 8.849738121032715, test loss: 6.3342814445495605\n",
      "h: 51 | epoch: 63, train loss: 8.828136444091797, test loss: 6.301886558532715\n",
      "h: 51 | epoch: 64, train loss: 8.808161735534668, test loss: 6.271397590637207\n",
      "h: 51 | epoch: 65, train loss: 8.789689064025879, test loss: 6.24268913269043\n",
      "h: 51 | epoch: 66, train loss: 8.772597312927246, test loss: 6.215641975402832\n",
      "h: 51 | epoch: 67, train loss: 8.756783485412598, test loss: 6.190148830413818\n",
      "h: 51 | epoch: 68, train loss: 8.742146492004395, test loss: 6.166108131408691\n",
      "h: 51 | epoch: 69, train loss: 8.728592872619629, test loss: 6.143426895141602\n",
      "h: 51 | epoch: 70, train loss: 8.716042518615723, test loss: 6.122018814086914\n",
      "h: 51 | epoch: 71, train loss: 8.704415321350098, test loss: 6.10180139541626\n",
      "h: 51 | epoch: 72, train loss: 8.693643569946289, test loss: 6.082699775695801\n",
      "h: 51 | epoch: 73, train loss: 8.6836576461792, test loss: 6.064644813537598\n",
      "h: 51 | epoch: 74, train loss: 8.674400329589844, test loss: 6.047569274902344\n",
      "h: 51 | epoch: 75, train loss: 8.665815353393555, test loss: 6.031414985656738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 51 | epoch: 76, train loss: 8.65785026550293, test loss: 6.0161237716674805\n",
      "h: 51 | epoch: 77, train loss: 8.650461196899414, test loss: 6.00164270401001\n",
      "h: 51 | epoch: 78, train loss: 8.643601417541504, test loss: 5.9879231452941895\n",
      "h: 51 | epoch: 79, train loss: 8.637231826782227, test loss: 5.974919319152832\n",
      "h: 51 | epoch: 80, train loss: 8.631317138671875, test loss: 5.962588787078857\n",
      "h: 51 | epoch: 81, train loss: 8.625821113586426, test loss: 5.950892925262451\n",
      "h: 51 | epoch: 82, train loss: 8.620715141296387, test loss: 5.939791202545166\n",
      "h: 51 | epoch: 83, train loss: 8.615967750549316, test loss: 5.929253101348877\n",
      "h: 51 | epoch: 84, train loss: 8.611554145812988, test loss: 5.919241905212402\n",
      "h: 51 | epoch: 85, train loss: 8.607447624206543, test loss: 5.909730434417725\n",
      "h: 51 | epoch: 86, train loss: 8.603628158569336, test loss: 5.900689125061035\n",
      "h: 51 | epoch: 87, train loss: 8.600072860717773, test loss: 5.892089366912842\n",
      "h: 51 | epoch: 88, train loss: 8.596763610839844, test loss: 5.883910179138184\n",
      "h: 51 | epoch: 89, train loss: 8.593681335449219, test loss: 5.876125812530518\n",
      "h: 51 | epoch: 90, train loss: 8.590810775756836, test loss: 5.868715286254883\n",
      "h: 51 | epoch: 91, train loss: 8.588135719299316, test loss: 5.861656665802002\n",
      "h: 51 | epoch: 92, train loss: 8.58564281463623, test loss: 5.85493278503418\n",
      "h: 51 | epoch: 93, train loss: 8.583318710327148, test loss: 5.84852409362793\n",
      "h: 51 | epoch: 94, train loss: 8.58115005493164, test loss: 5.842413425445557\n",
      "h: 51 | epoch: 95, train loss: 8.579129219055176, test loss: 5.836586952209473\n",
      "h: 51 | epoch: 96, train loss: 8.577241897583008, test loss: 5.831027507781982\n",
      "h: 51 | epoch: 97, train loss: 8.575482368469238, test loss: 5.825721740722656\n",
      "h: 51 | epoch: 98, train loss: 8.573838233947754, test loss: 5.820658206939697\n",
      "h: 51 | epoch: 99, train loss: 8.572303771972656, test loss: 5.815821647644043\n",
      "h: 52 | epoch: 0, train loss: 43.26456832885742, test loss: 37.47207260131836\n",
      "h: 52 | epoch: 1, train loss: 40.91230773925781, test loss: 35.403099060058594\n",
      "h: 52 | epoch: 2, train loss: 38.71842575073242, test loss: 33.47011184692383\n",
      "h: 52 | epoch: 3, train loss: 36.67136764526367, test loss: 31.663421630859375\n",
      "h: 52 | epoch: 4, train loss: 34.760643005371094, test loss: 29.97418212890625\n",
      "h: 52 | epoch: 5, train loss: 32.976680755615234, test loss: 28.394332885742188\n",
      "h: 52 | epoch: 6, train loss: 31.310733795166016, test loss: 26.91646385192871\n",
      "h: 52 | epoch: 7, train loss: 29.754779815673828, test loss: 25.533775329589844\n",
      "h: 52 | epoch: 8, train loss: 28.301433563232422, test loss: 24.239992141723633\n",
      "h: 52 | epoch: 9, train loss: 26.943899154663086, test loss: 23.02931785583496\n",
      "h: 52 | epoch: 10, train loss: 25.675884246826172, test loss: 21.896387100219727\n",
      "h: 52 | epoch: 11, train loss: 24.491558074951172, test loss: 20.83620834350586\n",
      "h: 52 | epoch: 12, train loss: 23.385517120361328, test loss: 19.844148635864258\n",
      "h: 52 | epoch: 13, train loss: 22.352718353271484, test loss: 18.915895462036133\n",
      "h: 52 | epoch: 14, train loss: 21.388479232788086, test loss: 18.047428131103516\n",
      "h: 52 | epoch: 15, train loss: 20.488420486450195, test loss: 17.234968185424805\n",
      "h: 52 | epoch: 16, train loss: 19.648452758789062, test loss: 16.47500991821289\n",
      "h: 52 | epoch: 17, train loss: 18.864755630493164, test loss: 15.764259338378906\n",
      "h: 52 | epoch: 18, train loss: 18.133743286132812, test loss: 15.099629402160645\n",
      "h: 52 | epoch: 19, train loss: 17.45206069946289, test loss: 14.478227615356445\n",
      "h: 52 | epoch: 20, train loss: 16.81656265258789, test loss: 13.897336959838867\n",
      "h: 52 | epoch: 21, train loss: 16.224292755126953, test loss: 13.354411125183105\n",
      "h: 52 | epoch: 22, train loss: 15.67248249053955, test loss: 12.847061157226562\n",
      "h: 52 | epoch: 23, train loss: 15.158526420593262, test loss: 12.373029708862305\n",
      "h: 52 | epoch: 24, train loss: 14.67998218536377, test loss: 11.93021297454834\n",
      "h: 52 | epoch: 25, train loss: 14.234550476074219, test loss: 11.51662540435791\n",
      "h: 52 | epoch: 26, train loss: 13.820080757141113, test loss: 11.130398750305176\n",
      "h: 52 | epoch: 27, train loss: 13.434539794921875, test loss: 10.76978588104248\n",
      "h: 52 | epoch: 28, train loss: 13.076029777526855, test loss: 10.433141708374023\n",
      "h: 52 | epoch: 29, train loss: 12.742764472961426, test loss: 10.118921279907227\n",
      "h: 52 | epoch: 30, train loss: 12.433065414428711, test loss: 9.82567024230957\n",
      "h: 52 | epoch: 31, train loss: 12.145359992980957, test loss: 9.552027702331543\n",
      "h: 52 | epoch: 32, train loss: 11.878171920776367, test loss: 9.29671573638916\n",
      "h: 52 | epoch: 33, train loss: 11.630114555358887, test loss: 9.058533668518066\n",
      "h: 52 | epoch: 34, train loss: 11.39989185333252, test loss: 8.83635139465332\n",
      "h: 52 | epoch: 35, train loss: 11.186286926269531, test loss: 8.629119873046875\n",
      "h: 52 | epoch: 36, train loss: 10.988158226013184, test loss: 8.435848236083984\n",
      "h: 52 | epoch: 37, train loss: 10.80444049835205, test loss: 8.255606651306152\n",
      "h: 52 | epoch: 38, train loss: 10.634133338928223, test loss: 8.08752727508545\n",
      "h: 52 | epoch: 39, train loss: 10.47630500793457, test loss: 7.9307966232299805\n",
      "h: 52 | epoch: 40, train loss: 10.33008098602295, test loss: 7.784655570983887\n",
      "h: 52 | epoch: 41, train loss: 10.194642066955566, test loss: 7.648387908935547\n",
      "h: 52 | epoch: 42, train loss: 10.069229125976562, test loss: 7.5213303565979\n",
      "h: 52 | epoch: 43, train loss: 9.953128814697266, test loss: 7.40286111831665\n",
      "h: 52 | epoch: 44, train loss: 9.84567642211914, test loss: 7.292394161224365\n",
      "h: 52 | epoch: 45, train loss: 9.746252059936523, test loss: 7.189389228820801\n",
      "h: 52 | epoch: 46, train loss: 9.654277801513672, test loss: 7.0933332443237305\n",
      "h: 52 | epoch: 47, train loss: 9.5692138671875, test loss: 7.003758907318115\n",
      "h: 52 | epoch: 48, train loss: 9.490560531616211, test loss: 6.920217990875244\n",
      "h: 52 | epoch: 49, train loss: 9.417848587036133, test loss: 6.842298984527588\n",
      "h: 52 | epoch: 50, train loss: 9.350645065307617, test loss: 6.769617557525635\n",
      "h: 52 | epoch: 51, train loss: 9.288542747497559, test loss: 6.701812744140625\n",
      "h: 52 | epoch: 52, train loss: 9.231167793273926, test loss: 6.638552188873291\n",
      "h: 52 | epoch: 53, train loss: 9.178170204162598, test loss: 6.579520225524902\n",
      "h: 52 | epoch: 54, train loss: 9.129222869873047, test loss: 6.5244269371032715\n",
      "h: 52 | epoch: 55, train loss: 9.084027290344238, test loss: 6.4730024337768555\n",
      "h: 52 | epoch: 56, train loss: 9.042299270629883, test loss: 6.424994468688965\n",
      "h: 52 | epoch: 57, train loss: 9.003782272338867, test loss: 6.380168437957764\n",
      "h: 52 | epoch: 58, train loss: 8.968234062194824, test loss: 6.338302135467529\n",
      "h: 52 | epoch: 59, train loss: 8.935430526733398, test loss: 6.299195289611816\n",
      "h: 52 | epoch: 60, train loss: 8.905162811279297, test loss: 6.262658596038818\n",
      "h: 52 | epoch: 61, train loss: 8.877241134643555, test loss: 6.228512763977051\n",
      "h: 52 | epoch: 62, train loss: 8.851486206054688, test loss: 6.196596622467041\n",
      "h: 52 | epoch: 63, train loss: 8.827733039855957, test loss: 6.166754722595215\n",
      "h: 52 | epoch: 64, train loss: 8.805827140808105, test loss: 6.138848304748535\n",
      "h: 52 | epoch: 65, train loss: 8.785628318786621, test loss: 6.112741470336914\n",
      "h: 52 | epoch: 66, train loss: 8.767007827758789, test loss: 6.088313579559326\n",
      "h: 52 | epoch: 67, train loss: 8.749841690063477, test loss: 6.065451145172119\n",
      "h: 52 | epoch: 68, train loss: 8.734018325805664, test loss: 6.044046401977539\n",
      "h: 52 | epoch: 69, train loss: 8.719435691833496, test loss: 6.02400016784668\n",
      "h: 52 | epoch: 70, train loss: 8.7059965133667, test loss: 6.005221366882324\n",
      "h: 52 | epoch: 71, train loss: 8.693613052368164, test loss: 5.987624168395996\n",
      "h: 52 | epoch: 72, train loss: 8.682201385498047, test loss: 5.971129417419434\n",
      "h: 52 | epoch: 73, train loss: 8.671688079833984, test loss: 5.955663681030273\n",
      "h: 52 | epoch: 74, train loss: 8.662001609802246, test loss: 5.941157817840576\n",
      "h: 52 | epoch: 75, train loss: 8.65307903289795, test loss: 5.927546501159668\n",
      "h: 52 | epoch: 76, train loss: 8.644861221313477, test loss: 5.91477108001709\n",
      "h: 52 | epoch: 77, train loss: 8.637292861938477, test loss: 5.902776718139648\n",
      "h: 52 | epoch: 78, train loss: 8.630321502685547, test loss: 5.891510486602783\n",
      "h: 52 | epoch: 79, train loss: 8.6239013671875, test loss: 5.880926609039307\n",
      "h: 52 | epoch: 80, train loss: 8.617989540100098, test loss: 5.8709797859191895\n",
      "h: 52 | epoch: 81, train loss: 8.61254596710205, test loss: 5.861626625061035\n",
      "h: 52 | epoch: 82, train loss: 8.60753345489502, test loss: 5.85283088684082\n",
      "h: 52 | epoch: 83, train loss: 8.602916717529297, test loss: 5.844554901123047\n",
      "h: 52 | epoch: 84, train loss: 8.598669052124023, test loss: 5.836766242980957\n",
      "h: 52 | epoch: 85, train loss: 8.594758033752441, test loss: 5.829433441162109\n",
      "h: 52 | epoch: 86, train loss: 8.591156005859375, test loss: 5.822526931762695\n",
      "h: 52 | epoch: 87, train loss: 8.58784008026123, test loss: 5.816019535064697\n",
      "h: 52 | epoch: 88, train loss: 8.58478832244873, test loss: 5.809886932373047\n",
      "h: 52 | epoch: 89, train loss: 8.581979751586914, test loss: 5.804104328155518\n",
      "h: 52 | epoch: 90, train loss: 8.579394340515137, test loss: 5.798651695251465\n",
      "h: 52 | epoch: 91, train loss: 8.577014923095703, test loss: 5.793507099151611\n",
      "h: 52 | epoch: 92, train loss: 8.574824333190918, test loss: 5.788651466369629\n",
      "h: 52 | epoch: 93, train loss: 8.572809219360352, test loss: 5.784067153930664\n",
      "h: 52 | epoch: 94, train loss: 8.570953369140625, test loss: 5.779737949371338\n",
      "h: 52 | epoch: 95, train loss: 8.569246292114258, test loss: 5.77564811706543\n",
      "h: 52 | epoch: 96, train loss: 8.56767463684082, test loss: 5.771782875061035\n",
      "h: 52 | epoch: 97, train loss: 8.566228866577148, test loss: 5.768129825592041\n",
      "h: 52 | epoch: 98, train loss: 8.564898490905762, test loss: 5.764673709869385\n",
      "h: 52 | epoch: 99, train loss: 8.563674926757812, test loss: 5.761404991149902\n",
      "h: 53 | epoch: 0, train loss: 46.993045806884766, test loss: 39.90840530395508\n",
      "h: 53 | epoch: 1, train loss: 43.4489860534668, test loss: 36.896644592285156\n",
      "h: 53 | epoch: 2, train loss: 40.231590270996094, test loss: 34.15435028076172\n",
      "h: 53 | epoch: 3, train loss: 37.3091926574707, test loss: 31.656112670898438\n",
      "h: 53 | epoch: 4, train loss: 34.653663635253906, test loss: 29.379261016845703\n",
      "h: 53 | epoch: 5, train loss: 32.23992919921875, test loss: 27.303508758544922\n",
      "h: 53 | epoch: 6, train loss: 30.045562744140625, test loss: 25.410640716552734\n",
      "h: 53 | epoch: 7, train loss: 28.0504207611084, test loss: 23.684247970581055\n",
      "h: 53 | epoch: 8, train loss: 26.23638343811035, test loss: 22.109508514404297\n",
      "h: 53 | epoch: 9, train loss: 24.587074279785156, test loss: 20.673015594482422\n",
      "h: 53 | epoch: 10, train loss: 23.087688446044922, test loss: 19.362606048583984\n",
      "h: 53 | epoch: 11, train loss: 21.724803924560547, test loss: 18.1672306060791\n",
      "h: 53 | epoch: 12, train loss: 20.486230850219727, test loss: 17.07682228088379\n",
      "h: 53 | epoch: 13, train loss: 19.36089324951172, test loss: 16.082233428955078\n",
      "h: 53 | epoch: 14, train loss: 18.338703155517578, test loss: 15.175107955932617\n",
      "h: 53 | epoch: 15, train loss: 17.410463333129883, test loss: 14.347826957702637\n",
      "h: 53 | epoch: 16, train loss: 16.567798614501953, test loss: 13.593424797058105\n",
      "h: 53 | epoch: 17, train loss: 15.803064346313477, test loss: 12.905550003051758\n",
      "h: 53 | epoch: 18, train loss: 15.109281539916992, test loss: 12.278385162353516\n",
      "h: 53 | epoch: 19, train loss: 14.480081558227539, test loss: 11.706623077392578\n",
      "h: 53 | epoch: 20, train loss: 13.909652709960938, test loss: 11.185410499572754\n",
      "h: 53 | epoch: 21, train loss: 13.392683029174805, test loss: 10.710305213928223\n",
      "h: 53 | epoch: 22, train loss: 12.92432975769043, test loss: 10.277251243591309\n",
      "h: 53 | epoch: 23, train loss: 12.500165939331055, test loss: 9.882545471191406\n",
      "h: 53 | epoch: 24, train loss: 12.116161346435547, test loss: 9.522793769836426\n",
      "h: 53 | epoch: 25, train loss: 11.76862907409668, test loss: 9.194902420043945\n",
      "h: 53 | epoch: 26, train loss: 11.454211235046387, test loss: 8.89604663848877\n",
      "h: 53 | epoch: 27, train loss: 11.169853210449219, test loss: 8.623641014099121\n",
      "h: 53 | epoch: 28, train loss: 10.912758827209473, test loss: 8.375333786010742\n",
      "h: 53 | epoch: 29, train loss: 10.680392265319824, test loss: 8.14897346496582\n",
      "h: 53 | epoch: 30, train loss: 10.470439910888672, test loss: 7.942597389221191\n",
      "h: 53 | epoch: 31, train loss: 10.280797004699707, test loss: 7.754417419433594\n",
      "h: 53 | epoch: 32, train loss: 10.109550476074219, test loss: 7.5828046798706055\n",
      "h: 53 | epoch: 33, train loss: 9.95495891571045, test loss: 7.426270484924316\n",
      "h: 53 | epoch: 34, train loss: 9.815442085266113, test loss: 7.283461093902588\n",
      "h: 53 | epoch: 35, train loss: 9.689562797546387, test loss: 7.153147220611572\n",
      "h: 53 | epoch: 36, train loss: 9.576014518737793, test loss: 7.034204006195068\n",
      "h: 53 | epoch: 37, train loss: 9.47362232208252, test loss: 6.925608158111572\n",
      "h: 53 | epoch: 38, train loss: 9.381304740905762, test loss: 6.826432228088379\n",
      "h: 53 | epoch: 39, train loss: 9.298093795776367, test loss: 6.735827445983887\n",
      "h: 53 | epoch: 40, train loss: 9.223106384277344, test loss: 6.653029441833496\n",
      "h: 53 | epoch: 41, train loss: 9.15554428100586, test loss: 6.577332496643066\n",
      "h: 53 | epoch: 42, train loss: 9.094685554504395, test loss: 6.508103847503662\n",
      "h: 53 | epoch: 43, train loss: 9.039874076843262, test loss: 6.44476318359375\n",
      "h: 53 | epoch: 44, train loss: 8.990519523620605, test loss: 6.386785507202148\n",
      "h: 53 | epoch: 45, train loss: 8.946087837219238, test loss: 6.333691596984863\n",
      "h: 53 | epoch: 46, train loss: 8.90609073638916, test loss: 6.285050392150879\n",
      "h: 53 | epoch: 47, train loss: 8.870094299316406, test loss: 6.240462303161621\n",
      "h: 53 | epoch: 48, train loss: 8.837703704833984, test loss: 6.19957160949707\n",
      "h: 53 | epoch: 49, train loss: 8.808561325073242, test loss: 6.162051200866699\n",
      "h: 53 | epoch: 50, train loss: 8.78234577178955, test loss: 6.1276044845581055\n",
      "h: 53 | epoch: 51, train loss: 8.758763313293457, test loss: 6.0959625244140625\n",
      "h: 53 | epoch: 52, train loss: 8.737556457519531, test loss: 6.066880702972412\n",
      "h: 53 | epoch: 53, train loss: 8.718485832214355, test loss: 6.040134906768799\n",
      "h: 53 | epoch: 54, train loss: 8.701338768005371, test loss: 6.015524387359619\n",
      "h: 53 | epoch: 55, train loss: 8.685922622680664, test loss: 5.99286413192749\n",
      "h: 53 | epoch: 56, train loss: 8.672064781188965, test loss: 5.971987724304199\n",
      "h: 53 | epoch: 57, train loss: 8.659608840942383, test loss: 5.952742099761963\n",
      "h: 53 | epoch: 58, train loss: 8.648412704467773, test loss: 5.934988975524902\n",
      "h: 53 | epoch: 59, train loss: 8.638352394104004, test loss: 5.91860294342041\n",
      "h: 53 | epoch: 60, train loss: 8.629310607910156, test loss: 5.903467655181885\n",
      "h: 53 | epoch: 61, train loss: 8.621187210083008, test loss: 5.889480113983154\n",
      "h: 53 | epoch: 62, train loss: 8.613886833190918, test loss: 5.876544952392578\n",
      "h: 53 | epoch: 63, train loss: 8.607330322265625, test loss: 5.864573955535889\n",
      "h: 53 | epoch: 64, train loss: 8.601438522338867, test loss: 5.853489875793457\n",
      "h: 53 | epoch: 65, train loss: 8.596147537231445, test loss: 5.8432207107543945\n",
      "h: 53 | epoch: 66, train loss: 8.59139347076416, test loss: 5.83369779586792\n",
      "h: 53 | epoch: 67, train loss: 8.587122917175293, test loss: 5.824865341186523\n",
      "h: 53 | epoch: 68, train loss: 8.583290100097656, test loss: 5.816665172576904\n",
      "h: 53 | epoch: 69, train loss: 8.579845428466797, test loss: 5.809048175811768\n",
      "h: 53 | epoch: 70, train loss: 8.576753616333008, test loss: 5.801969528198242\n",
      "h: 53 | epoch: 71, train loss: 8.573975563049316, test loss: 5.795387268066406\n",
      "h: 53 | epoch: 72, train loss: 8.571481704711914, test loss: 5.789260387420654\n",
      "h: 53 | epoch: 73, train loss: 8.569241523742676, test loss: 5.783556938171387\n",
      "h: 53 | epoch: 74, train loss: 8.567232131958008, test loss: 5.778244495391846\n",
      "h: 53 | epoch: 75, train loss: 8.565424919128418, test loss: 5.773291110992432\n",
      "h: 53 | epoch: 76, train loss: 8.563803672790527, test loss: 5.768671989440918\n",
      "h: 53 | epoch: 77, train loss: 8.562348365783691, test loss: 5.7643632888793945\n",
      "h: 53 | epoch: 78, train loss: 8.561041831970215, test loss: 5.760338306427002\n",
      "h: 53 | epoch: 79, train loss: 8.559868812561035, test loss: 5.7565813064575195\n",
      "h: 53 | epoch: 80, train loss: 8.558815956115723, test loss: 5.753067970275879\n",
      "h: 53 | epoch: 81, train loss: 8.557870864868164, test loss: 5.749783992767334\n",
      "h: 53 | epoch: 82, train loss: 8.557021141052246, test loss: 5.746712684631348\n",
      "h: 53 | epoch: 83, train loss: 8.556260108947754, test loss: 5.743837356567383\n",
      "h: 53 | epoch: 84, train loss: 8.555575370788574, test loss: 5.741147041320801\n",
      "h: 53 | epoch: 85, train loss: 8.554961204528809, test loss: 5.738626003265381\n",
      "h: 53 | epoch: 86, train loss: 8.554409980773926, test loss: 5.736263275146484\n",
      "h: 53 | epoch: 87, train loss: 8.553915023803711, test loss: 5.734047889709473\n",
      "h: 53 | epoch: 88, train loss: 8.553471565246582, test loss: 5.731972694396973\n",
      "h: 53 | epoch: 89, train loss: 8.553071975708008, test loss: 5.730025291442871\n",
      "h: 53 | epoch: 90, train loss: 8.552713394165039, test loss: 5.728196144104004\n",
      "h: 53 | epoch: 91, train loss: 8.552392959594727, test loss: 5.726481914520264\n",
      "h: 53 | epoch: 92, train loss: 8.552103996276855, test loss: 5.724869728088379\n",
      "h: 53 | epoch: 93, train loss: 8.55184555053711, test loss: 5.723355293273926\n",
      "h: 53 | epoch: 94, train loss: 8.551612854003906, test loss: 5.721933841705322\n",
      "h: 53 | epoch: 95, train loss: 8.55140495300293, test loss: 5.720596790313721\n",
      "h: 53 | epoch: 96, train loss: 8.551218032836914, test loss: 5.7193403244018555\n",
      "h: 53 | epoch: 97, train loss: 8.55104923248291, test loss: 5.718157768249512\n",
      "h: 53 | epoch: 98, train loss: 8.550898551940918, test loss: 5.71704626083374\n",
      "h: 53 | epoch: 99, train loss: 8.550763130187988, test loss: 5.716000556945801\n",
      "h: 54 | epoch: 0, train loss: 40.43170166015625, test loss: 35.149192810058594\n",
      "h: 54 | epoch: 1, train loss: 38.18661117553711, test loss: 33.177127838134766\n",
      "h: 54 | epoch: 2, train loss: 36.09724807739258, test loss: 31.33754539489746\n",
      "h: 54 | epoch: 3, train loss: 34.15217208862305, test loss: 29.621002197265625\n",
      "h: 54 | epoch: 4, train loss: 32.340980529785156, test loss: 28.018850326538086\n",
      "h: 54 | epoch: 5, train loss: 30.654163360595703, test loss: 26.5231876373291\n",
      "h: 54 | epoch: 6, train loss: 29.083011627197266, test loss: 25.126747131347656\n",
      "h: 54 | epoch: 7, train loss: 27.6195125579834, test loss: 23.82282066345215\n",
      "h: 54 | epoch: 8, train loss: 26.256311416625977, test loss: 22.60523796081543\n",
      "h: 54 | epoch: 9, train loss: 24.986587524414062, test loss: 21.468265533447266\n",
      "h: 54 | epoch: 10, train loss: 23.80405044555664, test loss: 20.406600952148438\n",
      "h: 54 | epoch: 11, train loss: 22.70285415649414, test loss: 19.41531753540039\n",
      "h: 54 | epoch: 12, train loss: 21.67757797241211, test loss: 18.489826202392578\n",
      "h: 54 | epoch: 13, train loss: 20.72317886352539, test loss: 17.62584114074707\n",
      "h: 54 | epoch: 14, train loss: 19.834949493408203, test loss: 16.819385528564453\n",
      "h: 54 | epoch: 15, train loss: 19.008506774902344, test loss: 16.06672477722168\n",
      "h: 54 | epoch: 16, train loss: 18.23975944519043, test loss: 15.364381790161133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 54 | epoch: 17, train loss: 17.52488136291504, test loss: 14.70909309387207\n",
      "h: 54 | epoch: 18, train loss: 16.860294342041016, test loss: 14.097810745239258\n",
      "h: 54 | epoch: 19, train loss: 16.242652893066406, test loss: 13.527673721313477\n",
      "h: 54 | epoch: 20, train loss: 15.668825149536133, test loss: 12.996007919311523\n",
      "h: 54 | epoch: 21, train loss: 15.135876655578613, test loss: 12.500297546386719\n",
      "h: 54 | epoch: 22, train loss: 14.6410551071167, test loss: 12.038196563720703\n",
      "h: 54 | epoch: 23, train loss: 14.181797981262207, test loss: 11.607495307922363\n",
      "h: 54 | epoch: 24, train loss: 13.755688667297363, test loss: 11.206123352050781\n",
      "h: 54 | epoch: 25, train loss: 13.360468864440918, test loss: 10.83215045928955\n",
      "h: 54 | epoch: 26, train loss: 12.994028091430664, test loss: 10.483750343322754\n",
      "h: 54 | epoch: 27, train loss: 12.654388427734375, test loss: 10.159223556518555\n",
      "h: 54 | epoch: 28, train loss: 12.339693069458008, test loss: 9.856975555419922\n",
      "h: 54 | epoch: 29, train loss: 12.048210144042969, test loss: 9.575511932373047\n",
      "h: 54 | epoch: 30, train loss: 11.778319358825684, test loss: 9.313430786132812\n",
      "h: 54 | epoch: 31, train loss: 11.528504371643066, test loss: 9.069422721862793\n",
      "h: 54 | epoch: 32, train loss: 11.297345161437988, test loss: 8.84226131439209\n",
      "h: 54 | epoch: 33, train loss: 11.083517074584961, test loss: 8.630800247192383\n",
      "h: 54 | epoch: 34, train loss: 10.885787963867188, test loss: 8.433965682983398\n",
      "h: 54 | epoch: 35, train loss: 10.702996253967285, test loss: 8.250755310058594\n",
      "h: 54 | epoch: 36, train loss: 10.534070014953613, test loss: 8.080229759216309\n",
      "h: 54 | epoch: 37, train loss: 10.37800121307373, test loss: 7.921516418457031\n",
      "h: 54 | epoch: 38, train loss: 10.233854293823242, test loss: 7.773796081542969\n",
      "h: 54 | epoch: 39, train loss: 10.10075855255127, test loss: 7.6363067626953125\n",
      "h: 54 | epoch: 40, train loss: 9.977897644042969, test loss: 7.508336067199707\n",
      "h: 54 | epoch: 41, train loss: 9.86451530456543, test loss: 7.389219760894775\n",
      "h: 54 | epoch: 42, train loss: 9.759910583496094, test loss: 7.278340816497803\n",
      "h: 54 | epoch: 43, train loss: 9.663426399230957, test loss: 7.175121307373047\n",
      "h: 54 | epoch: 44, train loss: 9.574456214904785, test loss: 7.0790228843688965\n",
      "h: 54 | epoch: 45, train loss: 9.492433547973633, test loss: 6.9895453453063965\n",
      "h: 54 | epoch: 46, train loss: 9.416833877563477, test loss: 6.906224250793457\n",
      "h: 54 | epoch: 47, train loss: 9.347169876098633, test loss: 6.828624725341797\n",
      "h: 54 | epoch: 48, train loss: 9.282990455627441, test loss: 6.756342887878418\n",
      "h: 54 | epoch: 49, train loss: 9.223875045776367, test loss: 6.689004421234131\n",
      "h: 54 | epoch: 50, train loss: 9.169438362121582, test loss: 6.62625789642334\n",
      "h: 54 | epoch: 51, train loss: 9.119318008422852, test loss: 6.567782402038574\n",
      "h: 54 | epoch: 52, train loss: 9.073179244995117, test loss: 6.513276100158691\n",
      "h: 54 | epoch: 53, train loss: 9.030715942382812, test loss: 6.462455749511719\n",
      "h: 54 | epoch: 54, train loss: 8.991643905639648, test loss: 6.415061950683594\n",
      "h: 54 | epoch: 55, train loss: 8.955694198608398, test loss: 6.370854377746582\n",
      "h: 54 | epoch: 56, train loss: 8.922626495361328, test loss: 6.329606533050537\n",
      "h: 54 | epoch: 57, train loss: 8.8922119140625, test loss: 6.291111946105957\n",
      "h: 54 | epoch: 58, train loss: 8.864246368408203, test loss: 6.2551751136779785\n",
      "h: 54 | epoch: 59, train loss: 8.838532447814941, test loss: 6.221615791320801\n",
      "h: 54 | epoch: 60, train loss: 8.81489372253418, test loss: 6.190268516540527\n",
      "h: 54 | epoch: 61, train loss: 8.793166160583496, test loss: 6.160978317260742\n",
      "h: 54 | epoch: 62, train loss: 8.773198127746582, test loss: 6.133601188659668\n",
      "h: 54 | epoch: 63, train loss: 8.754847526550293, test loss: 6.108004570007324\n",
      "h: 54 | epoch: 64, train loss: 8.737988471984863, test loss: 6.084061622619629\n",
      "h: 54 | epoch: 65, train loss: 8.722497940063477, test loss: 6.0616607666015625\n",
      "h: 54 | epoch: 66, train loss: 8.708269119262695, test loss: 6.040693283081055\n",
      "h: 54 | epoch: 67, train loss: 8.69520092010498, test loss: 6.021061897277832\n",
      "h: 54 | epoch: 68, train loss: 8.683198928833008, test loss: 6.002673625946045\n",
      "h: 54 | epoch: 69, train loss: 8.672178268432617, test loss: 5.985442161560059\n",
      "h: 54 | epoch: 70, train loss: 8.66205883026123, test loss: 5.969292163848877\n",
      "h: 54 | epoch: 71, train loss: 8.6527681350708, test loss: 5.954146385192871\n",
      "h: 54 | epoch: 72, train loss: 8.64423942565918, test loss: 5.939938545227051\n",
      "h: 54 | epoch: 73, train loss: 8.636408805847168, test loss: 5.926604270935059\n",
      "h: 54 | epoch: 74, train loss: 8.629222869873047, test loss: 5.91408634185791\n",
      "h: 54 | epoch: 75, train loss: 8.622626304626465, test loss: 5.902328014373779\n",
      "h: 54 | epoch: 76, train loss: 8.616573333740234, test loss: 5.891280174255371\n",
      "h: 54 | epoch: 77, train loss: 8.611019134521484, test loss: 5.88089656829834\n",
      "h: 54 | epoch: 78, train loss: 8.605921745300293, test loss: 5.871131420135498\n",
      "h: 54 | epoch: 79, train loss: 8.601245880126953, test loss: 5.8619465827941895\n",
      "h: 54 | epoch: 80, train loss: 8.596956253051758, test loss: 5.85330057144165\n",
      "h: 54 | epoch: 81, train loss: 8.593019485473633, test loss: 5.8451619148254395\n",
      "h: 54 | epoch: 82, train loss: 8.589408874511719, test loss: 5.837495803833008\n",
      "h: 54 | epoch: 83, train loss: 8.586097717285156, test loss: 5.830271244049072\n",
      "h: 54 | epoch: 84, train loss: 8.583059310913086, test loss: 5.82346248626709\n",
      "h: 54 | epoch: 85, train loss: 8.580273628234863, test loss: 5.817041873931885\n",
      "h: 54 | epoch: 86, train loss: 8.577716827392578, test loss: 5.810983657836914\n",
      "h: 54 | epoch: 87, train loss: 8.575373649597168, test loss: 5.805267333984375\n",
      "h: 54 | epoch: 88, train loss: 8.573225021362305, test loss: 5.799870491027832\n",
      "h: 54 | epoch: 89, train loss: 8.571253776550293, test loss: 5.794773578643799\n",
      "h: 54 | epoch: 90, train loss: 8.56944751739502, test loss: 5.7899580001831055\n",
      "h: 54 | epoch: 91, train loss: 8.567790985107422, test loss: 5.785407543182373\n",
      "h: 54 | epoch: 92, train loss: 8.56627082824707, test loss: 5.781103134155273\n",
      "h: 54 | epoch: 93, train loss: 8.5648775100708, test loss: 5.77703332901001\n",
      "h: 54 | epoch: 94, train loss: 8.563600540161133, test loss: 5.773182392120361\n",
      "h: 54 | epoch: 95, train loss: 8.562429428100586, test loss: 5.76953649520874\n",
      "h: 54 | epoch: 96, train loss: 8.561356544494629, test loss: 5.766085147857666\n",
      "h: 54 | epoch: 97, train loss: 8.560372352600098, test loss: 5.762816905975342\n",
      "h: 54 | epoch: 98, train loss: 8.559469223022461, test loss: 5.759718894958496\n",
      "h: 54 | epoch: 99, train loss: 8.558642387390137, test loss: 5.756784439086914\n",
      "h: 55 | epoch: 0, train loss: 46.38898849487305, test loss: 39.09634780883789\n",
      "h: 55 | epoch: 1, train loss: 43.55295181274414, test loss: 36.69375228881836\n",
      "h: 55 | epoch: 2, train loss: 40.93100357055664, test loss: 34.46742248535156\n",
      "h: 55 | epoch: 3, train loss: 38.505531311035156, test loss: 32.40325164794922\n",
      "h: 55 | epoch: 4, train loss: 36.260704040527344, test loss: 30.48854637145996\n",
      "h: 55 | epoch: 5, train loss: 34.1822395324707, test loss: 28.71181297302246\n",
      "h: 55 | epoch: 6, train loss: 32.2572135925293, test loss: 27.06258773803711\n",
      "h: 55 | epoch: 7, train loss: 30.473888397216797, test loss: 25.531352996826172\n",
      "h: 55 | epoch: 8, train loss: 28.82157325744629, test loss: 24.109416961669922\n",
      "h: 55 | epoch: 9, train loss: 27.29048728942871, test loss: 22.788806915283203\n",
      "h: 55 | epoch: 10, train loss: 25.871685028076172, test loss: 21.562198638916016\n",
      "h: 55 | epoch: 11, train loss: 24.556955337524414, test loss: 20.42285919189453\n",
      "h: 55 | epoch: 12, train loss: 23.33872413635254, test loss: 19.364578247070312\n",
      "h: 55 | epoch: 13, train loss: 22.210018157958984, test loss: 18.381616592407227\n",
      "h: 55 | epoch: 14, train loss: 21.164405822753906, test loss: 17.46865463256836\n",
      "h: 55 | epoch: 15, train loss: 20.195920944213867, test loss: 16.620779037475586\n",
      "h: 55 | epoch: 16, train loss: 19.299053192138672, test loss: 15.833417892456055\n",
      "h: 55 | epoch: 17, train loss: 18.468677520751953, test loss: 15.102340698242188\n",
      "h: 55 | epoch: 18, train loss: 17.700054168701172, test loss: 14.423601150512695\n",
      "h: 55 | epoch: 19, train loss: 16.9887638092041, test loss: 13.793533325195312\n",
      "h: 55 | epoch: 20, train loss: 16.330707550048828, test loss: 13.208727836608887\n",
      "h: 55 | epoch: 21, train loss: 15.722071647644043, test loss: 12.666013717651367\n",
      "h: 55 | epoch: 22, train loss: 15.15930461883545, test loss: 12.162433624267578\n",
      "h: 55 | epoch: 23, train loss: 14.639106750488281, test loss: 11.695226669311523\n",
      "h: 55 | epoch: 24, train loss: 14.158398628234863, test loss: 11.261833190917969\n",
      "h: 55 | epoch: 25, train loss: 13.714320182800293, test loss: 10.859858512878418\n",
      "h: 55 | epoch: 26, train loss: 13.304206848144531, test loss: 10.487074851989746\n",
      "h: 55 | epoch: 27, train loss: 12.925577163696289, test loss: 10.141408920288086\n",
      "h: 55 | epoch: 28, train loss: 12.57612419128418, test loss: 9.82091999053955\n",
      "h: 55 | epoch: 29, train loss: 12.253694534301758, test loss: 9.523810386657715\n",
      "h: 55 | epoch: 30, train loss: 11.956292152404785, test loss: 9.248403549194336\n",
      "h: 55 | epoch: 31, train loss: 11.682055473327637, test loss: 8.993135452270508\n",
      "h: 55 | epoch: 32, train loss: 11.429258346557617, test loss: 8.756551742553711\n",
      "h: 55 | epoch: 33, train loss: 11.196290969848633, test loss: 8.537301063537598\n",
      "h: 55 | epoch: 34, train loss: 10.981660842895508, test loss: 8.334123611450195\n",
      "h: 55 | epoch: 35, train loss: 10.783979415893555, test loss: 8.145849227905273\n",
      "h: 55 | epoch: 36, train loss: 10.601961135864258, test loss: 7.9713921546936035\n",
      "h: 55 | epoch: 37, train loss: 10.434408187866211, test loss: 7.809735298156738\n",
      "h: 55 | epoch: 38, train loss: 10.280213356018066, test loss: 7.659945011138916\n",
      "h: 55 | epoch: 39, train loss: 10.138347625732422, test loss: 7.52114200592041\n",
      "h: 55 | epoch: 40, train loss: 10.007858276367188, test loss: 7.392525672912598\n",
      "h: 55 | epoch: 41, train loss: 9.887863159179688, test loss: 7.273335933685303\n",
      "h: 55 | epoch: 42, train loss: 9.777544021606445, test loss: 7.16287899017334\n",
      "h: 55 | epoch: 43, train loss: 9.676142692565918, test loss: 7.060507297515869\n",
      "h: 55 | epoch: 44, train loss: 9.58296012878418, test loss: 6.965619087219238\n",
      "h: 55 | epoch: 45, train loss: 9.497350692749023, test loss: 6.877661228179932\n",
      "h: 55 | epoch: 46, train loss: 9.41871166229248, test loss: 6.796115875244141\n",
      "h: 55 | epoch: 47, train loss: 9.346491813659668, test loss: 6.720505714416504\n",
      "h: 55 | epoch: 48, train loss: 9.280180931091309, test loss: 6.650391578674316\n",
      "h: 55 | epoch: 49, train loss: 9.219305992126465, test loss: 6.5853590965271\n",
      "h: 55 | epoch: 50, train loss: 9.163431167602539, test loss: 6.525034427642822\n",
      "h: 55 | epoch: 51, train loss: 9.112154960632324, test loss: 6.469062805175781\n",
      "h: 55 | epoch: 52, train loss: 9.065108299255371, test loss: 6.417121887207031\n",
      "h: 55 | epoch: 53, train loss: 9.021947860717773, test loss: 6.368910312652588\n",
      "h: 55 | epoch: 54, train loss: 8.982357025146484, test loss: 6.324149131774902\n",
      "h: 55 | epoch: 55, train loss: 8.94604778289795, test loss: 6.2825846672058105\n",
      "h: 55 | epoch: 56, train loss: 8.91275405883789, test loss: 6.243973731994629\n",
      "h: 55 | epoch: 57, train loss: 8.882227897644043, test loss: 6.208102703094482\n",
      "h: 55 | epoch: 58, train loss: 8.854243278503418, test loss: 6.174765586853027\n",
      "h: 55 | epoch: 59, train loss: 8.828592300415039, test loss: 6.14377498626709\n",
      "h: 55 | epoch: 60, train loss: 8.805082321166992, test loss: 6.114955902099609\n",
      "h: 55 | epoch: 61, train loss: 8.783536911010742, test loss: 6.088150501251221\n",
      "h: 55 | epoch: 62, train loss: 8.7637939453125, test loss: 6.06320858001709\n",
      "h: 55 | epoch: 63, train loss: 8.745706558227539, test loss: 6.039992332458496\n",
      "h: 55 | epoch: 64, train loss: 8.729135513305664, test loss: 6.018378257751465\n",
      "h: 55 | epoch: 65, train loss: 8.713955879211426, test loss: 5.998246192932129\n",
      "h: 55 | epoch: 66, train loss: 8.700052261352539, test loss: 5.979489326477051\n",
      "h: 55 | epoch: 67, train loss: 8.687315940856934, test loss: 5.96200704574585\n",
      "h: 55 | epoch: 68, train loss: 8.675653457641602, test loss: 5.945706844329834\n",
      "h: 55 | epoch: 69, train loss: 8.664974212646484, test loss: 5.9305033683776855\n",
      "h: 55 | epoch: 70, train loss: 8.655195236206055, test loss: 5.916317462921143\n",
      "h: 55 | epoch: 71, train loss: 8.646242141723633, test loss: 5.903075695037842\n",
      "h: 55 | epoch: 72, train loss: 8.638042449951172, test loss: 5.890711784362793\n",
      "h: 55 | epoch: 73, train loss: 8.630537033081055, test loss: 5.879161834716797\n",
      "h: 55 | epoch: 74, train loss: 8.623666763305664, test loss: 5.868369102478027\n",
      "h: 55 | epoch: 75, train loss: 8.617377281188965, test loss: 5.858278274536133\n",
      "h: 55 | epoch: 76, train loss: 8.61161994934082, test loss: 5.848842144012451\n",
      "h: 55 | epoch: 77, train loss: 8.606350898742676, test loss: 5.8400139808654785\n",
      "h: 55 | epoch: 78, train loss: 8.601527214050293, test loss: 5.83174991607666\n",
      "h: 55 | epoch: 79, train loss: 8.597112655639648, test loss: 5.824012756347656\n",
      "h: 55 | epoch: 80, train loss: 8.593072891235352, test loss: 5.816765785217285\n",
      "h: 55 | epoch: 81, train loss: 8.589376449584961, test loss: 5.8099751472473145\n",
      "h: 55 | epoch: 82, train loss: 8.585992813110352, test loss: 5.8036088943481445\n",
      "h: 55 | epoch: 83, train loss: 8.582897186279297, test loss: 5.797638893127441\n",
      "h: 55 | epoch: 84, train loss: 8.58006477355957, test loss: 5.792036056518555\n",
      "h: 55 | epoch: 85, train loss: 8.577470779418945, test loss: 5.786780834197998\n",
      "h: 55 | epoch: 86, train loss: 8.575098037719727, test loss: 5.781844139099121\n",
      "h: 55 | epoch: 87, train loss: 8.572927474975586, test loss: 5.7772088050842285\n",
      "h: 55 | epoch: 88, train loss: 8.570941925048828, test loss: 5.772852897644043\n",
      "h: 55 | epoch: 89, train loss: 8.569125175476074, test loss: 5.768758773803711\n",
      "h: 55 | epoch: 90, train loss: 8.567461013793945, test loss: 5.764908313751221\n",
      "h: 55 | epoch: 91, train loss: 8.565940856933594, test loss: 5.761287689208984\n",
      "h: 55 | epoch: 92, train loss: 8.56454849243164, test loss: 5.757879734039307\n",
      "h: 55 | epoch: 93, train loss: 8.563275337219238, test loss: 5.7546706199646\n",
      "h: 55 | epoch: 94, train loss: 8.562108993530273, test loss: 5.751650810241699\n",
      "h: 55 | epoch: 95, train loss: 8.561043739318848, test loss: 5.748805999755859\n",
      "h: 55 | epoch: 96, train loss: 8.560068130493164, test loss: 5.746123790740967\n",
      "h: 55 | epoch: 97, train loss: 8.559176445007324, test loss: 5.743597030639648\n",
      "h: 55 | epoch: 98, train loss: 8.55836009979248, test loss: 5.741214275360107\n",
      "h: 55 | epoch: 99, train loss: 8.557612419128418, test loss: 5.738966941833496\n",
      "h: 56 | epoch: 0, train loss: 46.10435485839844, test loss: 38.90689468383789\n",
      "h: 56 | epoch: 1, train loss: 42.20702362060547, test loss: 35.694175720214844\n",
      "h: 56 | epoch: 2, train loss: 38.722904205322266, test loss: 32.80873107910156\n",
      "h: 56 | epoch: 3, train loss: 35.605987548828125, test loss: 30.21518898010254\n",
      "h: 56 | epoch: 4, train loss: 32.815975189208984, test loss: 27.882404327392578\n",
      "h: 56 | epoch: 5, train loss: 30.317453384399414, test loss: 25.782917022705078\n",
      "h: 56 | epoch: 6, train loss: 28.07915687561035, test loss: 23.89238166809082\n",
      "h: 56 | epoch: 7, train loss: 26.073421478271484, test loss: 22.189205169677734\n",
      "h: 56 | epoch: 8, train loss: 24.27570915222168, test loss: 20.65415382385254\n",
      "h: 56 | epoch: 9, train loss: 22.6641902923584, test loss: 19.27007484436035\n",
      "h: 56 | epoch: 10, train loss: 21.219425201416016, test loss: 18.02165412902832\n",
      "h: 56 | epoch: 11, train loss: 19.924049377441406, test loss: 16.895183563232422\n",
      "h: 56 | epoch: 12, train loss: 18.762563705444336, test loss: 15.878393173217773\n",
      "h: 56 | epoch: 13, train loss: 17.72109603881836, test loss: 14.960268020629883\n",
      "h: 56 | epoch: 14, train loss: 16.787220001220703, test loss: 14.13093376159668\n",
      "h: 56 | epoch: 15, train loss: 15.94981575012207, test loss: 13.3815279006958\n",
      "h: 56 | epoch: 16, train loss: 15.19890308380127, test loss: 12.704073905944824\n",
      "h: 56 | epoch: 17, train loss: 14.525538444519043, test loss: 12.09141731262207\n",
      "h: 56 | epoch: 18, train loss: 13.92170238494873, test loss: 11.537116050720215\n",
      "h: 56 | epoch: 19, train loss: 13.380197525024414, test loss: 11.035383224487305\n",
      "h: 56 | epoch: 20, train loss: 12.894561767578125, test loss: 10.581015586853027\n",
      "h: 56 | epoch: 21, train loss: 12.459007263183594, test loss: 10.169322967529297\n",
      "h: 56 | epoch: 22, train loss: 12.06833553314209, test loss: 9.796097755432129\n",
      "h: 56 | epoch: 23, train loss: 11.717888832092285, test loss: 9.457551002502441\n",
      "h: 56 | epoch: 24, train loss: 11.403482437133789, test loss: 9.150269508361816\n",
      "h: 56 | epoch: 25, train loss: 11.121369361877441, test loss: 8.871190071105957\n",
      "h: 56 | epoch: 26, train loss: 10.868189811706543, test loss: 8.617554664611816\n",
      "h: 56 | epoch: 27, train loss: 10.640932083129883, test loss: 8.386877059936523\n",
      "h: 56 | epoch: 28, train loss: 10.436894416809082, test loss: 8.17693042755127\n",
      "h: 56 | epoch: 29, train loss: 10.253660202026367, test loss: 7.985701084136963\n",
      "h: 56 | epoch: 30, train loss: 10.08906078338623, test loss: 7.811385154724121\n",
      "h: 56 | epoch: 31, train loss: 9.941156387329102, test loss: 7.6523542404174805\n",
      "h: 56 | epoch: 32, train loss: 9.808210372924805, test loss: 7.507145881652832\n",
      "h: 56 | epoch: 33, train loss: 9.688664436340332, test loss: 7.374443054199219\n",
      "h: 56 | epoch: 34, train loss: 9.58112907409668, test loss: 7.253060817718506\n",
      "h: 56 | epoch: 35, train loss: 9.484354019165039, test loss: 7.141932487487793\n",
      "h: 56 | epoch: 36, train loss: 9.39722728729248, test loss: 7.040095329284668\n",
      "h: 56 | epoch: 37, train loss: 9.318746566772461, test loss: 6.946684837341309\n",
      "h: 56 | epoch: 38, train loss: 9.248022079467773, test loss: 6.860922336578369\n",
      "h: 56 | epoch: 39, train loss: 9.18425178527832, test loss: 6.782103061676025\n",
      "h: 56 | epoch: 40, train loss: 9.126720428466797, test loss: 6.709595680236816\n",
      "h: 56 | epoch: 41, train loss: 9.074789047241211, test loss: 6.642828941345215\n",
      "h: 56 | epoch: 42, train loss: 9.027884483337402, test loss: 6.5812859535217285\n",
      "h: 56 | epoch: 43, train loss: 8.985494613647461, test loss: 6.524500846862793\n",
      "h: 56 | epoch: 44, train loss: 8.947157859802246, test loss: 6.472054481506348\n",
      "h: 56 | epoch: 45, train loss: 8.912467956542969, test loss: 6.4235687255859375\n",
      "h: 56 | epoch: 46, train loss: 8.881053924560547, test loss: 6.378697395324707\n",
      "h: 56 | epoch: 47, train loss: 8.85258674621582, test loss: 6.337131977081299\n",
      "h: 56 | epoch: 48, train loss: 8.826774597167969, test loss: 6.298589706420898\n",
      "h: 56 | epoch: 49, train loss: 8.803350448608398, test loss: 6.262816905975342\n",
      "h: 56 | epoch: 50, train loss: 8.782078742980957, test loss: 6.229583740234375\n",
      "h: 56 | epoch: 51, train loss: 8.762746810913086, test loss: 6.1986799240112305\n",
      "h: 56 | epoch: 52, train loss: 8.74516487121582, test loss: 6.169914722442627\n",
      "h: 56 | epoch: 53, train loss: 8.729164123535156, test loss: 6.1431169509887695\n",
      "h: 56 | epoch: 54, train loss: 8.71458911895752, test loss: 6.118129253387451\n",
      "h: 56 | epoch: 55, train loss: 8.701302528381348, test loss: 6.0948076248168945\n",
      "h: 56 | epoch: 56, train loss: 8.689180374145508, test loss: 6.073023796081543\n",
      "h: 56 | epoch: 57, train loss: 8.678114891052246, test loss: 6.052657604217529\n",
      "h: 56 | epoch: 58, train loss: 8.668004989624023, test loss: 6.033602714538574\n",
      "h: 56 | epoch: 59, train loss: 8.658760070800781, test loss: 6.015758991241455\n",
      "h: 56 | epoch: 60, train loss: 8.650300025939941, test loss: 5.999035835266113\n",
      "h: 56 | epoch: 61, train loss: 8.642553329467773, test loss: 5.983351707458496\n",
      "h: 56 | epoch: 62, train loss: 8.635454177856445, test loss: 5.968631744384766\n",
      "h: 56 | epoch: 63, train loss: 8.628942489624023, test loss: 5.95480489730835\n",
      "h: 56 | epoch: 64, train loss: 8.622964859008789, test loss: 5.941808223724365\n",
      "h: 56 | epoch: 65, train loss: 8.617476463317871, test loss: 5.929583549499512\n",
      "h: 56 | epoch: 66, train loss: 8.612431526184082, test loss: 5.918078899383545\n",
      "h: 56 | epoch: 67, train loss: 8.607789993286133, test loss: 5.907242774963379\n",
      "h: 56 | epoch: 68, train loss: 8.603520393371582, test loss: 5.897029399871826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 56 | epoch: 69, train loss: 8.59958553314209, test loss: 5.887398719787598\n",
      "h: 56 | epoch: 70, train loss: 8.59596061706543, test loss: 5.878312110900879\n",
      "h: 56 | epoch: 71, train loss: 8.592616081237793, test loss: 5.8697309494018555\n",
      "h: 56 | epoch: 72, train loss: 8.589530944824219, test loss: 5.861624717712402\n",
      "h: 56 | epoch: 73, train loss: 8.586681365966797, test loss: 5.853963851928711\n",
      "h: 56 | epoch: 74, train loss: 8.5840482711792, test loss: 5.846718788146973\n",
      "h: 56 | epoch: 75, train loss: 8.58161449432373, test loss: 5.83986234664917\n",
      "h: 56 | epoch: 76, train loss: 8.579364776611328, test loss: 5.833372116088867\n",
      "h: 56 | epoch: 77, train loss: 8.577280044555664, test loss: 5.8272247314453125\n",
      "h: 56 | epoch: 78, train loss: 8.575352668762207, test loss: 5.821398735046387\n",
      "h: 56 | epoch: 79, train loss: 8.573564529418945, test loss: 5.8158769607543945\n",
      "h: 56 | epoch: 80, train loss: 8.571908950805664, test loss: 5.810638904571533\n",
      "h: 56 | epoch: 81, train loss: 8.570371627807617, test loss: 5.805668830871582\n",
      "h: 56 | epoch: 82, train loss: 8.568946838378906, test loss: 5.800949573516846\n",
      "h: 56 | epoch: 83, train loss: 8.567625045776367, test loss: 5.796468257904053\n",
      "h: 56 | epoch: 84, train loss: 8.56639575958252, test loss: 5.792210578918457\n",
      "h: 56 | epoch: 85, train loss: 8.565255165100098, test loss: 5.788163185119629\n",
      "h: 56 | epoch: 86, train loss: 8.56419563293457, test loss: 5.784315586090088\n",
      "h: 56 | epoch: 87, train loss: 8.563209533691406, test loss: 5.780655860900879\n",
      "h: 56 | epoch: 88, train loss: 8.56229305267334, test loss: 5.777173042297363\n",
      "h: 56 | epoch: 89, train loss: 8.561440467834473, test loss: 5.773857116699219\n",
      "h: 56 | epoch: 90, train loss: 8.560647964477539, test loss: 5.770699501037598\n",
      "h: 56 | epoch: 91, train loss: 8.559908866882324, test loss: 5.7676920890808105\n",
      "h: 56 | epoch: 92, train loss: 8.559222221374512, test loss: 5.76482629776001\n",
      "h: 56 | epoch: 93, train loss: 8.55858039855957, test loss: 5.762094020843506\n",
      "h: 56 | epoch: 94, train loss: 8.557984352111816, test loss: 5.759489059448242\n",
      "h: 56 | epoch: 95, train loss: 8.557429313659668, test loss: 5.757004261016846\n",
      "h: 56 | epoch: 96, train loss: 8.556909561157227, test loss: 5.754633903503418\n",
      "h: 56 | epoch: 97, train loss: 8.556427001953125, test loss: 5.752370834350586\n",
      "h: 56 | epoch: 98, train loss: 8.555975914001465, test loss: 5.750210285186768\n",
      "h: 56 | epoch: 99, train loss: 8.555556297302246, test loss: 5.748147010803223\n",
      "h: 57 | epoch: 0, train loss: 46.46511459350586, test loss: 40.195228576660156\n",
      "h: 57 | epoch: 1, train loss: 43.44782257080078, test loss: 37.60428237915039\n",
      "h: 57 | epoch: 2, train loss: 40.67249298095703, test loss: 35.213279724121094\n",
      "h: 57 | epoch: 3, train loss: 38.118202209472656, test loss: 33.005489349365234\n",
      "h: 57 | epoch: 4, train loss: 35.76619338989258, test loss: 30.96587562561035\n",
      "h: 57 | epoch: 5, train loss: 33.599613189697266, test loss: 29.080856323242188\n",
      "h: 57 | epoch: 6, train loss: 31.603246688842773, test loss: 27.338144302368164\n",
      "h: 57 | epoch: 7, train loss: 29.763330459594727, test loss: 25.726572036743164\n",
      "h: 57 | epoch: 8, train loss: 28.067358016967773, test loss: 24.2359619140625\n",
      "h: 57 | epoch: 9, train loss: 26.503936767578125, test loss: 22.85702896118164\n",
      "h: 57 | epoch: 10, train loss: 25.06268310546875, test loss: 21.581270217895508\n",
      "h: 57 | epoch: 11, train loss: 23.73408317565918, test loss: 20.40087890625\n",
      "h: 57 | epoch: 12, train loss: 22.509424209594727, test loss: 19.308696746826172\n",
      "h: 57 | epoch: 13, train loss: 21.380687713623047, test loss: 18.298107147216797\n",
      "h: 57 | epoch: 14, train loss: 20.340518951416016, test loss: 17.363039016723633\n",
      "h: 57 | epoch: 15, train loss: 19.382131576538086, test loss: 16.497873306274414\n",
      "h: 57 | epoch: 16, train loss: 18.499271392822266, test loss: 15.697421073913574\n",
      "h: 57 | epoch: 17, train loss: 17.68616485595703, test loss: 14.956883430480957\n",
      "h: 57 | epoch: 18, train loss: 16.93747329711914, test loss: 14.271822929382324\n",
      "h: 57 | epoch: 19, train loss: 16.248271942138672, test loss: 13.638124465942383\n",
      "h: 57 | epoch: 20, train loss: 15.613993644714355, test loss: 13.051974296569824\n",
      "h: 57 | epoch: 21, train loss: 15.030424118041992, test loss: 12.509849548339844\n",
      "h: 57 | epoch: 22, train loss: 14.493659973144531, test loss: 12.008474349975586\n",
      "h: 57 | epoch: 23, train loss: 14.000082015991211, test loss: 11.544812202453613\n",
      "h: 57 | epoch: 24, train loss: 13.546350479125977, test loss: 11.116050720214844\n",
      "h: 57 | epoch: 25, train loss: 13.129362106323242, test loss: 10.719582557678223\n",
      "h: 57 | epoch: 26, train loss: 12.746259689331055, test loss: 10.352985382080078\n",
      "h: 57 | epoch: 27, train loss: 12.394383430480957, test loss: 10.014020919799805\n",
      "h: 57 | epoch: 28, train loss: 12.071282386779785, test loss: 9.700605392456055\n",
      "h: 57 | epoch: 29, train loss: 11.774686813354492, test loss: 9.410819053649902\n",
      "h: 57 | epoch: 30, train loss: 11.5024995803833, test loss: 9.142870903015137\n",
      "h: 57 | epoch: 31, train loss: 11.252774238586426, test loss: 8.895111083984375\n",
      "h: 57 | epoch: 32, train loss: 11.023720741271973, test loss: 8.666006088256836\n",
      "h: 57 | epoch: 33, train loss: 10.813684463500977, test loss: 8.454137802124023\n",
      "h: 57 | epoch: 34, train loss: 10.621132850646973, test loss: 8.258196830749512\n",
      "h: 57 | epoch: 35, train loss: 10.444653511047363, test loss: 8.07696533203125\n",
      "h: 57 | epoch: 36, train loss: 10.282944679260254, test loss: 7.9093217849731445\n",
      "h: 57 | epoch: 37, train loss: 10.13480281829834, test loss: 7.754225730895996\n",
      "h: 57 | epoch: 38, train loss: 9.999120712280273, test loss: 7.610718727111816\n",
      "h: 57 | epoch: 39, train loss: 9.874876976013184, test loss: 7.477911472320557\n",
      "h: 57 | epoch: 40, train loss: 9.76113224029541, test loss: 7.354985237121582\n",
      "h: 57 | epoch: 41, train loss: 9.657018661499023, test loss: 7.2411789894104\n",
      "h: 57 | epoch: 42, train loss: 9.561737060546875, test loss: 7.135795593261719\n",
      "h: 57 | epoch: 43, train loss: 9.474557876586914, test loss: 7.038186073303223\n",
      "h: 57 | epoch: 44, train loss: 9.394804000854492, test loss: 6.947756767272949\n",
      "h: 57 | epoch: 45, train loss: 9.321855545043945, test loss: 6.863954067230225\n",
      "h: 57 | epoch: 46, train loss: 9.255142211914062, test loss: 6.786271572113037\n",
      "h: 57 | epoch: 47, train loss: 9.194140434265137, test loss: 6.714242458343506\n",
      "h: 57 | epoch: 48, train loss: 9.138368606567383, test loss: 6.647430419921875\n",
      "h: 57 | epoch: 49, train loss: 9.087387084960938, test loss: 6.585436820983887\n",
      "h: 57 | epoch: 50, train loss: 9.040787696838379, test loss: 6.527896881103516\n",
      "h: 57 | epoch: 51, train loss: 8.998201370239258, test loss: 6.474469184875488\n",
      "h: 57 | epoch: 52, train loss: 8.9592866897583, test loss: 6.424841403961182\n",
      "h: 57 | epoch: 53, train loss: 8.923727989196777, test loss: 6.378726005554199\n",
      "h: 57 | epoch: 54, train loss: 8.891242980957031, test loss: 6.335854530334473\n",
      "h: 57 | epoch: 55, train loss: 8.861565589904785, test loss: 6.295984745025635\n",
      "h: 57 | epoch: 56, train loss: 8.834457397460938, test loss: 6.2588887214660645\n",
      "h: 57 | epoch: 57, train loss: 8.809698104858398, test loss: 6.224359512329102\n",
      "h: 57 | epoch: 58, train loss: 8.787084579467773, test loss: 6.192205429077148\n",
      "h: 57 | epoch: 59, train loss: 8.76643180847168, test loss: 6.162247657775879\n",
      "h: 57 | epoch: 60, train loss: 8.747575759887695, test loss: 6.134324550628662\n",
      "h: 57 | epoch: 61, train loss: 8.730354309082031, test loss: 6.108284950256348\n",
      "h: 57 | epoch: 62, train loss: 8.714631080627441, test loss: 6.083990573883057\n",
      "h: 57 | epoch: 63, train loss: 8.700273513793945, test loss: 6.061312675476074\n",
      "h: 57 | epoch: 64, train loss: 8.687166213989258, test loss: 6.040133476257324\n",
      "h: 57 | epoch: 65, train loss: 8.675199508666992, test loss: 6.020343780517578\n",
      "h: 57 | epoch: 66, train loss: 8.664274215698242, test loss: 6.001844882965088\n",
      "h: 57 | epoch: 67, train loss: 8.654300689697266, test loss: 5.984539985656738\n",
      "h: 57 | epoch: 68, train loss: 8.645194053649902, test loss: 5.968348026275635\n",
      "h: 57 | epoch: 69, train loss: 8.636881828308105, test loss: 5.95318603515625\n",
      "h: 57 | epoch: 70, train loss: 8.629295349121094, test loss: 5.938985347747803\n",
      "h: 57 | epoch: 71, train loss: 8.622368812561035, test loss: 5.925675392150879\n",
      "h: 57 | epoch: 72, train loss: 8.616044998168945, test loss: 5.91319465637207\n",
      "h: 57 | epoch: 73, train loss: 8.610273361206055, test loss: 5.901482582092285\n",
      "h: 57 | epoch: 74, train loss: 8.60500431060791, test loss: 5.8904924392700195\n",
      "h: 57 | epoch: 75, train loss: 8.600194931030273, test loss: 5.8801679611206055\n",
      "h: 57 | epoch: 76, train loss: 8.595804214477539, test loss: 5.8704681396484375\n",
      "h: 57 | epoch: 77, train loss: 8.591794967651367, test loss: 5.861350059509277\n",
      "h: 57 | epoch: 78, train loss: 8.588135719299316, test loss: 5.852774620056152\n",
      "h: 57 | epoch: 79, train loss: 8.584794998168945, test loss: 5.844703674316406\n",
      "h: 57 | epoch: 80, train loss: 8.581744194030762, test loss: 5.837106704711914\n",
      "h: 57 | epoch: 81, train loss: 8.578961372375488, test loss: 5.829948902130127\n",
      "h: 57 | epoch: 82, train loss: 8.57641887664795, test loss: 5.82320499420166\n",
      "h: 57 | epoch: 83, train loss: 8.574098587036133, test loss: 5.816847801208496\n",
      "h: 57 | epoch: 84, train loss: 8.571979522705078, test loss: 5.810851097106934\n",
      "h: 57 | epoch: 85, train loss: 8.57004451751709, test loss: 5.8051910400390625\n",
      "h: 57 | epoch: 86, train loss: 8.568278312683105, test loss: 5.799847602844238\n",
      "h: 57 | epoch: 87, train loss: 8.56666374206543, test loss: 5.794801712036133\n",
      "h: 57 | epoch: 88, train loss: 8.565191268920898, test loss: 5.790033340454102\n",
      "h: 57 | epoch: 89, train loss: 8.56384563446045, test loss: 5.785527229309082\n",
      "h: 57 | epoch: 90, train loss: 8.562618255615234, test loss: 5.781264305114746\n",
      "h: 57 | epoch: 91, train loss: 8.561495780944824, test loss: 5.7772321701049805\n",
      "h: 57 | epoch: 92, train loss: 8.560470581054688, test loss: 5.773415565490723\n",
      "h: 57 | epoch: 93, train loss: 8.559534072875977, test loss: 5.769803047180176\n",
      "h: 57 | epoch: 94, train loss: 8.55867862701416, test loss: 5.766380786895752\n",
      "h: 57 | epoch: 95, train loss: 8.557897567749023, test loss: 5.763138294219971\n",
      "h: 57 | epoch: 96, train loss: 8.557184219360352, test loss: 5.760065078735352\n",
      "h: 57 | epoch: 97, train loss: 8.55653190612793, test loss: 5.7571516036987305\n",
      "h: 57 | epoch: 98, train loss: 8.555936813354492, test loss: 5.754387378692627\n",
      "h: 57 | epoch: 99, train loss: 8.555392265319824, test loss: 5.751765727996826\n",
      "h: 58 | epoch: 0, train loss: 40.68109893798828, test loss: 34.61030960083008\n",
      "h: 58 | epoch: 1, train loss: 38.66912078857422, test loss: 32.852962493896484\n",
      "h: 58 | epoch: 2, train loss: 36.78614807128906, test loss: 31.206722259521484\n",
      "h: 58 | epoch: 3, train loss: 35.02286911010742, test loss: 29.663660049438477\n",
      "h: 58 | epoch: 4, train loss: 33.37083053588867, test loss: 28.216537475585938\n",
      "h: 58 | epoch: 5, train loss: 31.822330474853516, test loss: 26.858779907226562\n",
      "h: 58 | epoch: 6, train loss: 30.370325088500977, test loss: 25.5843505859375\n",
      "h: 58 | epoch: 7, train loss: 29.008358001708984, test loss: 24.38770866394043\n",
      "h: 58 | epoch: 8, train loss: 27.730510711669922, test loss: 23.263776779174805\n",
      "h: 58 | epoch: 9, train loss: 26.531326293945312, test loss: 22.207857131958008\n",
      "h: 58 | epoch: 10, train loss: 25.40576171875, test loss: 21.215620040893555\n",
      "h: 58 | epoch: 11, train loss: 24.34916114807129, test loss: 20.28305435180664\n",
      "h: 58 | epoch: 12, train loss: 23.357208251953125, test loss: 19.406448364257812\n",
      "h: 58 | epoch: 13, train loss: 22.425893783569336, test loss: 18.582340240478516\n",
      "h: 58 | epoch: 14, train loss: 21.55148696899414, test loss: 17.80752182006836\n",
      "h: 58 | epoch: 15, train loss: 20.730514526367188, test loss: 17.07899284362793\n",
      "h: 58 | epoch: 16, train loss: 19.959732055664062, test loss: 16.393970489501953\n",
      "h: 58 | epoch: 17, train loss: 19.23611068725586, test loss: 15.749834060668945\n",
      "h: 58 | epoch: 18, train loss: 18.55681037902832, test loss: 15.144142150878906\n",
      "h: 58 | epoch: 19, train loss: 17.9191837310791, test loss: 14.574612617492676\n",
      "h: 58 | epoch: 20, train loss: 17.32073402404785, test loss: 14.039097785949707\n",
      "h: 58 | epoch: 21, train loss: 16.759124755859375, test loss: 13.53558349609375\n",
      "h: 58 | epoch: 22, train loss: 16.23216438293457, test loss: 13.06218433380127\n",
      "h: 58 | epoch: 23, train loss: 15.737788200378418, test loss: 12.617121696472168\n",
      "h: 58 | epoch: 24, train loss: 15.274053573608398, test loss: 12.198736190795898\n",
      "h: 58 | epoch: 25, train loss: 14.839141845703125, test loss: 11.80544662475586\n",
      "h: 58 | epoch: 26, train loss: 14.431327819824219, test loss: 11.435785293579102\n",
      "h: 58 | epoch: 27, train loss: 14.04899787902832, test loss: 11.088351249694824\n",
      "h: 58 | epoch: 28, train loss: 13.690622329711914, test loss: 10.761846542358398\n",
      "h: 58 | epoch: 29, train loss: 13.354771614074707, test loss: 10.455031394958496\n",
      "h: 58 | epoch: 30, train loss: 13.040087699890137, test loss: 10.166744232177734\n",
      "h: 58 | epoch: 31, train loss: 12.745298385620117, test loss: 9.895890235900879\n",
      "h: 58 | epoch: 32, train loss: 12.469202041625977, test loss: 9.641437530517578\n",
      "h: 58 | epoch: 33, train loss: 12.210662841796875, test loss: 9.402413368225098\n",
      "h: 58 | epoch: 34, train loss: 11.968615531921387, test loss: 9.177903175354004\n",
      "h: 58 | epoch: 35, train loss: 11.742055892944336, test loss: 8.967042922973633\n",
      "h: 58 | epoch: 36, train loss: 11.530034065246582, test loss: 8.769015312194824\n",
      "h: 58 | epoch: 37, train loss: 11.331658363342285, test loss: 8.58305549621582\n",
      "h: 58 | epoch: 38, train loss: 11.146086692810059, test loss: 8.408441543579102\n",
      "h: 58 | epoch: 39, train loss: 10.972529411315918, test loss: 8.244490623474121\n",
      "h: 58 | epoch: 40, train loss: 10.8102388381958, test loss: 8.09056282043457\n",
      "h: 58 | epoch: 41, train loss: 10.658514022827148, test loss: 7.946054935455322\n",
      "h: 58 | epoch: 42, train loss: 10.516695022583008, test loss: 7.810394287109375\n",
      "h: 58 | epoch: 43, train loss: 10.384160041809082, test loss: 7.683046817779541\n",
      "h: 58 | epoch: 44, train loss: 10.260324478149414, test loss: 7.563508033752441\n",
      "h: 58 | epoch: 45, train loss: 10.144636154174805, test loss: 7.451303005218506\n",
      "h: 58 | epoch: 46, train loss: 10.036581039428711, test loss: 7.345987796783447\n",
      "h: 58 | epoch: 47, train loss: 9.935673713684082, test loss: 7.247138023376465\n",
      "h: 58 | epoch: 48, train loss: 9.84145450592041, test loss: 7.154360294342041\n",
      "h: 58 | epoch: 49, train loss: 9.753499031066895, test loss: 7.067279815673828\n",
      "h: 58 | epoch: 50, train loss: 9.671401023864746, test loss: 6.9855523109436035\n",
      "h: 58 | epoch: 51, train loss: 9.5947847366333, test loss: 6.908843040466309\n",
      "h: 58 | epoch: 52, train loss: 9.523295402526855, test loss: 6.83684778213501\n",
      "h: 58 | epoch: 53, train loss: 9.456599235534668, test loss: 6.7692718505859375\n",
      "h: 58 | epoch: 54, train loss: 9.39438533782959, test loss: 6.705844879150391\n",
      "h: 58 | epoch: 55, train loss: 9.336359024047852, test loss: 6.646309852600098\n",
      "h: 58 | epoch: 56, train loss: 9.282246589660645, test loss: 6.590423583984375\n",
      "h: 58 | epoch: 57, train loss: 9.231794357299805, test loss: 6.5379638671875\n",
      "h: 58 | epoch: 58, train loss: 9.184757232666016, test loss: 6.488717555999756\n",
      "h: 58 | epoch: 59, train loss: 9.140911102294922, test loss: 6.44248104095459\n",
      "h: 58 | epoch: 60, train loss: 9.100044250488281, test loss: 6.39907169342041\n",
      "h: 58 | epoch: 61, train loss: 9.061960220336914, test loss: 6.35831356048584\n",
      "h: 58 | epoch: 62, train loss: 9.026472091674805, test loss: 6.320040702819824\n",
      "h: 58 | epoch: 63, train loss: 8.993408203125, test loss: 6.284098148345947\n",
      "h: 58 | epoch: 64, train loss: 8.962605476379395, test loss: 6.250340461730957\n",
      "h: 58 | epoch: 65, train loss: 8.93391227722168, test loss: 6.218633651733398\n",
      "h: 58 | epoch: 66, train loss: 8.907186508178711, test loss: 6.18884801864624\n",
      "h: 58 | epoch: 67, train loss: 8.88229751586914, test loss: 6.1608662605285645\n",
      "h: 58 | epoch: 68, train loss: 8.859121322631836, test loss: 6.134572505950928\n",
      "h: 58 | epoch: 69, train loss: 8.837540626525879, test loss: 6.109864234924316\n",
      "h: 58 | epoch: 70, train loss: 8.8174467086792, test loss: 6.086643218994141\n",
      "h: 58 | epoch: 71, train loss: 8.798742294311523, test loss: 6.064815521240234\n",
      "h: 58 | epoch: 72, train loss: 8.781331062316895, test loss: 6.044295787811279\n",
      "h: 58 | epoch: 73, train loss: 8.765122413635254, test loss: 6.025001525878906\n",
      "h: 58 | epoch: 74, train loss: 8.750038146972656, test loss: 6.006856918334961\n",
      "h: 58 | epoch: 75, train loss: 8.73599910736084, test loss: 5.989792346954346\n",
      "h: 58 | epoch: 76, train loss: 8.722935676574707, test loss: 5.973738670349121\n",
      "h: 58 | epoch: 77, train loss: 8.710779190063477, test loss: 5.958635330200195\n",
      "h: 58 | epoch: 78, train loss: 8.699468612670898, test loss: 5.944422721862793\n",
      "h: 58 | epoch: 79, train loss: 8.688945770263672, test loss: 5.93104362487793\n",
      "h: 58 | epoch: 80, train loss: 8.679155349731445, test loss: 5.918450355529785\n",
      "h: 58 | epoch: 81, train loss: 8.67004680633545, test loss: 5.906591415405273\n",
      "h: 58 | epoch: 82, train loss: 8.661576271057129, test loss: 5.895424842834473\n",
      "h: 58 | epoch: 83, train loss: 8.653696060180664, test loss: 5.88490629196167\n",
      "h: 58 | epoch: 84, train loss: 8.646367073059082, test loss: 5.874995231628418\n",
      "h: 58 | epoch: 85, train loss: 8.639551162719727, test loss: 5.865655899047852\n",
      "h: 58 | epoch: 86, train loss: 8.633211135864258, test loss: 5.856853008270264\n",
      "h: 58 | epoch: 87, train loss: 8.62731647491455, test loss: 5.848555088043213\n",
      "h: 58 | epoch: 88, train loss: 8.621833801269531, test loss: 5.840728759765625\n",
      "h: 58 | epoch: 89, train loss: 8.616735458374023, test loss: 5.833346843719482\n",
      "h: 58 | epoch: 90, train loss: 8.611995697021484, test loss: 5.826383590698242\n",
      "h: 58 | epoch: 91, train loss: 8.607588768005371, test loss: 5.819812774658203\n",
      "h: 58 | epoch: 92, train loss: 8.60349178314209, test loss: 5.813611030578613\n",
      "h: 58 | epoch: 93, train loss: 8.59968376159668, test loss: 5.807755947113037\n",
      "h: 58 | epoch: 94, train loss: 8.59614086151123, test loss: 5.802225589752197\n",
      "h: 58 | epoch: 95, train loss: 8.592848777770996, test loss: 5.797002792358398\n",
      "h: 58 | epoch: 96, train loss: 8.589788436889648, test loss: 5.792067050933838\n",
      "h: 58 | epoch: 97, train loss: 8.586944580078125, test loss: 5.7874040603637695\n",
      "h: 58 | epoch: 98, train loss: 8.58430004119873, test loss: 5.782995223999023\n",
      "h: 58 | epoch: 99, train loss: 8.581842422485352, test loss: 5.778828144073486\n",
      "h: 59 | epoch: 0, train loss: 46.21416091918945, test loss: 39.508949279785156\n",
      "h: 59 | epoch: 1, train loss: 42.550819396972656, test loss: 36.414642333984375\n",
      "h: 59 | epoch: 2, train loss: 39.246307373046875, test loss: 33.61188888549805\n",
      "h: 59 | epoch: 3, train loss: 36.263710021972656, test loss: 31.071636199951172\n",
      "h: 59 | epoch: 4, train loss: 33.57041549682617, test loss: 28.768117904663086\n",
      "h: 59 | epoch: 5, train loss: 31.13753890991211, test loss: 26.678386688232422\n",
      "h: 59 | epoch: 6, train loss: 28.939355850219727, test loss: 24.781946182250977\n",
      "h: 59 | epoch: 7, train loss: 26.95292091369629, test loss: 23.060441970825195\n",
      "h: 59 | epoch: 8, train loss: 25.157684326171875, test loss: 21.49738883972168\n",
      "h: 59 | epoch: 9, train loss: 23.53521728515625, test loss: 20.077953338623047\n",
      "h: 59 | epoch: 10, train loss: 22.06894302368164, test loss: 18.788753509521484\n",
      "h: 59 | epoch: 11, train loss: 20.74393081665039, test loss: 17.61770248413086\n",
      "h: 59 | epoch: 12, train loss: 19.54673194885254, test loss: 16.553882598876953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 59 | epoch: 13, train loss: 18.465167999267578, test loss: 15.587396621704102\n",
      "h: 59 | epoch: 14, train loss: 17.488252639770508, test loss: 14.709274291992188\n",
      "h: 59 | epoch: 15, train loss: 16.606029510498047, test loss: 13.911386489868164\n",
      "h: 59 | epoch: 16, train loss: 15.809496879577637, test loss: 13.186355590820312\n",
      "h: 59 | epoch: 17, train loss: 15.090494155883789, test loss: 12.527478218078613\n",
      "h: 59 | epoch: 18, train loss: 14.441621780395508, test loss: 11.928679466247559\n",
      "h: 59 | epoch: 19, train loss: 13.856184005737305, test loss: 11.384425163269043\n",
      "h: 59 | epoch: 20, train loss: 13.328104972839355, test loss: 10.88969898223877\n",
      "h: 59 | epoch: 21, train loss: 12.85188102722168, test loss: 10.43994140625\n",
      "h: 59 | epoch: 22, train loss: 12.422523498535156, test loss: 10.031003952026367\n",
      "h: 59 | epoch: 23, train loss: 12.035510063171387, test loss: 9.659130096435547\n",
      "h: 59 | epoch: 24, train loss: 11.686744689941406, test loss: 9.320898056030273\n",
      "h: 59 | epoch: 25, train loss: 11.372518539428711, test loss: 9.013200759887695\n",
      "h: 59 | epoch: 26, train loss: 11.089470863342285, test loss: 8.733224868774414\n",
      "h: 59 | epoch: 27, train loss: 10.834562301635742, test loss: 8.47840690612793\n",
      "h: 59 | epoch: 28, train loss: 10.605037689208984, test loss: 8.246419906616211\n",
      "h: 59 | epoch: 29, train loss: 10.398411750793457, test loss: 8.035160064697266\n",
      "h: 59 | epoch: 30, train loss: 10.212430953979492, test loss: 7.842714786529541\n",
      "h: 59 | epoch: 31, train loss: 10.045059204101562, test loss: 7.667342185974121\n",
      "h: 59 | epoch: 32, train loss: 9.894460678100586, test loss: 7.507472038269043\n",
      "h: 59 | epoch: 33, train loss: 9.75897216796875, test loss: 7.361675262451172\n",
      "h: 59 | epoch: 34, train loss: 9.637094497680664, test loss: 7.228656768798828\n",
      "h: 59 | epoch: 35, train loss: 9.527473449707031, test loss: 7.107241153717041\n",
      "h: 59 | epoch: 36, train loss: 9.428885459899902, test loss: 6.996363639831543\n",
      "h: 59 | epoch: 37, train loss: 9.340232849121094, test loss: 6.895061492919922\n",
      "h: 59 | epoch: 38, train loss: 9.260519027709961, test loss: 6.802456855773926\n",
      "h: 59 | epoch: 39, train loss: 9.188848495483398, test loss: 6.717759132385254\n",
      "h: 59 | epoch: 40, train loss: 9.124412536621094, test loss: 6.640249729156494\n",
      "h: 59 | epoch: 41, train loss: 9.066486358642578, test loss: 6.56927490234375\n",
      "h: 59 | epoch: 42, train loss: 9.01441478729248, test loss: 6.504246711730957\n",
      "h: 59 | epoch: 43, train loss: 8.967605590820312, test loss: 6.444628715515137\n",
      "h: 59 | epoch: 44, train loss: 8.925530433654785, test loss: 6.389935493469238\n",
      "h: 59 | epoch: 45, train loss: 8.887707710266113, test loss: 6.339729309082031\n",
      "h: 59 | epoch: 46, train loss: 8.85371208190918, test loss: 6.293608665466309\n",
      "h: 59 | epoch: 47, train loss: 8.823152542114258, test loss: 6.251214027404785\n",
      "h: 59 | epoch: 48, train loss: 8.795684814453125, test loss: 6.212214469909668\n",
      "h: 59 | epoch: 49, train loss: 8.770992279052734, test loss: 6.176316261291504\n",
      "h: 59 | epoch: 50, train loss: 8.748796463012695, test loss: 6.143247127532959\n",
      "h: 59 | epoch: 51, train loss: 8.728842735290527, test loss: 6.112761497497559\n",
      "h: 59 | epoch: 52, train loss: 8.710904121398926, test loss: 6.084639072418213\n",
      "h: 59 | epoch: 53, train loss: 8.69477653503418, test loss: 6.058674335479736\n",
      "h: 59 | epoch: 54, train loss: 8.680274963378906, test loss: 6.034686088562012\n",
      "h: 59 | epoch: 55, train loss: 8.667238235473633, test loss: 6.012506484985352\n",
      "h: 59 | epoch: 56, train loss: 8.655512809753418, test loss: 5.991985321044922\n",
      "h: 59 | epoch: 57, train loss: 8.644968032836914, test loss: 5.972983360290527\n",
      "h: 59 | epoch: 58, train loss: 8.635485649108887, test loss: 5.955374240875244\n",
      "h: 59 | epoch: 59, train loss: 8.626956939697266, test loss: 5.93904447555542\n",
      "h: 59 | epoch: 60, train loss: 8.619283676147461, test loss: 5.923891544342041\n",
      "h: 59 | epoch: 61, train loss: 8.612380027770996, test loss: 5.909816741943359\n",
      "h: 59 | epoch: 62, train loss: 8.606169700622559, test loss: 5.896737575531006\n",
      "h: 59 | epoch: 63, train loss: 8.600580215454102, test loss: 5.8845720291137695\n",
      "h: 59 | epoch: 64, train loss: 8.595551490783691, test loss: 5.873249053955078\n",
      "h: 59 | epoch: 65, train loss: 8.591024398803711, test loss: 5.862704277038574\n",
      "h: 59 | epoch: 66, train loss: 8.586946487426758, test loss: 5.852875232696533\n",
      "h: 59 | epoch: 67, train loss: 8.58327865600586, test loss: 5.84370756149292\n",
      "h: 59 | epoch: 68, train loss: 8.579974174499512, test loss: 5.835154056549072\n",
      "h: 59 | epoch: 69, train loss: 8.576998710632324, test loss: 5.8271636962890625\n",
      "h: 59 | epoch: 70, train loss: 8.574319839477539, test loss: 5.819698333740234\n",
      "h: 59 | epoch: 71, train loss: 8.571905136108398, test loss: 5.812716484069824\n",
      "h: 59 | epoch: 72, train loss: 8.569730758666992, test loss: 5.806183338165283\n",
      "h: 59 | epoch: 73, train loss: 8.567770004272461, test loss: 5.80006742477417\n",
      "h: 59 | epoch: 74, train loss: 8.566003799438477, test loss: 5.794338226318359\n",
      "h: 59 | epoch: 75, train loss: 8.564412117004395, test loss: 5.788967609405518\n",
      "h: 59 | epoch: 76, train loss: 8.562975883483887, test loss: 5.783930778503418\n",
      "h: 59 | epoch: 77, train loss: 8.561681747436523, test loss: 5.77920389175415\n",
      "h: 59 | epoch: 78, train loss: 8.560514450073242, test loss: 5.774767875671387\n",
      "h: 59 | epoch: 79, train loss: 8.55946159362793, test loss: 5.770598411560059\n",
      "h: 59 | epoch: 80, train loss: 8.558511734008789, test loss: 5.766680717468262\n",
      "h: 59 | epoch: 81, train loss: 8.557653427124023, test loss: 5.762996673583984\n",
      "h: 59 | epoch: 82, train loss: 8.556879997253418, test loss: 5.759530544281006\n",
      "h: 59 | epoch: 83, train loss: 8.556182861328125, test loss: 5.756269454956055\n",
      "h: 59 | epoch: 84, train loss: 8.555551528930664, test loss: 5.75319766998291\n",
      "h: 59 | epoch: 85, train loss: 8.554981231689453, test loss: 5.750304222106934\n",
      "h: 59 | epoch: 86, train loss: 8.554468154907227, test loss: 5.747577667236328\n",
      "h: 59 | epoch: 87, train loss: 8.554003715515137, test loss: 5.745007514953613\n",
      "h: 59 | epoch: 88, train loss: 8.553584098815918, test loss: 5.742581844329834\n",
      "h: 59 | epoch: 89, train loss: 8.553204536437988, test loss: 5.740294456481934\n",
      "h: 59 | epoch: 90, train loss: 8.552862167358398, test loss: 5.73813533782959\n",
      "h: 59 | epoch: 91, train loss: 8.552552223205566, test loss: 5.736096382141113\n",
      "h: 59 | epoch: 92, train loss: 8.552271842956543, test loss: 5.734170913696289\n",
      "h: 59 | epoch: 93, train loss: 8.552019119262695, test loss: 5.732351303100586\n",
      "h: 59 | epoch: 94, train loss: 8.551789283752441, test loss: 5.730632305145264\n",
      "h: 59 | epoch: 95, train loss: 8.551582336425781, test loss: 5.729005813598633\n",
      "h: 59 | epoch: 96, train loss: 8.55139446258545, test loss: 5.7274675369262695\n",
      "h: 59 | epoch: 97, train loss: 8.551223754882812, test loss: 5.72601318359375\n",
      "h: 59 | epoch: 98, train loss: 8.551071166992188, test loss: 5.724637508392334\n",
      "h: 59 | epoch: 99, train loss: 8.550931930541992, test loss: 5.723334312438965\n",
      "h: 60 | epoch: 0, train loss: 36.220970153808594, test loss: 31.51009750366211\n",
      "h: 60 | epoch: 1, train loss: 34.02642822265625, test loss: 29.56868553161621\n",
      "h: 60 | epoch: 2, train loss: 32.003013610839844, test loss: 27.7740421295166\n",
      "h: 60 | epoch: 3, train loss: 30.13700294494629, test loss: 26.114665985107422\n",
      "h: 60 | epoch: 4, train loss: 28.41591453552246, test loss: 24.580089569091797\n",
      "h: 60 | epoch: 5, train loss: 26.82840347290039, test loss: 23.160741806030273\n",
      "h: 60 | epoch: 6, train loss: 25.364084243774414, test loss: 21.847883224487305\n",
      "h: 60 | epoch: 7, train loss: 24.01346206665039, test loss: 20.633464813232422\n",
      "h: 60 | epoch: 8, train loss: 22.767826080322266, test loss: 19.510103225708008\n",
      "h: 60 | epoch: 9, train loss: 21.619165420532227, test loss: 18.47099494934082\n",
      "h: 60 | epoch: 10, train loss: 20.56011199951172, test loss: 17.50987434387207\n",
      "h: 60 | epoch: 11, train loss: 19.58386993408203, test loss: 16.620952606201172\n",
      "h: 60 | epoch: 12, train loss: 18.68416404724121, test loss: 15.798873901367188\n",
      "h: 60 | epoch: 13, train loss: 17.85519790649414, test loss: 15.038679122924805\n",
      "h: 60 | epoch: 14, train loss: 17.09162139892578, test loss: 14.335803031921387\n",
      "h: 60 | epoch: 15, train loss: 16.388473510742188, test loss: 13.68598461151123\n",
      "h: 60 | epoch: 16, train loss: 15.741157531738281, test loss: 13.085294723510742\n",
      "h: 60 | epoch: 17, train loss: 15.145431518554688, test loss: 12.5300874710083\n",
      "h: 60 | epoch: 18, train loss: 14.597352981567383, test loss: 12.016973495483398\n",
      "h: 60 | epoch: 19, train loss: 14.093271255493164, test loss: 11.542816162109375\n",
      "h: 60 | epoch: 20, train loss: 13.62980842590332, test loss: 11.104703903198242\n",
      "h: 60 | epoch: 21, train loss: 13.203829765319824, test loss: 10.69993782043457\n",
      "h: 60 | epoch: 22, train loss: 12.812431335449219, test loss: 10.326016426086426\n",
      "h: 60 | epoch: 23, train loss: 12.452926635742188, test loss: 9.980610847473145\n",
      "h: 60 | epoch: 24, train loss: 12.122822761535645, test loss: 9.661572456359863\n",
      "h: 60 | epoch: 25, train loss: 11.81981372833252, test loss: 9.366905212402344\n",
      "h: 60 | epoch: 26, train loss: 11.541765213012695, test loss: 9.09476089477539\n",
      "h: 60 | epoch: 27, train loss: 11.286703109741211, test loss: 8.843420028686523\n",
      "h: 60 | epoch: 28, train loss: 11.052797317504883, test loss: 8.611299514770508\n",
      "h: 60 | epoch: 29, train loss: 10.838361740112305, test loss: 8.396926879882812\n",
      "h: 60 | epoch: 30, train loss: 10.641833305358887, test loss: 8.198944091796875\n",
      "h: 60 | epoch: 31, train loss: 10.461770057678223, test loss: 8.016090393066406\n",
      "h: 60 | epoch: 32, train loss: 10.29683780670166, test loss: 7.847198486328125\n",
      "h: 60 | epoch: 33, train loss: 10.145811080932617, test loss: 7.691196441650391\n",
      "h: 60 | epoch: 34, train loss: 10.007554054260254, test loss: 7.547083854675293\n",
      "h: 60 | epoch: 35, train loss: 9.881020545959473, test loss: 7.413943290710449\n",
      "h: 60 | epoch: 36, train loss: 9.765246391296387, test loss: 7.290924072265625\n",
      "h: 60 | epoch: 37, train loss: 9.659342765808105, test loss: 7.177238464355469\n",
      "h: 60 | epoch: 38, train loss: 9.562492370605469, test loss: 7.072165012359619\n",
      "h: 60 | epoch: 39, train loss: 9.4739408493042, test loss: 6.975032806396484\n",
      "h: 60 | epoch: 40, train loss: 9.392999649047852, test loss: 6.88522481918335\n",
      "h: 60 | epoch: 41, train loss: 9.319025039672852, test loss: 6.802170753479004\n",
      "h: 60 | epoch: 42, train loss: 9.251436233520508, test loss: 6.725344181060791\n",
      "h: 60 | epoch: 43, train loss: 9.18969440460205, test loss: 6.654262542724609\n",
      "h: 60 | epoch: 44, train loss: 9.133304595947266, test loss: 6.588476657867432\n",
      "h: 60 | epoch: 45, train loss: 9.081811904907227, test loss: 6.527577877044678\n",
      "h: 60 | epoch: 46, train loss: 9.03480052947998, test loss: 6.47118616104126\n",
      "h: 60 | epoch: 47, train loss: 8.991888046264648, test loss: 6.41895055770874\n",
      "h: 60 | epoch: 48, train loss: 8.952722549438477, test loss: 6.370549201965332\n",
      "h: 60 | epoch: 49, train loss: 8.916986465454102, test loss: 6.325686931610107\n",
      "h: 60 | epoch: 50, train loss: 8.884379386901855, test loss: 6.284088134765625\n",
      "h: 60 | epoch: 51, train loss: 8.854635238647461, test loss: 6.245502948760986\n",
      "h: 60 | epoch: 52, train loss: 8.827507019042969, test loss: 6.209699630737305\n",
      "h: 60 | epoch: 53, train loss: 8.802766799926758, test loss: 6.176462173461914\n",
      "h: 60 | epoch: 54, train loss: 8.780207633972168, test loss: 6.145598411560059\n",
      "h: 60 | epoch: 55, train loss: 8.75964069366455, test loss: 6.1169233322143555\n",
      "h: 60 | epoch: 56, train loss: 8.740891456604004, test loss: 6.090271472930908\n",
      "h: 60 | epoch: 57, train loss: 8.72380256652832, test loss: 6.065489768981934\n",
      "h: 60 | epoch: 58, train loss: 8.708226203918457, test loss: 6.042437553405762\n",
      "h: 60 | epoch: 59, train loss: 8.694032669067383, test loss: 6.020982265472412\n",
      "h: 60 | epoch: 60, train loss: 8.681100845336914, test loss: 6.001006126403809\n",
      "h: 60 | epoch: 61, train loss: 8.669317245483398, test loss: 5.982398509979248\n",
      "h: 60 | epoch: 62, train loss: 8.65858268737793, test loss: 5.9650559425354\n",
      "h: 60 | epoch: 63, train loss: 8.648804664611816, test loss: 5.9488844871521\n",
      "h: 60 | epoch: 64, train loss: 8.639898300170898, test loss: 5.933801174163818\n",
      "h: 60 | epoch: 65, train loss: 8.631787300109863, test loss: 5.919723987579346\n",
      "h: 60 | epoch: 66, train loss: 8.624399185180664, test loss: 5.9065775871276855\n",
      "h: 60 | epoch: 67, train loss: 8.617671012878418, test loss: 5.8942975997924805\n",
      "h: 60 | epoch: 68, train loss: 8.611547470092773, test loss: 5.882819652557373\n",
      "h: 60 | epoch: 69, train loss: 8.60597038269043, test loss: 5.872086524963379\n",
      "h: 60 | epoch: 70, train loss: 8.600893020629883, test loss: 5.862044811248779\n",
      "h: 60 | epoch: 71, train loss: 8.596269607543945, test loss: 5.852646350860596\n",
      "h: 60 | epoch: 72, train loss: 8.592062950134277, test loss: 5.843844413757324\n",
      "h: 60 | epoch: 73, train loss: 8.588232040405273, test loss: 5.835597515106201\n",
      "h: 60 | epoch: 74, train loss: 8.584745407104492, test loss: 5.827866554260254\n",
      "h: 60 | epoch: 75, train loss: 8.581574440002441, test loss: 5.820618629455566\n",
      "h: 60 | epoch: 76, train loss: 8.578686714172363, test loss: 5.813816547393799\n",
      "h: 60 | epoch: 77, train loss: 8.576058387756348, test loss: 5.807430267333984\n",
      "h: 60 | epoch: 78, train loss: 8.5736665725708, test loss: 5.801434516906738\n",
      "h: 60 | epoch: 79, train loss: 8.571491241455078, test loss: 5.795801162719727\n",
      "h: 60 | epoch: 80, train loss: 8.569509506225586, test loss: 5.790505409240723\n",
      "h: 60 | epoch: 81, train loss: 8.567708969116211, test loss: 5.785526752471924\n",
      "h: 60 | epoch: 82, train loss: 8.566069602966309, test loss: 5.780840873718262\n",
      "h: 60 | epoch: 83, train loss: 8.56457805633545, test loss: 5.776430606842041\n",
      "h: 60 | epoch: 84, train loss: 8.563220977783203, test loss: 5.772281169891357\n",
      "h: 60 | epoch: 85, train loss: 8.561986923217773, test loss: 5.768368721008301\n",
      "h: 60 | epoch: 86, train loss: 8.56086254119873, test loss: 5.764684677124023\n",
      "h: 60 | epoch: 87, train loss: 8.55984115600586, test loss: 5.761209964752197\n",
      "h: 60 | epoch: 88, train loss: 8.55891227722168, test loss: 5.757933616638184\n",
      "h: 60 | epoch: 89, train loss: 8.558065414428711, test loss: 5.7548418045043945\n",
      "h: 60 | epoch: 90, train loss: 8.557297706604004, test loss: 5.751925468444824\n",
      "h: 60 | epoch: 91, train loss: 8.556597709655762, test loss: 5.749170303344727\n",
      "h: 60 | epoch: 92, train loss: 8.555962562561035, test loss: 5.7465691566467285\n",
      "h: 60 | epoch: 93, train loss: 8.555383682250977, test loss: 5.744112491607666\n",
      "h: 60 | epoch: 94, train loss: 8.554856300354004, test loss: 5.741790294647217\n",
      "h: 60 | epoch: 95, train loss: 8.554377555847168, test loss: 5.739594459533691\n",
      "h: 60 | epoch: 96, train loss: 8.553942680358887, test loss: 5.737517356872559\n",
      "h: 60 | epoch: 97, train loss: 8.553547859191895, test loss: 5.73555326461792\n",
      "h: 60 | epoch: 98, train loss: 8.553186416625977, test loss: 5.733694553375244\n",
      "h: 60 | epoch: 99, train loss: 8.552858352661133, test loss: 5.731935501098633\n",
      "h: 61 | epoch: 0, train loss: 38.86293411254883, test loss: 32.894752502441406\n",
      "h: 61 | epoch: 1, train loss: 36.075889587402344, test loss: 30.55531883239746\n",
      "h: 61 | epoch: 2, train loss: 33.545631408691406, test loss: 28.42288589477539\n",
      "h: 61 | epoch: 3, train loss: 31.247583389282227, test loss: 26.478240966796875\n",
      "h: 61 | epoch: 4, train loss: 29.15981674194336, test loss: 24.70418930053711\n",
      "h: 61 | epoch: 5, train loss: 27.262691497802734, test loss: 23.085235595703125\n",
      "h: 61 | epoch: 6, train loss: 25.538558959960938, test loss: 21.60743522644043\n",
      "h: 61 | epoch: 7, train loss: 23.971519470214844, test loss: 20.258193969726562\n",
      "h: 61 | epoch: 8, train loss: 22.547225952148438, test loss: 19.026100158691406\n",
      "h: 61 | epoch: 9, train loss: 21.252704620361328, test loss: 17.900829315185547\n",
      "h: 61 | epoch: 10, train loss: 20.076208114624023, test loss: 16.872989654541016\n",
      "h: 61 | epoch: 11, train loss: 19.0070743560791, test loss: 15.934036254882812\n",
      "h: 61 | epoch: 12, train loss: 18.035625457763672, test loss: 15.076210021972656\n",
      "h: 61 | epoch: 13, train loss: 17.153064727783203, test loss: 14.292439460754395\n",
      "h: 61 | epoch: 14, train loss: 16.35138702392578, test loss: 13.576269149780273\n",
      "h: 61 | epoch: 15, train loss: 15.623309135437012, test loss: 12.921819686889648\n",
      "h: 61 | epoch: 16, train loss: 14.962199211120605, test loss: 12.323719024658203\n",
      "h: 61 | epoch: 17, train loss: 14.362014770507812, test loss: 11.777070999145508\n",
      "h: 61 | epoch: 18, train loss: 13.817245483398438, test loss: 11.277396202087402\n",
      "h: 61 | epoch: 19, train loss: 13.322880744934082, test loss: 10.8206148147583\n",
      "h: 61 | epoch: 20, train loss: 12.87434196472168, test loss: 10.402990341186523\n",
      "h: 61 | epoch: 21, train loss: 12.467466354370117, test loss: 10.021117210388184\n",
      "h: 61 | epoch: 22, train loss: 12.098456382751465, test loss: 9.671876907348633\n",
      "h: 61 | epoch: 23, train loss: 11.7638521194458, test loss: 9.35243034362793\n",
      "h: 61 | epoch: 24, train loss: 11.460504531860352, test loss: 9.060184478759766\n",
      "h: 61 | epoch: 25, train loss: 11.185540199279785, test loss: 8.792764663696289\n",
      "h: 61 | epoch: 26, train loss: 10.936348915100098, test loss: 8.548006057739258\n",
      "h: 61 | epoch: 27, train loss: 10.710553169250488, test loss: 8.323934555053711\n",
      "h: 61 | epoch: 28, train loss: 10.505987167358398, test loss: 8.118745803833008\n",
      "h: 61 | epoch: 29, train loss: 10.32068157196045, test loss: 7.930796146392822\n",
      "h: 61 | epoch: 30, train loss: 10.152849197387695, test loss: 7.75858211517334\n",
      "h: 61 | epoch: 31, train loss: 10.000861167907715, test loss: 7.600734710693359\n",
      "h: 61 | epoch: 32, train loss: 9.863240242004395, test loss: 7.4560041427612305\n",
      "h: 61 | epoch: 33, train loss: 9.738637924194336, test loss: 7.323251247406006\n",
      "h: 61 | epoch: 34, train loss: 9.625836372375488, test loss: 7.201437473297119\n",
      "h: 61 | epoch: 35, train loss: 9.523728370666504, test loss: 7.089611053466797\n",
      "h: 61 | epoch: 36, train loss: 9.431306838989258, test loss: 6.986913204193115\n",
      "h: 61 | epoch: 37, train loss: 9.347657203674316, test loss: 6.892555236816406\n",
      "h: 61 | epoch: 38, train loss: 9.271955490112305, test loss: 6.805816650390625\n",
      "h: 61 | epoch: 39, train loss: 9.203445434570312, test loss: 6.726044654846191\n",
      "h: 61 | epoch: 40, train loss: 9.141450881958008, test loss: 6.652640342712402\n",
      "h: 61 | epoch: 41, train loss: 9.085351943969727, test loss: 6.585061073303223\n",
      "h: 61 | epoch: 42, train loss: 9.034589767456055, test loss: 6.522810935974121\n",
      "h: 61 | epoch: 43, train loss: 8.98865795135498, test loss: 6.465437412261963\n",
      "h: 61 | epoch: 44, train loss: 8.94709587097168, test loss: 6.412528991699219\n",
      "h: 61 | epoch: 45, train loss: 8.909490585327148, test loss: 6.363708972930908\n",
      "h: 61 | epoch: 46, train loss: 8.875460624694824, test loss: 6.318633079528809\n",
      "h: 61 | epoch: 47, train loss: 8.844671249389648, test loss: 6.276991367340088\n",
      "h: 61 | epoch: 48, train loss: 8.816808700561523, test loss: 6.23849630355835\n",
      "h: 61 | epoch: 49, train loss: 8.791596412658691, test loss: 6.2028889656066895\n",
      "h: 61 | epoch: 50, train loss: 8.768781661987305, test loss: 6.169930458068848\n",
      "h: 61 | epoch: 51, train loss: 8.748133659362793, test loss: 6.139405250549316\n",
      "h: 61 | epoch: 52, train loss: 8.729447364807129, test loss: 6.111115455627441\n",
      "h: 61 | epoch: 53, train loss: 8.71253490447998, test loss: 6.0848798751831055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 61 | epoch: 54, train loss: 8.697227478027344, test loss: 6.060532093048096\n",
      "h: 61 | epoch: 55, train loss: 8.683371543884277, test loss: 6.037923336029053\n",
      "h: 61 | epoch: 56, train loss: 8.670827865600586, test loss: 6.016913890838623\n",
      "h: 61 | epoch: 57, train loss: 8.659472465515137, test loss: 5.997378349304199\n",
      "h: 61 | epoch: 58, train loss: 8.649192810058594, test loss: 5.979201793670654\n",
      "h: 61 | epoch: 59, train loss: 8.639883041381836, test loss: 5.962278366088867\n",
      "h: 61 | epoch: 60, train loss: 8.631454467773438, test loss: 5.94650936126709\n",
      "h: 61 | epoch: 61, train loss: 8.623819351196289, test loss: 5.931809425354004\n",
      "h: 61 | epoch: 62, train loss: 8.61690616607666, test loss: 5.918095111846924\n",
      "h: 61 | epoch: 63, train loss: 8.61064338684082, test loss: 5.905292510986328\n",
      "h: 61 | epoch: 64, train loss: 8.60496997833252, test loss: 5.89333438873291\n",
      "h: 61 | epoch: 65, train loss: 8.599828720092773, test loss: 5.882155418395996\n",
      "h: 61 | epoch: 66, train loss: 8.595171928405762, test loss: 5.871701240539551\n",
      "h: 61 | epoch: 67, train loss: 8.590951919555664, test loss: 5.8619184494018555\n",
      "h: 61 | epoch: 68, train loss: 8.587126731872559, test loss: 5.852755546569824\n",
      "h: 61 | epoch: 69, train loss: 8.583659172058105, test loss: 5.844171047210693\n",
      "h: 61 | epoch: 70, train loss: 8.580516815185547, test loss: 5.836123466491699\n",
      "h: 61 | epoch: 71, train loss: 8.577667236328125, test loss: 5.828573226928711\n",
      "h: 61 | epoch: 72, train loss: 8.57508373260498, test loss: 5.821485996246338\n",
      "h: 61 | epoch: 73, train loss: 8.572741508483887, test loss: 5.814831256866455\n",
      "h: 61 | epoch: 74, train loss: 8.570615768432617, test loss: 5.8085784912109375\n",
      "h: 61 | epoch: 75, train loss: 8.56868839263916, test loss: 5.802699089050293\n",
      "h: 61 | epoch: 76, train loss: 8.566940307617188, test loss: 5.797168731689453\n",
      "h: 61 | epoch: 77, train loss: 8.565352439880371, test loss: 5.791964530944824\n",
      "h: 61 | epoch: 78, train loss: 8.563913345336914, test loss: 5.7870635986328125\n",
      "h: 61 | epoch: 79, train loss: 8.562606811523438, test loss: 5.78244686126709\n",
      "h: 61 | epoch: 80, train loss: 8.561421394348145, test loss: 5.7780961990356445\n",
      "h: 61 | epoch: 81, train loss: 8.560344696044922, test loss: 5.773994445800781\n",
      "h: 61 | epoch: 82, train loss: 8.559367179870605, test loss: 5.770123481750488\n",
      "h: 61 | epoch: 83, train loss: 8.558479309082031, test loss: 5.7664713859558105\n",
      "h: 61 | epoch: 84, train loss: 8.557673454284668, test loss: 5.763022422790527\n",
      "h: 61 | epoch: 85, train loss: 8.556940078735352, test loss: 5.759765148162842\n",
      "h: 61 | epoch: 86, train loss: 8.556275367736816, test loss: 5.756688117980957\n",
      "h: 61 | epoch: 87, train loss: 8.555671691894531, test loss: 5.75377893447876\n",
      "h: 61 | epoch: 88, train loss: 8.555122375488281, test loss: 5.751028537750244\n",
      "h: 61 | epoch: 89, train loss: 8.5546236038208, test loss: 5.7484259605407715\n",
      "h: 61 | epoch: 90, train loss: 8.554170608520508, test loss: 5.745961666107178\n",
      "h: 61 | epoch: 91, train loss: 8.553757667541504, test loss: 5.743631362915039\n",
      "h: 61 | epoch: 92, train loss: 8.553382873535156, test loss: 5.741423606872559\n",
      "h: 61 | epoch: 93, train loss: 8.5530424118042, test loss: 5.73933219909668\n",
      "h: 61 | epoch: 94, train loss: 8.552732467651367, test loss: 5.737351417541504\n",
      "h: 61 | epoch: 95, train loss: 8.552450180053711, test loss: 5.735473155975342\n",
      "h: 61 | epoch: 96, train loss: 8.552193641662598, test loss: 5.733692646026611\n",
      "h: 61 | epoch: 97, train loss: 8.551960945129395, test loss: 5.732003688812256\n",
      "h: 61 | epoch: 98, train loss: 8.551748275756836, test loss: 5.730401515960693\n",
      "h: 61 | epoch: 99, train loss: 8.551555633544922, test loss: 5.728881359100342\n",
      "h: 62 | epoch: 0, train loss: 48.700592041015625, test loss: 41.297119140625\n",
      "h: 62 | epoch: 1, train loss: 45.085693359375, test loss: 38.22608184814453\n",
      "h: 62 | epoch: 2, train loss: 41.797855377197266, test loss: 35.42517852783203\n",
      "h: 62 | epoch: 3, train loss: 38.80561065673828, test loss: 32.8691291809082\n",
      "h: 62 | epoch: 4, train loss: 36.08103561401367, test loss: 30.535396575927734\n",
      "h: 62 | epoch: 5, train loss: 33.599266052246094, test loss: 28.403823852539062\n",
      "h: 62 | epoch: 6, train loss: 31.338058471679688, test loss: 26.456310272216797\n",
      "h: 62 | epoch: 7, train loss: 29.277454376220703, test loss: 24.67658042907715\n",
      "h: 62 | epoch: 8, train loss: 27.39948081970215, test loss: 23.049917221069336\n",
      "h: 62 | epoch: 9, train loss: 25.68792152404785, test loss: 21.562992095947266\n",
      "h: 62 | epoch: 10, train loss: 24.12809181213379, test loss: 20.203731536865234\n",
      "h: 62 | epoch: 11, train loss: 22.706676483154297, test loss: 18.961151123046875\n",
      "h: 62 | epoch: 12, train loss: 21.411579132080078, test loss: 17.825246810913086\n",
      "h: 62 | epoch: 13, train loss: 20.231792449951172, test loss: 16.78690528869629\n",
      "h: 62 | epoch: 14, train loss: 19.157276153564453, test loss: 15.837793350219727\n",
      "h: 62 | epoch: 15, train loss: 18.178874969482422, test loss: 14.970312118530273\n",
      "h: 62 | epoch: 16, train loss: 17.28823471069336, test loss: 14.177507400512695\n",
      "h: 62 | epoch: 17, train loss: 16.477710723876953, test loss: 13.453010559082031\n",
      "h: 62 | epoch: 18, train loss: 15.740318298339844, test loss: 12.790996551513672\n",
      "h: 62 | epoch: 19, train loss: 15.069668769836426, test loss: 12.186129570007324\n",
      "h: 62 | epoch: 20, train loss: 14.459918022155762, test loss: 11.633525848388672\n",
      "h: 62 | epoch: 21, train loss: 13.905717849731445, test loss: 11.128708839416504\n",
      "h: 62 | epoch: 22, train loss: 13.402170181274414, test loss: 10.667573928833008\n",
      "h: 62 | epoch: 23, train loss: 12.944796562194824, test loss: 10.246365547180176\n",
      "h: 62 | epoch: 24, train loss: 12.529504776000977, test loss: 9.86164665222168\n",
      "h: 62 | epoch: 25, train loss: 12.152544975280762, test loss: 9.510265350341797\n",
      "h: 62 | epoch: 26, train loss: 11.8104887008667, test loss: 9.189334869384766\n",
      "h: 62 | epoch: 27, train loss: 11.500205993652344, test loss: 8.896219253540039\n",
      "h: 62 | epoch: 28, train loss: 11.218835830688477, test loss: 8.628495216369629\n",
      "h: 62 | epoch: 29, train loss: 10.963757514953613, test loss: 8.38395881652832\n",
      "h: 62 | epoch: 30, train loss: 10.732587814331055, test loss: 8.16058349609375\n",
      "h: 62 | epoch: 31, train loss: 10.52314567565918, test loss: 7.956523895263672\n",
      "h: 62 | epoch: 32, train loss: 10.33344554901123, test loss: 7.770089149475098\n",
      "h: 62 | epoch: 33, train loss: 10.161672592163086, test loss: 7.599737644195557\n",
      "h: 62 | epoch: 34, train loss: 10.006174087524414, test loss: 7.444056510925293\n",
      "h: 62 | epoch: 35, train loss: 9.865445137023926, test loss: 7.301760673522949\n",
      "h: 62 | epoch: 36, train loss: 9.738115310668945, test loss: 7.171672821044922\n",
      "h: 62 | epoch: 37, train loss: 9.622937202453613, test loss: 7.052720546722412\n",
      "h: 62 | epoch: 38, train loss: 9.518773078918457, test loss: 6.9439263343811035\n",
      "h: 62 | epoch: 39, train loss: 9.424591064453125, test loss: 6.844395637512207\n",
      "h: 62 | epoch: 40, train loss: 9.339455604553223, test loss: 6.753316402435303\n",
      "h: 62 | epoch: 41, train loss: 9.262511253356934, test loss: 6.669943809509277\n",
      "h: 62 | epoch: 42, train loss: 9.19298267364502, test loss: 6.593603610992432\n",
      "h: 62 | epoch: 43, train loss: 9.130168914794922, test loss: 6.523679256439209\n",
      "h: 62 | epoch: 44, train loss: 9.073431015014648, test loss: 6.459607124328613\n",
      "h: 62 | epoch: 45, train loss: 9.022189140319824, test loss: 6.400875091552734\n",
      "h: 62 | epoch: 46, train loss: 8.975918769836426, test loss: 6.347017288208008\n",
      "h: 62 | epoch: 47, train loss: 8.934144020080566, test loss: 6.297610282897949\n",
      "h: 62 | epoch: 48, train loss: 8.896432876586914, test loss: 6.252264499664307\n",
      "h: 62 | epoch: 49, train loss: 8.862398147583008, test loss: 6.210629940032959\n",
      "h: 62 | epoch: 50, train loss: 8.831680297851562, test loss: 6.1723833084106445\n",
      "h: 62 | epoch: 51, train loss: 8.803962707519531, test loss: 6.137232303619385\n",
      "h: 62 | epoch: 52, train loss: 8.77895450592041, test loss: 6.1049113273620605\n",
      "h: 62 | epoch: 53, train loss: 8.756392478942871, test loss: 6.07517671585083\n",
      "h: 62 | epoch: 54, train loss: 8.736041069030762, test loss: 6.047807693481445\n",
      "h: 62 | epoch: 55, train loss: 8.717684745788574, test loss: 6.022604942321777\n",
      "h: 62 | epoch: 56, train loss: 8.701128005981445, test loss: 5.999382019042969\n",
      "h: 62 | epoch: 57, train loss: 8.686197280883789, test loss: 5.977972984313965\n",
      "h: 62 | epoch: 58, train loss: 8.672734260559082, test loss: 5.958226203918457\n",
      "h: 62 | epoch: 59, train loss: 8.66059398651123, test loss: 5.939999580383301\n",
      "h: 62 | epoch: 60, train loss: 8.64964771270752, test loss: 5.9231696128845215\n",
      "h: 62 | epoch: 61, train loss: 8.639780044555664, test loss: 5.907619953155518\n",
      "h: 62 | epoch: 62, train loss: 8.63088321685791, test loss: 5.893243789672852\n",
      "h: 62 | epoch: 63, train loss: 8.622862815856934, test loss: 5.879945278167725\n",
      "h: 62 | epoch: 64, train loss: 8.615633964538574, test loss: 5.867639064788818\n",
      "h: 62 | epoch: 65, train loss: 8.60911750793457, test loss: 5.856241226196289\n",
      "h: 62 | epoch: 66, train loss: 8.603242874145508, test loss: 5.845679759979248\n",
      "h: 62 | epoch: 67, train loss: 8.59794807434082, test loss: 5.83588981628418\n",
      "h: 62 | epoch: 68, train loss: 8.593175888061523, test loss: 5.826807975769043\n",
      "h: 62 | epoch: 69, train loss: 8.588875770568848, test loss: 5.818377494812012\n",
      "h: 62 | epoch: 70, train loss: 8.584999084472656, test loss: 5.810548305511475\n",
      "h: 62 | epoch: 71, train loss: 8.58150577545166, test loss: 5.803274154663086\n",
      "h: 62 | epoch: 72, train loss: 8.578357696533203, test loss: 5.796513557434082\n",
      "h: 62 | epoch: 73, train loss: 8.575518608093262, test loss: 5.790223121643066\n",
      "h: 62 | epoch: 74, train loss: 8.572962760925293, test loss: 5.784368515014648\n",
      "h: 62 | epoch: 75, train loss: 8.570657730102539, test loss: 5.7789177894592285\n",
      "h: 62 | epoch: 76, train loss: 8.56857967376709, test loss: 5.773839950561523\n",
      "h: 62 | epoch: 77, train loss: 8.5667085647583, test loss: 5.769106388092041\n",
      "h: 62 | epoch: 78, train loss: 8.565021514892578, test loss: 5.764692306518555\n",
      "h: 62 | epoch: 79, train loss: 8.563501358032227, test loss: 5.7605743408203125\n",
      "h: 62 | epoch: 80, train loss: 8.562129974365234, test loss: 5.7567315101623535\n",
      "h: 62 | epoch: 81, train loss: 8.560895919799805, test loss: 5.753141403198242\n",
      "h: 62 | epoch: 82, train loss: 8.559782028198242, test loss: 5.749786853790283\n",
      "h: 62 | epoch: 83, train loss: 8.558778762817383, test loss: 5.746653079986572\n",
      "h: 62 | epoch: 84, train loss: 8.55787467956543, test loss: 5.743722438812256\n",
      "h: 62 | epoch: 85, train loss: 8.557060241699219, test loss: 5.7409820556640625\n",
      "h: 62 | epoch: 86, train loss: 8.55632495880127, test loss: 5.738415718078613\n",
      "h: 62 | epoch: 87, train loss: 8.555662155151367, test loss: 5.736014366149902\n",
      "h: 62 | epoch: 88, train loss: 8.555066108703613, test loss: 5.733765125274658\n",
      "h: 62 | epoch: 89, train loss: 8.554527282714844, test loss: 5.731658458709717\n",
      "h: 62 | epoch: 90, train loss: 8.55404281616211, test loss: 5.729682445526123\n",
      "h: 62 | epoch: 91, train loss: 8.553606033325195, test loss: 5.727832794189453\n",
      "h: 62 | epoch: 92, train loss: 8.553210258483887, test loss: 5.7260966300964355\n",
      "h: 62 | epoch: 93, train loss: 8.552855491638184, test loss: 5.72446870803833\n",
      "h: 62 | epoch: 94, train loss: 8.552535057067871, test loss: 5.722940444946289\n",
      "h: 62 | epoch: 95, train loss: 8.552245140075684, test loss: 5.721506118774414\n",
      "h: 62 | epoch: 96, train loss: 8.551983833312988, test loss: 5.720160484313965\n",
      "h: 62 | epoch: 97, train loss: 8.551750183105469, test loss: 5.718896389007568\n",
      "h: 62 | epoch: 98, train loss: 8.551538467407227, test loss: 5.717708110809326\n",
      "h: 62 | epoch: 99, train loss: 8.551346778869629, test loss: 5.7165937423706055\n",
      "h: 63 | epoch: 0, train loss: 47.345252990722656, test loss: 42.03516387939453\n",
      "h: 63 | epoch: 1, train loss: 44.93506622314453, test loss: 39.87567138671875\n",
      "h: 63 | epoch: 2, train loss: 42.675994873046875, test loss: 37.847740173339844\n",
      "h: 63 | epoch: 3, train loss: 40.55725860595703, test loss: 35.942264556884766\n",
      "h: 63 | epoch: 4, train loss: 38.56910705566406, test loss: 34.15092086791992\n",
      "h: 63 | epoch: 5, train loss: 36.702659606933594, test loss: 32.4661750793457\n",
      "h: 63 | epoch: 6, train loss: 34.949825286865234, test loss: 30.881093978881836\n",
      "h: 63 | epoch: 7, train loss: 33.30322265625, test loss: 29.38934898376465\n",
      "h: 63 | epoch: 8, train loss: 31.75604820251465, test loss: 27.985118865966797\n",
      "h: 63 | epoch: 9, train loss: 30.30206871032715, test loss: 26.663005828857422\n",
      "h: 63 | epoch: 10, train loss: 28.935516357421875, test loss: 25.41805648803711\n",
      "h: 63 | epoch: 11, train loss: 27.651065826416016, test loss: 24.245643615722656\n",
      "h: 63 | epoch: 12, train loss: 26.44375991821289, test loss: 23.14148712158203\n",
      "h: 63 | epoch: 13, train loss: 25.309005737304688, test loss: 22.10159683227539\n",
      "h: 63 | epoch: 14, train loss: 24.242517471313477, test loss: 21.12224578857422\n",
      "h: 63 | epoch: 15, train loss: 23.24028968811035, test loss: 20.1999454498291\n",
      "h: 63 | epoch: 16, train loss: 22.298580169677734, test loss: 19.331432342529297\n",
      "h: 63 | epoch: 17, train loss: 21.41387939453125, test loss: 18.513647079467773\n",
      "h: 63 | epoch: 18, train loss: 20.58289337158203, test loss: 17.743703842163086\n",
      "h: 63 | epoch: 19, train loss: 19.80253028869629, test loss: 17.018905639648438\n",
      "h: 63 | epoch: 20, train loss: 19.069873809814453, test loss: 16.336692810058594\n",
      "h: 63 | epoch: 21, train loss: 18.38217544555664, test loss: 15.694669723510742\n",
      "h: 63 | epoch: 22, train loss: 17.736858367919922, test loss: 15.090560913085938\n",
      "h: 63 | epoch: 23, train loss: 17.1314697265625, test loss: 14.52222728729248\n",
      "h: 63 | epoch: 24, train loss: 16.56371307373047, test loss: 13.987640380859375\n",
      "h: 63 | epoch: 25, train loss: 16.031402587890625, test loss: 13.484891891479492\n",
      "h: 63 | epoch: 26, train loss: 15.53248405456543, test loss: 13.012173652648926\n",
      "h: 63 | epoch: 27, train loss: 15.065008163452148, test loss: 12.567767143249512\n",
      "h: 63 | epoch: 28, train loss: 14.6271390914917, test loss: 12.15005874633789\n",
      "h: 63 | epoch: 29, train loss: 14.217126846313477, test loss: 11.757518768310547\n",
      "h: 63 | epoch: 30, train loss: 13.833333015441895, test loss: 11.388690948486328\n",
      "h: 63 | epoch: 31, train loss: 13.47419548034668, test loss: 11.042207717895508\n",
      "h: 63 | epoch: 32, train loss: 13.13824462890625, test loss: 10.716772079467773\n",
      "h: 63 | epoch: 33, train loss: 12.824084281921387, test loss: 10.411152839660645\n",
      "h: 63 | epoch: 34, train loss: 12.530399322509766, test loss: 10.124192237854004\n",
      "h: 63 | epoch: 35, train loss: 12.255949020385742, test loss: 9.854791641235352\n",
      "h: 63 | epoch: 36, train loss: 11.999552726745605, test loss: 9.601911544799805\n",
      "h: 63 | epoch: 37, train loss: 11.760103225708008, test loss: 9.364575386047363\n",
      "h: 63 | epoch: 38, train loss: 11.53654956817627, test loss: 9.141851425170898\n",
      "h: 63 | epoch: 39, train loss: 11.327906608581543, test loss: 8.932867050170898\n",
      "h: 63 | epoch: 40, train loss: 11.133237838745117, test loss: 8.73679256439209\n",
      "h: 63 | epoch: 41, train loss: 10.951662063598633, test loss: 8.552851676940918\n",
      "h: 63 | epoch: 42, train loss: 10.782353401184082, test loss: 8.380308151245117\n",
      "h: 63 | epoch: 43, train loss: 10.624528884887695, test loss: 8.218465805053711\n",
      "h: 63 | epoch: 44, train loss: 10.477452278137207, test loss: 8.06667423248291\n",
      "h: 63 | epoch: 45, train loss: 10.340432167053223, test loss: 7.924312591552734\n",
      "h: 63 | epoch: 46, train loss: 10.212814331054688, test loss: 7.790799617767334\n",
      "h: 63 | epoch: 47, train loss: 10.093988418579102, test loss: 7.665596961975098\n",
      "h: 63 | epoch: 48, train loss: 9.983378410339355, test loss: 7.548178195953369\n",
      "h: 63 | epoch: 49, train loss: 9.88044261932373, test loss: 7.438066005706787\n",
      "h: 63 | epoch: 50, train loss: 9.784673690795898, test loss: 7.3348069190979\n",
      "h: 63 | epoch: 51, train loss: 9.695594787597656, test loss: 7.237967491149902\n",
      "h: 63 | epoch: 52, train loss: 9.612756729125977, test loss: 7.147149085998535\n",
      "h: 63 | epoch: 53, train loss: 9.535746574401855, test loss: 7.061971187591553\n",
      "h: 63 | epoch: 54, train loss: 9.464166641235352, test loss: 6.982079982757568\n",
      "h: 63 | epoch: 55, train loss: 9.397648811340332, test loss: 6.90714168548584\n",
      "h: 63 | epoch: 56, train loss: 9.335847854614258, test loss: 6.836844444274902\n",
      "h: 63 | epoch: 57, train loss: 9.27844524383545, test loss: 6.770893096923828\n",
      "h: 63 | epoch: 58, train loss: 9.225137710571289, test loss: 6.709013938903809\n",
      "h: 63 | epoch: 59, train loss: 9.175641059875488, test loss: 6.650949001312256\n",
      "h: 63 | epoch: 60, train loss: 9.129693984985352, test loss: 6.596454620361328\n",
      "h: 63 | epoch: 61, train loss: 9.08704948425293, test loss: 6.5453033447265625\n",
      "h: 63 | epoch: 62, train loss: 9.047475814819336, test loss: 6.497284889221191\n",
      "h: 63 | epoch: 63, train loss: 9.010761260986328, test loss: 6.452200412750244\n",
      "h: 63 | epoch: 64, train loss: 8.976703643798828, test loss: 6.409861087799072\n",
      "h: 63 | epoch: 65, train loss: 8.94511604309082, test loss: 6.370093822479248\n",
      "h: 63 | epoch: 66, train loss: 8.915822982788086, test loss: 6.332736492156982\n",
      "h: 63 | epoch: 67, train loss: 8.888665199279785, test loss: 6.297633171081543\n",
      "h: 63 | epoch: 68, train loss: 8.863487243652344, test loss: 6.264642715454102\n",
      "h: 63 | epoch: 69, train loss: 8.8401517868042, test loss: 6.233631134033203\n",
      "h: 63 | epoch: 70, train loss: 8.818525314331055, test loss: 6.204472541809082\n",
      "h: 63 | epoch: 71, train loss: 8.798482894897461, test loss: 6.177049160003662\n",
      "h: 63 | epoch: 72, train loss: 8.779914855957031, test loss: 6.151252269744873\n",
      "h: 63 | epoch: 73, train loss: 8.762713432312012, test loss: 6.126978397369385\n",
      "h: 63 | epoch: 74, train loss: 8.746781349182129, test loss: 6.104130744934082\n",
      "h: 63 | epoch: 75, train loss: 8.732024192810059, test loss: 6.0826239585876465\n",
      "h: 63 | epoch: 76, train loss: 8.718358993530273, test loss: 6.062367916107178\n",
      "h: 63 | epoch: 77, train loss: 8.705704689025879, test loss: 6.043290138244629\n",
      "h: 63 | epoch: 78, train loss: 8.693988800048828, test loss: 6.025313377380371\n",
      "h: 63 | epoch: 79, train loss: 8.683143615722656, test loss: 6.008368968963623\n",
      "h: 63 | epoch: 80, train loss: 8.673104286193848, test loss: 5.992394924163818\n",
      "h: 63 | epoch: 81, train loss: 8.663811683654785, test loss: 5.977329730987549\n",
      "h: 63 | epoch: 82, train loss: 8.655211448669434, test loss: 5.963118553161621\n",
      "h: 63 | epoch: 83, train loss: 8.647253036499023, test loss: 5.949706077575684\n",
      "h: 63 | epoch: 84, train loss: 8.63988971710205, test loss: 5.937046051025391\n",
      "h: 63 | epoch: 85, train loss: 8.633074760437012, test loss: 5.925092697143555\n",
      "h: 63 | epoch: 86, train loss: 8.626771926879883, test loss: 5.9138007164001465\n",
      "h: 63 | epoch: 87, train loss: 8.620939254760742, test loss: 5.903132438659668\n",
      "h: 63 | epoch: 88, train loss: 8.615546226501465, test loss: 5.893049240112305\n",
      "h: 63 | epoch: 89, train loss: 8.610556602478027, test loss: 5.883514404296875\n",
      "h: 63 | epoch: 90, train loss: 8.605940818786621, test loss: 5.874497890472412\n",
      "h: 63 | epoch: 91, train loss: 8.601673126220703, test loss: 5.865967750549316\n",
      "h: 63 | epoch: 92, train loss: 8.597726821899414, test loss: 5.857893943786621\n",
      "h: 63 | epoch: 93, train loss: 8.594076156616211, test loss: 5.850251197814941\n",
      "h: 63 | epoch: 94, train loss: 8.590702056884766, test loss: 5.843012809753418\n",
      "h: 63 | epoch: 95, train loss: 8.587581634521484, test loss: 5.836154937744141\n",
      "h: 63 | epoch: 96, train loss: 8.584696769714355, test loss: 5.829657554626465\n",
      "h: 63 | epoch: 97, train loss: 8.582030296325684, test loss: 5.823498249053955\n",
      "h: 63 | epoch: 98, train loss: 8.579565048217773, test loss: 5.817657470703125\n",
      "h: 63 | epoch: 99, train loss: 8.577284812927246, test loss: 5.812117576599121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 64 | epoch: 0, train loss: 50.941017150878906, test loss: 42.95611572265625\n",
      "h: 64 | epoch: 1, train loss: 47.56321334838867, test loss: 40.11436462402344\n",
      "h: 64 | epoch: 2, train loss: 44.45758056640625, test loss: 37.494564056396484\n",
      "h: 64 | epoch: 3, train loss: 41.60011672973633, test loss: 35.0777702331543\n",
      "h: 64 | epoch: 4, train loss: 38.96941375732422, test loss: 32.84699630737305\n",
      "h: 64 | epoch: 5, train loss: 36.54631805419922, test loss: 30.786991119384766\n",
      "h: 64 | epoch: 6, train loss: 34.3136100769043, test loss: 28.88397789001465\n",
      "h: 64 | epoch: 7, train loss: 32.25575637817383, test loss: 27.125478744506836\n",
      "h: 64 | epoch: 8, train loss: 30.35868263244629, test loss: 25.500173568725586\n",
      "h: 64 | epoch: 9, train loss: 28.609619140625, test loss: 23.997737884521484\n",
      "h: 64 | epoch: 10, train loss: 26.9969425201416, test loss: 22.608745574951172\n",
      "h: 64 | epoch: 11, train loss: 25.510019302368164, test loss: 21.324560165405273\n",
      "h: 64 | epoch: 12, train loss: 24.13912582397461, test loss: 20.13726806640625\n",
      "h: 64 | epoch: 13, train loss: 22.875347137451172, test loss: 19.039579391479492\n",
      "h: 64 | epoch: 14, train loss: 21.71048355102539, test loss: 18.024784088134766\n",
      "h: 64 | epoch: 15, train loss: 20.63698959350586, test loss: 17.08669090270996\n",
      "h: 64 | epoch: 16, train loss: 19.64790916442871, test loss: 16.2196044921875\n",
      "h: 64 | epoch: 17, train loss: 18.736827850341797, test loss: 15.418232917785645\n",
      "h: 64 | epoch: 18, train loss: 17.897811889648438, test loss: 14.67768669128418\n",
      "h: 64 | epoch: 19, train loss: 17.125377655029297, test loss: 13.993446350097656\n",
      "h: 64 | epoch: 20, train loss: 16.414459228515625, test loss: 13.36131477355957\n",
      "h: 64 | epoch: 21, train loss: 15.760353088378906, test loss: 12.777419090270996\n",
      "h: 64 | epoch: 22, train loss: 15.158717155456543, test loss: 12.238146781921387\n",
      "h: 64 | epoch: 23, train loss: 14.605524063110352, test loss: 11.740163803100586\n",
      "h: 64 | epoch: 24, train loss: 14.097040176391602, test loss: 11.28036880493164\n",
      "h: 64 | epoch: 25, train loss: 13.62981128692627, test loss: 10.855894088745117\n",
      "h: 64 | epoch: 26, train loss: 13.200639724731445, test loss: 10.464070320129395\n",
      "h: 64 | epoch: 27, train loss: 12.806553840637207, test loss: 10.102433204650879\n",
      "h: 64 | epoch: 28, train loss: 12.44481086730957, test loss: 9.768689155578613\n",
      "h: 64 | epoch: 29, train loss: 12.112871170043945, test loss: 9.46071720123291\n",
      "h: 64 | epoch: 30, train loss: 11.808375358581543, test loss: 9.176549911499023\n",
      "h: 64 | epoch: 31, train loss: 11.529153823852539, test loss: 8.914361953735352\n",
      "h: 64 | epoch: 32, train loss: 11.273187637329102, test loss: 8.672467231750488\n",
      "h: 64 | epoch: 33, train loss: 11.038616180419922, test loss: 8.449304580688477\n",
      "h: 64 | epoch: 34, train loss: 10.823718070983887, test loss: 8.243428230285645\n",
      "h: 64 | epoch: 35, train loss: 10.62690544128418, test loss: 8.053497314453125\n",
      "h: 64 | epoch: 36, train loss: 10.446709632873535, test loss: 7.878274440765381\n",
      "h: 64 | epoch: 37, train loss: 10.281778335571289, test loss: 7.716618537902832\n",
      "h: 64 | epoch: 38, train loss: 10.130859375, test loss: 7.567469596862793\n",
      "h: 64 | epoch: 39, train loss: 9.992803573608398, test loss: 7.4298505783081055\n",
      "h: 64 | epoch: 40, train loss: 9.866545677185059, test loss: 7.302861213684082\n",
      "h: 64 | epoch: 41, train loss: 9.751110076904297, test loss: 7.185665130615234\n",
      "h: 64 | epoch: 42, train loss: 9.64559555053711, test loss: 7.077497959136963\n",
      "h: 64 | epoch: 43, train loss: 9.549172401428223, test loss: 6.9776458740234375\n",
      "h: 64 | epoch: 44, train loss: 9.461082458496094, test loss: 6.885458946228027\n",
      "h: 64 | epoch: 45, train loss: 9.380620002746582, test loss: 6.800332069396973\n",
      "h: 64 | epoch: 46, train loss: 9.30714225769043, test loss: 6.721707344055176\n",
      "h: 64 | epoch: 47, train loss: 9.240058898925781, test loss: 6.649075984954834\n",
      "h: 64 | epoch: 48, train loss: 9.178825378417969, test loss: 6.581963539123535\n",
      "h: 64 | epoch: 49, train loss: 9.122941970825195, test loss: 6.519936561584473\n",
      "h: 64 | epoch: 50, train loss: 9.071952819824219, test loss: 6.462594032287598\n",
      "h: 64 | epoch: 51, train loss: 9.025436401367188, test loss: 6.4095659255981445\n",
      "h: 64 | epoch: 52, train loss: 8.983011245727539, test loss: 6.360514163970947\n",
      "h: 64 | epoch: 53, train loss: 8.944320678710938, test loss: 6.315126419067383\n",
      "h: 64 | epoch: 54, train loss: 8.909043312072754, test loss: 6.273116111755371\n",
      "h: 64 | epoch: 55, train loss: 8.876882553100586, test loss: 6.234216690063477\n",
      "h: 64 | epoch: 56, train loss: 8.84756851196289, test loss: 6.198185920715332\n",
      "h: 64 | epoch: 57, train loss: 8.820853233337402, test loss: 6.164799213409424\n",
      "h: 64 | epoch: 58, train loss: 8.7965087890625, test loss: 6.133851051330566\n",
      "h: 64 | epoch: 59, train loss: 8.774328231811523, test loss: 6.1051530838012695\n",
      "h: 64 | epoch: 60, train loss: 8.754122734069824, test loss: 6.078527927398682\n",
      "h: 64 | epoch: 61, train loss: 8.7357177734375, test loss: 6.053818702697754\n",
      "h: 64 | epoch: 62, train loss: 8.718954086303711, test loss: 6.0308756828308105\n",
      "h: 64 | epoch: 63, train loss: 8.703690528869629, test loss: 6.009564399719238\n",
      "h: 64 | epoch: 64, train loss: 8.689790725708008, test loss: 5.989757061004639\n",
      "h: 64 | epoch: 65, train loss: 8.677136421203613, test loss: 5.971343040466309\n",
      "h: 64 | epoch: 66, train loss: 8.665616035461426, test loss: 5.9542131423950195\n",
      "h: 64 | epoch: 67, train loss: 8.655128479003906, test loss: 5.9382734298706055\n",
      "h: 64 | epoch: 68, train loss: 8.645584106445312, test loss: 5.923430919647217\n",
      "h: 64 | epoch: 69, train loss: 8.636896133422852, test loss: 5.909605026245117\n",
      "h: 64 | epoch: 70, train loss: 8.62899112701416, test loss: 5.896721363067627\n",
      "h: 64 | epoch: 71, train loss: 8.621796607971191, test loss: 5.884706497192383\n",
      "h: 64 | epoch: 72, train loss: 8.615252494812012, test loss: 5.87349796295166\n",
      "h: 64 | epoch: 73, train loss: 8.609296798706055, test loss: 5.863036632537842\n",
      "h: 64 | epoch: 74, train loss: 8.603878021240234, test loss: 5.853269100189209\n",
      "h: 64 | epoch: 75, train loss: 8.59895133972168, test loss: 5.844141960144043\n",
      "h: 64 | epoch: 76, train loss: 8.594467163085938, test loss: 5.835610866546631\n",
      "h: 64 | epoch: 77, train loss: 8.5903902053833, test loss: 5.827632904052734\n",
      "h: 64 | epoch: 78, train loss: 8.58668041229248, test loss: 5.8201680183410645\n",
      "h: 64 | epoch: 79, train loss: 8.583307266235352, test loss: 5.813179969787598\n",
      "h: 64 | epoch: 80, train loss: 8.580240249633789, test loss: 5.80663537979126\n",
      "h: 64 | epoch: 81, train loss: 8.5774507522583, test loss: 5.800502777099609\n",
      "h: 64 | epoch: 82, train loss: 8.574913024902344, test loss: 5.794753551483154\n",
      "h: 64 | epoch: 83, train loss: 8.572607040405273, test loss: 5.7893595695495605\n",
      "h: 64 | epoch: 84, train loss: 8.570509910583496, test loss: 5.784299373626709\n",
      "h: 64 | epoch: 85, train loss: 8.568603515625, test loss: 5.779549598693848\n",
      "h: 64 | epoch: 86, train loss: 8.566869735717773, test loss: 5.775086879730225\n",
      "h: 64 | epoch: 87, train loss: 8.56529426574707, test loss: 5.770895004272461\n",
      "h: 64 | epoch: 88, train loss: 8.563860893249512, test loss: 5.766953945159912\n",
      "h: 64 | epoch: 89, train loss: 8.562559127807617, test loss: 5.763247013092041\n",
      "h: 64 | epoch: 90, train loss: 8.561373710632324, test loss: 5.759759902954102\n",
      "h: 64 | epoch: 91, train loss: 8.560297966003418, test loss: 5.756476402282715\n",
      "h: 64 | epoch: 92, train loss: 8.559320449829102, test loss: 5.753385066986084\n",
      "h: 64 | epoch: 93, train loss: 8.558430671691895, test loss: 5.750473976135254\n",
      "h: 64 | epoch: 94, train loss: 8.557622909545898, test loss: 5.74772834777832\n",
      "h: 64 | epoch: 95, train loss: 8.556886672973633, test loss: 5.745141506195068\n",
      "h: 64 | epoch: 96, train loss: 8.556220054626465, test loss: 5.742702960968018\n",
      "h: 64 | epoch: 97, train loss: 8.555612564086914, test loss: 5.740399360656738\n",
      "h: 64 | epoch: 98, train loss: 8.555062294006348, test loss: 5.738225936889648\n",
      "h: 64 | epoch: 99, train loss: 8.554560661315918, test loss: 5.736175537109375\n",
      "h: 65 | epoch: 0, train loss: 42.80336380004883, test loss: 35.9224853515625\n",
      "h: 65 | epoch: 1, train loss: 39.5977668762207, test loss: 33.22117233276367\n",
      "h: 65 | epoch: 2, train loss: 36.69097137451172, test loss: 30.764562606811523\n",
      "h: 65 | epoch: 3, train loss: 34.05403137207031, test loss: 28.529521942138672\n",
      "h: 65 | epoch: 4, train loss: 31.661174774169922, test loss: 26.495403289794922\n",
      "h: 65 | epoch: 5, train loss: 29.4893798828125, test loss: 24.643672943115234\n",
      "h: 65 | epoch: 6, train loss: 27.51799964904785, test loss: 22.957674026489258\n",
      "h: 65 | epoch: 7, train loss: 25.728479385375977, test loss: 21.422374725341797\n",
      "h: 65 | epoch: 8, train loss: 24.104097366333008, test loss: 20.02420997619629\n",
      "h: 65 | epoch: 9, train loss: 22.62974739074707, test loss: 18.750883102416992\n",
      "h: 65 | epoch: 10, train loss: 21.2917537689209, test loss: 17.591249465942383\n",
      "h: 65 | epoch: 11, train loss: 20.077728271484375, test loss: 16.535179138183594\n",
      "h: 65 | epoch: 12, train loss: 18.976423263549805, test loss: 15.57347583770752\n",
      "h: 65 | epoch: 13, train loss: 17.977619171142578, test loss: 14.69775104522705\n",
      "h: 65 | epoch: 14, train loss: 17.072017669677734, test loss: 13.900372505187988\n",
      "h: 65 | epoch: 15, train loss: 16.251161575317383, test loss: 13.174389839172363\n",
      "h: 65 | epoch: 16, train loss: 15.507345199584961, test loss: 12.513457298278809\n",
      "h: 65 | epoch: 17, train loss: 14.833552360534668, test loss: 11.911787033081055\n",
      "h: 65 | epoch: 18, train loss: 14.223390579223633, test loss: 11.364104270935059\n",
      "h: 65 | epoch: 19, train loss: 13.671030044555664, test loss: 10.865593910217285\n",
      "h: 65 | epoch: 20, train loss: 13.171163558959961, test loss: 10.411860466003418\n",
      "h: 65 | epoch: 21, train loss: 12.718954086303711, test loss: 9.998895645141602\n",
      "h: 65 | epoch: 22, train loss: 12.309995651245117, test loss: 9.62304401397705\n",
      "h: 65 | epoch: 23, train loss: 11.940272331237793, test loss: 9.28097152709961\n",
      "h: 65 | epoch: 24, train loss: 11.606130599975586, test loss: 8.969636917114258\n",
      "h: 65 | epoch: 25, train loss: 11.304245948791504, test loss: 8.68626594543457\n",
      "h: 65 | epoch: 26, train loss: 11.031587600708008, test loss: 8.428332328796387\n",
      "h: 65 | epoch: 27, train loss: 10.785406112670898, test loss: 8.1935396194458\n",
      "h: 65 | epoch: 28, train loss: 10.563196182250977, test loss: 7.9797844886779785\n",
      "h: 65 | epoch: 29, train loss: 10.362682342529297, test loss: 7.785160064697266\n",
      "h: 65 | epoch: 30, train loss: 10.18179988861084, test loss: 7.607929229736328\n",
      "h: 65 | epoch: 31, train loss: 10.018671035766602, test loss: 7.446507930755615\n",
      "h: 65 | epoch: 32, train loss: 9.871593475341797, test loss: 7.299458980560303\n",
      "h: 65 | epoch: 33, train loss: 9.739023208618164, test loss: 7.165472507476807\n",
      "h: 65 | epoch: 34, train loss: 9.619558334350586, test loss: 7.043360233306885\n",
      "h: 65 | epoch: 35, train loss: 9.511930465698242, test loss: 6.932039737701416\n",
      "h: 65 | epoch: 36, train loss: 9.414989471435547, test loss: 6.830527305603027\n",
      "h: 65 | epoch: 37, train loss: 9.327692985534668, test loss: 6.737928867340088\n",
      "h: 65 | epoch: 38, train loss: 9.249101638793945, test loss: 6.653435707092285\n",
      "h: 65 | epoch: 39, train loss: 9.178357124328613, test loss: 6.576308250427246\n",
      "h: 65 | epoch: 40, train loss: 9.114691734313965, test loss: 6.505878448486328\n",
      "h: 65 | epoch: 41, train loss: 9.057409286499023, test loss: 6.441538333892822\n",
      "h: 65 | epoch: 42, train loss: 9.005876541137695, test loss: 6.3827362060546875\n",
      "h: 65 | epoch: 43, train loss: 8.959524154663086, test loss: 6.328972339630127\n",
      "h: 65 | epoch: 44, train loss: 8.917840957641602, test loss: 6.279791355133057\n",
      "h: 65 | epoch: 45, train loss: 8.88036060333252, test loss: 6.234780311584473\n",
      "h: 65 | epoch: 46, train loss: 8.846665382385254, test loss: 6.19356632232666\n",
      "h: 65 | epoch: 47, train loss: 8.816376686096191, test loss: 6.155806541442871\n",
      "h: 65 | epoch: 48, train loss: 8.789155006408691, test loss: 6.121196269989014\n",
      "h: 65 | epoch: 49, train loss: 8.764692306518555, test loss: 6.08945369720459\n",
      "h: 65 | epoch: 50, train loss: 8.742712020874023, test loss: 6.060323238372803\n",
      "h: 65 | epoch: 51, train loss: 8.722963333129883, test loss: 6.033576488494873\n",
      "h: 65 | epoch: 52, train loss: 8.705222129821777, test loss: 6.009002208709717\n",
      "h: 65 | epoch: 53, train loss: 8.689288139343262, test loss: 5.986411094665527\n",
      "h: 65 | epoch: 54, train loss: 8.674975395202637, test loss: 5.965630531311035\n",
      "h: 65 | epoch: 55, train loss: 8.66212272644043, test loss: 5.946503162384033\n",
      "h: 65 | epoch: 56, train loss: 8.650582313537598, test loss: 5.928886890411377\n",
      "h: 65 | epoch: 57, train loss: 8.640219688415527, test loss: 5.912651538848877\n",
      "h: 65 | epoch: 58, train loss: 8.630916595458984, test loss: 5.897677898406982\n",
      "h: 65 | epoch: 59, train loss: 8.622564315795898, test loss: 5.883862495422363\n",
      "h: 65 | epoch: 60, train loss: 8.61506462097168, test loss: 5.871103763580322\n",
      "h: 65 | epoch: 61, train loss: 8.608335494995117, test loss: 5.859314918518066\n",
      "h: 65 | epoch: 62, train loss: 8.602294921875, test loss: 5.848414421081543\n",
      "h: 65 | epoch: 63, train loss: 8.59687328338623, test loss: 5.838329315185547\n",
      "h: 65 | epoch: 64, train loss: 8.59200668334961, test loss: 5.828993797302246\n",
      "h: 65 | epoch: 65, train loss: 8.587641716003418, test loss: 5.820344924926758\n",
      "h: 65 | epoch: 66, train loss: 8.583722114562988, test loss: 5.812326908111572\n",
      "h: 65 | epoch: 67, train loss: 8.580206871032715, test loss: 5.8048906326293945\n",
      "h: 65 | epoch: 68, train loss: 8.577052116394043, test loss: 5.797987937927246\n",
      "h: 65 | epoch: 69, train loss: 8.574222564697266, test loss: 5.791576862335205\n",
      "h: 65 | epoch: 70, train loss: 8.57168197631836, test loss: 5.785618305206299\n",
      "h: 65 | epoch: 71, train loss: 8.569403648376465, test loss: 5.780078411102295\n",
      "h: 65 | epoch: 72, train loss: 8.567360877990723, test loss: 5.774925231933594\n",
      "h: 65 | epoch: 73, train loss: 8.565526962280273, test loss: 5.770127296447754\n",
      "h: 65 | epoch: 74, train loss: 8.563881874084473, test loss: 5.765657424926758\n",
      "h: 65 | epoch: 75, train loss: 8.562406539916992, test loss: 5.761492729187012\n",
      "h: 65 | epoch: 76, train loss: 8.56108283996582, test loss: 5.757607460021973\n",
      "h: 65 | epoch: 77, train loss: 8.559896469116211, test loss: 5.753983974456787\n",
      "h: 65 | epoch: 78, train loss: 8.558831214904785, test loss: 5.750601291656494\n",
      "h: 65 | epoch: 79, train loss: 8.55787467956543, test loss: 5.74744176864624\n",
      "h: 65 | epoch: 80, train loss: 8.557019233703613, test loss: 5.744488716125488\n",
      "h: 65 | epoch: 81, train loss: 8.556251525878906, test loss: 5.741728782653809\n",
      "h: 65 | epoch: 82, train loss: 8.555562019348145, test loss: 5.739147663116455\n",
      "h: 65 | epoch: 83, train loss: 8.554944038391113, test loss: 5.73673152923584\n",
      "h: 65 | epoch: 84, train loss: 8.554388999938965, test loss: 5.734470367431641\n",
      "h: 65 | epoch: 85, train loss: 8.553892135620117, test loss: 5.732353210449219\n",
      "h: 65 | epoch: 86, train loss: 8.553445816040039, test loss: 5.730367660522461\n",
      "h: 65 | epoch: 87, train loss: 8.553046226501465, test loss: 5.728507041931152\n",
      "h: 65 | epoch: 88, train loss: 8.552687644958496, test loss: 5.7267632484436035\n",
      "h: 65 | epoch: 89, train loss: 8.552366256713867, test loss: 5.725127220153809\n",
      "h: 65 | epoch: 90, train loss: 8.552077293395996, test loss: 5.723592281341553\n",
      "h: 65 | epoch: 91, train loss: 8.551819801330566, test loss: 5.722150802612305\n",
      "h: 65 | epoch: 92, train loss: 8.551587104797363, test loss: 5.720797538757324\n",
      "h: 65 | epoch: 93, train loss: 8.55137825012207, test loss: 5.719526290893555\n",
      "h: 65 | epoch: 94, train loss: 8.551192283630371, test loss: 5.718332290649414\n",
      "h: 65 | epoch: 95, train loss: 8.551025390625, test loss: 5.717209815979004\n",
      "h: 65 | epoch: 96, train loss: 8.550875663757324, test loss: 5.7161545753479\n",
      "h: 65 | epoch: 97, train loss: 8.550741195678711, test loss: 5.71516227722168\n",
      "h: 65 | epoch: 98, train loss: 8.550620079040527, test loss: 5.714229106903076\n",
      "h: 65 | epoch: 99, train loss: 8.550512313842773, test loss: 5.713351249694824\n",
      "h: 66 | epoch: 0, train loss: 47.611106872558594, test loss: 39.80733108520508\n",
      "h: 66 | epoch: 1, train loss: 43.6059684753418, test loss: 36.411712646484375\n",
      "h: 66 | epoch: 2, train loss: 40.02503204345703, test loss: 33.370262145996094\n",
      "h: 66 | epoch: 3, train loss: 36.820343017578125, test loss: 30.643447875976562\n",
      "h: 66 | epoch: 4, train loss: 33.95008087158203, test loss: 28.196685791015625\n",
      "h: 66 | epoch: 5, train loss: 31.377593994140625, test loss: 25.99957847595215\n",
      "h: 66 | epoch: 6, train loss: 29.070663452148438, test loss: 24.025358200073242\n",
      "h: 66 | epoch: 7, train loss: 27.000865936279297, test loss: 22.25035858154297\n",
      "h: 66 | epoch: 8, train loss: 25.14305877685547, test loss: 20.653627395629883\n",
      "h: 66 | epoch: 9, train loss: 23.474952697753906, test loss: 19.216548919677734\n",
      "h: 66 | epoch: 10, train loss: 21.976728439331055, test loss: 17.922574996948242\n",
      "h: 66 | epoch: 11, train loss: 20.630754470825195, test loss: 16.756961822509766\n",
      "h: 66 | epoch: 12, train loss: 19.421295166015625, test loss: 15.706560134887695\n",
      "h: 66 | epoch: 13, train loss: 18.33430290222168, test loss: 14.759608268737793\n",
      "h: 66 | epoch: 14, train loss: 17.357219696044922, test loss: 13.905613899230957\n",
      "h: 66 | epoch: 15, train loss: 16.478809356689453, test loss: 13.135162353515625\n",
      "h: 66 | epoch: 16, train loss: 15.689002990722656, test loss: 12.439834594726562\n",
      "h: 66 | epoch: 17, train loss: 14.978776931762695, test loss: 11.812084197998047\n",
      "h: 66 | epoch: 18, train loss: 14.340042114257812, test loss: 11.245139122009277\n",
      "h: 66 | epoch: 19, train loss: 13.765538215637207, test loss: 10.732924461364746\n",
      "h: 66 | epoch: 20, train loss: 13.248751640319824, test loss: 10.269988059997559\n",
      "h: 66 | epoch: 21, train loss: 12.783836364746094, test loss: 9.85143756866455\n",
      "h: 66 | epoch: 22, train loss: 12.3655366897583, test loss: 9.472869873046875\n",
      "h: 66 | epoch: 23, train loss: 11.989133834838867, test loss: 9.130339622497559\n",
      "h: 66 | epoch: 24, train loss: 11.650398254394531, test loss: 8.820290565490723\n",
      "h: 66 | epoch: 25, train loss: 11.345518112182617, test loss: 8.539533615112305\n",
      "h: 66 | epoch: 26, train loss: 11.071073532104492, test loss: 8.285196304321289\n",
      "h: 66 | epoch: 27, train loss: 10.823997497558594, test loss: 8.05469799041748\n",
      "h: 66 | epoch: 28, train loss: 10.601524353027344, test loss: 7.845719337463379\n",
      "h: 66 | epoch: 29, train loss: 10.401174545288086, test loss: 7.656163692474365\n",
      "h: 66 | epoch: 30, train loss: 10.220721244812012, test loss: 7.484156608581543\n",
      "h: 66 | epoch: 31, train loss: 10.0581636428833, test loss: 7.328001499176025\n",
      "h: 66 | epoch: 32, train loss: 9.911699295043945, test loss: 7.186176300048828\n",
      "h: 66 | epoch: 33, train loss: 9.779712677001953, test loss: 7.057308197021484\n",
      "h: 66 | epoch: 34, train loss: 9.660751342773438, test loss: 6.940160274505615\n",
      "h: 66 | epoch: 35, train loss: 9.553510665893555, test loss: 6.833615779876709\n",
      "h: 66 | epoch: 36, train loss: 9.456812858581543, test loss: 6.736672878265381\n",
      "h: 66 | epoch: 37, train loss: 9.369607925415039, test loss: 6.648425102233887\n",
      "h: 66 | epoch: 38, train loss: 9.290948867797852, test loss: 6.568055629730225\n",
      "h: 66 | epoch: 39, train loss: 9.219979286193848, test loss: 6.4948272705078125\n",
      "h: 66 | epoch: 40, train loss: 9.155937194824219, test loss: 6.428072929382324\n",
      "h: 66 | epoch: 41, train loss: 9.09813117980957, test loss: 6.3671956062316895\n",
      "h: 66 | epoch: 42, train loss: 9.045945167541504, test loss: 6.311649322509766\n",
      "h: 66 | epoch: 43, train loss: 8.998821258544922, test loss: 6.260947227478027\n",
      "h: 66 | epoch: 44, train loss: 8.956259727478027, test loss: 6.214641571044922\n",
      "h: 66 | epoch: 45, train loss: 8.917808532714844, test loss: 6.17233419418335\n",
      "h: 66 | epoch: 46, train loss: 8.883064270019531, test loss: 6.133665561676025\n",
      "h: 66 | epoch: 47, train loss: 8.85166072845459, test loss: 6.098304271697998\n",
      "h: 66 | epoch: 48, train loss: 8.823272705078125, test loss: 6.065953254699707\n",
      "h: 66 | epoch: 49, train loss: 8.797601699829102, test loss: 6.036342620849609\n",
      "h: 66 | epoch: 50, train loss: 8.774385452270508, test loss: 6.009232044219971\n",
      "h: 66 | epoch: 51, train loss: 8.75338363647461, test loss: 5.984396457672119\n",
      "h: 66 | epoch: 52, train loss: 8.734379768371582, test loss: 5.961638450622559\n",
      "h: 66 | epoch: 53, train loss: 8.717180252075195, test loss: 5.940773010253906\n",
      "h: 66 | epoch: 54, train loss: 8.701611518859863, test loss: 5.921638488769531\n",
      "h: 66 | epoch: 55, train loss: 8.687514305114746, test loss: 5.90408182144165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 66 | epoch: 56, train loss: 8.674747467041016, test loss: 5.887966632843018\n",
      "h: 66 | epoch: 57, train loss: 8.663183212280273, test loss: 5.873169898986816\n",
      "h: 66 | epoch: 58, train loss: 8.652704238891602, test loss: 5.859577655792236\n",
      "h: 66 | epoch: 59, train loss: 8.643208503723145, test loss: 5.847088813781738\n",
      "h: 66 | epoch: 60, train loss: 8.634602546691895, test loss: 5.835607528686523\n",
      "h: 66 | epoch: 61, train loss: 8.626799583435059, test loss: 5.825052738189697\n",
      "h: 66 | epoch: 62, train loss: 8.619723320007324, test loss: 5.815342903137207\n",
      "h: 66 | epoch: 63, train loss: 8.613306045532227, test loss: 5.806407451629639\n",
      "h: 66 | epoch: 64, train loss: 8.607484817504883, test loss: 5.798183917999268\n",
      "h: 66 | epoch: 65, train loss: 8.602203369140625, test loss: 5.790611743927002\n",
      "h: 66 | epoch: 66, train loss: 8.597410202026367, test loss: 5.783636569976807\n",
      "h: 66 | epoch: 67, train loss: 8.593060493469238, test loss: 5.7772111892700195\n",
      "h: 66 | epoch: 68, train loss: 8.589111328125, test loss: 5.771289348602295\n",
      "h: 66 | epoch: 69, train loss: 8.585525512695312, test loss: 5.765830039978027\n",
      "h: 66 | epoch: 70, train loss: 8.582269668579102, test loss: 5.7607951164245605\n",
      "h: 66 | epoch: 71, train loss: 8.57931137084961, test loss: 5.756153106689453\n",
      "h: 66 | epoch: 72, train loss: 8.57662582397461, test loss: 5.75186824798584\n",
      "h: 66 | epoch: 73, train loss: 8.57418441772461, test loss: 5.747913360595703\n",
      "h: 66 | epoch: 74, train loss: 8.571966171264648, test loss: 5.744264125823975\n",
      "h: 66 | epoch: 75, train loss: 8.569951057434082, test loss: 5.740894317626953\n",
      "h: 66 | epoch: 76, train loss: 8.568117141723633, test loss: 5.737780570983887\n",
      "h: 66 | epoch: 77, train loss: 8.566452026367188, test loss: 5.734905242919922\n",
      "h: 66 | epoch: 78, train loss: 8.56493854522705, test loss: 5.732247352600098\n",
      "h: 66 | epoch: 79, train loss: 8.563559532165527, test loss: 5.729792594909668\n",
      "h: 66 | epoch: 80, train loss: 8.562307357788086, test loss: 5.7275214195251465\n",
      "h: 66 | epoch: 81, train loss: 8.561168670654297, test loss: 5.72542142868042\n",
      "h: 66 | epoch: 82, train loss: 8.560132026672363, test loss: 5.723479747772217\n",
      "h: 66 | epoch: 83, train loss: 8.559188842773438, test loss: 5.721683502197266\n",
      "h: 66 | epoch: 84, train loss: 8.558331489562988, test loss: 5.720020294189453\n",
      "h: 66 | epoch: 85, train loss: 8.557550430297852, test loss: 5.718482971191406\n",
      "h: 66 | epoch: 86, train loss: 8.556839942932129, test loss: 5.717058181762695\n",
      "h: 66 | epoch: 87, train loss: 8.556192398071289, test loss: 5.715740203857422\n",
      "h: 66 | epoch: 88, train loss: 8.555603981018066, test loss: 5.714519500732422\n",
      "h: 66 | epoch: 89, train loss: 8.55506706237793, test loss: 5.7133893966674805\n",
      "h: 66 | epoch: 90, train loss: 8.55457878112793, test loss: 5.712342262268066\n",
      "h: 66 | epoch: 91, train loss: 8.5541353225708, test loss: 5.71136999130249\n",
      "h: 66 | epoch: 92, train loss: 8.553730964660645, test loss: 5.710472106933594\n",
      "h: 66 | epoch: 93, train loss: 8.553361892700195, test loss: 5.709637641906738\n",
      "h: 66 | epoch: 94, train loss: 8.55302619934082, test loss: 5.708864688873291\n",
      "h: 66 | epoch: 95, train loss: 8.55272102355957, test loss: 5.70814847946167\n",
      "h: 66 | epoch: 96, train loss: 8.55244255065918, test loss: 5.707483768463135\n",
      "h: 66 | epoch: 97, train loss: 8.5521879196167, test loss: 5.706867694854736\n",
      "h: 66 | epoch: 98, train loss: 8.551957130432129, test loss: 5.706296920776367\n",
      "h: 66 | epoch: 99, train loss: 8.551746368408203, test loss: 5.705766201019287\n",
      "h: 67 | epoch: 0, train loss: 40.606361389160156, test loss: 34.5723876953125\n",
      "h: 67 | epoch: 1, train loss: 37.9122428894043, test loss: 32.31694030761719\n",
      "h: 67 | epoch: 2, train loss: 35.448631286621094, test loss: 30.245304107666016\n",
      "h: 67 | epoch: 3, train loss: 33.19438171386719, test loss: 28.341205596923828\n",
      "h: 67 | epoch: 4, train loss: 31.13059425354004, test loss: 26.59006118774414\n",
      "h: 67 | epoch: 5, train loss: 29.2403507232666, test loss: 24.978769302368164\n",
      "h: 67 | epoch: 6, train loss: 27.5084285736084, test loss: 23.495502471923828\n",
      "h: 67 | epoch: 7, train loss: 25.921117782592773, test loss: 22.129568099975586\n",
      "h: 67 | epoch: 8, train loss: 24.466012954711914, test loss: 20.87128257751465\n",
      "h: 67 | epoch: 9, train loss: 23.131885528564453, test loss: 19.71183204650879\n",
      "h: 67 | epoch: 10, train loss: 21.908531188964844, test loss: 18.643199920654297\n",
      "h: 67 | epoch: 11, train loss: 20.786678314208984, test loss: 17.65807342529297\n",
      "h: 67 | epoch: 12, train loss: 19.757863998413086, test loss: 16.749759674072266\n",
      "h: 67 | epoch: 13, train loss: 18.81437873840332, test loss: 15.912153244018555\n",
      "h: 67 | epoch: 14, train loss: 17.949169158935547, test loss: 15.139643669128418\n",
      "h: 67 | epoch: 15, train loss: 17.155790328979492, test loss: 14.427087783813477\n",
      "h: 67 | epoch: 16, train loss: 16.428329467773438, test loss: 13.769770622253418\n",
      "h: 67 | epoch: 17, train loss: 15.761377334594727, test loss: 13.163350105285645\n",
      "h: 67 | epoch: 18, train loss: 15.1499662399292, test loss: 12.603841781616211\n",
      "h: 67 | epoch: 19, train loss: 14.589548110961914, test loss: 12.087571144104004\n",
      "h: 67 | epoch: 20, train loss: 14.075933456420898, test loss: 11.611162185668945\n",
      "h: 67 | epoch: 21, train loss: 13.605283737182617, test loss: 11.171499252319336\n",
      "h: 67 | epoch: 22, train loss: 13.17407512664795, test loss: 10.765715599060059\n",
      "h: 67 | epoch: 23, train loss: 12.779057502746582, test loss: 10.391169548034668\n",
      "h: 67 | epoch: 24, train loss: 12.417255401611328, test loss: 10.04542350769043\n",
      "h: 67 | epoch: 25, train loss: 12.085928916931152, test loss: 9.726234436035156\n",
      "h: 67 | epoch: 26, train loss: 11.782560348510742, test loss: 9.431528091430664\n",
      "h: 67 | epoch: 27, train loss: 11.504838943481445, test loss: 9.159399032592773\n",
      "h: 67 | epoch: 28, train loss: 11.250636100769043, test loss: 8.908082962036133\n",
      "h: 67 | epoch: 29, train loss: 11.017998695373535, test loss: 8.675958633422852\n",
      "h: 67 | epoch: 30, train loss: 10.805130004882812, test loss: 8.461530685424805\n",
      "h: 67 | epoch: 31, train loss: 10.610383987426758, test loss: 8.263415336608887\n",
      "h: 67 | epoch: 32, train loss: 10.432243347167969, test loss: 8.080343246459961\n",
      "h: 67 | epoch: 33, train loss: 10.269318580627441, test loss: 7.91113805770874\n",
      "h: 67 | epoch: 34, train loss: 10.120330810546875, test loss: 7.754721641540527\n",
      "h: 67 | epoch: 35, train loss: 9.984108924865723, test loss: 7.6100969314575195\n",
      "h: 67 | epoch: 36, train loss: 9.859575271606445, test loss: 7.476346015930176\n",
      "h: 67 | epoch: 37, train loss: 9.745744705200195, test loss: 7.352618217468262\n",
      "h: 67 | epoch: 38, train loss: 9.641706466674805, test loss: 7.238138675689697\n",
      "h: 67 | epoch: 39, train loss: 9.546636581420898, test loss: 7.132184028625488\n",
      "h: 67 | epoch: 40, train loss: 9.459768295288086, test loss: 7.034094333648682\n",
      "h: 67 | epoch: 41, train loss: 9.38040542602539, test loss: 6.94326114654541\n",
      "h: 67 | epoch: 42, train loss: 9.307905197143555, test loss: 6.859117031097412\n",
      "h: 67 | epoch: 43, train loss: 9.24168586730957, test loss: 6.781149387359619\n",
      "h: 67 | epoch: 44, train loss: 9.181205749511719, test loss: 6.7088799476623535\n",
      "h: 67 | epoch: 45, train loss: 9.125974655151367, test loss: 6.641867637634277\n",
      "h: 67 | epoch: 46, train loss: 9.075540542602539, test loss: 6.579709529876709\n",
      "h: 67 | epoch: 47, train loss: 9.02949333190918, test loss: 6.52203369140625\n",
      "h: 67 | epoch: 48, train loss: 8.98745346069336, test loss: 6.4684929847717285\n",
      "h: 67 | epoch: 49, train loss: 8.949075698852539, test loss: 6.418774604797363\n",
      "h: 67 | epoch: 50, train loss: 8.914045333862305, test loss: 6.372588157653809\n",
      "h: 67 | epoch: 51, train loss: 8.882070541381836, test loss: 6.3296637535095215\n",
      "h: 67 | epoch: 52, train loss: 8.852887153625488, test loss: 6.28975248336792\n",
      "h: 67 | epoch: 53, train loss: 8.826253890991211, test loss: 6.25262975692749\n",
      "h: 67 | epoch: 54, train loss: 8.801950454711914, test loss: 6.218084812164307\n",
      "h: 67 | epoch: 55, train loss: 8.77977180480957, test loss: 6.1859235763549805\n",
      "h: 67 | epoch: 56, train loss: 8.759536743164062, test loss: 6.155969142913818\n",
      "h: 67 | epoch: 57, train loss: 8.741073608398438, test loss: 6.128056526184082\n",
      "h: 67 | epoch: 58, train loss: 8.724228858947754, test loss: 6.102034568786621\n",
      "h: 67 | epoch: 59, train loss: 8.708861351013184, test loss: 6.07776403427124\n",
      "h: 67 | epoch: 60, train loss: 8.694841384887695, test loss: 6.055117130279541\n",
      "h: 67 | epoch: 61, train loss: 8.682053565979004, test loss: 6.033973217010498\n",
      "h: 67 | epoch: 62, train loss: 8.670387268066406, test loss: 6.014224529266357\n",
      "h: 67 | epoch: 63, train loss: 8.659746170043945, test loss: 5.995769500732422\n",
      "h: 67 | epoch: 64, train loss: 8.650041580200195, test loss: 5.978515625\n",
      "h: 67 | epoch: 65, train loss: 8.641188621520996, test loss: 5.962375640869141\n",
      "h: 67 | epoch: 66, train loss: 8.6331148147583, test loss: 5.947271347045898\n",
      "h: 67 | epoch: 67, train loss: 8.625750541687012, test loss: 5.933128833770752\n",
      "h: 67 | epoch: 68, train loss: 8.619035720825195, test loss: 5.919882297515869\n",
      "h: 67 | epoch: 69, train loss: 8.612911224365234, test loss: 5.90746545791626\n",
      "h: 67 | epoch: 70, train loss: 8.607325553894043, test loss: 5.895822525024414\n",
      "h: 67 | epoch: 71, train loss: 8.602232933044434, test loss: 5.884900093078613\n",
      "h: 67 | epoch: 72, train loss: 8.597587585449219, test loss: 5.874650001525879\n",
      "h: 67 | epoch: 73, train loss: 8.593352317810059, test loss: 5.865024566650391\n",
      "h: 67 | epoch: 74, train loss: 8.58949089050293, test loss: 5.855982303619385\n",
      "h: 67 | epoch: 75, train loss: 8.585968971252441, test loss: 5.847481727600098\n",
      "h: 67 | epoch: 76, train loss: 8.582756996154785, test loss: 5.839488983154297\n",
      "h: 67 | epoch: 77, train loss: 8.579829216003418, test loss: 5.83197021484375\n",
      "h: 67 | epoch: 78, train loss: 8.57715892791748, test loss: 5.824893951416016\n",
      "h: 67 | epoch: 79, train loss: 8.574724197387695, test loss: 5.818230152130127\n",
      "h: 67 | epoch: 80, train loss: 8.572504043579102, test loss: 5.811954021453857\n",
      "h: 67 | epoch: 81, train loss: 8.570480346679688, test loss: 5.806038856506348\n",
      "h: 67 | epoch: 82, train loss: 8.568635940551758, test loss: 5.8004608154296875\n",
      "h: 67 | epoch: 83, train loss: 8.5669527053833, test loss: 5.795200824737549\n",
      "h: 67 | epoch: 84, train loss: 8.565418243408203, test loss: 5.790236949920654\n",
      "h: 67 | epoch: 85, train loss: 8.564019203186035, test loss: 5.78555154800415\n",
      "h: 67 | epoch: 86, train loss: 8.562744140625, test loss: 5.781126022338867\n",
      "h: 67 | epoch: 87, train loss: 8.5615816116333, test loss: 5.776947498321533\n",
      "h: 67 | epoch: 88, train loss: 8.560522079467773, test loss: 5.772996425628662\n",
      "h: 67 | epoch: 89, train loss: 8.559554100036621, test loss: 5.769261837005615\n",
      "h: 67 | epoch: 90, train loss: 8.558672904968262, test loss: 5.765730857849121\n",
      "h: 67 | epoch: 91, train loss: 8.557869911193848, test loss: 5.762389183044434\n",
      "h: 67 | epoch: 92, train loss: 8.557137489318848, test loss: 5.759227752685547\n",
      "h: 67 | epoch: 93, train loss: 8.556469917297363, test loss: 5.756234645843506\n",
      "h: 67 | epoch: 94, train loss: 8.55586051940918, test loss: 5.753399848937988\n",
      "h: 67 | epoch: 95, train loss: 8.555305480957031, test loss: 5.7507147789001465\n",
      "h: 67 | epoch: 96, train loss: 8.55479907989502, test loss: 5.748169898986816\n",
      "h: 67 | epoch: 97, train loss: 8.554338455200195, test loss: 5.745759010314941\n",
      "h: 67 | epoch: 98, train loss: 8.553916931152344, test loss: 5.743472099304199\n",
      "h: 67 | epoch: 99, train loss: 8.553533554077148, test loss: 5.741303443908691\n",
      "h: 68 | epoch: 0, train loss: 48.872337341308594, test loss: 40.65095138549805\n",
      "h: 68 | epoch: 1, train loss: 45.265037536621094, test loss: 37.63771057128906\n",
      "h: 68 | epoch: 2, train loss: 41.98386001586914, test loss: 34.890159606933594\n",
      "h: 68 | epoch: 3, train loss: 38.997283935546875, test loss: 32.383262634277344\n",
      "h: 68 | epoch: 4, train loss: 36.277366638183594, test loss: 30.094722747802734\n",
      "h: 68 | epoch: 5, train loss: 33.79924392700195, test loss: 28.004638671875\n",
      "h: 68 | epoch: 6, train loss: 31.540691375732422, test loss: 26.095144271850586\n",
      "h: 68 | epoch: 7, train loss: 29.481775283813477, test loss: 24.350168228149414\n",
      "h: 68 | epoch: 8, train loss: 27.60457420349121, test loss: 22.755216598510742\n",
      "h: 68 | epoch: 9, train loss: 25.892913818359375, test loss: 21.297182083129883\n",
      "h: 68 | epoch: 10, train loss: 24.332172393798828, test loss: 19.96416664123535\n",
      "h: 68 | epoch: 11, train loss: 22.909090042114258, test loss: 18.745389938354492\n",
      "h: 68 | epoch: 12, train loss: 21.611637115478516, test loss: 17.631032943725586\n",
      "h: 68 | epoch: 13, train loss: 20.42886734008789, test loss: 16.612144470214844\n",
      "h: 68 | epoch: 14, train loss: 19.350814819335938, test loss: 15.680569648742676\n",
      "h: 68 | epoch: 15, train loss: 18.368383407592773, test loss: 14.828857421875\n",
      "h: 68 | epoch: 16, train loss: 17.47327995300293, test loss: 14.050201416015625\n",
      "h: 68 | epoch: 17, train loss: 16.657926559448242, test loss: 13.338373184204102\n",
      "h: 68 | epoch: 18, train loss: 15.915397644042969, test loss: 12.687681198120117\n",
      "h: 68 | epoch: 19, train loss: 15.239355087280273, test loss: 12.09290885925293\n",
      "h: 68 | epoch: 20, train loss: 14.62401294708252, test loss: 11.549287796020508\n",
      "h: 68 | epoch: 21, train loss: 14.064062118530273, test loss: 11.052446365356445\n",
      "h: 68 | epoch: 22, train loss: 13.554662704467773, test loss: 10.598381042480469\n",
      "h: 68 | epoch: 23, train loss: 13.0913724899292, test loss: 10.183427810668945\n",
      "h: 68 | epoch: 24, train loss: 12.670135498046875, test loss: 9.804228782653809\n",
      "h: 68 | epoch: 25, train loss: 12.28723430633545, test loss: 9.457712173461914\n",
      "h: 68 | epoch: 26, train loss: 11.939275741577148, test loss: 9.14106273651123\n",
      "h: 68 | epoch: 27, train loss: 11.623151779174805, test loss: 8.851704597473145\n",
      "h: 68 | epoch: 28, train loss: 11.336024284362793, test loss: 8.58728313446045\n",
      "h: 68 | epoch: 29, train loss: 11.075301170349121, test loss: 8.3456392288208\n",
      "h: 68 | epoch: 30, train loss: 10.83860969543457, test loss: 8.124799728393555\n",
      "h: 68 | epoch: 31, train loss: 10.623785972595215, test loss: 7.922957420349121\n",
      "h: 68 | epoch: 32, train loss: 10.428855895996094, test loss: 7.738465785980225\n",
      "h: 68 | epoch: 33, train loss: 10.252013206481934, test loss: 7.569814205169678\n",
      "h: 68 | epoch: 34, train loss: 10.091617584228516, test loss: 7.415627479553223\n",
      "h: 68 | epoch: 35, train loss: 9.946165084838867, test loss: 7.2746429443359375\n",
      "h: 68 | epoch: 36, train loss: 9.814292907714844, test loss: 7.1457109451293945\n",
      "h: 68 | epoch: 37, train loss: 9.69474983215332, test loss: 7.027780055999756\n",
      "h: 68 | epoch: 38, train loss: 9.586407661437988, test loss: 6.919893741607666\n",
      "h: 68 | epoch: 39, train loss: 9.488229751586914, test loss: 6.821173667907715\n",
      "h: 68 | epoch: 40, train loss: 9.39927864074707, test loss: 6.7308220863342285\n",
      "h: 68 | epoch: 41, train loss: 9.318699836730957, test loss: 6.648110866546631\n",
      "h: 68 | epoch: 42, train loss: 9.245713233947754, test loss: 6.5723724365234375\n",
      "h: 68 | epoch: 43, train loss: 9.179612159729004, test loss: 6.5030012130737305\n",
      "h: 68 | epoch: 44, train loss: 9.119756698608398, test loss: 6.439444541931152\n",
      "h: 68 | epoch: 45, train loss: 9.065561294555664, test loss: 6.381196022033691\n",
      "h: 68 | epoch: 46, train loss: 9.016495704650879, test loss: 6.327796936035156\n",
      "h: 68 | epoch: 47, train loss: 8.97208023071289, test loss: 6.278826713562012\n",
      "h: 68 | epoch: 48, train loss: 8.931877136230469, test loss: 6.2339019775390625\n",
      "h: 68 | epoch: 49, train loss: 8.895490646362305, test loss: 6.19267463684082\n",
      "h: 68 | epoch: 50, train loss: 8.862558364868164, test loss: 6.154826641082764\n",
      "h: 68 | epoch: 51, train loss: 8.832756996154785, test loss: 6.1200666427612305\n",
      "h: 68 | epoch: 52, train loss: 8.805790901184082, test loss: 6.088131904602051\n",
      "h: 68 | epoch: 53, train loss: 8.781389236450195, test loss: 6.058778762817383\n",
      "h: 68 | epoch: 54, train loss: 8.75931167602539, test loss: 6.031790733337402\n",
      "h: 68 | epoch: 55, train loss: 8.739336967468262, test loss: 6.006964206695557\n",
      "h: 68 | epoch: 56, train loss: 8.72126579284668, test loss: 5.984117031097412\n",
      "h: 68 | epoch: 57, train loss: 8.704915046691895, test loss: 5.963082313537598\n",
      "h: 68 | epoch: 58, train loss: 8.690123558044434, test loss: 5.943707466125488\n",
      "h: 68 | epoch: 59, train loss: 8.676742553710938, test loss: 5.925854206085205\n",
      "h: 68 | epoch: 60, train loss: 8.66463851928711, test loss: 5.90939474105835\n",
      "h: 68 | epoch: 61, train loss: 8.653688430786133, test loss: 5.894215106964111\n",
      "h: 68 | epoch: 62, train loss: 8.643781661987305, test loss: 5.8802080154418945\n",
      "h: 68 | epoch: 63, train loss: 8.634820938110352, test loss: 5.867275238037109\n",
      "h: 68 | epoch: 64, train loss: 8.626714706420898, test loss: 5.855331897735596\n",
      "h: 68 | epoch: 65, train loss: 8.61938190460205, test loss: 5.844295978546143\n",
      "h: 68 | epoch: 66, train loss: 8.612748146057129, test loss: 5.834092617034912\n",
      "h: 68 | epoch: 67, train loss: 8.606746673583984, test loss: 5.824656009674072\n",
      "h: 68 | epoch: 68, train loss: 8.601316452026367, test loss: 5.815924644470215\n",
      "h: 68 | epoch: 69, train loss: 8.596405982971191, test loss: 5.8078413009643555\n",
      "h: 68 | epoch: 70, train loss: 8.591962814331055, test loss: 5.800354957580566\n",
      "h: 68 | epoch: 71, train loss: 8.587942123413086, test loss: 5.793417930603027\n",
      "h: 68 | epoch: 72, train loss: 8.584304809570312, test loss: 5.786986827850342\n",
      "h: 68 | epoch: 73, train loss: 8.581014633178711, test loss: 5.781022071838379\n",
      "h: 68 | epoch: 74, train loss: 8.578036308288574, test loss: 5.775488376617432\n",
      "h: 68 | epoch: 75, train loss: 8.575342178344727, test loss: 5.770350456237793\n",
      "h: 68 | epoch: 76, train loss: 8.572904586791992, test loss: 5.7655792236328125\n",
      "h: 68 | epoch: 77, train loss: 8.570697784423828, test loss: 5.76114559173584\n",
      "h: 68 | epoch: 78, train loss: 8.56870174407959, test loss: 5.757025718688965\n",
      "h: 68 | epoch: 79, train loss: 8.566896438598633, test loss: 5.753194332122803\n",
      "h: 68 | epoch: 80, train loss: 8.56525993347168, test loss: 5.749630928039551\n",
      "h: 68 | epoch: 81, train loss: 8.563779830932617, test loss: 5.74631404876709\n",
      "h: 68 | epoch: 82, train loss: 8.562440872192383, test loss: 5.74322509765625\n",
      "h: 68 | epoch: 83, train loss: 8.561227798461914, test loss: 5.740349769592285\n",
      "h: 68 | epoch: 84, train loss: 8.56013011932373, test loss: 5.737669944763184\n",
      "h: 68 | epoch: 85, train loss: 8.559137344360352, test loss: 5.735171794891357\n",
      "h: 68 | epoch: 86, train loss: 8.558237075805664, test loss: 5.732843399047852\n",
      "h: 68 | epoch: 87, train loss: 8.55742359161377, test loss: 5.73067045211792\n",
      "h: 68 | epoch: 88, train loss: 8.556685447692871, test loss: 5.728643894195557\n",
      "h: 68 | epoch: 89, train loss: 8.55601692199707, test loss: 5.726751804351807\n",
      "h: 68 | epoch: 90, train loss: 8.555412292480469, test loss: 5.724984169006348\n",
      "h: 68 | epoch: 91, train loss: 8.554864883422852, test loss: 5.723334312438965\n",
      "h: 68 | epoch: 92, train loss: 8.554369926452637, test loss: 5.721793174743652\n",
      "h: 68 | epoch: 93, train loss: 8.553919792175293, test loss: 5.720353126525879\n",
      "h: 68 | epoch: 94, train loss: 8.55351448059082, test loss: 5.7190046310424805\n",
      "h: 68 | epoch: 95, train loss: 8.553144454956055, test loss: 5.717744827270508\n",
      "h: 68 | epoch: 96, train loss: 8.552810668945312, test loss: 5.716567039489746\n",
      "h: 68 | epoch: 97, train loss: 8.552508354187012, test loss: 5.715464115142822\n",
      "h: 68 | epoch: 98, train loss: 8.552234649658203, test loss: 5.714432716369629\n",
      "h: 68 | epoch: 99, train loss: 8.551986694335938, test loss: 5.713466644287109\n",
      "h: 69 | epoch: 0, train loss: 49.31233596801758, test loss: 42.162757873535156\n",
      "h: 69 | epoch: 1, train loss: 45.54175567626953, test loss: 38.89187240600586\n",
      "h: 69 | epoch: 2, train loss: 42.12861633300781, test loss: 35.92597198486328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 69 | epoch: 3, train loss: 39.03645706176758, test loss: 33.23440170288086\n",
      "h: 69 | epoch: 4, train loss: 36.23310852050781, test loss: 30.790081024169922\n",
      "h: 69 | epoch: 5, train loss: 33.69013214111328, test loss: 28.568960189819336\n",
      "h: 69 | epoch: 6, train loss: 31.382251739501953, test loss: 26.54961585998535\n",
      "h: 69 | epoch: 7, train loss: 29.286945343017578, test loss: 24.712913513183594\n",
      "h: 69 | epoch: 8, train loss: 27.384069442749023, test loss: 23.041709899902344\n",
      "h: 69 | epoch: 9, train loss: 25.655569076538086, test loss: 21.52060317993164\n",
      "h: 69 | epoch: 10, train loss: 24.085205078125, test loss: 20.135738372802734\n",
      "h: 69 | epoch: 11, train loss: 22.65835189819336, test loss: 18.874614715576172\n",
      "h: 69 | epoch: 12, train loss: 21.3618221282959, test loss: 17.725948333740234\n",
      "h: 69 | epoch: 13, train loss: 20.183679580688477, test loss: 16.679521560668945\n",
      "h: 69 | epoch: 14, train loss: 19.113117218017578, test loss: 15.72608757019043\n",
      "h: 69 | epoch: 15, train loss: 18.140350341796875, test loss: 14.857259750366211\n",
      "h: 69 | epoch: 16, train loss: 17.256492614746094, test loss: 14.065427780151367\n",
      "h: 69 | epoch: 17, train loss: 16.453479766845703, test loss: 13.343676567077637\n",
      "h: 69 | epoch: 18, train loss: 15.723978042602539, test loss: 12.685731887817383\n",
      "h: 69 | epoch: 19, train loss: 15.061327934265137, test loss: 12.085882186889648\n",
      "h: 69 | epoch: 20, train loss: 14.459465026855469, test loss: 11.538928985595703\n",
      "h: 69 | epoch: 21, train loss: 13.912874221801758, test loss: 11.04014778137207\n",
      "h: 69 | epoch: 22, train loss: 13.416537284851074, test loss: 10.585238456726074\n",
      "h: 69 | epoch: 23, train loss: 12.965883255004883, test loss: 10.170286178588867\n",
      "h: 69 | epoch: 24, train loss: 12.556756973266602, test loss: 9.791727066040039\n",
      "h: 69 | epoch: 25, train loss: 12.185373306274414, test loss: 9.446316719055176\n",
      "h: 69 | epoch: 26, train loss: 11.848283767700195, test loss: 9.131099700927734\n",
      "h: 69 | epoch: 27, train loss: 11.542356491088867, test loss: 8.843385696411133\n",
      "h: 69 | epoch: 28, train loss: 11.264738082885742, test loss: 8.580728530883789\n",
      "h: 69 | epoch: 29, train loss: 11.012829780578613, test loss: 8.340895652770996\n",
      "h: 69 | epoch: 30, train loss: 10.784269332885742, test loss: 8.121856689453125\n",
      "h: 69 | epoch: 31, train loss: 10.576913833618164, test loss: 7.9217658042907715\n",
      "h: 69 | epoch: 32, train loss: 10.38880443572998, test loss: 7.7389397621154785\n",
      "h: 69 | epoch: 33, train loss: 10.218168258666992, test loss: 7.571844577789307\n",
      "h: 69 | epoch: 34, train loss: 10.063386917114258, test loss: 7.419088840484619\n",
      "h: 69 | epoch: 35, train loss: 9.922998428344727, test loss: 7.2794036865234375\n",
      "h: 69 | epoch: 36, train loss: 9.795666694641113, test loss: 7.151630401611328\n",
      "h: 69 | epoch: 37, train loss: 9.680180549621582, test loss: 7.034721374511719\n",
      "h: 69 | epoch: 38, train loss: 9.575440406799316, test loss: 6.927718162536621\n",
      "h: 69 | epoch: 39, train loss: 9.480447769165039, test loss: 6.829750061035156\n",
      "h: 69 | epoch: 40, train loss: 9.394296646118164, test loss: 6.740023136138916\n",
      "h: 69 | epoch: 41, train loss: 9.316163063049316, test loss: 6.657816410064697\n",
      "h: 69 | epoch: 42, train loss: 9.245301246643066, test loss: 6.582472801208496\n",
      "h: 69 | epoch: 43, train loss: 9.181031227111816, test loss: 6.513394355773926\n",
      "h: 69 | epoch: 44, train loss: 9.122739791870117, test loss: 6.45003604888916\n",
      "h: 69 | epoch: 45, train loss: 9.069869041442871, test loss: 6.391903877258301\n",
      "h: 69 | epoch: 46, train loss: 9.0219144821167, test loss: 6.338542461395264\n",
      "h: 69 | epoch: 47, train loss: 8.978414535522461, test loss: 6.289543628692627\n",
      "h: 69 | epoch: 48, train loss: 8.938955307006836, test loss: 6.244531154632568\n",
      "h: 69 | epoch: 49, train loss: 8.903158187866211, test loss: 6.2031660079956055\n",
      "h: 69 | epoch: 50, train loss: 8.870680809020996, test loss: 6.165134906768799\n",
      "h: 69 | epoch: 51, train loss: 8.841216087341309, test loss: 6.130154609680176\n",
      "h: 69 | epoch: 52, train loss: 8.81447982788086, test loss: 6.097970008850098\n",
      "h: 69 | epoch: 53, train loss: 8.790220260620117, test loss: 6.068339824676514\n",
      "h: 69 | epoch: 54, train loss: 8.768203735351562, test loss: 6.041054725646973\n",
      "h: 69 | epoch: 55, train loss: 8.748223304748535, test loss: 6.015914440155029\n",
      "h: 69 | epoch: 56, train loss: 8.730086326599121, test loss: 5.992743015289307\n",
      "h: 69 | epoch: 57, train loss: 8.713623046875, test loss: 5.971375942230225\n",
      "h: 69 | epoch: 58, train loss: 8.698677062988281, test loss: 5.951663970947266\n",
      "h: 69 | epoch: 59, train loss: 8.685108184814453, test loss: 5.933471202850342\n",
      "h: 69 | epoch: 60, train loss: 8.6727876663208, test loss: 5.916675090789795\n",
      "h: 69 | epoch: 61, train loss: 8.661599159240723, test loss: 5.901158332824707\n",
      "h: 69 | epoch: 62, train loss: 8.651437759399414, test loss: 5.886821746826172\n",
      "h: 69 | epoch: 63, train loss: 8.64220905303955, test loss: 5.873566627502441\n",
      "h: 69 | epoch: 64, train loss: 8.633825302124023, test loss: 5.861306190490723\n",
      "h: 69 | epoch: 65, train loss: 8.626208305358887, test loss: 5.849963188171387\n",
      "h: 69 | epoch: 66, train loss: 8.619288444519043, test loss: 5.839461326599121\n",
      "h: 69 | epoch: 67, train loss: 8.61299991607666, test loss: 5.829737186431885\n",
      "h: 69 | epoch: 68, train loss: 8.60728645324707, test loss: 5.820727348327637\n",
      "h: 69 | epoch: 69, train loss: 8.602090835571289, test loss: 5.8123779296875\n",
      "h: 69 | epoch: 70, train loss: 8.597370147705078, test loss: 5.804636478424072\n",
      "h: 69 | epoch: 71, train loss: 8.59307861328125, test loss: 5.797455787658691\n",
      "h: 69 | epoch: 72, train loss: 8.589177131652832, test loss: 5.790791988372803\n",
      "h: 69 | epoch: 73, train loss: 8.585627555847168, test loss: 5.784605979919434\n",
      "h: 69 | epoch: 74, train loss: 8.58240032196045, test loss: 5.7788615226745605\n",
      "h: 69 | epoch: 75, train loss: 8.579465866088867, test loss: 5.773524761199951\n",
      "h: 69 | epoch: 76, train loss: 8.57679557800293, test loss: 5.7685651779174805\n",
      "h: 69 | epoch: 77, train loss: 8.57436752319336, test loss: 5.763955116271973\n",
      "h: 69 | epoch: 78, train loss: 8.57215690612793, test loss: 5.7596659660339355\n",
      "h: 69 | epoch: 79, train loss: 8.570147514343262, test loss: 5.755677223205566\n",
      "h: 69 | epoch: 80, train loss: 8.568316459655762, test loss: 5.751963138580322\n",
      "h: 69 | epoch: 81, train loss: 8.566651344299316, test loss: 5.748505592346191\n",
      "h: 69 | epoch: 82, train loss: 8.565135955810547, test loss: 5.745286464691162\n",
      "h: 69 | epoch: 83, train loss: 8.563755989074707, test loss: 5.742286682128906\n",
      "h: 69 | epoch: 84, train loss: 8.562499046325684, test loss: 5.739490985870361\n",
      "h: 69 | epoch: 85, train loss: 8.561355590820312, test loss: 5.736885070800781\n",
      "h: 69 | epoch: 86, train loss: 8.56031322479248, test loss: 5.734454154968262\n",
      "h: 69 | epoch: 87, train loss: 8.559364318847656, test loss: 5.732186317443848\n",
      "h: 69 | epoch: 88, train loss: 8.558500289916992, test loss: 5.7300705909729\n",
      "h: 69 | epoch: 89, train loss: 8.557713508605957, test loss: 5.728095054626465\n",
      "h: 69 | epoch: 90, train loss: 8.556995391845703, test loss: 5.726251602172852\n",
      "h: 69 | epoch: 91, train loss: 8.556342124938965, test loss: 5.72452974319458\n",
      "h: 69 | epoch: 92, train loss: 8.555747985839844, test loss: 5.7229204177856445\n",
      "h: 69 | epoch: 93, train loss: 8.555204391479492, test loss: 5.72141695022583\n",
      "h: 69 | epoch: 94, train loss: 8.554710388183594, test loss: 5.720012187957764\n",
      "h: 69 | epoch: 95, train loss: 8.554259300231934, test loss: 5.718697547912598\n",
      "h: 69 | epoch: 96, train loss: 8.553849220275879, test loss: 5.717469215393066\n",
      "h: 69 | epoch: 97, train loss: 8.553474426269531, test loss: 5.7163190841674805\n",
      "h: 69 | epoch: 98, train loss: 8.553133010864258, test loss: 5.715243339538574\n",
      "h: 69 | epoch: 99, train loss: 8.55282211303711, test loss: 5.714236259460449\n",
      "h: 70 | epoch: 0, train loss: 48.3360595703125, test loss: 40.42703628540039\n",
      "h: 70 | epoch: 1, train loss: 44.29921340942383, test loss: 37.08729934692383\n",
      "h: 70 | epoch: 2, train loss: 40.67408752441406, test loss: 34.076473236083984\n",
      "h: 70 | epoch: 3, train loss: 37.41661834716797, test loss: 31.360427856445312\n",
      "h: 70 | epoch: 4, train loss: 34.48810577392578, test loss: 28.90903091430664\n",
      "h: 70 | epoch: 5, train loss: 31.85439682006836, test loss: 26.69557762145996\n",
      "h: 70 | epoch: 6, train loss: 29.485254287719727, test loss: 24.696338653564453\n",
      "h: 70 | epoch: 7, train loss: 27.35379409790039, test loss: 22.890117645263672\n",
      "h: 70 | epoch: 8, train loss: 25.43606185913086, test loss: 21.257976531982422\n",
      "h: 70 | epoch: 9, train loss: 23.71064567565918, test loss: 19.782939910888672\n",
      "h: 70 | epoch: 10, train loss: 22.158382415771484, test loss: 18.44974136352539\n",
      "h: 70 | epoch: 11, train loss: 20.762069702148438, test loss: 17.244670867919922\n",
      "h: 70 | epoch: 12, train loss: 19.50625228881836, test loss: 16.155363082885742\n",
      "h: 70 | epoch: 13, train loss: 18.377025604248047, test loss: 15.170679092407227\n",
      "h: 70 | epoch: 14, train loss: 17.361865997314453, test loss: 14.280537605285645\n",
      "h: 70 | epoch: 15, train loss: 16.44948959350586, test loss: 13.475858688354492\n",
      "h: 70 | epoch: 16, train loss: 15.6297025680542, test loss: 12.748424530029297\n",
      "h: 70 | epoch: 17, train loss: 14.893335342407227, test loss: 12.090806007385254\n",
      "h: 70 | epoch: 18, train loss: 14.232083320617676, test loss: 11.496286392211914\n",
      "h: 70 | epoch: 19, train loss: 13.638470649719238, test loss: 10.958785057067871\n",
      "h: 70 | epoch: 20, train loss: 13.105745315551758, test loss: 10.472805976867676\n",
      "h: 70 | epoch: 21, train loss: 12.627799987792969, test loss: 10.033380508422852\n",
      "h: 70 | epoch: 22, train loss: 12.199139595031738, test loss: 9.636008262634277\n",
      "h: 70 | epoch: 23, train loss: 11.81479549407959, test loss: 9.276622772216797\n",
      "h: 70 | epoch: 24, train loss: 11.470291137695312, test loss: 8.951544761657715\n",
      "h: 70 | epoch: 25, train loss: 11.161584854125977, test loss: 8.657450675964355\n",
      "h: 70 | epoch: 26, train loss: 10.885037422180176, test loss: 8.391336441040039\n",
      "h: 70 | epoch: 27, train loss: 10.637365341186523, test loss: 8.150487899780273\n",
      "h: 70 | epoch: 28, train loss: 10.415616989135742, test loss: 7.9324493408203125\n",
      "h: 70 | epoch: 29, train loss: 10.217126846313477, test loss: 7.735007286071777\n",
      "h: 70 | epoch: 30, train loss: 10.039502143859863, test loss: 7.556160926818848\n",
      "h: 70 | epoch: 31, train loss: 9.880586624145508, test loss: 7.394103050231934\n",
      "h: 70 | epoch: 32, train loss: 9.738441467285156, test loss: 7.247203826904297\n",
      "h: 70 | epoch: 33, train loss: 9.611327171325684, test loss: 7.113994598388672\n",
      "h: 70 | epoch: 34, train loss: 9.497678756713867, test loss: 6.993147373199463\n",
      "h: 70 | epoch: 35, train loss: 9.396088600158691, test loss: 6.8834638595581055\n",
      "h: 70 | epoch: 36, train loss: 9.305294036865234, test loss: 6.783867835998535\n",
      "h: 70 | epoch: 37, train loss: 9.224164962768555, test loss: 6.693382263183594\n",
      "h: 70 | epoch: 38, train loss: 9.151681900024414, test loss: 6.611130714416504\n",
      "h: 70 | epoch: 39, train loss: 9.086935043334961, test loss: 6.53632116317749\n",
      "h: 70 | epoch: 40, train loss: 9.029109001159668, test loss: 6.468240261077881\n",
      "h: 70 | epoch: 41, train loss: 8.977468490600586, test loss: 6.406246185302734\n",
      "h: 70 | epoch: 42, train loss: 8.93136215209961, test loss: 6.349757194519043\n",
      "h: 70 | epoch: 43, train loss: 8.890199661254883, test loss: 6.298251152038574\n",
      "h: 70 | epoch: 44, train loss: 8.853453636169434, test loss: 6.251256465911865\n",
      "h: 70 | epoch: 45, train loss: 8.820655822753906, test loss: 6.208347797393799\n",
      "h: 70 | epoch: 46, train loss: 8.791385650634766, test loss: 6.169142723083496\n",
      "h: 70 | epoch: 47, train loss: 8.765265464782715, test loss: 6.133293628692627\n",
      "h: 70 | epoch: 48, train loss: 8.741959571838379, test loss: 6.100489616394043\n",
      "h: 70 | epoch: 49, train loss: 8.721162796020508, test loss: 6.070449352264404\n",
      "h: 70 | epoch: 50, train loss: 8.70261001586914, test loss: 6.042917728424072\n",
      "h: 70 | epoch: 51, train loss: 8.68605899810791, test loss: 6.017665386199951\n",
      "h: 70 | epoch: 52, train loss: 8.671294212341309, test loss: 5.99448823928833\n",
      "h: 70 | epoch: 53, train loss: 8.658124923706055, test loss: 5.9731950759887695\n",
      "h: 70 | epoch: 54, train loss: 8.646378517150879, test loss: 5.953618049621582\n",
      "h: 70 | epoch: 55, train loss: 8.635900497436523, test loss: 5.935605049133301\n",
      "h: 70 | epoch: 56, train loss: 8.626558303833008, test loss: 5.9190168380737305\n",
      "h: 70 | epoch: 57, train loss: 8.618224143981934, test loss: 5.903728485107422\n",
      "h: 70 | epoch: 58, train loss: 8.610793113708496, test loss: 5.8896284103393555\n",
      "h: 70 | epoch: 59, train loss: 8.604166030883789, test loss: 5.876611709594727\n",
      "h: 70 | epoch: 60, train loss: 8.59825611114502, test loss: 5.86458683013916\n",
      "h: 70 | epoch: 61, train loss: 8.592985153198242, test loss: 5.853469371795654\n",
      "h: 70 | epoch: 62, train loss: 8.588286399841309, test loss: 5.843182563781738\n",
      "h: 70 | epoch: 63, train loss: 8.584096908569336, test loss: 5.83365535736084\n",
      "h: 70 | epoch: 64, train loss: 8.580358505249023, test loss: 5.8248291015625\n",
      "h: 70 | epoch: 65, train loss: 8.577027320861816, test loss: 5.816643714904785\n",
      "h: 70 | epoch: 66, train loss: 8.574056625366211, test loss: 5.809044361114502\n",
      "h: 70 | epoch: 67, train loss: 8.571407318115234, test loss: 5.801988124847412\n",
      "h: 70 | epoch: 68, train loss: 8.569045066833496, test loss: 5.795429229736328\n",
      "h: 70 | epoch: 69, train loss: 8.566938400268555, test loss: 5.789329528808594\n",
      "h: 70 | epoch: 70, train loss: 8.565059661865234, test loss: 5.7836527824401855\n",
      "h: 70 | epoch: 71, train loss: 8.563385963439941, test loss: 5.778365612030029\n",
      "h: 70 | epoch: 72, train loss: 8.561891555786133, test loss: 5.773439407348633\n",
      "h: 70 | epoch: 73, train loss: 8.56056022644043, test loss: 5.768845558166504\n",
      "h: 70 | epoch: 74, train loss: 8.559372901916504, test loss: 5.764559268951416\n",
      "h: 70 | epoch: 75, train loss: 8.558314323425293, test loss: 5.760557174682617\n",
      "h: 70 | epoch: 76, train loss: 8.557369232177734, test loss: 5.756819725036621\n",
      "h: 70 | epoch: 77, train loss: 8.556528091430664, test loss: 5.753325462341309\n",
      "h: 70 | epoch: 78, train loss: 8.555776596069336, test loss: 5.750058174133301\n",
      "h: 70 | epoch: 79, train loss: 8.555108070373535, test loss: 5.747002601623535\n",
      "h: 70 | epoch: 80, train loss: 8.554510116577148, test loss: 5.744141578674316\n",
      "h: 70 | epoch: 81, train loss: 8.553977012634277, test loss: 5.741461277008057\n",
      "h: 70 | epoch: 82, train loss: 8.553503036499023, test loss: 5.738951206207275\n",
      "h: 70 | epoch: 83, train loss: 8.553077697753906, test loss: 5.736598014831543\n",
      "h: 70 | epoch: 84, train loss: 8.552700996398926, test loss: 5.734391212463379\n",
      "h: 70 | epoch: 85, train loss: 8.552363395690918, test loss: 5.732320785522461\n",
      "h: 70 | epoch: 86, train loss: 8.55206298828125, test loss: 5.730377197265625\n",
      "h: 70 | epoch: 87, train loss: 8.551794052124023, test loss: 5.7285542488098145\n",
      "h: 70 | epoch: 88, train loss: 8.551555633544922, test loss: 5.726840019226074\n",
      "h: 70 | epoch: 89, train loss: 8.551342010498047, test loss: 5.7252302169799805\n",
      "h: 70 | epoch: 90, train loss: 8.551151275634766, test loss: 5.72371768951416\n",
      "h: 70 | epoch: 91, train loss: 8.550981521606445, test loss: 5.72229528427124\n",
      "h: 70 | epoch: 92, train loss: 8.550829887390137, test loss: 5.720957279205322\n",
      "h: 70 | epoch: 93, train loss: 8.550694465637207, test loss: 5.719699859619141\n",
      "h: 70 | epoch: 94, train loss: 8.55057430267334, test loss: 5.7185163497924805\n",
      "h: 70 | epoch: 95, train loss: 8.550466537475586, test loss: 5.717401504516602\n",
      "h: 70 | epoch: 96, train loss: 8.550371170043945, test loss: 5.716353416442871\n",
      "h: 70 | epoch: 97, train loss: 8.550285339355469, test loss: 5.715364933013916\n",
      "h: 70 | epoch: 98, train loss: 8.55020809173584, test loss: 5.714436054229736\n",
      "h: 70 | epoch: 99, train loss: 8.550140380859375, test loss: 5.713558197021484\n",
      "h: 71 | epoch: 0, train loss: 42.861576080322266, test loss: 36.8123664855957\n",
      "h: 71 | epoch: 1, train loss: 39.4749870300293, test loss: 33.901329040527344\n",
      "h: 71 | epoch: 2, train loss: 36.42153549194336, test loss: 31.267353057861328\n",
      "h: 71 | epoch: 3, train loss: 33.66730880737305, test loss: 28.883005142211914\n",
      "h: 71 | epoch: 4, train loss: 31.1822509765625, test loss: 26.723840713500977\n",
      "h: 71 | epoch: 5, train loss: 28.93963623046875, test loss: 24.768062591552734\n",
      "h: 71 | epoch: 6, train loss: 26.915613174438477, test loss: 22.996158599853516\n",
      "h: 71 | epoch: 7, train loss: 25.088836669921875, test loss: 21.390588760375977\n",
      "h: 71 | epoch: 8, train loss: 23.440168380737305, test loss: 19.93560218811035\n",
      "h: 71 | epoch: 9, train loss: 21.952404022216797, test loss: 18.616987228393555\n",
      "h: 71 | epoch: 10, train loss: 20.61004066467285, test loss: 17.42193603515625\n",
      "h: 71 | epoch: 11, train loss: 19.399110794067383, test loss: 16.338848114013672\n",
      "h: 71 | epoch: 12, train loss: 18.306995391845703, test loss: 15.357236862182617\n",
      "h: 71 | epoch: 13, train loss: 17.322288513183594, test loss: 14.467620849609375\n",
      "h: 71 | epoch: 14, train loss: 16.434680938720703, test loss: 13.661384582519531\n",
      "h: 71 | epoch: 15, train loss: 15.634828567504883, test loss: 12.930730819702148\n",
      "h: 71 | epoch: 16, train loss: 14.914285659790039, test loss: 12.268589973449707\n",
      "h: 71 | epoch: 17, train loss: 14.265390396118164, test loss: 11.668536186218262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 71 | epoch: 18, train loss: 13.681215286254883, test loss: 11.124750137329102\n",
      "h: 71 | epoch: 19, train loss: 13.155477523803711, test loss: 10.63194465637207\n",
      "h: 71 | epoch: 20, train loss: 12.682488441467285, test loss: 10.185330390930176\n",
      "h: 71 | epoch: 21, train loss: 12.257098197937012, test loss: 9.78055191040039\n",
      "h: 71 | epoch: 22, train loss: 11.874639511108398, test loss: 9.41366958618164\n",
      "h: 71 | epoch: 23, train loss: 11.530895233154297, test loss: 9.08110237121582\n",
      "h: 71 | epoch: 24, train loss: 11.22204303741455, test loss: 8.7796049118042\n",
      "h: 71 | epoch: 25, train loss: 10.944628715515137, test loss: 8.50623607635498\n",
      "h: 71 | epoch: 26, train loss: 10.69552993774414, test loss: 8.258332252502441\n",
      "h: 71 | epoch: 27, train loss: 10.471921920776367, test loss: 8.033476829528809\n",
      "h: 71 | epoch: 28, train loss: 10.271254539489746, test loss: 7.829482078552246\n",
      "h: 71 | epoch: 29, train loss: 10.091227531433105, test loss: 7.64437198638916\n",
      "h: 71 | epoch: 30, train loss: 9.929758071899414, test loss: 7.476342678070068\n",
      "h: 71 | epoch: 31, train loss: 9.784975051879883, test loss: 7.323780059814453\n",
      "h: 71 | epoch: 32, train loss: 9.655183792114258, test loss: 7.185211181640625\n",
      "h: 71 | epoch: 33, train loss: 9.538864135742188, test loss: 7.059309959411621\n",
      "h: 71 | epoch: 34, train loss: 9.434638023376465, test loss: 6.944871425628662\n",
      "h: 71 | epoch: 35, train loss: 9.341273307800293, test loss: 6.840810298919678\n",
      "h: 71 | epoch: 36, train loss: 9.257654190063477, test loss: 6.746145725250244\n",
      "h: 71 | epoch: 37, train loss: 9.182779312133789, test loss: 6.6599860191345215\n",
      "h: 71 | epoch: 38, train loss: 9.115745544433594, test loss: 6.581530570983887\n",
      "h: 71 | epoch: 39, train loss: 9.055746078491211, test loss: 6.510054111480713\n",
      "h: 71 | epoch: 40, train loss: 9.002050399780273, test loss: 6.444897651672363\n",
      "h: 71 | epoch: 41, train loss: 8.95400619506836, test loss: 6.385472297668457\n",
      "h: 71 | epoch: 42, train loss: 8.91102409362793, test loss: 6.331240653991699\n",
      "h: 71 | epoch: 43, train loss: 8.872578620910645, test loss: 6.281718730926514\n",
      "h: 71 | epoch: 44, train loss: 8.838193893432617, test loss: 6.236469268798828\n",
      "h: 71 | epoch: 45, train loss: 8.807446479797363, test loss: 6.195096015930176\n",
      "h: 71 | epoch: 46, train loss: 8.77995491027832, test loss: 6.157243251800537\n",
      "h: 71 | epoch: 47, train loss: 8.755376815795898, test loss: 6.122589111328125\n",
      "h: 71 | epoch: 48, train loss: 8.733407974243164, test loss: 6.0908379554748535\n",
      "h: 71 | epoch: 49, train loss: 8.713772773742676, test loss: 6.061728477478027\n",
      "h: 71 | epoch: 50, train loss: 8.696224212646484, test loss: 6.035020351409912\n",
      "h: 71 | epoch: 51, train loss: 8.680543899536133, test loss: 6.010498523712158\n",
      "h: 71 | epoch: 52, train loss: 8.666531562805176, test loss: 5.987967491149902\n",
      "h: 71 | epoch: 53, train loss: 8.654014587402344, test loss: 5.967251300811768\n",
      "h: 71 | epoch: 54, train loss: 8.642831802368164, test loss: 5.948185920715332\n",
      "h: 71 | epoch: 55, train loss: 8.632841110229492, test loss: 5.930629730224609\n",
      "h: 71 | epoch: 56, train loss: 8.623918533325195, test loss: 5.914450168609619\n",
      "h: 71 | epoch: 57, train loss: 8.615949630737305, test loss: 5.8995280265808105\n",
      "h: 71 | epoch: 58, train loss: 8.608831405639648, test loss: 5.88575553894043\n",
      "h: 71 | epoch: 59, train loss: 8.602476119995117, test loss: 5.873034477233887\n",
      "h: 71 | epoch: 60, train loss: 8.596799850463867, test loss: 5.861275672912598\n",
      "h: 71 | epoch: 61, train loss: 8.591732025146484, test loss: 5.850396633148193\n",
      "h: 71 | epoch: 62, train loss: 8.587206840515137, test loss: 5.840327262878418\n",
      "h: 71 | epoch: 63, train loss: 8.583166122436523, test loss: 5.830998420715332\n",
      "h: 71 | epoch: 64, train loss: 8.579559326171875, test loss: 5.82235050201416\n",
      "h: 71 | epoch: 65, train loss: 8.576337814331055, test loss: 5.814328193664551\n",
      "h: 71 | epoch: 66, train loss: 8.573463439941406, test loss: 5.806879997253418\n",
      "h: 71 | epoch: 67, train loss: 8.57089614868164, test loss: 5.799960136413574\n",
      "h: 71 | epoch: 68, train loss: 8.568605422973633, test loss: 5.793528079986572\n",
      "h: 71 | epoch: 69, train loss: 8.566560745239258, test loss: 5.787545204162598\n",
      "h: 71 | epoch: 70, train loss: 8.56473445892334, test loss: 5.781975269317627\n",
      "h: 71 | epoch: 71, train loss: 8.563104629516602, test loss: 5.7767863273620605\n",
      "h: 71 | epoch: 72, train loss: 8.561650276184082, test loss: 5.771952152252197\n",
      "h: 71 | epoch: 73, train loss: 8.560352325439453, test loss: 5.7674431800842285\n",
      "h: 71 | epoch: 74, train loss: 8.55919361114502, test loss: 5.763236045837402\n",
      "h: 71 | epoch: 75, train loss: 8.558159828186035, test loss: 5.759307861328125\n",
      "h: 71 | epoch: 76, train loss: 8.557236671447754, test loss: 5.75563907623291\n",
      "h: 71 | epoch: 77, train loss: 8.556411743164062, test loss: 5.752209663391113\n",
      "h: 71 | epoch: 78, train loss: 8.55567741394043, test loss: 5.7490034103393555\n",
      "h: 71 | epoch: 79, train loss: 8.555021286010742, test loss: 5.7460036277771\n",
      "h: 71 | epoch: 80, train loss: 8.554435729980469, test loss: 5.743195056915283\n",
      "h: 71 | epoch: 81, train loss: 8.553912162780762, test loss: 5.740565776824951\n",
      "h: 71 | epoch: 82, train loss: 8.553445816040039, test loss: 5.738102912902832\n",
      "h: 71 | epoch: 83, train loss: 8.553030014038086, test loss: 5.735793590545654\n",
      "h: 71 | epoch: 84, train loss: 8.552658081054688, test loss: 5.733628273010254\n",
      "h: 71 | epoch: 85, train loss: 8.552325248718262, test loss: 5.731598377227783\n",
      "h: 71 | epoch: 86, train loss: 8.552030563354492, test loss: 5.7296929359436035\n",
      "h: 71 | epoch: 87, train loss: 8.551767349243164, test loss: 5.727903842926025\n",
      "h: 71 | epoch: 88, train loss: 8.551530838012695, test loss: 5.726223945617676\n",
      "h: 71 | epoch: 89, train loss: 8.55132007598877, test loss: 5.724646091461182\n",
      "h: 71 | epoch: 90, train loss: 8.551133155822754, test loss: 5.723162651062012\n",
      "h: 71 | epoch: 91, train loss: 8.550965309143066, test loss: 5.721768856048584\n",
      "h: 71 | epoch: 92, train loss: 8.550816535949707, test loss: 5.720459461212158\n",
      "h: 71 | epoch: 93, train loss: 8.550682067871094, test loss: 5.719226837158203\n",
      "h: 71 | epoch: 94, train loss: 8.550561904907227, test loss: 5.718067169189453\n",
      "h: 71 | epoch: 95, train loss: 8.550456047058105, test loss: 5.716976642608643\n",
      "h: 71 | epoch: 96, train loss: 8.550361633300781, test loss: 5.715948104858398\n",
      "h: 71 | epoch: 97, train loss: 8.550276756286621, test loss: 5.714981555938721\n",
      "h: 71 | epoch: 98, train loss: 8.550201416015625, test loss: 5.714071750640869\n",
      "h: 71 | epoch: 99, train loss: 8.55013370513916, test loss: 5.7132134437561035\n",
      "h: 72 | epoch: 0, train loss: 46.24445724487305, test loss: 39.265254974365234\n",
      "h: 72 | epoch: 1, train loss: 42.66454315185547, test loss: 36.212791442871094\n",
      "h: 72 | epoch: 2, train loss: 39.42476272583008, test loss: 33.442691802978516\n",
      "h: 72 | epoch: 3, train loss: 36.49126434326172, test loss: 30.927499771118164\n",
      "h: 72 | epoch: 4, train loss: 33.83403396606445, test loss: 28.642807006835938\n",
      "h: 72 | epoch: 5, train loss: 31.426380157470703, test loss: 26.566818237304688\n",
      "h: 72 | epoch: 6, train loss: 29.244461059570312, test loss: 24.680007934570312\n",
      "h: 72 | epoch: 7, train loss: 27.26692771911621, test loss: 22.964824676513672\n",
      "h: 72 | epoch: 8, train loss: 25.474605560302734, test loss: 21.405475616455078\n",
      "h: 72 | epoch: 9, train loss: 23.8502254486084, test loss: 19.98769760131836\n",
      "h: 72 | epoch: 10, train loss: 22.378204345703125, test loss: 18.69858741760254\n",
      "h: 72 | epoch: 11, train loss: 21.04445457458496, test loss: 17.526473999023438\n",
      "h: 72 | epoch: 12, train loss: 19.836217880249023, test loss: 16.46075439453125\n",
      "h: 72 | epoch: 13, train loss: 18.741933822631836, test loss: 15.491813659667969\n",
      "h: 72 | epoch: 14, train loss: 17.751108169555664, test loss: 14.6109037399292\n",
      "h: 72 | epoch: 15, train loss: 16.854204177856445, test loss: 13.810078620910645\n",
      "h: 72 | epoch: 16, train loss: 16.042560577392578, test loss: 13.08210277557373\n",
      "h: 72 | epoch: 17, train loss: 15.308300971984863, test loss: 12.420385360717773\n",
      "h: 72 | epoch: 18, train loss: 14.644254684448242, test loss: 11.818934440612793\n",
      "h: 72 | epoch: 19, train loss: 14.043910026550293, test loss: 11.272285461425781\n",
      "h: 72 | epoch: 20, train loss: 13.50133228302002, test loss: 10.775466918945312\n",
      "h: 72 | epoch: 21, train loss: 13.011128425598145, test loss: 10.323943138122559\n",
      "h: 72 | epoch: 22, train loss: 12.568388938903809, test loss: 9.913596153259277\n",
      "h: 72 | epoch: 23, train loss: 12.168651580810547, test loss: 9.540667533874512\n",
      "h: 72 | epoch: 24, train loss: 11.80785846710205, test loss: 9.201735496520996\n",
      "h: 72 | epoch: 25, train loss: 11.482321739196777, test loss: 8.893689155578613\n",
      "h: 72 | epoch: 26, train loss: 11.18868637084961, test loss: 8.613696098327637\n",
      "h: 72 | epoch: 27, train loss: 10.92391300201416, test loss: 8.359180450439453\n",
      "h: 72 | epoch: 28, train loss: 10.685234069824219, test loss: 8.127798080444336\n",
      "h: 72 | epoch: 29, train loss: 10.47014331817627, test loss: 7.917420387268066\n",
      "h: 72 | epoch: 30, train loss: 10.276363372802734, test loss: 7.72611141204834\n",
      "h: 72 | epoch: 31, train loss: 10.101835250854492, test loss: 7.552109718322754\n",
      "h: 72 | epoch: 32, train loss: 9.944685935974121, test loss: 7.393820762634277\n",
      "h: 72 | epoch: 33, train loss: 9.803220748901367, test loss: 7.249790191650391\n",
      "h: 72 | epoch: 34, train loss: 9.675909042358398, test loss: 7.118703365325928\n",
      "h: 72 | epoch: 35, train loss: 9.561360359191895, test loss: 6.999361991882324\n",
      "h: 72 | epoch: 36, train loss: 9.458319664001465, test loss: 6.89068078994751\n",
      "h: 72 | epoch: 37, train loss: 9.365652084350586, test loss: 6.791677951812744\n",
      "h: 72 | epoch: 38, train loss: 9.282331466674805, test loss: 6.7014594078063965\n",
      "h: 72 | epoch: 39, train loss: 9.207429885864258, test loss: 6.6192145347595215\n",
      "h: 72 | epoch: 40, train loss: 9.140109062194824, test loss: 6.544210910797119\n",
      "h: 72 | epoch: 41, train loss: 9.07961368560791, test loss: 6.475779056549072\n",
      "h: 72 | epoch: 42, train loss: 9.025260925292969, test loss: 6.413317680358887\n",
      "h: 72 | epoch: 43, train loss: 8.976436614990234, test loss: 6.356281757354736\n",
      "h: 72 | epoch: 44, train loss: 8.932583808898926, test loss: 6.304172515869141\n",
      "h: 72 | epoch: 45, train loss: 8.893202781677246, test loss: 6.256542682647705\n",
      "h: 72 | epoch: 46, train loss: 8.857843399047852, test loss: 6.212981224060059\n",
      "h: 72 | epoch: 47, train loss: 8.826098442077637, test loss: 6.17312479019165\n",
      "h: 72 | epoch: 48, train loss: 8.797603607177734, test loss: 6.136634349822998\n",
      "h: 72 | epoch: 49, train loss: 8.772027015686035, test loss: 6.103206634521484\n",
      "h: 72 | epoch: 50, train loss: 8.74907398223877, test loss: 6.072569847106934\n",
      "h: 72 | epoch: 51, train loss: 8.728477478027344, test loss: 6.04447078704834\n",
      "h: 72 | epoch: 52, train loss: 8.709997177124023, test loss: 6.018686294555664\n",
      "h: 72 | epoch: 53, train loss: 8.693418502807617, test loss: 5.9950103759765625\n",
      "h: 72 | epoch: 54, train loss: 8.678544998168945, test loss: 5.9732561111450195\n",
      "h: 72 | epoch: 55, train loss: 8.665205001831055, test loss: 5.953255653381348\n",
      "h: 72 | epoch: 56, train loss: 8.653238296508789, test loss: 5.934857368469238\n",
      "h: 72 | epoch: 57, train loss: 8.642507553100586, test loss: 5.917920112609863\n",
      "h: 72 | epoch: 58, train loss: 8.632883071899414, test loss: 5.902317523956299\n",
      "h: 72 | epoch: 59, train loss: 8.624253273010254, test loss: 5.887937068939209\n",
      "h: 72 | epoch: 60, train loss: 8.616515159606934, test loss: 5.874671936035156\n",
      "h: 72 | epoch: 61, train loss: 8.609576225280762, test loss: 5.862429618835449\n",
      "h: 72 | epoch: 62, train loss: 8.603355407714844, test loss: 5.851122856140137\n",
      "h: 72 | epoch: 63, train loss: 8.597779273986816, test loss: 5.840672969818115\n",
      "h: 72 | epoch: 64, train loss: 8.592780113220215, test loss: 5.831009387969971\n",
      "h: 72 | epoch: 65, train loss: 8.588296890258789, test loss: 5.822068214416504\n",
      "h: 72 | epoch: 66, train loss: 8.58427906036377, test loss: 5.813788890838623\n",
      "h: 72 | epoch: 67, train loss: 8.580678939819336, test loss: 5.806116104125977\n",
      "h: 72 | epoch: 68, train loss: 8.5774507522583, test loss: 5.799003601074219\n",
      "h: 72 | epoch: 69, train loss: 8.574557304382324, test loss: 5.7924065589904785\n",
      "h: 72 | epoch: 70, train loss: 8.5719633102417, test loss: 5.786281585693359\n",
      "h: 72 | epoch: 71, train loss: 8.569639205932617, test loss: 5.780593395233154\n",
      "h: 72 | epoch: 72, train loss: 8.567556381225586, test loss: 5.77530574798584\n",
      "h: 72 | epoch: 73, train loss: 8.565690040588379, test loss: 5.770390033721924\n",
      "h: 72 | epoch: 74, train loss: 8.564016342163086, test loss: 5.7658162117004395\n",
      "h: 72 | epoch: 75, train loss: 8.562517166137695, test loss: 5.761557579040527\n",
      "h: 72 | epoch: 76, train loss: 8.561172485351562, test loss: 5.757591724395752\n",
      "h: 72 | epoch: 77, train loss: 8.559967994689941, test loss: 5.753894805908203\n",
      "h: 72 | epoch: 78, train loss: 8.558889389038086, test loss: 5.750448226928711\n",
      "h: 72 | epoch: 79, train loss: 8.55792236328125, test loss: 5.74723482131958\n",
      "h: 72 | epoch: 80, train loss: 8.557055473327637, test loss: 5.744233131408691\n",
      "h: 72 | epoch: 81, train loss: 8.556278228759766, test loss: 5.74143123626709\n",
      "h: 72 | epoch: 82, train loss: 8.555582046508789, test loss: 5.738812446594238\n",
      "h: 72 | epoch: 83, train loss: 8.554957389831543, test loss: 5.7363667488098145\n",
      "h: 72 | epoch: 84, train loss: 8.554399490356445, test loss: 5.734078884124756\n",
      "h: 72 | epoch: 85, train loss: 8.553897857666016, test loss: 5.73193883895874\n",
      "h: 72 | epoch: 86, train loss: 8.553449630737305, test loss: 5.7299370765686035\n",
      "h: 72 | epoch: 87, train loss: 8.553047180175781, test loss: 5.728061199188232\n",
      "h: 72 | epoch: 88, train loss: 8.55268669128418, test loss: 5.726305961608887\n",
      "h: 72 | epoch: 89, train loss: 8.552364349365234, test loss: 5.724661350250244\n",
      "h: 72 | epoch: 90, train loss: 8.55207347869873, test loss: 5.723119258880615\n",
      "h: 72 | epoch: 91, train loss: 8.5518159866333, test loss: 5.721673965454102\n",
      "h: 72 | epoch: 92, train loss: 8.551582336425781, test loss: 5.720318794250488\n",
      "h: 72 | epoch: 93, train loss: 8.551374435424805, test loss: 5.719047546386719\n",
      "h: 72 | epoch: 94, train loss: 8.551187515258789, test loss: 5.717854022979736\n",
      "h: 72 | epoch: 95, train loss: 8.551019668579102, test loss: 5.716734409332275\n",
      "h: 72 | epoch: 96, train loss: 8.550870895385742, test loss: 5.7156829833984375\n",
      "h: 72 | epoch: 97, train loss: 8.550735473632812, test loss: 5.714694976806641\n",
      "h: 72 | epoch: 98, train loss: 8.550615310668945, test loss: 5.713766574859619\n",
      "h: 72 | epoch: 99, train loss: 8.550507545471191, test loss: 5.712894916534424\n",
      "h: 73 | epoch: 0, train loss: 43.621788024902344, test loss: 37.61167907714844\n",
      "h: 73 | epoch: 1, train loss: 40.83247756958008, test loss: 35.162811279296875\n",
      "h: 73 | epoch: 2, train loss: 38.26740264892578, test loss: 32.90726852416992\n",
      "h: 73 | epoch: 3, train loss: 35.90712356567383, test loss: 30.82854652404785\n",
      "h: 73 | epoch: 4, train loss: 33.73421096801758, test loss: 28.911794662475586\n",
      "h: 73 | epoch: 5, train loss: 31.732995986938477, test loss: 27.14363670349121\n",
      "h: 73 | epoch: 6, train loss: 29.889331817626953, test loss: 25.51195526123047\n",
      "h: 73 | epoch: 7, train loss: 28.190399169921875, test loss: 24.005783081054688\n",
      "h: 73 | epoch: 8, train loss: 26.624582290649414, test loss: 22.615131378173828\n",
      "h: 73 | epoch: 9, train loss: 25.181285858154297, test loss: 21.33089256286621\n",
      "h: 73 | epoch: 10, train loss: 23.850849151611328, test loss: 20.144756317138672\n",
      "h: 73 | epoch: 11, train loss: 22.624439239501953, test loss: 19.049100875854492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 73 | epoch: 12, train loss: 21.493959426879883, test loss: 18.036962509155273\n",
      "h: 73 | epoch: 13, train loss: 20.451984405517578, test loss: 17.10192108154297\n",
      "h: 73 | epoch: 14, train loss: 19.491683959960938, test loss: 16.238086700439453\n",
      "h: 73 | epoch: 15, train loss: 18.606769561767578, test loss: 15.440025329589844\n",
      "h: 73 | epoch: 16, train loss: 17.791460037231445, test loss: 14.70274829864502\n",
      "h: 73 | epoch: 17, train loss: 17.040409088134766, test loss: 14.021636962890625\n",
      "h: 73 | epoch: 18, train loss: 16.34868812561035, test loss: 13.392433166503906\n",
      "h: 73 | epoch: 19, train loss: 15.71174430847168, test loss: 12.811208724975586\n",
      "h: 73 | epoch: 20, train loss: 15.125370979309082, test loss: 12.274327278137207\n",
      "h: 73 | epoch: 21, train loss: 14.585680961608887, test loss: 11.778428077697754\n",
      "h: 73 | epoch: 22, train loss: 14.089078903198242, test loss: 11.320404052734375\n",
      "h: 73 | epoch: 23, train loss: 13.632232666015625, test loss: 10.897383689880371\n",
      "h: 73 | epoch: 24, train loss: 13.212068557739258, test loss: 10.506708145141602\n",
      "h: 73 | epoch: 25, train loss: 12.825736999511719, test loss: 10.145917892456055\n",
      "h: 73 | epoch: 26, train loss: 12.470606803894043, test loss: 9.812736511230469\n",
      "h: 73 | epoch: 27, train loss: 12.144242286682129, test loss: 9.505060195922852\n",
      "h: 73 | epoch: 28, train loss: 11.844388008117676, test loss: 9.220939636230469\n",
      "h: 73 | epoch: 29, train loss: 11.568961143493652, test loss: 8.958574295043945\n",
      "h: 73 | epoch: 30, train loss: 11.316034317016602, test loss: 8.71629524230957\n",
      "h: 73 | epoch: 31, train loss: 11.08382797241211, test loss: 8.49256420135498\n",
      "h: 73 | epoch: 32, train loss: 10.870697021484375, test loss: 8.285955429077148\n",
      "h: 73 | epoch: 33, train loss: 10.67512035369873, test loss: 8.0951509475708\n",
      "h: 73 | epoch: 34, train loss: 10.495695114135742, test loss: 7.918932914733887\n",
      "h: 73 | epoch: 35, train loss: 10.331125259399414, test loss: 7.756174564361572\n",
      "h: 73 | epoch: 36, train loss: 10.180212020874023, test loss: 7.605839729309082\n",
      "h: 73 | epoch: 37, train loss: 10.041855812072754, test loss: 7.466965675354004\n",
      "h: 73 | epoch: 38, train loss: 9.915038108825684, test loss: 7.3386640548706055\n",
      "h: 73 | epoch: 39, train loss: 9.798818588256836, test loss: 7.220118522644043\n",
      "h: 73 | epoch: 40, train loss: 9.69233512878418, test loss: 7.110573768615723\n",
      "h: 73 | epoch: 41, train loss: 9.594789505004883, test loss: 7.009326934814453\n",
      "h: 73 | epoch: 42, train loss: 9.505449295043945, test loss: 6.9157395362854\n",
      "h: 73 | epoch: 43, train loss: 9.423639297485352, test loss: 6.829213619232178\n",
      "h: 73 | epoch: 44, train loss: 9.348737716674805, test loss: 6.749205589294434\n",
      "h: 73 | epoch: 45, train loss: 9.280172348022461, test loss: 6.675206184387207\n",
      "h: 73 | epoch: 46, train loss: 9.217415809631348, test loss: 6.606749534606934\n",
      "h: 73 | epoch: 47, train loss: 9.159988403320312, test loss: 6.5434088706970215\n",
      "h: 73 | epoch: 48, train loss: 9.107443809509277, test loss: 6.4847846031188965\n",
      "h: 73 | epoch: 49, train loss: 9.05937385559082, test loss: 6.430514335632324\n",
      "h: 73 | epoch: 50, train loss: 9.015403747558594, test loss: 6.380259990692139\n",
      "h: 73 | epoch: 51, train loss: 8.975191116333008, test loss: 6.333711624145508\n",
      "h: 73 | epoch: 52, train loss: 8.938413619995117, test loss: 6.290583610534668\n",
      "h: 73 | epoch: 53, train loss: 8.90478801727295, test loss: 6.250613689422607\n",
      "h: 73 | epoch: 54, train loss: 8.87404727935791, test loss: 6.213555812835693\n",
      "h: 73 | epoch: 55, train loss: 8.84594440460205, test loss: 6.179190635681152\n",
      "h: 73 | epoch: 56, train loss: 8.820257186889648, test loss: 6.147308349609375\n",
      "h: 73 | epoch: 57, train loss: 8.79677963256836, test loss: 6.117721080780029\n",
      "h: 73 | epoch: 58, train loss: 8.775324821472168, test loss: 6.090254306793213\n",
      "h: 73 | epoch: 59, train loss: 8.755720138549805, test loss: 6.064746379852295\n",
      "h: 73 | epoch: 60, train loss: 8.73780632019043, test loss: 6.0410475730896\n",
      "h: 73 | epoch: 61, train loss: 8.721441268920898, test loss: 6.019021987915039\n",
      "h: 73 | epoch: 62, train loss: 8.706491470336914, test loss: 5.998544216156006\n",
      "h: 73 | epoch: 63, train loss: 8.69283390045166, test loss: 5.979496955871582\n",
      "h: 73 | epoch: 64, train loss: 8.68035888671875, test loss: 5.961773872375488\n",
      "h: 73 | epoch: 65, train loss: 8.668966293334961, test loss: 5.945276260375977\n",
      "h: 73 | epoch: 66, train loss: 8.658560752868652, test loss: 5.929912090301514\n",
      "h: 73 | epoch: 67, train loss: 8.649057388305664, test loss: 5.9155988693237305\n",
      "h: 73 | epoch: 68, train loss: 8.640380859375, test loss: 5.902257919311523\n",
      "h: 73 | epoch: 69, train loss: 8.632457733154297, test loss: 5.889817714691162\n",
      "h: 73 | epoch: 70, train loss: 8.625224113464355, test loss: 5.8782124519348145\n",
      "h: 73 | epoch: 71, train loss: 8.61861801147461, test loss: 5.867382049560547\n",
      "h: 73 | epoch: 72, train loss: 8.612588882446289, test loss: 5.857269763946533\n",
      "h: 73 | epoch: 73, train loss: 8.607084274291992, test loss: 5.847824573516846\n",
      "h: 73 | epoch: 74, train loss: 8.602057456970215, test loss: 5.838998317718506\n",
      "h: 73 | epoch: 75, train loss: 8.5974702835083, test loss: 5.830746650695801\n",
      "h: 73 | epoch: 76, train loss: 8.593282699584961, test loss: 5.823029041290283\n",
      "h: 73 | epoch: 77, train loss: 8.589460372924805, test loss: 5.815806865692139\n",
      "h: 73 | epoch: 78, train loss: 8.585970878601074, test loss: 5.809045791625977\n",
      "h: 73 | epoch: 79, train loss: 8.58278751373291, test loss: 5.802712917327881\n",
      "h: 73 | epoch: 80, train loss: 8.579880714416504, test loss: 5.796780586242676\n",
      "h: 73 | epoch: 81, train loss: 8.577227592468262, test loss: 5.791219711303711\n",
      "h: 73 | epoch: 82, train loss: 8.574807167053223, test loss: 5.786004543304443\n",
      "h: 73 | epoch: 83, train loss: 8.57259750366211, test loss: 5.781111240386963\n",
      "h: 73 | epoch: 84, train loss: 8.570581436157227, test loss: 5.776518821716309\n",
      "h: 73 | epoch: 85, train loss: 8.568741798400879, test loss: 5.772207260131836\n",
      "h: 73 | epoch: 86, train loss: 8.567063331604004, test loss: 5.768157005310059\n",
      "h: 73 | epoch: 87, train loss: 8.565531730651855, test loss: 5.764349937438965\n",
      "h: 73 | epoch: 88, train loss: 8.564133644104004, test loss: 5.760770797729492\n",
      "h: 73 | epoch: 89, train loss: 8.562857627868652, test loss: 5.757406234741211\n",
      "h: 73 | epoch: 90, train loss: 8.56169319152832, test loss: 5.754239082336426\n",
      "h: 73 | epoch: 91, train loss: 8.560630798339844, test loss: 5.75125789642334\n",
      "h: 73 | epoch: 92, train loss: 8.559662818908691, test loss: 5.748451232910156\n",
      "h: 73 | epoch: 93, train loss: 8.558777809143066, test loss: 5.74580717086792\n",
      "h: 73 | epoch: 94, train loss: 8.55797004699707, test loss: 5.743315696716309\n",
      "h: 73 | epoch: 95, train loss: 8.557235717773438, test loss: 5.740966796875\n",
      "h: 73 | epoch: 96, train loss: 8.556564331054688, test loss: 5.738752365112305\n",
      "h: 73 | epoch: 97, train loss: 8.555951118469238, test loss: 5.736663341522217\n",
      "h: 73 | epoch: 98, train loss: 8.555391311645508, test loss: 5.734691143035889\n",
      "h: 73 | epoch: 99, train loss: 8.55488109588623, test loss: 5.7328290939331055\n",
      "h: 74 | epoch: 0, train loss: 49.98176956176758, test loss: 42.19817352294922\n",
      "h: 74 | epoch: 1, train loss: 45.97441101074219, test loss: 38.90936279296875\n",
      "h: 74 | epoch: 2, train loss: 42.36801528930664, test loss: 35.93513870239258\n",
      "h: 74 | epoch: 3, train loss: 39.11924743652344, test loss: 33.242618560791016\n",
      "h: 74 | epoch: 4, train loss: 36.19015121459961, test loss: 30.802886962890625\n",
      "h: 74 | epoch: 5, train loss: 33.54739761352539, test loss: 28.59042739868164\n",
      "h: 74 | epoch: 6, train loss: 31.161550521850586, test loss: 26.582630157470703\n",
      "h: 74 | epoch: 7, train loss: 29.006549835205078, test loss: 24.759410858154297\n",
      "h: 74 | epoch: 8, train loss: 27.059246063232422, test loss: 23.102863311767578\n",
      "h: 74 | epoch: 9, train loss: 25.29901695251465, test loss: 21.59698486328125\n",
      "h: 74 | epoch: 10, train loss: 23.707433700561523, test loss: 20.227428436279297\n",
      "h: 74 | epoch: 11, train loss: 22.268016815185547, test loss: 18.981338500976562\n",
      "h: 74 | epoch: 12, train loss: 20.965980529785156, test loss: 17.84713363647461\n",
      "h: 74 | epoch: 13, train loss: 19.78803825378418, test loss: 16.814393997192383\n",
      "h: 74 | epoch: 14, train loss: 18.72224998474121, test loss: 15.873716354370117\n",
      "h: 74 | epoch: 15, train loss: 17.757854461669922, test loss: 15.016609191894531\n",
      "h: 74 | epoch: 16, train loss: 16.885149002075195, test loss: 14.23539924621582\n",
      "h: 74 | epoch: 17, train loss: 16.09538459777832, test loss: 13.523141860961914\n",
      "h: 74 | epoch: 18, train loss: 15.380651473999023, test loss: 12.87354850769043\n",
      "h: 74 | epoch: 19, train loss: 14.733810424804688, test loss: 12.280923843383789\n",
      "h: 74 | epoch: 20, train loss: 14.148404121398926, test loss: 11.740099906921387\n",
      "h: 74 | epoch: 21, train loss: 13.6185941696167, test loss: 11.246390342712402\n",
      "h: 74 | epoch: 22, train loss: 13.1390962600708, test loss: 10.795548439025879\n",
      "h: 74 | epoch: 23, train loss: 12.705133438110352, test loss: 10.383711814880371\n",
      "h: 74 | epoch: 24, train loss: 12.312378883361816, test loss: 10.007380485534668\n",
      "h: 74 | epoch: 25, train loss: 11.956917762756348, test loss: 9.663370132446289\n",
      "h: 74 | epoch: 26, train loss: 11.63520622253418, test loss: 9.34878921508789\n",
      "h: 74 | epoch: 27, train loss: 11.344035148620605, test loss: 9.061014175415039\n",
      "h: 74 | epoch: 28, train loss: 11.080503463745117, test loss: 8.79765796661377\n",
      "h: 74 | epoch: 29, train loss: 10.841980934143066, test loss: 8.55655288696289\n",
      "h: 74 | epoch: 30, train loss: 10.626091003417969, test loss: 8.335726737976074\n",
      "h: 74 | epoch: 31, train loss: 10.43067741394043, test loss: 8.133383750915527\n",
      "h: 74 | epoch: 32, train loss: 10.253791809082031, test loss: 7.947902679443359\n",
      "h: 74 | epoch: 33, train loss: 10.093671798706055, test loss: 7.777795314788818\n",
      "h: 74 | epoch: 34, train loss: 9.948722839355469, test loss: 7.621718406677246\n",
      "h: 74 | epoch: 35, train loss: 9.817497253417969, test loss: 7.478442192077637\n",
      "h: 74 | epoch: 36, train loss: 9.698689460754395, test loss: 7.346855163574219\n",
      "h: 74 | epoch: 37, train loss: 9.591119766235352, test loss: 7.225943565368652\n",
      "h: 74 | epoch: 38, train loss: 9.493715286254883, test loss: 7.114782810211182\n",
      "h: 74 | epoch: 39, train loss: 9.405510902404785, test loss: 7.012533664703369\n",
      "h: 74 | epoch: 40, train loss: 9.325629234313965, test loss: 6.91842794418335\n",
      "h: 74 | epoch: 41, train loss: 9.253279685974121, test loss: 6.831772804260254\n",
      "h: 74 | epoch: 42, train loss: 9.18774700164795, test loss: 6.751934051513672\n",
      "h: 74 | epoch: 43, train loss: 9.128382682800293, test loss: 6.6783342361450195\n",
      "h: 74 | epoch: 44, train loss: 9.074602127075195, test loss: 6.61044454574585\n",
      "h: 74 | epoch: 45, train loss: 9.025870323181152, test loss: 6.547787666320801\n",
      "h: 74 | epoch: 46, train loss: 8.981714248657227, test loss: 6.489926338195801\n",
      "h: 74 | epoch: 47, train loss: 8.94169807434082, test loss: 6.436463356018066\n",
      "h: 74 | epoch: 48, train loss: 8.905427932739258, test loss: 6.387030124664307\n",
      "h: 74 | epoch: 49, train loss: 8.872552871704102, test loss: 6.341297626495361\n",
      "h: 74 | epoch: 50, train loss: 8.842748641967773, test loss: 6.298966407775879\n",
      "h: 74 | epoch: 51, train loss: 8.815725326538086, test loss: 6.259757041931152\n",
      "h: 74 | epoch: 52, train loss: 8.791223526000977, test loss: 6.223415374755859\n",
      "h: 74 | epoch: 53, train loss: 8.769002914428711, test loss: 6.189713478088379\n",
      "h: 74 | epoch: 54, train loss: 8.748847961425781, test loss: 6.158439636230469\n",
      "h: 74 | epoch: 55, train loss: 8.730566024780273, test loss: 6.12939977645874\n",
      "h: 74 | epoch: 56, train loss: 8.713980674743652, test loss: 6.102419376373291\n",
      "h: 74 | epoch: 57, train loss: 8.698931694030762, test loss: 6.077336311340332\n",
      "h: 74 | epoch: 58, train loss: 8.685275077819824, test loss: 6.054003715515137\n",
      "h: 74 | epoch: 59, train loss: 8.672883033752441, test loss: 6.032284736633301\n",
      "h: 74 | epoch: 60, train loss: 8.661633491516113, test loss: 6.012056350708008\n",
      "h: 74 | epoch: 61, train loss: 8.651422500610352, test loss: 5.993204593658447\n",
      "h: 74 | epoch: 62, train loss: 8.642151832580566, test loss: 5.975625514984131\n",
      "h: 74 | epoch: 63, train loss: 8.633733749389648, test loss: 5.959222316741943\n",
      "h: 74 | epoch: 64, train loss: 8.62608814239502, test loss: 5.94390869140625\n",
      "h: 74 | epoch: 65, train loss: 8.619146347045898, test loss: 5.92960262298584\n",
      "h: 74 | epoch: 66, train loss: 8.612839698791504, test loss: 5.916232585906982\n",
      "h: 74 | epoch: 67, train loss: 8.607110977172852, test loss: 5.9037275314331055\n",
      "h: 74 | epoch: 68, train loss: 8.60190486907959, test loss: 5.892024517059326\n",
      "h: 74 | epoch: 69, train loss: 8.597176551818848, test loss: 5.881068706512451\n",
      "h: 74 | epoch: 70, train loss: 8.592878341674805, test loss: 5.870804309844971\n",
      "h: 74 | epoch: 71, train loss: 8.588972091674805, test loss: 5.861184597015381\n",
      "h: 74 | epoch: 72, train loss: 8.585420608520508, test loss: 5.852161407470703\n",
      "h: 74 | epoch: 73, train loss: 8.582193374633789, test loss: 5.843695640563965\n",
      "h: 74 | epoch: 74, train loss: 8.579259872436523, test loss: 5.835747718811035\n",
      "h: 74 | epoch: 75, train loss: 8.576590538024902, test loss: 5.828283309936523\n",
      "h: 74 | epoch: 76, train loss: 8.574164390563965, test loss: 5.821267604827881\n",
      "h: 74 | epoch: 77, train loss: 8.571958541870117, test loss: 5.814671516418457\n",
      "h: 74 | epoch: 78, train loss: 8.569952011108398, test loss: 5.808467388153076\n",
      "h: 74 | epoch: 79, train loss: 8.56812572479248, test loss: 5.802628040313721\n",
      "h: 74 | epoch: 80, train loss: 8.566465377807617, test loss: 5.797131061553955\n",
      "h: 74 | epoch: 81, train loss: 8.56495475769043, test loss: 5.791952133178711\n",
      "h: 74 | epoch: 82, train loss: 8.563579559326172, test loss: 5.787071704864502\n",
      "h: 74 | epoch: 83, train loss: 8.562328338623047, test loss: 5.782472610473633\n",
      "h: 74 | epoch: 84, train loss: 8.561190605163574, test loss: 5.778132438659668\n",
      "h: 74 | epoch: 85, train loss: 8.560154914855957, test loss: 5.774038314819336\n",
      "h: 74 | epoch: 86, train loss: 8.559209823608398, test loss: 5.7701735496521\n",
      "h: 74 | epoch: 87, train loss: 8.55835247039795, test loss: 5.766524314880371\n",
      "h: 74 | epoch: 88, train loss: 8.557570457458496, test loss: 5.763075828552246\n",
      "h: 74 | epoch: 89, train loss: 8.556859970092773, test loss: 5.759819030761719\n",
      "h: 74 | epoch: 90, train loss: 8.556211471557617, test loss: 5.756739616394043\n",
      "h: 74 | epoch: 91, train loss: 8.555622100830078, test loss: 5.7538275718688965\n",
      "h: 74 | epoch: 92, train loss: 8.555086135864258, test loss: 5.751072406768799\n",
      "h: 74 | epoch: 93, train loss: 8.554595947265625, test loss: 5.748465061187744\n",
      "h: 74 | epoch: 94, train loss: 8.554149627685547, test loss: 5.745995998382568\n",
      "h: 74 | epoch: 95, train loss: 8.553744316101074, test loss: 5.743659973144531\n",
      "h: 74 | epoch: 96, train loss: 8.553374290466309, test loss: 5.741446495056152\n",
      "h: 74 | epoch: 97, train loss: 8.553037643432617, test loss: 5.739348411560059\n",
      "h: 74 | epoch: 98, train loss: 8.55273151397705, test loss: 5.737361431121826\n",
      "h: 74 | epoch: 99, train loss: 8.55245304107666, test loss: 5.735476493835449\n",
      "h: 75 | epoch: 0, train loss: 49.359222412109375, test loss: 40.085018157958984\n",
      "h: 75 | epoch: 1, train loss: 43.7970085144043, test loss: 35.67845153808594\n",
      "h: 75 | epoch: 2, train loss: 39.002342224121094, test loss: 31.858606338500977\n",
      "h: 75 | epoch: 3, train loss: 34.86609649658203, test loss: 28.544265747070312\n",
      "h: 75 | epoch: 4, train loss: 31.295780181884766, test loss: 25.666250228881836\n",
      "h: 75 | epoch: 5, train loss: 28.212635040283203, test loss: 23.165374755859375\n",
      "h: 75 | epoch: 6, train loss: 25.549388885498047, test loss: 20.9908504486084\n",
      "h: 75 | epoch: 7, train loss: 23.248382568359375, test loss: 19.099002838134766\n",
      "h: 75 | epoch: 8, train loss: 21.260089874267578, test loss: 17.452180862426758\n",
      "h: 75 | epoch: 9, train loss: 19.54189109802246, test loss: 16.017879486083984\n",
      "h: 75 | epoch: 10, train loss: 18.057043075561523, test loss: 14.767992973327637\n",
      "h: 75 | epoch: 11, train loss: 16.773849487304688, test loss: 13.678207397460938\n",
      "h: 75 | epoch: 12, train loss: 15.664929389953613, test loss: 12.727449417114258\n",
      "h: 75 | epoch: 13, train loss: 14.706629753112793, test loss: 11.897469520568848\n",
      "h: 75 | epoch: 14, train loss: 13.878497123718262, test loss: 11.172441482543945\n",
      "h: 75 | epoch: 15, train loss: 13.162851333618164, test loss: 10.538647651672363\n",
      "h: 75 | epoch: 16, train loss: 12.544401168823242, test loss: 9.984173774719238\n",
      "h: 75 | epoch: 17, train loss: 12.009927749633789, test loss: 9.498698234558105\n",
      "h: 75 | epoch: 18, train loss: 11.547996520996094, test loss: 9.073251724243164\n",
      "h: 75 | epoch: 19, train loss: 11.148725509643555, test loss: 8.700063705444336\n",
      "h: 75 | epoch: 20, train loss: 10.803573608398438, test loss: 8.372380256652832\n",
      "h: 75 | epoch: 21, train loss: 10.505158424377441, test loss: 8.084344863891602\n",
      "h: 75 | epoch: 22, train loss: 10.247100830078125, test loss: 7.83087158203125\n",
      "h: 75 | epoch: 23, train loss: 10.023893356323242, test loss: 7.6075439453125\n",
      "h: 75 | epoch: 24, train loss: 9.830778121948242, test loss: 7.410527229309082\n",
      "h: 75 | epoch: 25, train loss: 9.663647651672363, test loss: 7.2364935874938965\n",
      "h: 75 | epoch: 26, train loss: 9.518954277038574, test loss: 7.082551002502441\n",
      "h: 75 | epoch: 27, train loss: 9.393638610839844, test loss: 6.946183204650879\n",
      "h: 75 | epoch: 28, train loss: 9.285059928894043, test loss: 6.8252058029174805\n",
      "h: 75 | epoch: 29, train loss: 9.190939903259277, test loss: 6.717722415924072\n",
      "h: 75 | epoch: 30, train loss: 9.109312057495117, test loss: 6.622075080871582\n",
      "h: 75 | epoch: 31, train loss: 9.038477897644043, test loss: 6.536825656890869\n",
      "h: 75 | epoch: 32, train loss: 8.976977348327637, test loss: 6.460721492767334\n",
      "h: 75 | epoch: 33, train loss: 8.923544883728027, test loss: 6.392668724060059\n",
      "h: 75 | epoch: 34, train loss: 8.877094268798828, test loss: 6.3317155838012695\n",
      "h: 75 | epoch: 35, train loss: 8.836681365966797, test loss: 6.277028560638428\n",
      "h: 75 | epoch: 36, train loss: 8.801498413085938, test loss: 6.227883338928223\n",
      "h: 75 | epoch: 37, train loss: 8.770843505859375, test loss: 6.1836419105529785\n",
      "h: 75 | epoch: 38, train loss: 8.744114875793457, test loss: 6.1437482833862305\n",
      "h: 75 | epoch: 39, train loss: 8.72078800201416, test loss: 6.1077165603637695\n",
      "h: 75 | epoch: 40, train loss: 8.70041275024414, test loss: 6.075117588043213\n",
      "h: 75 | epoch: 41, train loss: 8.682600021362305, test loss: 6.045577049255371\n",
      "h: 75 | epoch: 42, train loss: 8.667013168334961, test loss: 6.018764495849609\n",
      "h: 75 | epoch: 43, train loss: 8.653361320495605, test loss: 5.994390487670898\n",
      "h: 75 | epoch: 44, train loss: 8.641393661499023, test loss: 5.972196102142334\n",
      "h: 75 | epoch: 45, train loss: 8.630888938903809, test loss: 5.951958656311035\n",
      "h: 75 | epoch: 46, train loss: 8.621663093566895, test loss: 5.933477401733398\n",
      "h: 75 | epoch: 47, train loss: 8.613550186157227, test loss: 5.916574478149414\n",
      "h: 75 | epoch: 48, train loss: 8.606406211853027, test loss: 5.90109395980835\n",
      "h: 75 | epoch: 49, train loss: 8.600113868713379, test loss: 5.886896133422852\n",
      "h: 75 | epoch: 50, train loss: 8.594561576843262, test loss: 5.873859405517578\n",
      "h: 75 | epoch: 51, train loss: 8.589658737182617, test loss: 5.861871242523193\n",
      "h: 75 | epoch: 52, train loss: 8.5853271484375, test loss: 5.850834846496582\n",
      "h: 75 | epoch: 53, train loss: 8.581491470336914, test loss: 5.840661525726318\n",
      "h: 75 | epoch: 54, train loss: 8.578093528747559, test loss: 5.831273555755615\n",
      "h: 75 | epoch: 55, train loss: 8.575082778930664, test loss: 5.822600841522217\n",
      "h: 75 | epoch: 56, train loss: 8.572408676147461, test loss: 5.814579963684082\n",
      "h: 75 | epoch: 57, train loss: 8.570030212402344, test loss: 5.807154655456543\n",
      "h: 75 | epoch: 58, train loss: 8.567916870117188, test loss: 5.800273895263672\n",
      "h: 75 | epoch: 59, train loss: 8.566035270690918, test loss: 5.79388952255249\n",
      "h: 75 | epoch: 60, train loss: 8.564358711242676, test loss: 5.787963390350342\n",
      "h: 75 | epoch: 61, train loss: 8.562861442565918, test loss: 5.7824554443359375\n",
      "h: 75 | epoch: 62, train loss: 8.561525344848633, test loss: 5.7773332595825195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 75 | epoch: 63, train loss: 8.560330390930176, test loss: 5.77256441116333\n",
      "h: 75 | epoch: 64, train loss: 8.559263229370117, test loss: 5.768122673034668\n",
      "h: 75 | epoch: 65, train loss: 8.558304786682129, test loss: 5.763981342315674\n",
      "h: 75 | epoch: 66, train loss: 8.557448387145996, test loss: 5.760116100311279\n",
      "h: 75 | epoch: 67, train loss: 8.556680679321289, test loss: 5.756510257720947\n",
      "h: 75 | epoch: 68, train loss: 8.555988311767578, test loss: 5.753139495849609\n",
      "h: 75 | epoch: 69, train loss: 8.555368423461914, test loss: 5.749987602233887\n",
      "h: 75 | epoch: 70, train loss: 8.554811477661133, test loss: 5.747040748596191\n",
      "h: 75 | epoch: 71, train loss: 8.554309844970703, test loss: 5.74428129196167\n",
      "h: 75 | epoch: 72, train loss: 8.553858757019043, test loss: 5.741696357727051\n",
      "h: 75 | epoch: 73, train loss: 8.553451538085938, test loss: 5.739274501800537\n",
      "h: 75 | epoch: 74, train loss: 8.553084373474121, test loss: 5.737003803253174\n",
      "h: 75 | epoch: 75, train loss: 8.552754402160645, test loss: 5.734873294830322\n",
      "h: 75 | epoch: 76, train loss: 8.552454948425293, test loss: 5.732873439788818\n",
      "h: 75 | epoch: 77, train loss: 8.552186965942383, test loss: 5.730994701385498\n",
      "h: 75 | epoch: 78, train loss: 8.551942825317383, test loss: 5.7292304039001465\n",
      "h: 75 | epoch: 79, train loss: 8.551721572875977, test loss: 5.727571964263916\n",
      "h: 75 | epoch: 80, train loss: 8.551523208618164, test loss: 5.726012706756592\n",
      "h: 75 | epoch: 81, train loss: 8.551342964172363, test loss: 5.724544525146484\n",
      "h: 75 | epoch: 82, train loss: 8.551179885864258, test loss: 5.723161697387695\n",
      "h: 75 | epoch: 83, train loss: 8.551031112670898, test loss: 5.721861839294434\n",
      "h: 75 | epoch: 84, train loss: 8.550898551940918, test loss: 5.720635890960693\n",
      "h: 75 | epoch: 85, train loss: 8.550777435302734, test loss: 5.719481468200684\n",
      "h: 75 | epoch: 86, train loss: 8.550667762756348, test loss: 5.7183918952941895\n",
      "h: 75 | epoch: 87, train loss: 8.550567626953125, test loss: 5.71736478805542\n",
      "h: 75 | epoch: 88, train loss: 8.550477027893066, test loss: 5.716395378112793\n",
      "h: 75 | epoch: 89, train loss: 8.550394058227539, test loss: 5.715481281280518\n",
      "h: 75 | epoch: 90, train loss: 8.550320625305176, test loss: 5.714617729187012\n",
      "h: 75 | epoch: 91, train loss: 8.550252914428711, test loss: 5.713801383972168\n",
      "h: 75 | epoch: 92, train loss: 8.550191879272461, test loss: 5.713031768798828\n",
      "h: 75 | epoch: 93, train loss: 8.550135612487793, test loss: 5.71230411529541\n",
      "h: 75 | epoch: 94, train loss: 8.550085067749023, test loss: 5.711615085601807\n",
      "h: 75 | epoch: 95, train loss: 8.550039291381836, test loss: 5.710964679718018\n",
      "h: 75 | epoch: 96, train loss: 8.549997329711914, test loss: 5.710348606109619\n",
      "h: 75 | epoch: 97, train loss: 8.549958229064941, test loss: 5.709766864776611\n",
      "h: 75 | epoch: 98, train loss: 8.54992389678955, test loss: 5.709214687347412\n",
      "h: 75 | epoch: 99, train loss: 8.549893379211426, test loss: 5.70869255065918\n",
      "h: 76 | epoch: 0, train loss: 36.28704071044922, test loss: 30.106826782226562\n",
      "h: 76 | epoch: 1, train loss: 33.52541732788086, test loss: 27.823883056640625\n",
      "h: 76 | epoch: 2, train loss: 31.037342071533203, test loss: 25.758920669555664\n",
      "h: 76 | epoch: 3, train loss: 28.795150756835938, test loss: 23.890464782714844\n",
      "h: 76 | epoch: 4, train loss: 26.774211883544922, test loss: 22.199338912963867\n",
      "h: 76 | epoch: 5, train loss: 24.952529907226562, test loss: 20.66836929321289\n",
      "h: 76 | epoch: 6, train loss: 23.31043243408203, test loss: 19.282140731811523\n",
      "h: 76 | epoch: 7, train loss: 21.830270767211914, test loss: 18.026790618896484\n",
      "h: 76 | epoch: 8, train loss: 20.496191024780273, test loss: 16.889850616455078\n",
      "h: 76 | epoch: 9, train loss: 19.293930053710938, test loss: 15.860048294067383\n",
      "h: 76 | epoch: 10, train loss: 18.21063804626465, test loss: 14.927223205566406\n",
      "h: 76 | epoch: 11, train loss: 17.234722137451172, test loss: 14.082186698913574\n",
      "h: 76 | epoch: 12, train loss: 16.355724334716797, test loss: 13.316633224487305\n",
      "h: 76 | epoch: 13, train loss: 15.564203262329102, test loss: 12.623041152954102\n",
      "h: 76 | epoch: 14, train loss: 14.851617813110352, test loss: 11.99460506439209\n",
      "h: 76 | epoch: 15, train loss: 14.210253715515137, test loss: 11.425165176391602\n",
      "h: 76 | epoch: 16, train loss: 13.633143424987793, test loss: 10.909135818481445\n",
      "h: 76 | epoch: 17, train loss: 13.113980293273926, test loss: 10.441458702087402\n",
      "h: 76 | epoch: 18, train loss: 12.647066116333008, test loss: 10.01755428314209\n",
      "h: 76 | epoch: 19, train loss: 12.22724723815918, test loss: 9.633273124694824\n",
      "h: 76 | epoch: 20, train loss: 11.849872589111328, test loss: 9.284857749938965\n",
      "h: 76 | epoch: 21, train loss: 11.510730743408203, test loss: 8.968900680541992\n",
      "h: 76 | epoch: 22, train loss: 11.206022262573242, test loss: 8.682321548461914\n",
      "h: 76 | epoch: 23, train loss: 10.932318687438965, test loss: 8.422329902648926\n",
      "h: 76 | epoch: 24, train loss: 10.686515808105469, test loss: 8.186395645141602\n",
      "h: 76 | epoch: 25, train loss: 10.465821266174316, test loss: 7.972237586975098\n",
      "h: 76 | epoch: 26, train loss: 10.267709732055664, test loss: 7.7777814865112305\n",
      "h: 76 | epoch: 27, train loss: 10.089903831481934, test loss: 7.60115909576416\n",
      "h: 76 | epoch: 28, train loss: 9.930356979370117, test loss: 7.440673828125\n",
      "h: 76 | epoch: 29, train loss: 9.78721809387207, test loss: 7.294797420501709\n",
      "h: 76 | epoch: 30, train loss: 9.658819198608398, test loss: 7.162140846252441\n",
      "h: 76 | epoch: 31, train loss: 9.543663024902344, test loss: 7.0414557456970215\n",
      "h: 76 | epoch: 32, train loss: 9.440398216247559, test loss: 6.931609153747559\n",
      "h: 76 | epoch: 33, train loss: 9.347811698913574, test loss: 6.831575870513916\n",
      "h: 76 | epoch: 34, train loss: 9.264805793762207, test loss: 6.7404351234436035\n",
      "h: 76 | epoch: 35, train loss: 9.190402030944824, test loss: 6.657347679138184\n",
      "h: 76 | epoch: 36, train loss: 9.123714447021484, test loss: 6.581561088562012\n",
      "h: 76 | epoch: 37, train loss: 9.063949584960938, test loss: 6.512391567230225\n",
      "h: 76 | epoch: 38, train loss: 9.010393142700195, test loss: 6.449222564697266\n",
      "h: 76 | epoch: 39, train loss: 8.962404251098633, test loss: 6.391495704650879\n",
      "h: 76 | epoch: 40, train loss: 8.91940689086914, test loss: 6.338708877563477\n",
      "h: 76 | epoch: 41, train loss: 8.88088607788086, test loss: 6.290403842926025\n",
      "h: 76 | epoch: 42, train loss: 8.846376419067383, test loss: 6.246171474456787\n",
      "h: 76 | epoch: 43, train loss: 8.815461158752441, test loss: 6.2056403160095215\n",
      "h: 76 | epoch: 44, train loss: 8.78776741027832, test loss: 6.168471813201904\n",
      "h: 76 | epoch: 45, train loss: 8.762961387634277, test loss: 6.134362697601318\n",
      "h: 76 | epoch: 46, train loss: 8.740741729736328, test loss: 6.103036403656006\n",
      "h: 76 | epoch: 47, train loss: 8.720839500427246, test loss: 6.074246406555176\n",
      "h: 76 | epoch: 48, train loss: 8.70301342010498, test loss: 6.047763347625732\n",
      "h: 76 | epoch: 49, train loss: 8.687047958374023, test loss: 6.023385524749756\n",
      "h: 76 | epoch: 50, train loss: 8.672746658325195, test loss: 6.000929355621338\n",
      "h: 76 | epoch: 51, train loss: 8.659936904907227, test loss: 5.980224132537842\n",
      "h: 76 | epoch: 52, train loss: 8.64846420288086, test loss: 5.961119651794434\n",
      "h: 76 | epoch: 53, train loss: 8.638187408447266, test loss: 5.943478584289551\n",
      "h: 76 | epoch: 54, train loss: 8.628981590270996, test loss: 5.927175045013428\n",
      "h: 76 | epoch: 55, train loss: 8.62073802947998, test loss: 5.912096977233887\n",
      "h: 76 | epoch: 56, train loss: 8.613351821899414, test loss: 5.898139476776123\n",
      "h: 76 | epoch: 57, train loss: 8.60673713684082, test loss: 5.8852105140686035\n",
      "h: 76 | epoch: 58, train loss: 8.600809097290039, test loss: 5.873225212097168\n",
      "h: 76 | epoch: 59, train loss: 8.595499038696289, test loss: 5.862104892730713\n",
      "h: 76 | epoch: 60, train loss: 8.590742111206055, test loss: 5.851780891418457\n",
      "h: 76 | epoch: 61, train loss: 8.586481094360352, test loss: 5.842188835144043\n",
      "h: 76 | epoch: 62, train loss: 8.582662582397461, test loss: 5.833266735076904\n",
      "h: 76 | epoch: 63, train loss: 8.579241752624512, test loss: 5.824967384338379\n",
      "h: 76 | epoch: 64, train loss: 8.576177597045898, test loss: 5.817237854003906\n",
      "h: 76 | epoch: 65, train loss: 8.573429107666016, test loss: 5.8100361824035645\n",
      "h: 76 | epoch: 66, train loss: 8.570966720581055, test loss: 5.803319454193115\n",
      "h: 76 | epoch: 67, train loss: 8.568760871887207, test loss: 5.797053337097168\n",
      "h: 76 | epoch: 68, train loss: 8.56678295135498, test loss: 5.791201591491699\n",
      "h: 76 | epoch: 69, train loss: 8.565011024475098, test loss: 5.785735130310059\n",
      "h: 76 | epoch: 70, train loss: 8.563421249389648, test loss: 5.780625343322754\n",
      "h: 76 | epoch: 71, train loss: 8.561997413635254, test loss: 5.775846004486084\n",
      "h: 76 | epoch: 72, train loss: 8.56071949005127, test loss: 5.771370887756348\n",
      "h: 76 | epoch: 73, train loss: 8.559575080871582, test loss: 5.767180442810059\n",
      "h: 76 | epoch: 74, train loss: 8.558547973632812, test loss: 5.763253211975098\n",
      "h: 76 | epoch: 75, train loss: 8.55762767791748, test loss: 5.7595720291137695\n",
      "h: 76 | epoch: 76, train loss: 8.556801795959473, test loss: 5.756119728088379\n",
      "h: 76 | epoch: 77, train loss: 8.556060791015625, test loss: 5.752878189086914\n",
      "h: 76 | epoch: 78, train loss: 8.555397033691406, test loss: 5.749835014343262\n",
      "h: 76 | epoch: 79, train loss: 8.554800987243652, test loss: 5.74697732925415\n",
      "h: 76 | epoch: 80, train loss: 8.554266929626465, test loss: 5.744290351867676\n",
      "h: 76 | epoch: 81, train loss: 8.553787231445312, test loss: 5.741764068603516\n",
      "h: 76 | epoch: 82, train loss: 8.553357124328613, test loss: 5.739388465881348\n",
      "h: 76 | epoch: 83, train loss: 8.552971839904785, test loss: 5.737153053283691\n",
      "h: 76 | epoch: 84, train loss: 8.552624702453613, test loss: 5.735047340393066\n",
      "h: 76 | epoch: 85, train loss: 8.552313804626465, test loss: 5.733066082000732\n",
      "h: 76 | epoch: 86, train loss: 8.552035331726074, test loss: 5.731200218200684\n",
      "h: 76 | epoch: 87, train loss: 8.551785469055176, test loss: 5.729439735412598\n",
      "h: 76 | epoch: 88, train loss: 8.55156135559082, test loss: 5.727781772613525\n",
      "h: 76 | epoch: 89, train loss: 8.551358222961426, test loss: 5.726218223571777\n",
      "h: 76 | epoch: 90, train loss: 8.551177024841309, test loss: 5.7247419357299805\n",
      "h: 76 | epoch: 91, train loss: 8.55101490020752, test loss: 5.7233500480651855\n",
      "h: 76 | epoch: 92, train loss: 8.55086898803711, test loss: 5.722036361694336\n",
      "h: 76 | epoch: 93, train loss: 8.550737380981445, test loss: 5.720795631408691\n",
      "h: 76 | epoch: 94, train loss: 8.550619125366211, test loss: 5.7196245193481445\n",
      "h: 76 | epoch: 95, train loss: 8.55051326751709, test loss: 5.718518257141113\n",
      "h: 76 | epoch: 96, train loss: 8.550418853759766, test loss: 5.717474460601807\n",
      "h: 76 | epoch: 97, train loss: 8.550333023071289, test loss: 5.716485977172852\n",
      "h: 76 | epoch: 98, train loss: 8.550256729125977, test loss: 5.715552806854248\n",
      "h: 76 | epoch: 99, train loss: 8.550188064575195, test loss: 5.714670181274414\n",
      "h: 77 | epoch: 0, train loss: 45.212669372558594, test loss: 38.440460205078125\n",
      "h: 77 | epoch: 1, train loss: 41.60983657836914, test loss: 35.327720642089844\n",
      "h: 77 | epoch: 2, train loss: 38.36774444580078, test loss: 32.52164077758789\n",
      "h: 77 | epoch: 3, train loss: 35.448089599609375, test loss: 29.990060806274414\n",
      "h: 77 | epoch: 4, train loss: 32.817161560058594, test loss: 27.704599380493164\n",
      "h: 77 | epoch: 5, train loss: 30.445215225219727, test loss: 25.640146255493164\n",
      "h: 77 | epoch: 6, train loss: 28.305883407592773, test loss: 23.7744083404541\n",
      "h: 77 | epoch: 7, train loss: 26.375757217407227, test loss: 22.087541580200195\n",
      "h: 77 | epoch: 8, train loss: 24.633953094482422, test loss: 20.56184196472168\n",
      "h: 77 | epoch: 9, train loss: 23.06182098388672, test loss: 19.181468963623047\n",
      "h: 77 | epoch: 10, train loss: 21.642671585083008, test loss: 17.93223762512207\n",
      "h: 77 | epoch: 11, train loss: 20.36152458190918, test loss: 16.80140495300293\n",
      "h: 77 | epoch: 12, train loss: 19.204927444458008, test loss: 15.777532577514648\n",
      "h: 77 | epoch: 13, train loss: 18.160778045654297, test loss: 14.850321769714355\n",
      "h: 77 | epoch: 14, train loss: 17.21817398071289, test loss: 14.010488510131836\n",
      "h: 77 | epoch: 15, train loss: 16.36728858947754, test loss: 13.249674797058105\n",
      "h: 77 | epoch: 16, train loss: 15.59925651550293, test loss: 12.560328483581543\n",
      "h: 77 | epoch: 17, train loss: 14.906074523925781, test loss: 11.935637474060059\n",
      "h: 77 | epoch: 18, train loss: 14.280509948730469, test loss: 11.369449615478516\n",
      "h: 77 | epoch: 19, train loss: 13.716028213500977, test loss: 10.85619831085205\n",
      "h: 77 | epoch: 20, train loss: 13.206727981567383, test loss: 10.390859603881836\n",
      "h: 77 | epoch: 21, train loss: 12.74726676940918, test loss: 9.968889236450195\n",
      "h: 77 | epoch: 22, train loss: 12.33282470703125, test loss: 9.586175918579102\n",
      "h: 77 | epoch: 23, train loss: 11.959030151367188, test loss: 9.23900032043457\n",
      "h: 77 | epoch: 24, train loss: 11.621941566467285, test loss: 8.924001693725586\n",
      "h: 77 | epoch: 25, train loss: 11.317988395690918, test loss: 8.638134956359863\n",
      "h: 77 | epoch: 26, train loss: 11.04394817352295, test loss: 8.378644943237305\n",
      "h: 77 | epoch: 27, train loss: 10.796902656555176, test loss: 8.143047332763672\n",
      "h: 77 | epoch: 28, train loss: 10.574216842651367, test loss: 7.929085731506348\n",
      "h: 77 | epoch: 29, train loss: 10.373510360717773, test loss: 7.734723091125488\n",
      "h: 77 | epoch: 30, train loss: 10.192631721496582, test loss: 7.558112144470215\n",
      "h: 77 | epoch: 31, train loss: 10.029634475708008, test loss: 7.397585868835449\n",
      "h: 77 | epoch: 32, train loss: 9.882765769958496, test loss: 7.251636505126953\n",
      "h: 77 | epoch: 33, train loss: 9.750439643859863, test loss: 7.118894100189209\n",
      "h: 77 | epoch: 34, train loss: 9.631223678588867, test loss: 6.998126029968262\n",
      "h: 77 | epoch: 35, train loss: 9.523826599121094, test loss: 6.888208866119385\n",
      "h: 77 | epoch: 36, train loss: 9.427083015441895, test loss: 6.788135528564453\n",
      "h: 77 | epoch: 37, train loss: 9.33993911743164, test loss: 6.696990013122559\n",
      "h: 77 | epoch: 38, train loss: 9.261446952819824, test loss: 6.6139397621154785\n",
      "h: 77 | epoch: 39, train loss: 9.190752029418945, test loss: 6.538237571716309\n",
      "h: 77 | epoch: 40, train loss: 9.12707805633545, test loss: 6.469204902648926\n",
      "h: 77 | epoch: 41, train loss: 9.069732666015625, test loss: 6.406225681304932\n",
      "h: 77 | epoch: 42, train loss: 9.018087387084961, test loss: 6.348744869232178\n",
      "h: 77 | epoch: 43, train loss: 8.971575736999512, test loss: 6.296258449554443\n",
      "h: 77 | epoch: 44, train loss: 8.929689407348633, test loss: 6.24830961227417\n",
      "h: 77 | epoch: 45, train loss: 8.891966819763184, test loss: 6.20448637008667\n",
      "h: 77 | epoch: 46, train loss: 8.857995986938477, test loss: 6.164413928985596\n",
      "h: 77 | epoch: 47, train loss: 8.827402114868164, test loss: 6.127753734588623\n",
      "h: 77 | epoch: 48, train loss: 8.799851417541504, test loss: 6.094198703765869\n",
      "h: 77 | epoch: 49, train loss: 8.77503776550293, test loss: 6.063468933105469\n",
      "h: 77 | epoch: 50, train loss: 8.752692222595215, test loss: 6.035313606262207\n",
      "h: 77 | epoch: 51, train loss: 8.73256778717041, test loss: 6.009503364562988\n",
      "h: 77 | epoch: 52, train loss: 8.714442253112793, test loss: 5.985829830169678\n",
      "h: 77 | epoch: 53, train loss: 8.698119163513184, test loss: 5.96410608291626\n",
      "h: 77 | epoch: 54, train loss: 8.683415412902832, test loss: 5.944157600402832\n",
      "h: 77 | epoch: 55, train loss: 8.670173645019531, test loss: 5.925833225250244\n",
      "h: 77 | epoch: 56, train loss: 8.658246040344238, test loss: 5.908989429473877\n",
      "h: 77 | epoch: 57, train loss: 8.647501945495605, test loss: 5.893499374389648\n",
      "h: 77 | epoch: 58, train loss: 8.637824058532715, test loss: 5.879245281219482\n",
      "h: 77 | epoch: 59, train loss: 8.629106521606445, test loss: 5.866122722625732\n",
      "h: 77 | epoch: 60, train loss: 8.621252059936523, test loss: 5.854034900665283\n",
      "h: 77 | epoch: 61, train loss: 8.614176750183105, test loss: 5.842892646789551\n",
      "h: 77 | epoch: 62, train loss: 8.607802391052246, test loss: 5.8326191902160645\n",
      "h: 77 | epoch: 63, train loss: 8.602060317993164, test loss: 5.823138236999512\n",
      "h: 77 | epoch: 64, train loss: 8.596885681152344, test loss: 5.81438684463501\n",
      "h: 77 | epoch: 65, train loss: 8.592223167419434, test loss: 5.806302547454834\n",
      "h: 77 | epoch: 66, train loss: 8.588022232055664, test loss: 5.798830986022949\n",
      "h: 77 | epoch: 67, train loss: 8.584238052368164, test loss: 5.7919230461120605\n",
      "h: 77 | epoch: 68, train loss: 8.580826759338379, test loss: 5.785531997680664\n",
      "h: 77 | epoch: 69, train loss: 8.577751159667969, test loss: 5.7796149253845215\n",
      "h: 77 | epoch: 70, train loss: 8.574979782104492, test loss: 5.774135112762451\n",
      "h: 77 | epoch: 71, train loss: 8.57248306274414, test loss: 5.769057750701904\n",
      "h: 77 | epoch: 72, train loss: 8.570232391357422, test loss: 5.764350414276123\n",
      "h: 77 | epoch: 73, train loss: 8.568203926086426, test loss: 5.75998592376709\n",
      "h: 77 | epoch: 74, train loss: 8.566373825073242, test loss: 5.755932807922363\n",
      "h: 77 | epoch: 75, train loss: 8.564724922180176, test loss: 5.752171039581299\n",
      "h: 77 | epoch: 76, train loss: 8.563240051269531, test loss: 5.74867582321167\n",
      "h: 77 | epoch: 77, train loss: 8.561899185180664, test loss: 5.745427131652832\n",
      "h: 77 | epoch: 78, train loss: 8.560691833496094, test loss: 5.742407321929932\n",
      "h: 77 | epoch: 79, train loss: 8.559602737426758, test loss: 5.739596366882324\n",
      "h: 77 | epoch: 80, train loss: 8.558619499206543, test loss: 5.736981391906738\n",
      "h: 77 | epoch: 81, train loss: 8.557733535766602, test loss: 5.734545707702637\n",
      "h: 77 | epoch: 82, train loss: 8.55693531036377, test loss: 5.732278347015381\n",
      "h: 77 | epoch: 83, train loss: 8.556215286254883, test loss: 5.73016357421875\n",
      "h: 77 | epoch: 84, train loss: 8.555564880371094, test loss: 5.728193283081055\n",
      "h: 77 | epoch: 85, train loss: 8.55497932434082, test loss: 5.726355075836182\n",
      "h: 77 | epoch: 86, train loss: 8.554451942443848, test loss: 5.724640369415283\n",
      "h: 77 | epoch: 87, train loss: 8.553975105285645, test loss: 5.723038196563721\n",
      "h: 77 | epoch: 88, train loss: 8.553545951843262, test loss: 5.721542835235596\n",
      "h: 77 | epoch: 89, train loss: 8.553156852722168, test loss: 5.720145225524902\n",
      "h: 77 | epoch: 90, train loss: 8.552807807922363, test loss: 5.718838691711426\n",
      "h: 77 | epoch: 91, train loss: 8.552492141723633, test loss: 5.717617988586426\n",
      "h: 77 | epoch: 92, train loss: 8.552206993103027, test loss: 5.7164764404296875\n",
      "h: 77 | epoch: 93, train loss: 8.551950454711914, test loss: 5.715408802032471\n",
      "h: 77 | epoch: 94, train loss: 8.551718711853027, test loss: 5.714407444000244\n",
      "h: 77 | epoch: 95, train loss: 8.551509857177734, test loss: 5.713471412658691\n",
      "h: 77 | epoch: 96, train loss: 8.551321029663086, test loss: 5.712594985961914\n",
      "h: 77 | epoch: 97, train loss: 8.55115032196045, test loss: 5.711773872375488\n",
      "h: 77 | epoch: 98, train loss: 8.550997734069824, test loss: 5.711003303527832\n",
      "h: 77 | epoch: 99, train loss: 8.550858497619629, test loss: 5.710280895233154\n",
      "h: 78 | epoch: 0, train loss: 43.197410583496094, test loss: 36.04570388793945\n",
      "h: 78 | epoch: 1, train loss: 39.76622772216797, test loss: 33.179439544677734\n",
      "h: 78 | epoch: 2, train loss: 36.673377990722656, test loss: 30.587600708007812\n",
      "h: 78 | epoch: 3, train loss: 33.884376525878906, test loss: 28.242908477783203\n",
      "h: 78 | epoch: 4, train loss: 31.3686466217041, test loss: 26.121097564697266\n",
      "h: 78 | epoch: 5, train loss: 29.09902000427246, test loss: 24.20050811767578\n",
      "h: 78 | epoch: 6, train loss: 27.051233291625977, test loss: 22.46175765991211\n",
      "h: 78 | epoch: 7, train loss: 25.203598022460938, test loss: 20.88743782043457\n",
      "h: 78 | epoch: 8, train loss: 23.53665542602539, test loss: 19.4619083404541\n",
      "h: 78 | epoch: 9, train loss: 22.032915115356445, test loss: 18.17106819152832\n",
      "h: 78 | epoch: 10, train loss: 20.676626205444336, test loss: 17.002193450927734\n",
      "h: 78 | epoch: 11, train loss: 19.45359230041504, test loss: 15.943784713745117\n",
      "h: 78 | epoch: 12, train loss: 18.350988388061523, test loss: 14.985440254211426\n",
      "h: 78 | epoch: 13, train loss: 17.357227325439453, test loss: 14.11775016784668\n",
      "h: 78 | epoch: 14, train loss: 16.461833953857422, test loss: 13.332176208496094\n",
      "h: 78 | epoch: 15, train loss: 15.655316352844238, test loss: 12.620986938476562\n",
      "h: 78 | epoch: 16, train loss: 14.929097175598145, test loss: 11.977179527282715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 78 | epoch: 17, train loss: 14.275396347045898, test loss: 11.394391059875488\n",
      "h: 78 | epoch: 18, train loss: 13.687176704406738, test loss: 10.866857528686523\n",
      "h: 78 | epoch: 19, train loss: 13.158060073852539, test loss: 10.389352798461914\n",
      "h: 78 | epoch: 20, train loss: 12.682271957397461, test loss: 9.957130432128906\n",
      "h: 78 | epoch: 21, train loss: 12.254589080810547, test loss: 9.565893173217773\n",
      "h: 78 | epoch: 22, train loss: 11.870277404785156, test loss: 9.21174430847168\n",
      "h: 78 | epoch: 23, train loss: 11.525055885314941, test loss: 8.89115047454834\n",
      "h: 78 | epoch: 24, train loss: 11.215049743652344, test loss: 8.600908279418945\n",
      "h: 78 | epoch: 25, train loss: 10.936761856079102, test loss: 8.338118553161621\n",
      "h: 78 | epoch: 26, train loss: 10.68702507019043, test loss: 8.100152969360352\n",
      "h: 78 | epoch: 27, train loss: 10.46297836303711, test loss: 7.884634971618652\n",
      "h: 78 | epoch: 28, train loss: 10.262042999267578, test loss: 7.689411163330078\n",
      "h: 78 | epoch: 29, train loss: 10.08188533782959, test loss: 7.512535095214844\n",
      "h: 78 | epoch: 30, train loss: 9.920405387878418, test loss: 7.352242469787598\n",
      "h: 78 | epoch: 31, train loss: 9.775701522827148, test loss: 7.206942081451416\n",
      "h: 78 | epoch: 32, train loss: 9.646069526672363, test loss: 7.075192451477051\n",
      "h: 78 | epoch: 33, train loss: 9.52996826171875, test loss: 6.955693244934082\n",
      "h: 78 | epoch: 34, train loss: 9.42601203918457, test loss: 6.847267150878906\n",
      "h: 78 | epoch: 35, train loss: 9.332950592041016, test loss: 6.748848915100098\n",
      "h: 78 | epoch: 36, train loss: 9.24966049194336, test loss: 6.659480094909668\n",
      "h: 78 | epoch: 37, train loss: 9.175132751464844, test loss: 6.578296661376953\n",
      "h: 78 | epoch: 38, train loss: 9.108461380004883, test loss: 6.504511833190918\n",
      "h: 78 | epoch: 39, train loss: 9.048827171325684, test loss: 6.437422752380371\n",
      "h: 78 | epoch: 40, train loss: 8.995500564575195, test loss: 6.376387596130371\n",
      "h: 78 | epoch: 41, train loss: 8.947818756103516, test loss: 6.320831298828125\n",
      "h: 78 | epoch: 42, train loss: 8.905195236206055, test loss: 6.270236015319824\n",
      "h: 78 | epoch: 43, train loss: 8.867097854614258, test loss: 6.224131107330322\n",
      "h: 78 | epoch: 44, train loss: 8.833051681518555, test loss: 6.182093143463135\n",
      "h: 78 | epoch: 45, train loss: 8.802632331848145, test loss: 6.143739700317383\n",
      "h: 78 | epoch: 46, train loss: 8.7754545211792, test loss: 6.108724594116211\n",
      "h: 78 | epoch: 47, train loss: 8.751177787780762, test loss: 6.076739311218262\n",
      "h: 78 | epoch: 48, train loss: 8.72949504852295, test loss: 6.047499656677246\n",
      "h: 78 | epoch: 49, train loss: 8.71013069152832, test loss: 6.020752906799316\n",
      "h: 78 | epoch: 50, train loss: 8.692839622497559, test loss: 5.996269226074219\n",
      "h: 78 | epoch: 51, train loss: 8.677400588989258, test loss: 5.973840713500977\n",
      "h: 78 | epoch: 52, train loss: 8.663618087768555, test loss: 5.953282356262207\n",
      "h: 78 | epoch: 53, train loss: 8.651315689086914, test loss: 5.934422492980957\n",
      "h: 78 | epoch: 54, train loss: 8.640334129333496, test loss: 5.917107105255127\n",
      "h: 78 | epoch: 55, train loss: 8.630533218383789, test loss: 5.901201248168945\n",
      "h: 78 | epoch: 56, train loss: 8.621788024902344, test loss: 5.886576175689697\n",
      "h: 78 | epoch: 57, train loss: 8.613981246948242, test loss: 5.87312126159668\n",
      "h: 78 | epoch: 58, train loss: 8.607017517089844, test loss: 5.8607330322265625\n",
      "h: 78 | epoch: 59, train loss: 8.60080337524414, test loss: 5.849318027496338\n",
      "h: 78 | epoch: 60, train loss: 8.595259666442871, test loss: 5.838791370391846\n",
      "h: 78 | epoch: 61, train loss: 8.590314865112305, test loss: 5.829078197479248\n",
      "h: 78 | epoch: 62, train loss: 8.58590316772461, test loss: 5.8201093673706055\n",
      "h: 78 | epoch: 63, train loss: 8.581968307495117, test loss: 5.8118181228637695\n",
      "h: 78 | epoch: 64, train loss: 8.578458786010742, test loss: 5.8041534423828125\n",
      "h: 78 | epoch: 65, train loss: 8.57532787322998, test loss: 5.797058582305908\n",
      "h: 78 | epoch: 66, train loss: 8.57253646850586, test loss: 5.790488243103027\n",
      "h: 78 | epoch: 67, train loss: 8.570045471191406, test loss: 5.784399032592773\n",
      "h: 78 | epoch: 68, train loss: 8.567825317382812, test loss: 5.778753280639648\n",
      "h: 78 | epoch: 69, train loss: 8.565845489501953, test loss: 5.773514747619629\n",
      "h: 78 | epoch: 70, train loss: 8.564081192016602, test loss: 5.768648147583008\n",
      "h: 78 | epoch: 71, train loss: 8.562505722045898, test loss: 5.764127731323242\n",
      "h: 78 | epoch: 72, train loss: 8.561101913452148, test loss: 5.759923934936523\n",
      "h: 78 | epoch: 73, train loss: 8.559850692749023, test loss: 5.756015300750732\n",
      "h: 78 | epoch: 74, train loss: 8.558733940124512, test loss: 5.75237512588501\n",
      "h: 78 | epoch: 75, train loss: 8.557740211486816, test loss: 5.748985290527344\n",
      "h: 78 | epoch: 76, train loss: 8.556852340698242, test loss: 5.7458271980285645\n",
      "h: 78 | epoch: 77, train loss: 8.556062698364258, test loss: 5.7428812980651855\n",
      "h: 78 | epoch: 78, train loss: 8.555356979370117, test loss: 5.740133762359619\n",
      "h: 78 | epoch: 79, train loss: 8.554728507995605, test loss: 5.737569808959961\n",
      "h: 78 | epoch: 80, train loss: 8.554168701171875, test loss: 5.735175132751465\n",
      "h: 78 | epoch: 81, train loss: 8.553668975830078, test loss: 5.732937812805176\n",
      "h: 78 | epoch: 82, train loss: 8.553223609924316, test loss: 5.730845928192139\n",
      "h: 78 | epoch: 83, train loss: 8.552825927734375, test loss: 5.728890895843506\n",
      "h: 78 | epoch: 84, train loss: 8.552473068237305, test loss: 5.7270612716674805\n",
      "h: 78 | epoch: 85, train loss: 8.55215835571289, test loss: 5.725348949432373\n",
      "h: 78 | epoch: 86, train loss: 8.55187702178955, test loss: 5.723746299743652\n",
      "h: 78 | epoch: 87, train loss: 8.551626205444336, test loss: 5.722245693206787\n",
      "h: 78 | epoch: 88, train loss: 8.55140209197998, test loss: 5.720839500427246\n",
      "h: 78 | epoch: 89, train loss: 8.551204681396484, test loss: 5.719521522521973\n",
      "h: 78 | epoch: 90, train loss: 8.551027297973633, test loss: 5.718286514282227\n",
      "h: 78 | epoch: 91, train loss: 8.550868034362793, test loss: 5.717127799987793\n",
      "h: 78 | epoch: 92, train loss: 8.550727844238281, test loss: 5.716040134429932\n",
      "h: 78 | epoch: 93, train loss: 8.550601959228516, test loss: 5.715019702911377\n",
      "h: 78 | epoch: 94, train loss: 8.550490379333496, test loss: 5.714062690734863\n",
      "h: 78 | epoch: 95, train loss: 8.550390243530273, test loss: 5.713162899017334\n",
      "h: 78 | epoch: 96, train loss: 8.550301551818848, test loss: 5.712319850921631\n",
      "h: 78 | epoch: 97, train loss: 8.550222396850586, test loss: 5.711525917053223\n",
      "h: 78 | epoch: 98, train loss: 8.550151824951172, test loss: 5.710780620574951\n",
      "h: 78 | epoch: 99, train loss: 8.550088882446289, test loss: 5.710080146789551\n",
      "h: 79 | epoch: 0, train loss: 46.91367721557617, test loss: 39.43975830078125\n",
      "h: 79 | epoch: 1, train loss: 42.60804748535156, test loss: 35.84893035888672\n",
      "h: 79 | epoch: 2, train loss: 38.78573989868164, test loss: 32.648006439208984\n",
      "h: 79 | epoch: 3, train loss: 35.39075469970703, test loss: 29.793045043945312\n",
      "h: 79 | epoch: 4, train loss: 32.37419509887695, test loss: 27.245508193969727\n",
      "h: 79 | epoch: 5, train loss: 29.69327735900879, test loss: 24.971513748168945\n",
      "h: 79 | epoch: 6, train loss: 27.31035804748535, test loss: 22.941146850585938\n",
      "h: 79 | epoch: 7, train loss: 25.19226837158203, test loss: 21.127948760986328\n",
      "h: 79 | epoch: 8, train loss: 23.309677124023438, test loss: 19.508445739746094\n",
      "h: 79 | epoch: 9, train loss: 21.636600494384766, test loss: 18.06179428100586\n",
      "h: 79 | epoch: 10, train loss: 20.149980545043945, test loss: 16.769439697265625\n",
      "h: 79 | epoch: 11, train loss: 18.82933235168457, test loss: 15.614842414855957\n",
      "h: 79 | epoch: 12, train loss: 17.656414031982422, test loss: 14.583270072937012\n",
      "h: 79 | epoch: 13, train loss: 16.614999771118164, test loss: 13.6615629196167\n",
      "h: 79 | epoch: 14, train loss: 15.690622329711914, test loss: 12.837972640991211\n",
      "h: 79 | epoch: 15, train loss: 14.870391845703125, test loss: 12.102008819580078\n",
      "h: 79 | epoch: 16, train loss: 14.142807006835938, test loss: 11.444292068481445\n",
      "h: 79 | epoch: 17, train loss: 13.497624397277832, test loss: 10.856447219848633\n",
      "h: 79 | epoch: 18, train loss: 12.925697326660156, test loss: 10.330984115600586\n",
      "h: 79 | epoch: 19, train loss: 12.4188814163208, test loss: 9.861212730407715\n",
      "h: 79 | epoch: 20, train loss: 11.969911575317383, test loss: 9.441160202026367\n",
      "h: 79 | epoch: 21, train loss: 11.57231616973877, test loss: 9.065483093261719\n",
      "h: 79 | epoch: 22, train loss: 11.220327377319336, test loss: 8.729413986206055\n",
      "h: 79 | epoch: 23, train loss: 10.908811569213867, test loss: 8.428693771362305\n",
      "h: 79 | epoch: 24, train loss: 10.633197784423828, test loss: 8.15952205657959\n",
      "h: 79 | epoch: 25, train loss: 10.389420509338379, test loss: 7.918501853942871\n",
      "h: 79 | epoch: 26, train loss: 10.173859596252441, test loss: 7.702610969543457\n",
      "h: 79 | epoch: 27, train loss: 9.983304977416992, test loss: 7.509145259857178\n",
      "h: 79 | epoch: 28, train loss: 9.814894676208496, test loss: 7.335695743560791\n",
      "h: 79 | epoch: 29, train loss: 9.666093826293945, test loss: 7.180113315582275\n",
      "h: 79 | epoch: 30, train loss: 9.534649848937988, test loss: 7.0404839515686035\n",
      "h: 79 | epoch: 31, train loss: 9.418561935424805, test loss: 6.915098667144775\n",
      "h: 79 | epoch: 32, train loss: 9.316060066223145, test loss: 6.802438259124756\n",
      "h: 79 | epoch: 33, train loss: 9.225569725036621, test loss: 6.70114278793335\n",
      "h: 79 | epoch: 34, train loss: 9.145700454711914, test loss: 6.610005855560303\n",
      "h: 79 | epoch: 35, train loss: 9.075216293334961, test loss: 6.527947425842285\n",
      "h: 79 | epoch: 36, train loss: 9.013025283813477, test loss: 6.454010963439941\n",
      "h: 79 | epoch: 37, train loss: 8.958158493041992, test loss: 6.387337684631348\n",
      "h: 79 | epoch: 38, train loss: 8.909761428833008, test loss: 6.327167987823486\n",
      "h: 79 | epoch: 39, train loss: 8.867077827453613, test loss: 6.272818565368652\n",
      "h: 79 | epoch: 40, train loss: 8.829435348510742, test loss: 6.223688125610352\n",
      "h: 79 | epoch: 41, train loss: 8.796243667602539, test loss: 6.179233551025391\n",
      "h: 79 | epoch: 42, train loss: 8.766980171203613, test loss: 6.138973712921143\n",
      "h: 79 | epoch: 43, train loss: 8.741179466247559, test loss: 6.102478981018066\n",
      "h: 79 | epoch: 44, train loss: 8.718437194824219, test loss: 6.069365501403809\n",
      "h: 79 | epoch: 45, train loss: 8.698390007019043, test loss: 6.039292335510254\n",
      "h: 79 | epoch: 46, train loss: 8.680723190307617, test loss: 6.011950492858887\n",
      "h: 79 | epoch: 47, train loss: 8.66515064239502, test loss: 5.987071990966797\n",
      "h: 79 | epoch: 48, train loss: 8.651426315307617, test loss: 5.964409828186035\n",
      "h: 79 | epoch: 49, train loss: 8.639330863952637, test loss: 5.943745136260986\n",
      "h: 79 | epoch: 50, train loss: 8.628673553466797, test loss: 5.9248857498168945\n",
      "h: 79 | epoch: 51, train loss: 8.619282722473145, test loss: 5.907653331756592\n",
      "h: 79 | epoch: 52, train loss: 8.611006736755371, test loss: 5.8918962478637695\n",
      "h: 79 | epoch: 53, train loss: 8.603715896606445, test loss: 5.877469062805176\n",
      "h: 79 | epoch: 54, train loss: 8.597289085388184, test loss: 5.864249229431152\n",
      "h: 79 | epoch: 55, train loss: 8.59162712097168, test loss: 5.85212516784668\n",
      "h: 79 | epoch: 56, train loss: 8.586639404296875, test loss: 5.840993404388428\n",
      "h: 79 | epoch: 57, train loss: 8.582243919372559, test loss: 5.83076286315918\n",
      "h: 79 | epoch: 58, train loss: 8.578371047973633, test loss: 5.821352005004883\n",
      "h: 79 | epoch: 59, train loss: 8.574957847595215, test loss: 5.81268835067749\n",
      "h: 79 | epoch: 60, train loss: 8.571950912475586, test loss: 5.804704189300537\n",
      "h: 79 | epoch: 61, train loss: 8.569300651550293, test loss: 5.797339916229248\n",
      "h: 79 | epoch: 62, train loss: 8.566965103149414, test loss: 5.790542125701904\n",
      "h: 79 | epoch: 63, train loss: 8.564908981323242, test loss: 5.7842607498168945\n",
      "h: 79 | epoch: 64, train loss: 8.563095092773438, test loss: 5.778453826904297\n",
      "h: 79 | epoch: 65, train loss: 8.56149673461914, test loss: 5.773078918457031\n",
      "h: 79 | epoch: 66, train loss: 8.560087203979492, test loss: 5.768102645874023\n",
      "h: 79 | epoch: 67, train loss: 8.558846473693848, test loss: 5.763489246368408\n",
      "h: 79 | epoch: 68, train loss: 8.55775260925293, test loss: 5.759211540222168\n",
      "h: 79 | epoch: 69, train loss: 8.556787490844727, test loss: 5.7552409172058105\n",
      "h: 79 | epoch: 70, train loss: 8.555936813354492, test loss: 5.751552581787109\n",
      "h: 79 | epoch: 71, train loss: 8.555188179016113, test loss: 5.7481255531311035\n",
      "h: 79 | epoch: 72, train loss: 8.554527282714844, test loss: 5.744938850402832\n",
      "h: 79 | epoch: 73, train loss: 8.553943634033203, test loss: 5.741973876953125\n",
      "h: 79 | epoch: 74, train loss: 8.553430557250977, test loss: 5.739212512969971\n",
      "h: 79 | epoch: 75, train loss: 8.552977561950684, test loss: 5.7366414070129395\n",
      "h: 79 | epoch: 76, train loss: 8.552577018737793, test loss: 5.734243869781494\n",
      "h: 79 | epoch: 77, train loss: 8.552225112915039, test loss: 5.732008934020996\n",
      "h: 79 | epoch: 78, train loss: 8.551915168762207, test loss: 5.729924201965332\n",
      "h: 79 | epoch: 79, train loss: 8.551640510559082, test loss: 5.727977275848389\n",
      "h: 79 | epoch: 80, train loss: 8.551399230957031, test loss: 5.726160526275635\n",
      "h: 79 | epoch: 81, train loss: 8.551185607910156, test loss: 5.724462509155273\n",
      "h: 79 | epoch: 82, train loss: 8.550996780395508, test loss: 5.72287654876709\n",
      "h: 79 | epoch: 83, train loss: 8.550830841064453, test loss: 5.7213921546936035\n",
      "h: 79 | epoch: 84, train loss: 8.550683975219727, test loss: 5.720004081726074\n",
      "h: 79 | epoch: 85, train loss: 8.550555229187012, test loss: 5.718705654144287\n",
      "h: 79 | epoch: 86, train loss: 8.550440788269043, test loss: 5.717490196228027\n",
      "h: 79 | epoch: 87, train loss: 8.55034065246582, test loss: 5.716353416442871\n",
      "h: 79 | epoch: 88, train loss: 8.550251007080078, test loss: 5.7152886390686035\n",
      "h: 79 | epoch: 89, train loss: 8.550172805786133, test loss: 5.714289665222168\n",
      "h: 79 | epoch: 90, train loss: 8.550102233886719, test loss: 5.713353157043457\n",
      "h: 79 | epoch: 91, train loss: 8.550042152404785, test loss: 5.712477684020996\n",
      "h: 79 | epoch: 92, train loss: 8.549985885620117, test loss: 5.711655616760254\n",
      "h: 79 | epoch: 93, train loss: 8.549939155578613, test loss: 5.7108845710754395\n",
      "h: 79 | epoch: 94, train loss: 8.549897193908691, test loss: 5.710162162780762\n",
      "h: 79 | epoch: 95, train loss: 8.549860000610352, test loss: 5.7094831466674805\n",
      "h: 79 | epoch: 96, train loss: 8.549826622009277, test loss: 5.708847522735596\n",
      "h: 79 | epoch: 97, train loss: 8.549798011779785, test loss: 5.708249568939209\n",
      "h: 79 | epoch: 98, train loss: 8.549772262573242, test loss: 5.707688808441162\n",
      "h: 79 | epoch: 99, train loss: 8.549748420715332, test loss: 5.707161903381348\n",
      "h: 80 | epoch: 0, train loss: 36.32746124267578, test loss: 30.684261322021484\n",
      "h: 80 | epoch: 1, train loss: 33.4336051940918, test loss: 28.213558197021484\n",
      "h: 80 | epoch: 2, train loss: 30.838220596313477, test loss: 25.990169525146484\n",
      "h: 80 | epoch: 3, train loss: 28.510095596313477, test loss: 23.98879623413086\n",
      "h: 80 | epoch: 4, train loss: 26.421527862548828, test loss: 22.186901092529297\n",
      "h: 80 | epoch: 5, train loss: 24.5478515625, test loss: 20.56435775756836\n",
      "h: 80 | epoch: 6, train loss: 22.8670597076416, test loss: 19.10316276550293\n",
      "h: 80 | epoch: 7, train loss: 21.359479904174805, test loss: 17.78719711303711\n",
      "h: 80 | epoch: 8, train loss: 20.007492065429688, test loss: 16.60198402404785\n",
      "h: 80 | epoch: 9, train loss: 18.79529571533203, test loss: 15.534510612487793\n",
      "h: 80 | epoch: 10, train loss: 17.708698272705078, test loss: 14.573080062866211\n",
      "h: 80 | epoch: 11, train loss: 16.734956741333008, test loss: 13.707165718078613\n",
      "h: 80 | epoch: 12, train loss: 15.86259937286377, test loss: 12.927279472351074\n",
      "h: 80 | epoch: 13, train loss: 15.081311225891113, test loss: 12.22487735748291\n",
      "h: 80 | epoch: 14, train loss: 14.381808280944824, test loss: 11.59225845336914\n",
      "h: 80 | epoch: 15, train loss: 13.755739212036133, test loss: 11.022480964660645\n",
      "h: 80 | epoch: 16, train loss: 13.195574760437012, test loss: 10.50929069519043\n",
      "h: 80 | epoch: 17, train loss: 12.694551467895508, test loss: 10.047037124633789\n",
      "h: 80 | epoch: 18, train loss: 12.246572494506836, test loss: 9.630643844604492\n",
      "h: 80 | epoch: 19, train loss: 11.846155166625977, test loss: 9.255525588989258\n",
      "h: 80 | epoch: 20, train loss: 11.488367080688477, test loss: 8.917551040649414\n",
      "h: 80 | epoch: 21, train loss: 11.168769836425781, test loss: 8.613001823425293\n",
      "h: 80 | epoch: 22, train loss: 10.883382797241211, test loss: 8.338529586791992\n",
      "h: 80 | epoch: 23, train loss: 10.628621101379395, test loss: 8.09111213684082\n",
      "h: 80 | epoch: 24, train loss: 10.401264190673828, test loss: 7.868034362792969\n",
      "h: 80 | epoch: 25, train loss: 10.198426246643066, test loss: 7.6668524742126465\n",
      "h: 80 | epoch: 26, train loss: 10.017512321472168, test loss: 7.485365390777588\n",
      "h: 80 | epoch: 27, train loss: 9.856195449829102, test loss: 7.321597099304199\n",
      "h: 80 | epoch: 28, train loss: 9.712395668029785, test loss: 7.173762321472168\n",
      "h: 80 | epoch: 29, train loss: 9.584238052368164, test loss: 7.040266990661621\n",
      "h: 80 | epoch: 30, train loss: 9.470050811767578, test loss: 6.919668674468994\n",
      "h: 80 | epoch: 31, train loss: 9.36833381652832, test loss: 6.810675144195557\n",
      "h: 80 | epoch: 32, train loss: 9.277748107910156, test loss: 6.712125301361084\n",
      "h: 80 | epoch: 33, train loss: 9.197091102600098, test loss: 6.622975826263428\n",
      "h: 80 | epoch: 34, train loss: 9.125288009643555, test loss: 6.5422868728637695\n",
      "h: 80 | epoch: 35, train loss: 9.061380386352539, test loss: 6.46921443939209\n",
      "h: 80 | epoch: 36, train loss: 9.004510879516602, test loss: 6.403005123138428\n",
      "h: 80 | epoch: 37, train loss: 8.953913688659668, test loss: 6.342978000640869\n",
      "h: 80 | epoch: 38, train loss: 8.908904075622559, test loss: 6.288518905639648\n",
      "h: 80 | epoch: 39, train loss: 8.868870735168457, test loss: 6.239083290100098\n",
      "h: 80 | epoch: 40, train loss: 8.833269119262695, test loss: 6.194177627563477\n",
      "h: 80 | epoch: 41, train loss: 8.801613807678223, test loss: 6.153358459472656\n",
      "h: 80 | epoch: 42, train loss: 8.773470878601074, test loss: 6.1162261962890625\n",
      "h: 80 | epoch: 43, train loss: 8.748453140258789, test loss: 6.082425117492676\n",
      "h: 80 | epoch: 44, train loss: 8.726218223571777, test loss: 6.051633358001709\n",
      "h: 80 | epoch: 45, train loss: 8.706457138061523, test loss: 6.023561477661133\n",
      "h: 80 | epoch: 46, train loss: 8.688896179199219, test loss: 5.997950553894043\n",
      "h: 80 | epoch: 47, train loss: 8.673293113708496, test loss: 5.974566459655762\n",
      "h: 80 | epoch: 48, train loss: 8.659431457519531, test loss: 5.953197956085205\n",
      "h: 80 | epoch: 49, train loss: 8.647117614746094, test loss: 5.933656692504883\n",
      "h: 80 | epoch: 50, train loss: 8.636177062988281, test loss: 5.915772438049316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 80 | epoch: 51, train loss: 8.626461029052734, test loss: 5.899392127990723\n",
      "h: 80 | epoch: 52, train loss: 8.617833137512207, test loss: 5.884375095367432\n",
      "h: 80 | epoch: 53, train loss: 8.610169410705566, test loss: 5.870599746704102\n",
      "h: 80 | epoch: 54, train loss: 8.603363037109375, test loss: 5.857950687408447\n",
      "h: 80 | epoch: 55, train loss: 8.597320556640625, test loss: 5.846328258514404\n",
      "h: 80 | epoch: 56, train loss: 8.591955184936523, test loss: 5.835640907287598\n",
      "h: 80 | epoch: 57, train loss: 8.587191581726074, test loss: 5.825804710388184\n",
      "h: 80 | epoch: 58, train loss: 8.582962036132812, test loss: 5.816743850708008\n",
      "h: 80 | epoch: 59, train loss: 8.579206466674805, test loss: 5.808393478393555\n",
      "h: 80 | epoch: 60, train loss: 8.575873374938965, test loss: 5.800690650939941\n",
      "h: 80 | epoch: 61, train loss: 8.572915077209473, test loss: 5.793580055236816\n",
      "h: 80 | epoch: 62, train loss: 8.570287704467773, test loss: 5.78701114654541\n",
      "h: 80 | epoch: 63, train loss: 8.567956924438477, test loss: 5.780939102172852\n",
      "h: 80 | epoch: 64, train loss: 8.565887451171875, test loss: 5.775321006774902\n",
      "h: 80 | epoch: 65, train loss: 8.564050674438477, test loss: 5.770120620727539\n",
      "h: 80 | epoch: 66, train loss: 8.562420845031738, test loss: 5.7653021812438965\n",
      "h: 80 | epoch: 67, train loss: 8.560975074768066, test loss: 5.760837078094482\n",
      "h: 80 | epoch: 68, train loss: 8.559690475463867, test loss: 5.75669527053833\n",
      "h: 80 | epoch: 69, train loss: 8.558550834655762, test loss: 5.752850532531738\n",
      "h: 80 | epoch: 70, train loss: 8.557538986206055, test loss: 5.7492804527282715\n",
      "h: 80 | epoch: 71, train loss: 8.556642532348633, test loss: 5.745963096618652\n",
      "h: 80 | epoch: 72, train loss: 8.555846214294434, test loss: 5.742878437042236\n",
      "h: 80 | epoch: 73, train loss: 8.555139541625977, test loss: 5.740009784698486\n",
      "h: 80 | epoch: 74, train loss: 8.554511070251465, test loss: 5.737339019775391\n",
      "h: 80 | epoch: 75, train loss: 8.553956985473633, test loss: 5.734852313995361\n",
      "h: 80 | epoch: 76, train loss: 8.553462028503418, test loss: 5.732536315917969\n",
      "h: 80 | epoch: 77, train loss: 8.553023338317871, test loss: 5.730377674102783\n",
      "h: 80 | epoch: 78, train loss: 8.552635192871094, test loss: 5.728363990783691\n",
      "h: 80 | epoch: 79, train loss: 8.552289962768555, test loss: 5.726485252380371\n",
      "h: 80 | epoch: 80, train loss: 8.551984786987305, test loss: 5.724732875823975\n",
      "h: 80 | epoch: 81, train loss: 8.551712036132812, test loss: 5.723095893859863\n",
      "h: 80 | epoch: 82, train loss: 8.551470756530762, test loss: 5.721567153930664\n",
      "h: 80 | epoch: 83, train loss: 8.551257133483887, test loss: 5.7201385498046875\n",
      "h: 80 | epoch: 84, train loss: 8.551067352294922, test loss: 5.718803882598877\n",
      "h: 80 | epoch: 85, train loss: 8.550898551940918, test loss: 5.717555999755859\n",
      "h: 80 | epoch: 86, train loss: 8.550748825073242, test loss: 5.716389179229736\n",
      "h: 80 | epoch: 87, train loss: 8.550617218017578, test loss: 5.715297698974609\n",
      "h: 80 | epoch: 88, train loss: 8.550498962402344, test loss: 5.714276313781738\n",
      "h: 80 | epoch: 89, train loss: 8.550394058227539, test loss: 5.713318824768066\n",
      "h: 80 | epoch: 90, train loss: 8.550302505493164, test loss: 5.712423324584961\n",
      "h: 80 | epoch: 91, train loss: 8.550219535827637, test loss: 5.71158504486084\n",
      "h: 80 | epoch: 92, train loss: 8.550146102905273, test loss: 5.710799217224121\n",
      "h: 80 | epoch: 93, train loss: 8.550082206726074, test loss: 5.7100629806518555\n",
      "h: 80 | epoch: 94, train loss: 8.55002498626709, test loss: 5.709373474121094\n",
      "h: 80 | epoch: 95, train loss: 8.54997444152832, test loss: 5.70872688293457\n",
      "h: 80 | epoch: 96, train loss: 8.549928665161133, test loss: 5.708120346069336\n",
      "h: 80 | epoch: 97, train loss: 8.549888610839844, test loss: 5.707550525665283\n",
      "h: 80 | epoch: 98, train loss: 8.54985237121582, test loss: 5.707018852233887\n",
      "h: 80 | epoch: 99, train loss: 8.549821853637695, test loss: 5.706518650054932\n",
      "h: 81 | epoch: 0, train loss: 48.0615234375, test loss: 39.62639236450195\n",
      "h: 81 | epoch: 1, train loss: 43.023067474365234, test loss: 35.586341857910156\n",
      "h: 81 | epoch: 2, train loss: 38.639976501464844, test loss: 32.05080795288086\n",
      "h: 81 | epoch: 3, train loss: 34.8236198425293, test loss: 28.953582763671875\n",
      "h: 81 | epoch: 4, train loss: 31.49834632873535, test loss: 26.23783302307129\n",
      "h: 81 | epoch: 5, train loss: 28.599319458007812, test loss: 23.854610443115234\n",
      "h: 81 | epoch: 6, train loss: 26.07073974609375, test loss: 21.76163673400879\n",
      "h: 81 | epoch: 7, train loss: 23.86447525024414, test loss: 19.922290802001953\n",
      "h: 81 | epoch: 8, train loss: 21.938879013061523, test loss: 18.304767608642578\n",
      "h: 81 | epoch: 9, train loss: 20.25784683227539, test loss: 16.88140869140625\n",
      "h: 81 | epoch: 10, train loss: 18.790040969848633, test loss: 15.628122329711914\n",
      "h: 81 | epoch: 11, train loss: 17.508195877075195, test loss: 14.523890495300293\n",
      "h: 81 | epoch: 12, train loss: 16.388599395751953, test loss: 13.550361633300781\n",
      "h: 81 | epoch: 13, train loss: 15.410577774047852, test loss: 12.691503524780273\n",
      "h: 81 | epoch: 14, train loss: 14.556121826171875, test loss: 11.933294296264648\n",
      "h: 81 | epoch: 15, train loss: 13.809521675109863, test loss: 11.263463973999023\n",
      "h: 81 | epoch: 16, train loss: 13.157072067260742, test loss: 10.67126750946045\n",
      "h: 81 | epoch: 17, train loss: 12.58681583404541, test loss: 10.147305488586426\n",
      "h: 81 | epoch: 18, train loss: 12.088315963745117, test loss: 9.683340072631836\n",
      "h: 81 | epoch: 19, train loss: 11.652464866638184, test loss: 9.272147178649902\n",
      "h: 81 | epoch: 20, train loss: 11.27131175994873, test loss: 8.907405853271484\n",
      "h: 81 | epoch: 21, train loss: 10.937919616699219, test loss: 8.583563804626465\n",
      "h: 81 | epoch: 22, train loss: 10.646233558654785, test loss: 8.295762062072754\n",
      "h: 81 | epoch: 23, train loss: 10.39096736907959, test loss: 8.039724349975586\n",
      "h: 81 | epoch: 24, train loss: 10.167510032653809, test loss: 7.811715126037598\n",
      "h: 81 | epoch: 25, train loss: 9.97183609008789, test loss: 7.608442783355713\n",
      "h: 81 | epoch: 26, train loss: 9.800435066223145, test loss: 7.427027702331543\n",
      "h: 81 | epoch: 27, train loss: 9.65024185180664, test loss: 7.26492977142334\n",
      "h: 81 | epoch: 28, train loss: 9.51858139038086, test loss: 7.119925498962402\n",
      "h: 81 | epoch: 29, train loss: 9.40312385559082, test loss: 6.99005651473999\n",
      "h: 81 | epoch: 30, train loss: 9.301828384399414, test loss: 6.873602867126465\n",
      "h: 81 | epoch: 31, train loss: 9.212919235229492, test loss: 6.76904821395874\n",
      "h: 81 | epoch: 32, train loss: 9.134848594665527, test loss: 6.675060272216797\n",
      "h: 81 | epoch: 33, train loss: 9.06626033782959, test loss: 6.590461730957031\n",
      "h: 81 | epoch: 34, train loss: 9.005973815917969, test loss: 6.514220237731934\n",
      "h: 81 | epoch: 35, train loss: 8.952954292297363, test loss: 6.445420742034912\n",
      "h: 81 | epoch: 36, train loss: 8.906305313110352, test loss: 6.383255481719971\n",
      "h: 81 | epoch: 37, train loss: 8.865235328674316, test loss: 6.3270158767700195\n",
      "h: 81 | epoch: 38, train loss: 8.829060554504395, test loss: 6.276068210601807\n",
      "h: 81 | epoch: 39, train loss: 8.7971773147583, test loss: 6.229855537414551\n",
      "h: 81 | epoch: 40, train loss: 8.769060134887695, test loss: 6.187886714935303\n",
      "h: 81 | epoch: 41, train loss: 8.744251251220703, test loss: 6.149720191955566\n",
      "h: 81 | epoch: 42, train loss: 8.722347259521484, test loss: 6.11497163772583\n",
      "h: 81 | epoch: 43, train loss: 8.702998161315918, test loss: 6.0832929611206055\n",
      "h: 81 | epoch: 44, train loss: 8.685894966125488, test loss: 6.054378986358643\n",
      "h: 81 | epoch: 45, train loss: 8.670766830444336, test loss: 6.027956962585449\n",
      "h: 81 | epoch: 46, train loss: 8.657379150390625, test loss: 6.003782749176025\n",
      "h: 81 | epoch: 47, train loss: 8.645523071289062, test loss: 5.981637954711914\n",
      "h: 81 | epoch: 48, train loss: 8.635019302368164, test loss: 5.961331367492676\n",
      "h: 81 | epoch: 49, train loss: 8.625704765319824, test loss: 5.942690372467041\n",
      "h: 81 | epoch: 50, train loss: 8.61744213104248, test loss: 5.925558090209961\n",
      "h: 81 | epoch: 51, train loss: 8.610109329223633, test loss: 5.909794330596924\n",
      "h: 81 | epoch: 52, train loss: 8.603594779968262, test loss: 5.895277500152588\n",
      "h: 81 | epoch: 53, train loss: 8.597805976867676, test loss: 5.881893157958984\n",
      "h: 81 | epoch: 54, train loss: 8.592658996582031, test loss: 5.869540691375732\n",
      "h: 81 | epoch: 55, train loss: 8.588077545166016, test loss: 5.858132362365723\n",
      "h: 81 | epoch: 56, train loss: 8.584001541137695, test loss: 5.847582817077637\n",
      "h: 81 | epoch: 57, train loss: 8.58036994934082, test loss: 5.8378214836120605\n",
      "h: 81 | epoch: 58, train loss: 8.577133178710938, test loss: 5.828780174255371\n",
      "h: 81 | epoch: 59, train loss: 8.574247360229492, test loss: 5.82039737701416\n",
      "h: 81 | epoch: 60, train loss: 8.571672439575195, test loss: 5.812621116638184\n",
      "h: 81 | epoch: 61, train loss: 8.569375991821289, test loss: 5.8053998947143555\n",
      "h: 81 | epoch: 62, train loss: 8.567323684692383, test loss: 5.798689842224121\n",
      "h: 81 | epoch: 63, train loss: 8.56549072265625, test loss: 5.792448997497559\n",
      "h: 81 | epoch: 64, train loss: 8.563852310180664, test loss: 5.786641597747803\n",
      "h: 81 | epoch: 65, train loss: 8.562386512756348, test loss: 5.781233310699463\n",
      "h: 81 | epoch: 66, train loss: 8.561075210571289, test loss: 5.776192665100098\n",
      "h: 81 | epoch: 67, train loss: 8.559901237487793, test loss: 5.771493911743164\n",
      "h: 81 | epoch: 68, train loss: 8.558849334716797, test loss: 5.767107963562012\n",
      "h: 81 | epoch: 69, train loss: 8.55790901184082, test loss: 5.763012886047363\n",
      "h: 81 | epoch: 70, train loss: 8.557065963745117, test loss: 5.759186744689941\n",
      "h: 81 | epoch: 71, train loss: 8.55630874633789, test loss: 5.755610466003418\n",
      "h: 81 | epoch: 72, train loss: 8.55562973022461, test loss: 5.752264022827148\n",
      "h: 81 | epoch: 73, train loss: 8.555021286010742, test loss: 5.749133110046387\n",
      "h: 81 | epoch: 74, train loss: 8.554474830627441, test loss: 5.74620246887207\n",
      "h: 81 | epoch: 75, train loss: 8.553984642028809, test loss: 5.74345588684082\n",
      "h: 81 | epoch: 76, train loss: 8.553544044494629, test loss: 5.740882396697998\n",
      "h: 81 | epoch: 77, train loss: 8.55314826965332, test loss: 5.738468647003174\n",
      "h: 81 | epoch: 78, train loss: 8.552793502807617, test loss: 5.736205101013184\n",
      "h: 81 | epoch: 79, train loss: 8.552473068237305, test loss: 5.7340803146362305\n",
      "h: 81 | epoch: 80, train loss: 8.552186965942383, test loss: 5.732085227966309\n",
      "h: 81 | epoch: 81, train loss: 8.55192756652832, test loss: 5.730210304260254\n",
      "h: 81 | epoch: 82, train loss: 8.551694869995117, test loss: 5.728450298309326\n",
      "h: 81 | epoch: 83, train loss: 8.551485061645508, test loss: 5.726794719696045\n",
      "h: 81 | epoch: 84, train loss: 8.551298141479492, test loss: 5.725237846374512\n",
      "h: 81 | epoch: 85, train loss: 8.551127433776855, test loss: 5.723773002624512\n",
      "h: 81 | epoch: 86, train loss: 8.55097484588623, test loss: 5.722393989562988\n",
      "h: 81 | epoch: 87, train loss: 8.550836563110352, test loss: 5.721097469329834\n",
      "h: 81 | epoch: 88, train loss: 8.550713539123535, test loss: 5.719875812530518\n",
      "h: 81 | epoch: 89, train loss: 8.550601959228516, test loss: 5.718724250793457\n",
      "h: 81 | epoch: 90, train loss: 8.550501823425293, test loss: 5.717640399932861\n",
      "h: 81 | epoch: 91, train loss: 8.550410270690918, test loss: 5.716617584228516\n",
      "h: 81 | epoch: 92, train loss: 8.550329208374023, test loss: 5.715654373168945\n",
      "h: 81 | epoch: 93, train loss: 8.55025577545166, test loss: 5.714745998382568\n",
      "h: 81 | epoch: 94, train loss: 8.550189018249512, test loss: 5.713887691497803\n",
      "h: 81 | epoch: 95, train loss: 8.550127983093262, test loss: 5.713078498840332\n",
      "h: 81 | epoch: 96, train loss: 8.550074577331543, test loss: 5.712314605712891\n",
      "h: 81 | epoch: 97, train loss: 8.550025939941406, test loss: 5.711594581604004\n",
      "h: 81 | epoch: 98, train loss: 8.549982070922852, test loss: 5.710914134979248\n",
      "h: 81 | epoch: 99, train loss: 8.549942016601562, test loss: 5.710270404815674\n",
      "h: 82 | epoch: 0, train loss: 44.3570556640625, test loss: 36.54943084716797\n",
      "h: 82 | epoch: 1, train loss: 40.16838073730469, test loss: 33.108314514160156\n",
      "h: 82 | epoch: 2, train loss: 36.46868896484375, test loss: 30.057397842407227\n",
      "h: 82 | epoch: 3, train loss: 33.19947052001953, test loss: 27.3510799407959\n",
      "h: 82 | epoch: 4, train loss: 30.309789657592773, test loss: 24.949527740478516\n",
      "h: 82 | epoch: 5, train loss: 27.75518226623535, test loss: 22.81779670715332\n",
      "h: 82 | epoch: 6, train loss: 25.496662139892578, test loss: 20.925172805786133\n",
      "h: 82 | epoch: 7, train loss: 23.499977111816406, test loss: 19.244564056396484\n",
      "h: 82 | epoch: 8, train loss: 21.734960556030273, test loss: 17.752042770385742\n",
      "h: 82 | epoch: 9, train loss: 20.17500114440918, test loss: 16.426456451416016\n",
      "h: 82 | epoch: 10, train loss: 18.79656410217285, test loss: 15.249061584472656\n",
      "h: 82 | epoch: 11, train loss: 17.57884407043457, test loss: 14.203229904174805\n",
      "h: 82 | epoch: 12, train loss: 16.503406524658203, test loss: 13.2742280960083\n",
      "h: 82 | epoch: 13, train loss: 15.553921699523926, test loss: 12.448954582214355\n",
      "h: 82 | epoch: 14, train loss: 14.715911865234375, test loss: 11.715788841247559\n",
      "h: 82 | epoch: 15, train loss: 13.976536750793457, test loss: 11.064397811889648\n",
      "h: 82 | epoch: 16, train loss: 13.324414253234863, test loss: 10.485611915588379\n",
      "h: 82 | epoch: 17, train loss: 12.749445915222168, test loss: 9.971269607543945\n",
      "h: 82 | epoch: 18, train loss: 12.242682456970215, test loss: 9.514134407043457\n",
      "h: 82 | epoch: 19, train loss: 11.796185493469238, test loss: 9.107772827148438\n",
      "h: 82 | epoch: 20, train loss: 11.402920722961426, test loss: 8.746469497680664\n",
      "h: 82 | epoch: 21, train loss: 11.056661605834961, test loss: 8.425153732299805\n",
      "h: 82 | epoch: 22, train loss: 10.751886367797852, test loss: 8.139314651489258\n",
      "h: 82 | epoch: 23, train loss: 10.483709335327148, test loss: 7.8849592208862305\n",
      "h: 82 | epoch: 24, train loss: 10.247810363769531, test loss: 7.658538818359375\n",
      "h: 82 | epoch: 25, train loss: 10.040365219116211, test loss: 7.456908226013184\n",
      "h: 82 | epoch: 26, train loss: 9.857995986938477, test loss: 7.277273654937744\n",
      "h: 82 | epoch: 27, train loss: 9.697712898254395, test loss: 7.117159843444824\n",
      "h: 82 | epoch: 28, train loss: 9.556877136230469, test loss: 6.974375247955322\n",
      "h: 82 | epoch: 29, train loss: 9.433165550231934, test loss: 6.846971035003662\n",
      "h: 82 | epoch: 30, train loss: 9.324518203735352, test loss: 6.7332258224487305\n",
      "h: 82 | epoch: 31, train loss: 9.229120254516602, test loss: 6.631608486175537\n",
      "h: 82 | epoch: 32, train loss: 9.145380020141602, test loss: 6.540766716003418\n",
      "h: 82 | epoch: 33, train loss: 9.07188606262207, test loss: 6.459502220153809\n",
      "h: 82 | epoch: 34, train loss: 9.00739574432373, test loss: 6.386749744415283\n",
      "h: 82 | epoch: 35, train loss: 8.950817108154297, test loss: 6.321567058563232\n",
      "h: 82 | epoch: 36, train loss: 8.901189804077148, test loss: 6.263120651245117\n",
      "h: 82 | epoch: 37, train loss: 8.857666015625, test loss: 6.21066951751709\n",
      "h: 82 | epoch: 38, train loss: 8.819500923156738, test loss: 6.163558006286621\n",
      "h: 82 | epoch: 39, train loss: 8.786040306091309, test loss: 6.121203899383545\n",
      "h: 82 | epoch: 40, train loss: 8.756708145141602, test loss: 6.083093643188477\n",
      "h: 82 | epoch: 41, train loss: 8.730998992919922, test loss: 6.048769474029541\n",
      "h: 82 | epoch: 42, train loss: 8.708467483520508, test loss: 6.017823219299316\n",
      "h: 82 | epoch: 43, train loss: 8.68872356414795, test loss: 5.98989725112915\n",
      "h: 82 | epoch: 44, train loss: 8.67142391204834, test loss: 5.964672088623047\n",
      "h: 82 | epoch: 45, train loss: 8.656268119812012, test loss: 5.941861629486084\n",
      "h: 82 | epoch: 46, train loss: 8.642990112304688, test loss: 5.921215534210205\n",
      "h: 82 | epoch: 47, train loss: 8.63136100769043, test loss: 5.902510643005371\n",
      "h: 82 | epoch: 48, train loss: 8.621174812316895, test loss: 5.885543346405029\n",
      "h: 82 | epoch: 49, train loss: 8.61225414276123, test loss: 5.870139122009277\n",
      "h: 82 | epoch: 50, train loss: 8.60444164276123, test loss: 5.856139183044434\n",
      "h: 82 | epoch: 51, train loss: 8.597600936889648, test loss: 5.843402862548828\n",
      "h: 82 | epoch: 52, train loss: 8.591611862182617, test loss: 5.831803798675537\n",
      "h: 82 | epoch: 53, train loss: 8.586368560791016, test loss: 5.821229934692383\n",
      "h: 82 | epoch: 54, train loss: 8.581777572631836, test loss: 5.811580657958984\n",
      "h: 82 | epoch: 55, train loss: 8.577757835388184, test loss: 5.8027663230896\n",
      "h: 82 | epoch: 56, train loss: 8.574238777160645, test loss: 5.7947096824646\n",
      "h: 82 | epoch: 57, train loss: 8.571160316467285, test loss: 5.7873358726501465\n",
      "h: 82 | epoch: 58, train loss: 8.568464279174805, test loss: 5.780582427978516\n",
      "h: 82 | epoch: 59, train loss: 8.5661039352417, test loss: 5.77439022064209\n",
      "h: 82 | epoch: 60, train loss: 8.56403923034668, test loss: 5.768707752227783\n",
      "h: 82 | epoch: 61, train loss: 8.562231063842773, test loss: 5.763489723205566\n",
      "h: 82 | epoch: 62, train loss: 8.560648918151855, test loss: 5.758693218231201\n",
      "h: 82 | epoch: 63, train loss: 8.55926513671875, test loss: 5.754281044006348\n",
      "h: 82 | epoch: 64, train loss: 8.558053970336914, test loss: 5.750219345092773\n",
      "h: 82 | epoch: 65, train loss: 8.55699348449707, test loss: 5.746477127075195\n",
      "h: 82 | epoch: 66, train loss: 8.556065559387207, test loss: 5.7430267333984375\n",
      "h: 82 | epoch: 67, train loss: 8.555253028869629, test loss: 5.739842891693115\n",
      "h: 82 | epoch: 68, train loss: 8.554543495178223, test loss: 5.736905097961426\n",
      "h: 82 | epoch: 69, train loss: 8.553921699523926, test loss: 5.734189987182617\n",
      "h: 82 | epoch: 70, train loss: 8.553377151489258, test loss: 5.731679916381836\n",
      "h: 82 | epoch: 71, train loss: 8.552901268005371, test loss: 5.729358673095703\n",
      "h: 82 | epoch: 72, train loss: 8.552484512329102, test loss: 5.72721004486084\n",
      "h: 82 | epoch: 73, train loss: 8.55212116241455, test loss: 5.725220680236816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 82 | epoch: 74, train loss: 8.551801681518555, test loss: 5.7233781814575195\n",
      "h: 82 | epoch: 75, train loss: 8.551523208618164, test loss: 5.721669673919678\n",
      "h: 82 | epoch: 76, train loss: 8.551279067993164, test loss: 5.720085144042969\n",
      "h: 82 | epoch: 77, train loss: 8.551065444946289, test loss: 5.718616008758545\n",
      "h: 82 | epoch: 78, train loss: 8.550878524780273, test loss: 5.717250823974609\n",
      "h: 82 | epoch: 79, train loss: 8.550715446472168, test loss: 5.715985298156738\n",
      "h: 82 | epoch: 80, train loss: 8.550572395324707, test loss: 5.714807510375977\n",
      "h: 82 | epoch: 81, train loss: 8.550447463989258, test loss: 5.713715076446533\n",
      "h: 82 | epoch: 82, train loss: 8.550337791442871, test loss: 5.712698936462402\n",
      "h: 82 | epoch: 83, train loss: 8.550241470336914, test loss: 5.711753845214844\n",
      "h: 82 | epoch: 84, train loss: 8.550158500671387, test loss: 5.710875511169434\n",
      "h: 82 | epoch: 85, train loss: 8.550085067749023, test loss: 5.710056781768799\n",
      "h: 82 | epoch: 86, train loss: 8.550021171569824, test loss: 5.709296226501465\n",
      "h: 82 | epoch: 87, train loss: 8.54996395111084, test loss: 5.708588123321533\n",
      "h: 82 | epoch: 88, train loss: 8.54991626739502, test loss: 5.7079291343688965\n",
      "h: 82 | epoch: 89, train loss: 8.549871444702148, test loss: 5.707314491271973\n",
      "h: 82 | epoch: 90, train loss: 8.549835205078125, test loss: 5.706743240356445\n",
      "h: 82 | epoch: 91, train loss: 8.54980182647705, test loss: 5.706209182739258\n",
      "h: 82 | epoch: 92, train loss: 8.549774169921875, test loss: 5.705713272094727\n",
      "h: 82 | epoch: 93, train loss: 8.549748420715332, test loss: 5.705250263214111\n",
      "h: 82 | epoch: 94, train loss: 8.549725532531738, test loss: 5.704819202423096\n",
      "h: 82 | epoch: 95, train loss: 8.549707412719727, test loss: 5.704415321350098\n",
      "h: 82 | epoch: 96, train loss: 8.549690246582031, test loss: 5.704041481018066\n",
      "h: 82 | epoch: 97, train loss: 8.549675941467285, test loss: 5.703691482543945\n",
      "h: 82 | epoch: 98, train loss: 8.549661636352539, test loss: 5.703364849090576\n",
      "h: 82 | epoch: 99, train loss: 8.549651145935059, test loss: 5.703061580657959\n",
      "h: 83 | epoch: 0, train loss: 51.436317443847656, test loss: 43.078834533691406\n",
      "h: 83 | epoch: 1, train loss: 47.05854415893555, test loss: 39.44639587402344\n",
      "h: 83 | epoch: 2, train loss: 43.131309509277344, test loss: 36.17567443847656\n",
      "h: 83 | epoch: 3, train loss: 39.60560607910156, test loss: 33.228519439697266\n",
      "h: 83 | epoch: 4, train loss: 36.43854522705078, test loss: 30.571365356445312\n",
      "h: 83 | epoch: 5, train loss: 33.5924072265625, test loss: 28.174545288085938\n",
      "h: 83 | epoch: 6, train loss: 31.033885955810547, test loss: 26.011770248413086\n",
      "h: 83 | epoch: 7, train loss: 28.733469009399414, test loss: 24.059619903564453\n",
      "h: 83 | epoch: 8, train loss: 26.66489028930664, test loss: 22.297216415405273\n",
      "h: 83 | epoch: 9, train loss: 24.80475616455078, test loss: 20.705869674682617\n",
      "h: 83 | epoch: 10, train loss: 23.13214111328125, test loss: 19.268827438354492\n",
      "h: 83 | epoch: 11, train loss: 21.628297805786133, test loss: 17.97103500366211\n",
      "h: 83 | epoch: 12, train loss: 20.276409149169922, test loss: 16.798961639404297\n",
      "h: 83 | epoch: 13, train loss: 19.06136131286621, test loss: 15.740399360656738\n",
      "h: 83 | epoch: 14, train loss: 17.969558715820312, test loss: 14.784357070922852\n",
      "h: 83 | epoch: 15, train loss: 16.988750457763672, test loss: 13.92090892791748\n",
      "h: 83 | epoch: 16, train loss: 16.107898712158203, test loss: 13.141093254089355\n",
      "h: 83 | epoch: 17, train loss: 15.317056655883789, test loss: 12.43681526184082\n",
      "h: 83 | epoch: 18, train loss: 14.607244491577148, test loss: 11.800756454467773\n",
      "h: 83 | epoch: 19, train loss: 13.970365524291992, test loss: 11.226302146911621\n",
      "h: 83 | epoch: 20, train loss: 13.399106979370117, test loss: 10.707473754882812\n",
      "h: 83 | epoch: 21, train loss: 12.886880874633789, test loss: 10.23886489868164\n",
      "h: 83 | epoch: 22, train loss: 12.427735328674316, test loss: 9.815592765808105\n",
      "h: 83 | epoch: 23, train loss: 12.016304016113281, test loss: 9.43323802947998\n",
      "h: 83 | epoch: 24, train loss: 11.647750854492188, test loss: 9.087812423706055\n",
      "h: 83 | epoch: 25, train loss: 11.317708015441895, test loss: 8.775707244873047\n",
      "h: 83 | epoch: 26, train loss: 11.022247314453125, test loss: 8.49366569519043\n",
      "h: 83 | epoch: 27, train loss: 10.757824897766113, test loss: 8.238748550415039\n",
      "h: 83 | epoch: 28, train loss: 10.52125072479248, test loss: 8.008295059204102\n",
      "h: 83 | epoch: 29, train loss: 10.309654235839844, test loss: 7.799912929534912\n",
      "h: 83 | epoch: 30, train loss: 10.120450973510742, test loss: 7.6114373207092285\n",
      "h: 83 | epoch: 31, train loss: 9.951318740844727, test loss: 7.440913200378418\n",
      "h: 83 | epoch: 32, train loss: 9.800167083740234, test loss: 7.286585330963135\n",
      "h: 83 | epoch: 33, train loss: 9.665115356445312, test loss: 7.1468634605407715\n",
      "h: 83 | epoch: 34, train loss: 9.54448413848877, test loss: 7.020317077636719\n",
      "h: 83 | epoch: 35, train loss: 9.43675422668457, test loss: 6.905656337738037\n",
      "h: 83 | epoch: 36, train loss: 9.340568542480469, test loss: 6.801718711853027\n",
      "h: 83 | epoch: 37, train loss: 9.254708290100098, test loss: 6.70745849609375\n",
      "h: 83 | epoch: 38, train loss: 9.178083419799805, test loss: 6.621929168701172\n",
      "h: 83 | epoch: 39, train loss: 9.109708786010742, test loss: 6.54428243637085\n",
      "h: 83 | epoch: 40, train loss: 9.048712730407715, test loss: 6.473753452301025\n",
      "h: 83 | epoch: 41, train loss: 8.994306564331055, test loss: 6.409649848937988\n",
      "h: 83 | epoch: 42, train loss: 8.945784568786621, test loss: 6.351354122161865\n",
      "h: 83 | epoch: 43, train loss: 8.902521133422852, test loss: 6.298305988311768\n",
      "h: 83 | epoch: 44, train loss: 8.863950729370117, test loss: 6.250000476837158\n",
      "h: 83 | epoch: 45, train loss: 8.829566955566406, test loss: 6.205985069274902\n",
      "h: 83 | epoch: 46, train loss: 8.798922538757324, test loss: 6.165849685668945\n",
      "h: 83 | epoch: 47, train loss: 8.771614074707031, test loss: 6.129226207733154\n",
      "h: 83 | epoch: 48, train loss: 8.747279167175293, test loss: 6.0957841873168945\n",
      "h: 83 | epoch: 49, train loss: 8.72559928894043, test loss: 6.065223693847656\n",
      "h: 83 | epoch: 50, train loss: 8.70628547668457, test loss: 6.037276268005371\n",
      "h: 83 | epoch: 51, train loss: 8.689081192016602, test loss: 6.011697292327881\n",
      "h: 83 | epoch: 52, train loss: 8.673757553100586, test loss: 5.988268852233887\n",
      "h: 83 | epoch: 53, train loss: 8.660112380981445, test loss: 5.966792583465576\n",
      "h: 83 | epoch: 54, train loss: 8.647958755493164, test loss: 5.947091579437256\n",
      "h: 83 | epoch: 55, train loss: 8.637138366699219, test loss: 5.929002285003662\n",
      "h: 83 | epoch: 56, train loss: 8.6275053024292, test loss: 5.912381172180176\n",
      "h: 83 | epoch: 57, train loss: 8.618927001953125, test loss: 5.897095680236816\n",
      "h: 83 | epoch: 58, train loss: 8.611289978027344, test loss: 5.883028030395508\n",
      "h: 83 | epoch: 59, train loss: 8.604494094848633, test loss: 5.870070457458496\n",
      "h: 83 | epoch: 60, train loss: 8.598443031311035, test loss: 5.858126640319824\n",
      "h: 83 | epoch: 61, train loss: 8.593057632446289, test loss: 5.84710693359375\n",
      "h: 83 | epoch: 62, train loss: 8.588264465332031, test loss: 5.8369340896606445\n",
      "h: 83 | epoch: 63, train loss: 8.58399772644043, test loss: 5.82753324508667\n",
      "h: 83 | epoch: 64, train loss: 8.580202102661133, test loss: 5.818840503692627\n",
      "h: 83 | epoch: 65, train loss: 8.576823234558105, test loss: 5.810795783996582\n",
      "h: 83 | epoch: 66, train loss: 8.573816299438477, test loss: 5.803347110748291\n",
      "h: 83 | epoch: 67, train loss: 8.571142196655273, test loss: 5.79644250869751\n",
      "h: 83 | epoch: 68, train loss: 8.568761825561523, test loss: 5.790038585662842\n",
      "h: 83 | epoch: 69, train loss: 8.566644668579102, test loss: 5.784095764160156\n",
      "h: 83 | epoch: 70, train loss: 8.564760208129883, test loss: 5.778576374053955\n",
      "h: 83 | epoch: 71, train loss: 8.563082695007324, test loss: 5.7734479904174805\n",
      "h: 83 | epoch: 72, train loss: 8.561591148376465, test loss: 5.768678188323975\n",
      "h: 83 | epoch: 73, train loss: 8.560263633728027, test loss: 5.764239311218262\n",
      "h: 83 | epoch: 74, train loss: 8.559083938598633, test loss: 5.760105609893799\n",
      "h: 83 | epoch: 75, train loss: 8.558032035827637, test loss: 5.756255149841309\n",
      "h: 83 | epoch: 76, train loss: 8.557097434997559, test loss: 5.752665996551514\n",
      "h: 83 | epoch: 77, train loss: 8.556266784667969, test loss: 5.7493181228637695\n",
      "h: 83 | epoch: 78, train loss: 8.555527687072754, test loss: 5.746192932128906\n",
      "h: 83 | epoch: 79, train loss: 8.554869651794434, test loss: 5.7432756423950195\n",
      "h: 83 | epoch: 80, train loss: 8.554285049438477, test loss: 5.7405500411987305\n",
      "h: 83 | epoch: 81, train loss: 8.553764343261719, test loss: 5.738001823425293\n",
      "h: 83 | epoch: 82, train loss: 8.553300857543945, test loss: 5.735620021820068\n",
      "h: 83 | epoch: 83, train loss: 8.552888870239258, test loss: 5.733391284942627\n",
      "h: 83 | epoch: 84, train loss: 8.552522659301758, test loss: 5.731305122375488\n",
      "h: 83 | epoch: 85, train loss: 8.552196502685547, test loss: 5.729352951049805\n",
      "h: 83 | epoch: 86, train loss: 8.551905632019043, test loss: 5.727523326873779\n",
      "h: 83 | epoch: 87, train loss: 8.55164909362793, test loss: 5.725808620452881\n",
      "h: 83 | epoch: 88, train loss: 8.551420211791992, test loss: 5.724202632904053\n",
      "h: 83 | epoch: 89, train loss: 8.551216125488281, test loss: 5.722695827484131\n",
      "h: 83 | epoch: 90, train loss: 8.551033973693848, test loss: 5.721282482147217\n",
      "h: 83 | epoch: 91, train loss: 8.550872802734375, test loss: 5.719956398010254\n",
      "h: 83 | epoch: 92, train loss: 8.550729751586914, test loss: 5.7187113761901855\n",
      "h: 83 | epoch: 93, train loss: 8.5506010055542, test loss: 5.717541694641113\n",
      "h: 83 | epoch: 94, train loss: 8.550488471984863, test loss: 5.716444969177246\n",
      "h: 83 | epoch: 95, train loss: 8.550386428833008, test loss: 5.7154130935668945\n",
      "h: 83 | epoch: 96, train loss: 8.550296783447266, test loss: 5.7144455909729\n",
      "h: 83 | epoch: 97, train loss: 8.550216674804688, test loss: 5.713534355163574\n",
      "h: 83 | epoch: 98, train loss: 8.550145149230957, test loss: 5.7126784324646\n",
      "h: 83 | epoch: 99, train loss: 8.550082206726074, test loss: 5.711874485015869\n",
      "h: 84 | epoch: 0, train loss: 48.28759002685547, test loss: 40.768463134765625\n",
      "h: 84 | epoch: 1, train loss: 44.057376861572266, test loss: 37.18024444580078\n",
      "h: 84 | epoch: 2, train loss: 40.280296325683594, test loss: 33.967525482177734\n",
      "h: 84 | epoch: 3, train loss: 36.90561294555664, test loss: 31.089136123657227\n",
      "h: 84 | epoch: 4, train loss: 33.888946533203125, test loss: 28.508874893188477\n",
      "h: 84 | epoch: 5, train loss: 31.19134521484375, test loss: 26.19488525390625\n",
      "h: 84 | epoch: 6, train loss: 28.778472900390625, test loss: 24.11897087097168\n",
      "h: 84 | epoch: 7, train loss: 26.619958877563477, test loss: 22.256147384643555\n",
      "h: 84 | epoch: 8, train loss: 24.688873291015625, test loss: 20.58418846130371\n",
      "h: 84 | epoch: 9, train loss: 22.96126937866211, test loss: 19.083314895629883\n",
      "h: 84 | epoch: 10, train loss: 21.415821075439453, test loss: 17.735843658447266\n",
      "h: 84 | epoch: 11, train loss: 20.03348731994629, test loss: 16.525999069213867\n",
      "h: 84 | epoch: 12, train loss: 18.797258377075195, test loss: 15.439648628234863\n",
      "h: 84 | epoch: 13, train loss: 17.691905975341797, test loss: 14.464123725891113\n",
      "h: 84 | epoch: 14, train loss: 16.703794479370117, test loss: 13.58808422088623\n",
      "h: 84 | epoch: 15, train loss: 15.820712089538574, test loss: 12.801351547241211\n",
      "h: 84 | epoch: 16, train loss: 15.031692504882812, test loss: 12.094781875610352\n",
      "h: 84 | epoch: 17, train loss: 14.326911926269531, test loss: 11.460174560546875\n",
      "h: 84 | epoch: 18, train loss: 13.697555541992188, test loss: 10.890158653259277\n",
      "h: 84 | epoch: 19, train loss: 13.135705947875977, test loss: 10.378124237060547\n",
      "h: 84 | epoch: 20, train loss: 12.634267807006836, test loss: 9.91812515258789\n",
      "h: 84 | epoch: 21, train loss: 12.186872482299805, test loss: 9.504826545715332\n",
      "h: 84 | epoch: 22, train loss: 11.787809371948242, test loss: 9.133439064025879\n",
      "h: 84 | epoch: 23, train loss: 11.431953430175781, test loss: 8.799654960632324\n",
      "h: 84 | epoch: 24, train loss: 11.114713668823242, test loss: 8.499613761901855\n",
      "h: 84 | epoch: 25, train loss: 10.831975936889648, test loss: 8.229849815368652\n",
      "h: 84 | epoch: 26, train loss: 10.580049514770508, test loss: 7.987246513366699\n",
      "h: 84 | epoch: 27, train loss: 10.355633735656738, test loss: 7.769015312194824\n",
      "h: 84 | epoch: 28, train loss: 10.155773162841797, test loss: 7.5726494789123535\n",
      "h: 84 | epoch: 29, train loss: 9.977819442749023, test loss: 7.395902156829834\n",
      "h: 84 | epoch: 30, train loss: 9.819406509399414, test loss: 7.236758232116699\n",
      "h: 84 | epoch: 31, train loss: 9.678417205810547, test loss: 7.093415260314941\n",
      "h: 84 | epoch: 32, train loss: 9.552962303161621, test loss: 6.9642438888549805\n",
      "h: 84 | epoch: 33, train loss: 9.441349029541016, test loss: 6.84780216217041\n",
      "h: 84 | epoch: 34, train loss: 9.342068672180176, test loss: 6.742784023284912\n",
      "h: 84 | epoch: 35, train loss: 9.25377368927002, test loss: 6.648019313812256\n",
      "h: 84 | epoch: 36, train loss: 9.17525863647461, test loss: 6.562469482421875\n",
      "h: 84 | epoch: 37, train loss: 9.105453491210938, test loss: 6.485196113586426\n",
      "h: 84 | epoch: 38, train loss: 9.043397903442383, test loss: 6.415358066558838\n",
      "h: 84 | epoch: 39, train loss: 8.988242149353027, test loss: 6.352203369140625\n",
      "h: 84 | epoch: 40, train loss: 8.939221382141113, test loss: 6.295058250427246\n",
      "h: 84 | epoch: 41, train loss: 8.895660400390625, test loss: 6.243319034576416\n",
      "h: 84 | epoch: 42, train loss: 8.856951713562012, test loss: 6.196444511413574\n",
      "h: 84 | epoch: 43, train loss: 8.822563171386719, test loss: 6.153947830200195\n",
      "h: 84 | epoch: 44, train loss: 8.792012214660645, test loss: 6.115396022796631\n",
      "h: 84 | epoch: 45, train loss: 8.764871597290039, test loss: 6.080397605895996\n",
      "h: 84 | epoch: 46, train loss: 8.740764617919922, test loss: 6.0486016273498535\n",
      "h: 84 | epoch: 47, train loss: 8.719353675842285, test loss: 6.01969575881958\n",
      "h: 84 | epoch: 48, train loss: 8.700337409973145, test loss: 5.9933953285217285\n",
      "h: 84 | epoch: 49, train loss: 8.683448791503906, test loss: 5.9694504737854\n",
      "h: 84 | epoch: 50, train loss: 8.668450355529785, test loss: 5.947632789611816\n",
      "h: 84 | epoch: 51, train loss: 8.655131340026855, test loss: 5.927738189697266\n",
      "h: 84 | epoch: 52, train loss: 8.643304824829102, test loss: 5.909584045410156\n",
      "h: 84 | epoch: 53, train loss: 8.632802963256836, test loss: 5.893003463745117\n",
      "h: 84 | epoch: 54, train loss: 8.623478889465332, test loss: 5.877851486206055\n",
      "h: 84 | epoch: 55, train loss: 8.615199089050293, test loss: 5.86398983001709\n",
      "h: 84 | epoch: 56, train loss: 8.607847213745117, test loss: 5.851303577423096\n",
      "h: 84 | epoch: 57, train loss: 8.601319313049316, test loss: 5.839681148529053\n",
      "h: 84 | epoch: 58, train loss: 8.595524787902832, test loss: 5.82902717590332\n",
      "h: 84 | epoch: 59, train loss: 8.590376853942871, test loss: 5.8192524909973145\n",
      "h: 84 | epoch: 60, train loss: 8.585808753967285, test loss: 5.810276985168457\n",
      "h: 84 | epoch: 61, train loss: 8.581751823425293, test loss: 5.802031517028809\n",
      "h: 84 | epoch: 62, train loss: 8.578150749206543, test loss: 5.7944488525390625\n",
      "h: 84 | epoch: 63, train loss: 8.574953079223633, test loss: 5.787472724914551\n",
      "h: 84 | epoch: 64, train loss: 8.572113037109375, test loss: 5.7810468673706055\n",
      "h: 84 | epoch: 65, train loss: 8.56959056854248, test loss: 5.775128364562988\n",
      "h: 84 | epoch: 66, train loss: 8.567352294921875, test loss: 5.769669532775879\n",
      "h: 84 | epoch: 67, train loss: 8.565363883972168, test loss: 5.764632225036621\n",
      "h: 84 | epoch: 68, train loss: 8.563599586486816, test loss: 5.759982109069824\n",
      "h: 84 | epoch: 69, train loss: 8.562031745910645, test loss: 5.755686283111572\n",
      "h: 84 | epoch: 70, train loss: 8.560639381408691, test loss: 5.75171422958374\n",
      "h: 84 | epoch: 71, train loss: 8.559404373168945, test loss: 5.748040199279785\n",
      "h: 84 | epoch: 72, train loss: 8.558305740356445, test loss: 5.7446393966674805\n",
      "h: 84 | epoch: 73, train loss: 8.557331085205078, test loss: 5.741489887237549\n",
      "h: 84 | epoch: 74, train loss: 8.556465148925781, test loss: 5.7385711669921875\n",
      "h: 84 | epoch: 75, train loss: 8.555696487426758, test loss: 5.735864162445068\n",
      "h: 84 | epoch: 76, train loss: 8.555013656616211, test loss: 5.733354091644287\n",
      "h: 84 | epoch: 77, train loss: 8.554407119750977, test loss: 5.731024742126465\n",
      "h: 84 | epoch: 78, train loss: 8.553868293762207, test loss: 5.7288618087768555\n",
      "h: 84 | epoch: 79, train loss: 8.553389549255371, test loss: 5.726851463317871\n",
      "h: 84 | epoch: 80, train loss: 8.55296516418457, test loss: 5.724984169006348\n",
      "h: 84 | epoch: 81, train loss: 8.552586555480957, test loss: 5.723247051239014\n",
      "h: 84 | epoch: 82, train loss: 8.552251815795898, test loss: 5.721630573272705\n",
      "h: 84 | epoch: 83, train loss: 8.55195426940918, test loss: 5.720127582550049\n",
      "h: 84 | epoch: 84, train loss: 8.551688194274902, test loss: 5.718728065490723\n",
      "h: 84 | epoch: 85, train loss: 8.551454544067383, test loss: 5.717423439025879\n",
      "h: 84 | epoch: 86, train loss: 8.551244735717773, test loss: 5.716209411621094\n",
      "h: 84 | epoch: 87, train loss: 8.551058769226074, test loss: 5.715076923370361\n",
      "h: 84 | epoch: 88, train loss: 8.550893783569336, test loss: 5.7140212059021\n",
      "h: 84 | epoch: 89, train loss: 8.55074691772461, test loss: 5.71303653717041\n",
      "h: 84 | epoch: 90, train loss: 8.550616264343262, test loss: 5.712118625640869\n",
      "h: 84 | epoch: 91, train loss: 8.550500869750977, test loss: 5.7112603187561035\n",
      "h: 84 | epoch: 92, train loss: 8.550397872924805, test loss: 5.710461616516113\n",
      "h: 84 | epoch: 93, train loss: 8.55030632019043, test loss: 5.709714412689209\n",
      "h: 84 | epoch: 94, train loss: 8.550225257873535, test loss: 5.709016799926758\n",
      "h: 84 | epoch: 95, train loss: 8.550152778625488, test loss: 5.708365440368652\n",
      "h: 84 | epoch: 96, train loss: 8.550088882446289, test loss: 5.707756042480469\n",
      "h: 84 | epoch: 97, train loss: 8.550031661987305, test loss: 5.707186698913574\n",
      "h: 84 | epoch: 98, train loss: 8.549981117248535, test loss: 5.7066545486450195\n",
      "h: 84 | epoch: 99, train loss: 8.549936294555664, test loss: 5.706158638000488\n",
      "h: 85 | epoch: 0, train loss: 48.52634048461914, test loss: 41.427528381347656\n",
      "h: 85 | epoch: 1, train loss: 44.873165130615234, test loss: 38.29221725463867\n",
      "h: 85 | epoch: 2, train loss: 41.55637741088867, test loss: 35.4388542175293\n",
      "h: 85 | epoch: 3, train loss: 38.54294967651367, test loss: 32.84033203125\n",
      "h: 85 | epoch: 4, train loss: 35.803672790527344, test loss: 30.472614288330078\n",
      "h: 85 | epoch: 5, train loss: 33.31258010864258, test loss: 28.314205169677734\n",
      "h: 85 | epoch: 6, train loss: 31.046497344970703, test loss: 26.34591293334961\n",
      "h: 85 | epoch: 7, train loss: 28.984668731689453, test loss: 24.55048179626465\n",
      "h: 85 | epoch: 8, train loss: 27.108455657958984, test loss: 22.912391662597656\n",
      "h: 85 | epoch: 9, train loss: 25.401071548461914, test loss: 21.417606353759766\n",
      "h: 85 | epoch: 10, train loss: 23.84734535217285, test loss: 20.053457260131836\n",
      "h: 85 | epoch: 11, train loss: 22.433549880981445, test loss: 18.808435440063477\n",
      "h: 85 | epoch: 12, train loss: 21.147235870361328, test loss: 17.67210578918457\n",
      "h: 85 | epoch: 13, train loss: 19.977100372314453, test loss: 16.634973526000977\n",
      "h: 85 | epoch: 14, train loss: 18.912857055664062, test loss: 15.688390731811523\n",
      "h: 85 | epoch: 15, train loss: 17.945140838623047, test loss: 14.824481010437012\n",
      "h: 85 | epoch: 16, train loss: 17.065418243408203, test loss: 14.03605842590332\n",
      "h: 85 | epoch: 17, train loss: 16.265901565551758, test loss: 13.316556930541992\n",
      "h: 85 | epoch: 18, train loss: 15.53948974609375, test loss: 12.659994125366211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 85 | epoch: 19, train loss: 14.879693984985352, test loss: 12.060890197753906\n",
      "h: 85 | epoch: 20, train loss: 14.280588150024414, test loss: 11.514241218566895\n",
      "h: 85 | epoch: 21, train loss: 13.736763000488281, test loss: 11.015480041503906\n",
      "h: 85 | epoch: 22, train loss: 13.243276596069336, test loss: 10.560425758361816\n",
      "h: 85 | epoch: 23, train loss: 12.795613288879395, test loss: 10.145255088806152\n",
      "h: 85 | epoch: 24, train loss: 12.389646530151367, test loss: 9.76647663116455\n",
      "h: 85 | epoch: 25, train loss: 12.021612167358398, test loss: 9.420900344848633\n",
      "h: 85 | epoch: 26, train loss: 11.688072204589844, test loss: 9.105607986450195\n",
      "h: 85 | epoch: 27, train loss: 11.385889053344727, test loss: 8.817938804626465\n",
      "h: 85 | epoch: 28, train loss: 11.112199783325195, test loss: 8.555453300476074\n",
      "h: 85 | epoch: 29, train loss: 10.86439323425293, test loss: 8.31593132019043\n",
      "h: 85 | epoch: 30, train loss: 10.64008617401123, test loss: 8.097342491149902\n",
      "h: 85 | epoch: 31, train loss: 10.437113761901855, test loss: 7.897834777832031\n",
      "h: 85 | epoch: 32, train loss: 10.253496170043945, test loss: 7.715718746185303\n",
      "h: 85 | epoch: 33, train loss: 10.087434768676758, test loss: 7.549448490142822\n",
      "h: 85 | epoch: 34, train loss: 9.937289237976074, test loss: 7.397619724273682\n",
      "h: 85 | epoch: 35, train loss: 9.8015718460083, test loss: 7.258950233459473\n",
      "h: 85 | epoch: 36, train loss: 9.678926467895508, test loss: 7.132272243499756\n",
      "h: 85 | epoch: 37, train loss: 9.568120956420898, test loss: 7.016519069671631\n",
      "h: 85 | epoch: 38, train loss: 9.468035697937012, test loss: 6.910719394683838\n",
      "h: 85 | epoch: 39, train loss: 9.377655982971191, test loss: 6.81398868560791\n",
      "h: 85 | epoch: 40, train loss: 9.296055793762207, test loss: 6.725523471832275\n",
      "h: 85 | epoch: 41, train loss: 9.22239875793457, test loss: 6.644589424133301\n",
      "h: 85 | epoch: 42, train loss: 9.155924797058105, test loss: 6.570518493652344\n",
      "h: 85 | epoch: 43, train loss: 9.095947265625, test loss: 6.502704620361328\n",
      "h: 85 | epoch: 44, train loss: 9.041837692260742, test loss: 6.440592288970947\n",
      "h: 85 | epoch: 45, train loss: 8.993034362792969, test loss: 6.383681297302246\n",
      "h: 85 | epoch: 46, train loss: 8.94902229309082, test loss: 6.3315110206604\n",
      "h: 85 | epoch: 47, train loss: 8.909337997436523, test loss: 6.283663272857666\n",
      "h: 85 | epoch: 48, train loss: 8.873563766479492, test loss: 6.239760398864746\n",
      "h: 85 | epoch: 49, train loss: 8.841314315795898, test loss: 6.199460029602051\n",
      "h: 85 | epoch: 50, train loss: 8.812252044677734, test loss: 6.1624436378479\n",
      "h: 85 | epoch: 51, train loss: 8.786062240600586, test loss: 6.1284284591674805\n",
      "h: 85 | epoch: 52, train loss: 8.76246452331543, test loss: 6.0971527099609375\n",
      "h: 85 | epoch: 53, train loss: 8.741206169128418, test loss: 6.068381309509277\n",
      "h: 85 | epoch: 54, train loss: 8.722055435180664, test loss: 6.041895866394043\n",
      "h: 85 | epoch: 55, train loss: 8.704808235168457, test loss: 6.0175042152404785\n",
      "h: 85 | epoch: 56, train loss: 8.689274787902832, test loss: 5.995026111602783\n",
      "h: 85 | epoch: 57, train loss: 8.675287246704102, test loss: 5.97429895401001\n",
      "h: 85 | epoch: 58, train loss: 8.66269302368164, test loss: 5.955175876617432\n",
      "h: 85 | epoch: 59, train loss: 8.65135383605957, test loss: 5.9375200271606445\n",
      "h: 85 | epoch: 60, train loss: 8.641145706176758, test loss: 5.921210289001465\n",
      "h: 85 | epoch: 61, train loss: 8.631956100463867, test loss: 5.906132698059082\n",
      "h: 85 | epoch: 62, train loss: 8.623685836791992, test loss: 5.892188549041748\n",
      "h: 85 | epoch: 63, train loss: 8.616241455078125, test loss: 5.879282474517822\n",
      "h: 85 | epoch: 64, train loss: 8.609542846679688, test loss: 5.8673295974731445\n",
      "h: 85 | epoch: 65, train loss: 8.603514671325684, test loss: 5.856252193450928\n",
      "h: 85 | epoch: 66, train loss: 8.598089218139648, test loss: 5.845980644226074\n",
      "h: 85 | epoch: 67, train loss: 8.593209266662598, test loss: 5.836450099945068\n",
      "h: 85 | epoch: 68, train loss: 8.588817596435547, test loss: 5.827602863311768\n",
      "h: 85 | epoch: 69, train loss: 8.584866523742676, test loss: 5.8193817138671875\n",
      "h: 85 | epoch: 70, train loss: 8.58131217956543, test loss: 5.811739444732666\n",
      "h: 85 | epoch: 71, train loss: 8.578115463256836, test loss: 5.80463171005249\n",
      "h: 85 | epoch: 72, train loss: 8.575239181518555, test loss: 5.798016548156738\n",
      "h: 85 | epoch: 73, train loss: 8.572651863098145, test loss: 5.791854381561279\n",
      "h: 85 | epoch: 74, train loss: 8.57032585144043, test loss: 5.786114692687988\n",
      "h: 85 | epoch: 75, train loss: 8.568232536315918, test loss: 5.780762195587158\n",
      "h: 85 | epoch: 76, train loss: 8.566350936889648, test loss: 5.775769233703613\n",
      "h: 85 | epoch: 77, train loss: 8.564657211303711, test loss: 5.771106243133545\n",
      "h: 85 | epoch: 78, train loss: 8.563136100769043, test loss: 5.766753196716309\n",
      "h: 85 | epoch: 79, train loss: 8.561767578125, test loss: 5.7626848220825195\n",
      "h: 85 | epoch: 80, train loss: 8.56053638458252, test loss: 5.758881568908691\n",
      "h: 85 | epoch: 81, train loss: 8.559428215026855, test loss: 5.755323886871338\n",
      "h: 85 | epoch: 82, train loss: 8.558433532714844, test loss: 5.7519941329956055\n",
      "h: 85 | epoch: 83, train loss: 8.557538986206055, test loss: 5.748875617980957\n",
      "h: 85 | epoch: 84, train loss: 8.556734085083008, test loss: 5.745955467224121\n",
      "h: 85 | epoch: 85, train loss: 8.556010246276855, test loss: 5.743217945098877\n",
      "h: 85 | epoch: 86, train loss: 8.555360794067383, test loss: 5.740651607513428\n",
      "h: 85 | epoch: 87, train loss: 8.554774284362793, test loss: 5.738243103027344\n",
      "h: 85 | epoch: 88, train loss: 8.554248809814453, test loss: 5.7359843254089355\n",
      "h: 85 | epoch: 89, train loss: 8.553776741027832, test loss: 5.733862400054932\n",
      "h: 85 | epoch: 90, train loss: 8.553351402282715, test loss: 5.731870174407959\n",
      "h: 85 | epoch: 91, train loss: 8.552968978881836, test loss: 5.729997634887695\n",
      "h: 85 | epoch: 92, train loss: 8.552626609802246, test loss: 5.728238105773926\n",
      "h: 85 | epoch: 93, train loss: 8.552316665649414, test loss: 5.726583480834961\n",
      "h: 85 | epoch: 94, train loss: 8.552040100097656, test loss: 5.725027084350586\n",
      "h: 85 | epoch: 95, train loss: 8.551790237426758, test loss: 5.723562717437744\n",
      "h: 85 | epoch: 96, train loss: 8.551566123962402, test loss: 5.722184181213379\n",
      "h: 85 | epoch: 97, train loss: 8.551363945007324, test loss: 5.720885276794434\n",
      "h: 85 | epoch: 98, train loss: 8.551182746887207, test loss: 5.719664573669434\n",
      "h: 85 | epoch: 99, train loss: 8.551019668579102, test loss: 5.718512535095215\n",
      "h: 86 | epoch: 0, train loss: 42.864410400390625, test loss: 34.65163040161133\n",
      "h: 86 | epoch: 1, train loss: 38.08251190185547, test loss: 30.8477783203125\n",
      "h: 86 | epoch: 2, train loss: 33.96863555908203, test loss: 27.557077407836914\n",
      "h: 86 | epoch: 3, train loss: 30.42776107788086, test loss: 24.708385467529297\n",
      "h: 86 | epoch: 4, train loss: 27.37911605834961, test loss: 22.240947723388672\n",
      "h: 86 | epoch: 5, train loss: 24.753793716430664, test loss: 20.102691650390625\n",
      "h: 86 | epoch: 6, train loss: 22.49283790588379, test loss: 18.248920440673828\n",
      "h: 86 | epoch: 7, train loss: 20.545696258544922, test loss: 16.64113998413086\n",
      "h: 86 | epoch: 8, train loss: 18.86892318725586, test loss: 15.246185302734375\n",
      "h: 86 | epoch: 9, train loss: 17.425153732299805, test loss: 14.035435676574707\n",
      "h: 86 | epoch: 10, train loss: 16.182201385498047, test loss: 12.984156608581543\n",
      "h: 86 | epoch: 11, train loss: 15.112329483032227, test loss: 12.070981979370117\n",
      "h: 86 | epoch: 12, train loss: 14.191614151000977, test loss: 11.277427673339844\n",
      "h: 86 | epoch: 13, train loss: 13.399421691894531, test loss: 10.587498664855957\n",
      "h: 86 | epoch: 14, train loss: 12.71795654296875, test loss: 9.98735523223877\n",
      "h: 86 | epoch: 15, train loss: 12.131858825683594, test loss: 9.4650239944458\n",
      "h: 86 | epoch: 16, train loss: 11.627883911132812, test loss: 9.010139465332031\n",
      "h: 86 | epoch: 17, train loss: 11.194602012634277, test loss: 8.613729476928711\n",
      "h: 86 | epoch: 18, train loss: 10.82216739654541, test loss: 8.268026351928711\n",
      "h: 86 | epoch: 19, train loss: 10.502084732055664, test loss: 7.966309547424316\n",
      "h: 86 | epoch: 20, train loss: 10.227035522460938, test loss: 7.7027587890625\n",
      "h: 86 | epoch: 21, train loss: 9.990715026855469, test loss: 7.4723405838012695\n",
      "h: 86 | epoch: 22, train loss: 9.787692070007324, test loss: 7.270691871643066\n",
      "h: 86 | epoch: 23, train loss: 9.613290786743164, test loss: 7.094040870666504\n",
      "h: 86 | epoch: 24, train loss: 9.46349048614502, test loss: 6.939121246337891\n",
      "h: 86 | epoch: 25, train loss: 9.33482551574707, test loss: 6.803102970123291\n",
      "h: 86 | epoch: 26, train loss: 9.224319458007812, test loss: 6.683537483215332\n",
      "h: 86 | epoch: 27, train loss: 9.129411697387695, test loss: 6.578300476074219\n",
      "h: 86 | epoch: 28, train loss: 9.04790210723877, test loss: 6.485556602478027\n",
      "h: 86 | epoch: 29, train loss: 8.977896690368652, test loss: 6.403710842132568\n",
      "h: 86 | epoch: 30, train loss: 8.91777229309082, test loss: 6.3313798904418945\n",
      "h: 86 | epoch: 31, train loss: 8.866128921508789, test loss: 6.267366409301758\n",
      "h: 86 | epoch: 32, train loss: 8.821770668029785, test loss: 6.210631847381592\n",
      "h: 86 | epoch: 33, train loss: 8.783666610717773, test loss: 6.160272121429443\n",
      "h: 86 | epoch: 34, train loss: 8.75092887878418, test loss: 6.115501880645752\n",
      "h: 86 | epoch: 35, train loss: 8.72280216217041, test loss: 6.075638771057129\n",
      "h: 86 | epoch: 36, train loss: 8.698633193969727, test loss: 6.0400896072387695\n",
      "h: 86 | epoch: 37, train loss: 8.677862167358398, test loss: 6.008338451385498\n",
      "h: 86 | epoch: 38, train loss: 8.660009384155273, test loss: 5.97993278503418\n",
      "h: 86 | epoch: 39, train loss: 8.644662857055664, test loss: 5.954482078552246\n",
      "h: 86 | epoch: 40, train loss: 8.63146686553955, test loss: 5.931640625\n",
      "h: 86 | epoch: 41, train loss: 8.620119094848633, test loss: 5.911110877990723\n",
      "h: 86 | epoch: 42, train loss: 8.610359191894531, test loss: 5.8926286697387695\n",
      "h: 86 | epoch: 43, train loss: 8.60196304321289, test loss: 5.875965118408203\n",
      "h: 86 | epoch: 44, train loss: 8.594738006591797, test loss: 5.8609185218811035\n",
      "h: 86 | epoch: 45, train loss: 8.588520050048828, test loss: 5.847311973571777\n",
      "h: 86 | epoch: 46, train loss: 8.58316707611084, test loss: 5.834989547729492\n",
      "h: 86 | epoch: 47, train loss: 8.578558921813965, test loss: 5.8238139152526855\n",
      "h: 86 | epoch: 48, train loss: 8.574588775634766, test loss: 5.813665866851807\n",
      "h: 86 | epoch: 49, train loss: 8.571168899536133, test loss: 5.804438591003418\n",
      "h: 86 | epoch: 50, train loss: 8.56822395324707, test loss: 5.796036243438721\n",
      "h: 86 | epoch: 51, train loss: 8.565683364868164, test loss: 5.7883734703063965\n",
      "h: 86 | epoch: 52, train loss: 8.563493728637695, test loss: 5.781382083892822\n",
      "h: 86 | epoch: 53, train loss: 8.561605453491211, test loss: 5.7749924659729\n",
      "h: 86 | epoch: 54, train loss: 8.559977531433105, test loss: 5.7691473960876465\n",
      "h: 86 | epoch: 55, train loss: 8.558571815490723, test loss: 5.763793468475342\n",
      "h: 86 | epoch: 56, train loss: 8.557357788085938, test loss: 5.758885383605957\n",
      "h: 86 | epoch: 57, train loss: 8.556310653686523, test loss: 5.75438117980957\n",
      "h: 86 | epoch: 58, train loss: 8.55540657043457, test loss: 5.750243186950684\n",
      "h: 86 | epoch: 59, train loss: 8.554624557495117, test loss: 5.746440887451172\n",
      "h: 86 | epoch: 60, train loss: 8.553949356079102, test loss: 5.742941379547119\n",
      "h: 86 | epoch: 61, train loss: 8.553365707397461, test loss: 5.739717483520508\n",
      "h: 86 | epoch: 62, train loss: 8.552861213684082, test loss: 5.736747741699219\n",
      "h: 86 | epoch: 63, train loss: 8.552424430847168, test loss: 5.734008312225342\n",
      "h: 86 | epoch: 64, train loss: 8.552045822143555, test loss: 5.731480121612549\n",
      "h: 86 | epoch: 65, train loss: 8.551717758178711, test loss: 5.729145050048828\n",
      "h: 86 | epoch: 66, train loss: 8.551435470581055, test loss: 5.726988315582275\n",
      "h: 86 | epoch: 67, train loss: 8.551191329956055, test loss: 5.724992275238037\n",
      "h: 86 | epoch: 68, train loss: 8.55097770690918, test loss: 5.723146438598633\n",
      "h: 86 | epoch: 69, train loss: 8.550793647766113, test loss: 5.721437454223633\n",
      "h: 86 | epoch: 70, train loss: 8.550633430480957, test loss: 5.71985387802124\n",
      "h: 86 | epoch: 71, train loss: 8.550495147705078, test loss: 5.718388557434082\n",
      "h: 86 | epoch: 72, train loss: 8.550374984741211, test loss: 5.717029571533203\n",
      "h: 86 | epoch: 73, train loss: 8.550270080566406, test loss: 5.715767860412598\n",
      "h: 86 | epoch: 74, train loss: 8.550179481506348, test loss: 5.714598655700684\n",
      "h: 86 | epoch: 75, train loss: 8.550102233886719, test loss: 5.713512420654297\n",
      "h: 86 | epoch: 76, train loss: 8.550032615661621, test loss: 5.712504863739014\n",
      "h: 86 | epoch: 77, train loss: 8.54997444152832, test loss: 5.7115678787231445\n",
      "h: 86 | epoch: 78, train loss: 8.549921989440918, test loss: 5.710698127746582\n",
      "h: 86 | epoch: 79, train loss: 8.549878120422363, test loss: 5.709889888763428\n",
      "h: 86 | epoch: 80, train loss: 8.54983901977539, test loss: 5.709136962890625\n",
      "h: 86 | epoch: 81, train loss: 8.549805641174316, test loss: 5.708437919616699\n",
      "h: 86 | epoch: 82, train loss: 8.549775123596191, test loss: 5.707787990570068\n",
      "h: 86 | epoch: 83, train loss: 8.549749374389648, test loss: 5.707182884216309\n",
      "h: 86 | epoch: 84, train loss: 8.549726486206055, test loss: 5.706620216369629\n",
      "h: 86 | epoch: 85, train loss: 8.549707412719727, test loss: 5.706096172332764\n",
      "h: 86 | epoch: 86, train loss: 8.549691200256348, test loss: 5.705606937408447\n",
      "h: 86 | epoch: 87, train loss: 8.549674987792969, test loss: 5.70515251159668\n",
      "h: 86 | epoch: 88, train loss: 8.549662590026855, test loss: 5.7047295570373535\n",
      "h: 86 | epoch: 89, train loss: 8.549652099609375, test loss: 5.704334735870361\n",
      "h: 86 | epoch: 90, train loss: 8.549641609191895, test loss: 5.703967094421387\n",
      "h: 86 | epoch: 91, train loss: 8.54963207244873, test loss: 5.7036237716674805\n",
      "h: 86 | epoch: 92, train loss: 8.549625396728516, test loss: 5.703304767608643\n",
      "h: 86 | epoch: 93, train loss: 8.5496187210083, test loss: 5.703005313873291\n",
      "h: 86 | epoch: 94, train loss: 8.549612998962402, test loss: 5.702728271484375\n",
      "h: 86 | epoch: 95, train loss: 8.54960823059082, test loss: 5.7024688720703125\n",
      "h: 86 | epoch: 96, train loss: 8.549603462219238, test loss: 5.7022271156311035\n",
      "h: 86 | epoch: 97, train loss: 8.549599647521973, test loss: 5.702000617980957\n",
      "h: 86 | epoch: 98, train loss: 8.549595832824707, test loss: 5.701791763305664\n",
      "h: 86 | epoch: 99, train loss: 8.549592971801758, test loss: 5.701594352722168\n",
      "h: 87 | epoch: 0, train loss: 49.83600616455078, test loss: 40.652320861816406\n",
      "h: 87 | epoch: 1, train loss: 44.705841064453125, test loss: 36.4583740234375\n",
      "h: 87 | epoch: 2, train loss: 40.221229553222656, test loss: 32.78126907348633\n",
      "h: 87 | epoch: 3, train loss: 36.29784393310547, test loss: 29.55466651916504\n",
      "h: 87 | epoch: 4, train loss: 32.86334228515625, test loss: 26.72145652770996\n",
      "h: 87 | epoch: 5, train loss: 29.855422973632812, test loss: 24.232250213623047\n",
      "h: 87 | epoch: 6, train loss: 27.220233917236328, test loss: 22.04424476623535\n",
      "h: 87 | epoch: 7, train loss: 24.911056518554688, test loss: 20.120189666748047\n",
      "h: 87 | epoch: 8, train loss: 22.88726234436035, test loss: 18.42764663696289\n",
      "h: 87 | epoch: 9, train loss: 21.113445281982422, test loss: 16.938282012939453\n",
      "h: 87 | epoch: 10, train loss: 19.55869483947754, test loss: 15.627313613891602\n",
      "h: 87 | epoch: 11, train loss: 18.195972442626953, test loss: 14.473057746887207\n",
      "h: 87 | epoch: 12, train loss: 17.001623153686523, test loss: 13.456494331359863\n",
      "h: 87 | epoch: 13, train loss: 15.954913139343262, test loss: 12.560954093933105\n",
      "h: 87 | epoch: 14, train loss: 15.037671089172363, test loss: 11.771807670593262\n",
      "h: 87 | epoch: 15, train loss: 14.233953475952148, test loss: 11.07620620727539\n",
      "h: 87 | epoch: 16, train loss: 13.529777526855469, test loss: 10.462871551513672\n",
      "h: 87 | epoch: 17, train loss: 12.912870407104492, test loss: 9.92188835144043\n",
      "h: 87 | epoch: 18, train loss: 12.372467994689941, test loss: 9.444552421569824\n",
      "h: 87 | epoch: 19, train loss: 11.899118423461914, test loss: 9.023214340209961\n",
      "h: 87 | epoch: 20, train loss: 11.484525680541992, test loss: 8.65114974975586\n",
      "h: 87 | epoch: 21, train loss: 11.121424674987793, test loss: 8.322450637817383\n",
      "h: 87 | epoch: 22, train loss: 10.80342960357666, test loss: 8.031929016113281\n",
      "h: 87 | epoch: 23, train loss: 10.524945259094238, test loss: 7.775019645690918\n",
      "h: 87 | epoch: 24, train loss: 10.281064987182617, test loss: 7.547715663909912\n",
      "h: 87 | epoch: 25, train loss: 10.067487716674805, test loss: 7.34649133682251\n",
      "h: 87 | epoch: 26, train loss: 9.880444526672363, test loss: 7.168252468109131\n",
      "h: 87 | epoch: 27, train loss: 9.716633796691895, test loss: 7.010272979736328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 87 | epoch: 28, train loss: 9.57315731048584, test loss: 6.870164394378662\n",
      "h: 87 | epoch: 29, train loss: 9.447486877441406, test loss: 6.745821952819824\n",
      "h: 87 | epoch: 30, train loss: 9.337400436401367, test loss: 6.635397434234619\n",
      "h: 87 | epoch: 31, train loss: 9.24095630645752, test loss: 6.537259578704834\n",
      "h: 87 | epoch: 32, train loss: 9.156452178955078, test loss: 6.449982643127441\n",
      "h: 87 | epoch: 33, train loss: 9.082401275634766, test loss: 6.372302532196045\n",
      "h: 87 | epoch: 34, train loss: 9.017498970031738, test loss: 6.303114891052246\n",
      "h: 87 | epoch: 35, train loss: 8.96060562133789, test loss: 6.24144172668457\n",
      "h: 87 | epoch: 36, train loss: 8.910725593566895, test loss: 6.186425685882568\n",
      "h: 87 | epoch: 37, train loss: 8.866983413696289, test loss: 6.1373090744018555\n",
      "h: 87 | epoch: 38, train loss: 8.828617095947266, test loss: 6.093424320220947\n",
      "h: 87 | epoch: 39, train loss: 8.794960021972656, test loss: 6.054181098937988\n",
      "h: 87 | epoch: 40, train loss: 8.765423774719238, test loss: 6.019061088562012\n",
      "h: 87 | epoch: 41, train loss: 8.739500999450684, test loss: 5.987607479095459\n",
      "h: 87 | epoch: 42, train loss: 8.716741561889648, test loss: 5.959413051605225\n",
      "h: 87 | epoch: 43, train loss: 8.696755409240723, test loss: 5.9341206550598145\n",
      "h: 87 | epoch: 44, train loss: 8.679200172424316, test loss: 5.91141414642334\n",
      "h: 87 | epoch: 45, train loss: 8.663774490356445, test loss: 5.891011714935303\n",
      "h: 87 | epoch: 46, train loss: 8.650217056274414, test loss: 5.872665882110596\n",
      "h: 87 | epoch: 47, train loss: 8.638298034667969, test loss: 5.856158256530762\n",
      "h: 87 | epoch: 48, train loss: 8.627816200256348, test loss: 5.841291904449463\n",
      "h: 87 | epoch: 49, train loss: 8.618595123291016, test loss: 5.827895164489746\n",
      "h: 87 | epoch: 50, train loss: 8.610481262207031, test loss: 5.815812110900879\n",
      "h: 87 | epoch: 51, train loss: 8.603338241577148, test loss: 5.804908752441406\n",
      "h: 87 | epoch: 52, train loss: 8.597049713134766, test loss: 5.795060157775879\n",
      "h: 87 | epoch: 53, train loss: 8.591509819030762, test loss: 5.786162376403809\n",
      "h: 87 | epoch: 54, train loss: 8.586629867553711, test loss: 5.778115272521973\n",
      "h: 87 | epoch: 55, train loss: 8.582326889038086, test loss: 5.770833492279053\n",
      "h: 87 | epoch: 56, train loss: 8.578535079956055, test loss: 5.764241695404053\n",
      "h: 87 | epoch: 57, train loss: 8.575188636779785, test loss: 5.758270263671875\n",
      "h: 87 | epoch: 58, train loss: 8.572237014770508, test loss: 5.752856254577637\n",
      "h: 87 | epoch: 59, train loss: 8.569632530212402, test loss: 5.74794864654541\n",
      "h: 87 | epoch: 60, train loss: 8.56733226776123, test loss: 5.743494510650635\n",
      "h: 87 | epoch: 61, train loss: 8.565301895141602, test loss: 5.739451885223389\n",
      "h: 87 | epoch: 62, train loss: 8.563507080078125, test loss: 5.735780239105225\n",
      "h: 87 | epoch: 63, train loss: 8.561920166015625, test loss: 5.732445240020752\n",
      "h: 87 | epoch: 64, train loss: 8.560518264770508, test loss: 5.729411602020264\n",
      "h: 87 | epoch: 65, train loss: 8.55927848815918, test loss: 5.726655006408691\n",
      "h: 87 | epoch: 66, train loss: 8.558180809020996, test loss: 5.724147319793701\n",
      "h: 87 | epoch: 67, train loss: 8.557210922241211, test loss: 5.721865653991699\n",
      "h: 87 | epoch: 68, train loss: 8.556351661682129, test loss: 5.719789028167725\n",
      "h: 87 | epoch: 69, train loss: 8.555590629577637, test loss: 5.717898368835449\n",
      "h: 87 | epoch: 70, train loss: 8.554917335510254, test loss: 5.7161760330200195\n",
      "h: 87 | epoch: 71, train loss: 8.554319381713867, test loss: 5.714608192443848\n",
      "h: 87 | epoch: 72, train loss: 8.553791046142578, test loss: 5.713179588317871\n",
      "h: 87 | epoch: 73, train loss: 8.553321838378906, test loss: 5.71187686920166\n",
      "h: 87 | epoch: 74, train loss: 8.55290412902832, test loss: 5.710690498352051\n",
      "h: 87 | epoch: 75, train loss: 8.552536010742188, test loss: 5.709608554840088\n",
      "h: 87 | epoch: 76, train loss: 8.55220890045166, test loss: 5.708622455596924\n",
      "h: 87 | epoch: 77, train loss: 8.551918029785156, test loss: 5.7077250480651855\n",
      "h: 87 | epoch: 78, train loss: 8.55165958404541, test loss: 5.706905364990234\n",
      "h: 87 | epoch: 79, train loss: 8.551429748535156, test loss: 5.70615816116333\n",
      "h: 87 | epoch: 80, train loss: 8.551225662231445, test loss: 5.7054762840271\n",
      "h: 87 | epoch: 81, train loss: 8.551045417785645, test loss: 5.704855442047119\n",
      "h: 87 | epoch: 82, train loss: 8.550885200500488, test loss: 5.704289436340332\n",
      "h: 87 | epoch: 83, train loss: 8.550741195678711, test loss: 5.7037739753723145\n",
      "h: 87 | epoch: 84, train loss: 8.550613403320312, test loss: 5.703303337097168\n",
      "h: 87 | epoch: 85, train loss: 8.550500869750977, test loss: 5.702873706817627\n",
      "h: 87 | epoch: 86, train loss: 8.550399780273438, test loss: 5.7024827003479\n",
      "h: 87 | epoch: 87, train loss: 8.550310134887695, test loss: 5.702127456665039\n",
      "h: 87 | epoch: 88, train loss: 8.550230026245117, test loss: 5.701801776885986\n",
      "h: 87 | epoch: 89, train loss: 8.550158500671387, test loss: 5.701506614685059\n",
      "h: 87 | epoch: 90, train loss: 8.55009651184082, test loss: 5.701238632202148\n",
      "h: 87 | epoch: 91, train loss: 8.550039291381836, test loss: 5.700993061065674\n",
      "h: 87 | epoch: 92, train loss: 8.549989700317383, test loss: 5.700770378112793\n",
      "h: 87 | epoch: 93, train loss: 8.549944877624512, test loss: 5.700568199157715\n",
      "h: 87 | epoch: 94, train loss: 8.549904823303223, test loss: 5.700383186340332\n",
      "h: 87 | epoch: 95, train loss: 8.5498685836792, test loss: 5.700216293334961\n",
      "h: 87 | epoch: 96, train loss: 8.549837112426758, test loss: 5.700064659118652\n",
      "h: 87 | epoch: 97, train loss: 8.549808502197266, test loss: 5.699925899505615\n",
      "h: 87 | epoch: 98, train loss: 8.549783706665039, test loss: 5.699800968170166\n",
      "h: 87 | epoch: 99, train loss: 8.549760818481445, test loss: 5.699686050415039\n",
      "h: 88 | epoch: 0, train loss: 42.91620635986328, test loss: 35.69463348388672\n",
      "h: 88 | epoch: 1, train loss: 39.07841491699219, test loss: 32.51968765258789\n",
      "h: 88 | epoch: 2, train loss: 35.6682014465332, test loss: 29.6868839263916\n",
      "h: 88 | epoch: 3, train loss: 32.63666915893555, test loss: 27.158145904541016\n",
      "h: 88 | epoch: 4, train loss: 29.941030502319336, test loss: 24.899988174438477\n",
      "h: 88 | epoch: 5, train loss: 27.54366111755371, test loss: 22.882884979248047\n",
      "h: 88 | epoch: 6, train loss: 25.411439895629883, test loss: 21.08071517944336\n",
      "h: 88 | epoch: 7, train loss: 23.515071868896484, test loss: 19.47031021118164\n",
      "h: 88 | epoch: 8, train loss: 21.828617095947266, test loss: 18.031097412109375\n",
      "h: 88 | epoch: 9, train loss: 20.32906150817871, test loss: 16.744760513305664\n",
      "h: 88 | epoch: 10, train loss: 18.995948791503906, test loss: 15.594987869262695\n",
      "h: 88 | epoch: 11, train loss: 17.811084747314453, test loss: 14.567222595214844\n",
      "h: 88 | epoch: 12, train loss: 16.758249282836914, test loss: 13.648477554321289\n",
      "h: 88 | epoch: 13, train loss: 15.823007583618164, test loss: 12.8271484375\n",
      "h: 88 | epoch: 14, train loss: 14.992467880249023, test loss: 12.092864036560059\n",
      "h: 88 | epoch: 15, train loss: 14.255149841308594, test loss: 11.436357498168945\n",
      "h: 88 | epoch: 16, train loss: 13.600797653198242, test loss: 10.849343299865723\n",
      "h: 88 | epoch: 17, train loss: 13.020265579223633, test loss: 10.324408531188965\n",
      "h: 88 | epoch: 18, train loss: 12.505399703979492, test loss: 9.85493278503418\n",
      "h: 88 | epoch: 19, train loss: 12.048918724060059, test loss: 9.434989929199219\n",
      "h: 88 | epoch: 20, train loss: 11.64433479309082, test loss: 9.059290885925293\n",
      "h: 88 | epoch: 21, train loss: 11.285863876342773, test loss: 8.723099708557129\n",
      "h: 88 | epoch: 22, train loss: 10.968348503112793, test loss: 8.422192573547363\n",
      "h: 88 | epoch: 23, train loss: 10.687196731567383, test loss: 8.152791023254395\n",
      "h: 88 | epoch: 24, train loss: 10.438318252563477, test loss: 7.911521911621094\n",
      "h: 88 | epoch: 25, train loss: 10.218072891235352, test loss: 7.695372581481934\n",
      "h: 88 | epoch: 26, train loss: 10.023218154907227, test loss: 7.501657009124756\n",
      "h: 88 | epoch: 27, train loss: 9.850874900817871, test loss: 7.327972412109375\n",
      "h: 88 | epoch: 28, train loss: 9.698481559753418, test loss: 7.172176361083984\n",
      "h: 88 | epoch: 29, train loss: 9.563760757446289, test loss: 7.032358646392822\n",
      "h: 88 | epoch: 30, train loss: 9.444694519042969, test loss: 6.906815528869629\n",
      "h: 88 | epoch: 31, train loss: 9.339485168457031, test loss: 6.794025421142578\n",
      "h: 88 | epoch: 32, train loss: 9.246539115905762, test loss: 6.692634582519531\n",
      "h: 88 | epoch: 33, train loss: 9.164445877075195, test loss: 6.6014299392700195\n",
      "h: 88 | epoch: 34, train loss: 9.091952323913574, test loss: 6.519335746765137\n",
      "h: 88 | epoch: 35, train loss: 9.027947425842285, test loss: 6.445389747619629\n",
      "h: 88 | epoch: 36, train loss: 8.97144603729248, test loss: 6.378735542297363\n",
      "h: 88 | epoch: 37, train loss: 8.921577453613281, test loss: 6.318609714508057\n",
      "h: 88 | epoch: 38, train loss: 8.877571105957031, test loss: 6.264327526092529\n",
      "h: 88 | epoch: 39, train loss: 8.838743209838867, test loss: 6.215282917022705\n",
      "h: 88 | epoch: 40, train loss: 8.804486274719238, test loss: 6.170932292938232\n",
      "h: 88 | epoch: 41, train loss: 8.774270057678223, test loss: 6.130794048309326\n",
      "h: 88 | epoch: 42, train loss: 8.747618675231934, test loss: 6.094433784484863\n",
      "h: 88 | epoch: 43, train loss: 8.724116325378418, test loss: 6.061468124389648\n",
      "h: 88 | epoch: 44, train loss: 8.703392028808594, test loss: 6.031551361083984\n",
      "h: 88 | epoch: 45, train loss: 8.68511962890625, test loss: 6.004378318786621\n",
      "h: 88 | epoch: 46, train loss: 8.669011116027832, test loss: 5.979672431945801\n",
      "h: 88 | epoch: 47, train loss: 8.654809951782227, test loss: 5.957189083099365\n",
      "h: 88 | epoch: 48, train loss: 8.642292022705078, test loss: 5.936707973480225\n",
      "h: 88 | epoch: 49, train loss: 8.631258964538574, test loss: 5.918034553527832\n",
      "h: 88 | epoch: 50, train loss: 8.621537208557129, test loss: 5.900992393493652\n",
      "h: 88 | epoch: 51, train loss: 8.612968444824219, test loss: 5.885422706604004\n",
      "h: 88 | epoch: 52, train loss: 8.60541820526123, test loss: 5.871186256408691\n",
      "h: 88 | epoch: 53, train loss: 8.598764419555664, test loss: 5.858156681060791\n",
      "h: 88 | epoch: 54, train loss: 8.592903137207031, test loss: 5.846219539642334\n",
      "h: 88 | epoch: 55, train loss: 8.587739944458008, test loss: 5.835272789001465\n",
      "h: 88 | epoch: 56, train loss: 8.583189010620117, test loss: 5.825225353240967\n",
      "h: 88 | epoch: 57, train loss: 8.579180717468262, test loss: 5.815996170043945\n",
      "h: 88 | epoch: 58, train loss: 8.57564926147461, test loss: 5.8075103759765625\n",
      "h: 88 | epoch: 59, train loss: 8.572538375854492, test loss: 5.799699783325195\n",
      "h: 88 | epoch: 60, train loss: 8.569799423217773, test loss: 5.792506217956543\n",
      "h: 88 | epoch: 61, train loss: 8.567386627197266, test loss: 5.78587532043457\n",
      "h: 88 | epoch: 62, train loss: 8.56525993347168, test loss: 5.779757022857666\n",
      "h: 88 | epoch: 63, train loss: 8.563386917114258, test loss: 5.774107933044434\n",
      "h: 88 | epoch: 64, train loss: 8.561738967895508, test loss: 5.768888473510742\n",
      "h: 88 | epoch: 65, train loss: 8.560286521911621, test loss: 5.764061450958252\n",
      "h: 88 | epoch: 66, train loss: 8.55900764465332, test loss: 5.759594440460205\n",
      "h: 88 | epoch: 67, train loss: 8.557880401611328, test loss: 5.755456924438477\n",
      "h: 88 | epoch: 68, train loss: 8.556889533996582, test loss: 5.751623153686523\n",
      "h: 88 | epoch: 69, train loss: 8.556015014648438, test loss: 5.748068332672119\n",
      "h: 88 | epoch: 70, train loss: 8.555246353149414, test loss: 5.744770526885986\n",
      "h: 88 | epoch: 71, train loss: 8.55456829071045, test loss: 5.741707801818848\n",
      "h: 88 | epoch: 72, train loss: 8.553972244262695, test loss: 5.73886251449585\n",
      "h: 88 | epoch: 73, train loss: 8.553445816040039, test loss: 5.736218452453613\n",
      "h: 88 | epoch: 74, train loss: 8.552984237670898, test loss: 5.733759880065918\n",
      "h: 88 | epoch: 75, train loss: 8.552577018737793, test loss: 5.731471061706543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 88 | epoch: 76, train loss: 8.552218437194824, test loss: 5.729340553283691\n",
      "h: 88 | epoch: 77, train loss: 8.551901817321777, test loss: 5.727356910705566\n",
      "h: 88 | epoch: 78, train loss: 8.551623344421387, test loss: 5.725508213043213\n",
      "h: 88 | epoch: 79, train loss: 8.55137825012207, test loss: 5.723784446716309\n",
      "h: 88 | epoch: 80, train loss: 8.551162719726562, test loss: 5.7221784591674805\n",
      "h: 88 | epoch: 81, train loss: 8.550973892211914, test loss: 5.720678806304932\n",
      "h: 88 | epoch: 82, train loss: 8.550806045532227, test loss: 5.719280242919922\n",
      "h: 88 | epoch: 83, train loss: 8.550658226013184, test loss: 5.717972755432129\n",
      "h: 88 | epoch: 84, train loss: 8.550529479980469, test loss: 5.7167534828186035\n",
      "h: 88 | epoch: 85, train loss: 8.550414085388184, test loss: 5.7156147956848145\n",
      "h: 88 | epoch: 86, train loss: 8.550313949584961, test loss: 5.714548587799072\n",
      "h: 88 | epoch: 87, train loss: 8.550226211547852, test loss: 5.713554382324219\n",
      "h: 88 | epoch: 88, train loss: 8.550146102905273, test loss: 5.712623596191406\n",
      "h: 88 | epoch: 89, train loss: 8.550079345703125, test loss: 5.711752891540527\n",
      "h: 88 | epoch: 90, train loss: 8.550018310546875, test loss: 5.710938453674316\n",
      "h: 88 | epoch: 91, train loss: 8.549964904785156, test loss: 5.710177898406982\n",
      "h: 88 | epoch: 92, train loss: 8.549918174743652, test loss: 5.709465026855469\n",
      "h: 88 | epoch: 93, train loss: 8.549877166748047, test loss: 5.708796501159668\n",
      "h: 88 | epoch: 94, train loss: 8.549840927124023, test loss: 5.708171844482422\n",
      "h: 88 | epoch: 95, train loss: 8.549809455871582, test loss: 5.707587718963623\n",
      "h: 88 | epoch: 96, train loss: 8.54978084564209, test loss: 5.707040309906006\n",
      "h: 88 | epoch: 97, train loss: 8.549756050109863, test loss: 5.706527233123779\n",
      "h: 88 | epoch: 98, train loss: 8.549734115600586, test loss: 5.706045627593994\n",
      "h: 88 | epoch: 99, train loss: 8.549715042114258, test loss: 5.705596923828125\n",
      "h: 89 | epoch: 0, train loss: 42.83210754394531, test loss: 34.8656005859375\n",
      "h: 89 | epoch: 1, train loss: 38.027183532714844, test loss: 31.008569717407227\n",
      "h: 89 | epoch: 2, train loss: 33.89573287963867, test loss: 27.6737060546875\n",
      "h: 89 | epoch: 3, train loss: 30.34189796447754, test loss: 24.788591384887695\n",
      "h: 89 | epoch: 4, train loss: 27.2841796875, test loss: 22.29132652282715\n",
      "h: 89 | epoch: 5, train loss: 24.653003692626953, test loss: 20.12885856628418\n",
      "h: 89 | epoch: 6, train loss: 22.388858795166016, test loss: 18.255619049072266\n",
      "h: 89 | epoch: 7, train loss: 20.440673828125, test loss: 16.63237762451172\n",
      "h: 89 | epoch: 8, train loss: 18.764591217041016, test loss: 15.225326538085938\n",
      "h: 89 | epoch: 9, train loss: 17.322866439819336, test loss: 14.005281448364258\n",
      "h: 89 | epoch: 10, train loss: 16.083003997802734, test loss: 12.947038650512695\n",
      "h: 89 | epoch: 11, train loss: 15.016998291015625, test loss: 12.028825759887695\n",
      "h: 89 | epoch: 12, train loss: 14.100702285766602, test loss: 11.231801986694336\n",
      "h: 89 | epoch: 13, train loss: 13.313302993774414, test loss: 10.53968620300293\n",
      "h: 89 | epoch: 14, train loss: 12.636845588684082, test loss: 9.93839168548584\n",
      "h: 89 | epoch: 15, train loss: 12.055855751037598, test loss: 9.415729522705078\n",
      "h: 89 | epoch: 16, train loss: 11.556984901428223, test loss: 8.961162567138672\n",
      "h: 89 | epoch: 17, train loss: 11.12873363494873, test loss: 8.565572738647461\n",
      "h: 89 | epoch: 18, train loss: 10.761191368103027, test loss: 8.221067428588867\n",
      "h: 89 | epoch: 19, train loss: 10.445821762084961, test loss: 7.920830726623535\n",
      "h: 89 | epoch: 20, train loss: 10.175272941589355, test loss: 7.65895938873291\n",
      "h: 89 | epoch: 21, train loss: 9.943219184875488, test loss: 7.430356025695801\n",
      "h: 89 | epoch: 22, train loss: 9.744218826293945, test loss: 7.230602264404297\n",
      "h: 89 | epoch: 23, train loss: 9.573587417602539, test loss: 7.055887699127197\n",
      "h: 89 | epoch: 24, train loss: 9.4273042678833, test loss: 6.902907371520996\n",
      "h: 89 | epoch: 25, train loss: 9.301907539367676, test loss: 6.768808841705322\n",
      "h: 89 | epoch: 26, train loss: 9.194425582885742, test loss: 6.651122093200684\n",
      "h: 89 | epoch: 27, train loss: 9.102307319641113, test loss: 6.547711372375488\n",
      "h: 89 | epoch: 28, train loss: 9.02336311340332, test loss: 6.456727027893066\n",
      "h: 89 | epoch: 29, train loss: 8.955710411071777, test loss: 6.3765668869018555\n",
      "h: 89 | epoch: 30, train loss: 8.89773941040039, test loss: 6.305846214294434\n",
      "h: 89 | epoch: 31, train loss: 8.848062515258789, test loss: 6.243364334106445\n",
      "h: 89 | epoch: 32, train loss: 8.80549430847168, test loss: 6.188080787658691\n",
      "h: 89 | epoch: 33, train loss: 8.769017219543457, test loss: 6.1390910148620605\n",
      "h: 89 | epoch: 34, train loss: 8.73775863647461, test loss: 6.095614433288574\n",
      "h: 89 | epoch: 35, train loss: 8.710970878601074, test loss: 6.0569682121276855\n",
      "h: 89 | epoch: 36, train loss: 8.68801498413086, test loss: 6.0225629806518555\n",
      "h: 89 | epoch: 37, train loss: 8.668339729309082, test loss: 5.991884231567383\n",
      "h: 89 | epoch: 38, train loss: 8.65147590637207, test loss: 5.964486122131348\n",
      "h: 89 | epoch: 39, train loss: 8.637018203735352, test loss: 5.939977645874023\n",
      "h: 89 | epoch: 40, train loss: 8.624626159667969, test loss: 5.9180192947387695\n",
      "h: 89 | epoch: 41, train loss: 8.61400032043457, test loss: 5.898315906524658\n",
      "h: 89 | epoch: 42, train loss: 8.604888916015625, test loss: 5.8806071281433105\n",
      "h: 89 | epoch: 43, train loss: 8.597074508666992, test loss: 5.864667892456055\n",
      "h: 89 | epoch: 44, train loss: 8.590373039245605, test loss: 5.850298881530762\n",
      "h: 89 | epoch: 45, train loss: 8.584624290466309, test loss: 5.837326526641846\n",
      "h: 89 | epoch: 46, train loss: 8.579691886901855, test loss: 5.825596332550049\n",
      "h: 89 | epoch: 47, train loss: 8.575459480285645, test loss: 5.814976692199707\n",
      "h: 89 | epoch: 48, train loss: 8.571825981140137, test loss: 5.8053483963012695\n",
      "h: 89 | epoch: 49, train loss: 8.568707466125488, test loss: 5.79660701751709\n",
      "h: 89 | epoch: 50, train loss: 8.566030502319336, test loss: 5.788661003112793\n",
      "h: 89 | epoch: 51, train loss: 8.563730239868164, test loss: 5.781428337097168\n",
      "h: 89 | epoch: 52, train loss: 8.561756134033203, test loss: 5.774837017059326\n",
      "h: 89 | epoch: 53, train loss: 8.56005859375, test loss: 5.76882266998291\n",
      "h: 89 | epoch: 54, train loss: 8.558600425720215, test loss: 5.763329982757568\n",
      "h: 89 | epoch: 55, train loss: 8.557348251342773, test loss: 5.7583088874816895\n",
      "h: 89 | epoch: 56, train loss: 8.556269645690918, test loss: 5.753711700439453\n",
      "h: 89 | epoch: 57, train loss: 8.555342674255371, test loss: 5.749500274658203\n",
      "h: 89 | epoch: 58, train loss: 8.554546356201172, test loss: 5.745638847351074\n",
      "h: 89 | epoch: 59, train loss: 8.553861618041992, test loss: 5.742093563079834\n",
      "h: 89 | epoch: 60, train loss: 8.55327033996582, test loss: 5.738837242126465\n",
      "h: 89 | epoch: 61, train loss: 8.552762985229492, test loss: 5.735843181610107\n",
      "h: 89 | epoch: 62, train loss: 8.552326202392578, test loss: 5.733089923858643\n",
      "h: 89 | epoch: 63, train loss: 8.551948547363281, test loss: 5.73055362701416\n",
      "h: 89 | epoch: 64, train loss: 8.551624298095703, test loss: 5.728217124938965\n",
      "h: 89 | epoch: 65, train loss: 8.551344871520996, test loss: 5.726061820983887\n",
      "h: 89 | epoch: 66, train loss: 8.551103591918945, test loss: 5.7240753173828125\n",
      "h: 89 | epoch: 67, train loss: 8.550895690917969, test loss: 5.722239971160889\n",
      "h: 89 | epoch: 68, train loss: 8.550716400146484, test loss: 5.720545768737793\n",
      "h: 89 | epoch: 69, train loss: 8.550561904907227, test loss: 5.7189812660217285\n",
      "h: 89 | epoch: 70, train loss: 8.55042839050293, test loss: 5.717532157897949\n",
      "h: 89 | epoch: 71, train loss: 8.550312995910645, test loss: 5.716193199157715\n",
      "h: 89 | epoch: 72, train loss: 8.550213813781738, test loss: 5.714955806732178\n",
      "h: 89 | epoch: 73, train loss: 8.550127029418945, test loss: 5.713808536529541\n",
      "h: 89 | epoch: 74, train loss: 8.550054550170898, test loss: 5.712746620178223\n",
      "h: 89 | epoch: 75, train loss: 8.549989700317383, test loss: 5.711761474609375\n",
      "h: 89 | epoch: 76, train loss: 8.549933433532715, test loss: 5.710850238800049\n",
      "h: 89 | epoch: 77, train loss: 8.549885749816895, test loss: 5.7100043296813965\n",
      "h: 89 | epoch: 78, train loss: 8.549844741821289, test loss: 5.709221363067627\n",
      "h: 89 | epoch: 79, train loss: 8.549808502197266, test loss: 5.708492279052734\n",
      "h: 89 | epoch: 80, train loss: 8.54977798461914, test loss: 5.707817554473877\n",
      "h: 89 | epoch: 81, train loss: 8.549749374389648, test loss: 5.7071919441223145\n",
      "h: 89 | epoch: 82, train loss: 8.549726486206055, test loss: 5.706608772277832\n",
      "h: 89 | epoch: 83, train loss: 8.54970645904541, test loss: 5.706068515777588\n",
      "h: 89 | epoch: 84, train loss: 8.549688339233398, test loss: 5.705567359924316\n",
      "h: 89 | epoch: 85, train loss: 8.549674034118652, test loss: 5.7051005363464355\n",
      "h: 89 | epoch: 86, train loss: 8.549661636352539, test loss: 5.704667091369629\n",
      "h: 89 | epoch: 87, train loss: 8.549649238586426, test loss: 5.7042646408081055\n",
      "h: 89 | epoch: 88, train loss: 8.549639701843262, test loss: 5.7038893699646\n",
      "h: 89 | epoch: 89, train loss: 8.549630165100098, test loss: 5.703541278839111\n",
      "h: 89 | epoch: 90, train loss: 8.549623489379883, test loss: 5.703217506408691\n",
      "h: 89 | epoch: 91, train loss: 8.549616813659668, test loss: 5.702916145324707\n",
      "h: 89 | epoch: 92, train loss: 8.54961109161377, test loss: 5.702636241912842\n",
      "h: 89 | epoch: 93, train loss: 8.549606323242188, test loss: 5.702374458312988\n",
      "h: 89 | epoch: 94, train loss: 8.549601554870605, test loss: 5.702132225036621\n",
      "h: 89 | epoch: 95, train loss: 8.54959774017334, test loss: 5.701906204223633\n",
      "h: 89 | epoch: 96, train loss: 8.549593925476074, test loss: 5.701695442199707\n",
      "h: 89 | epoch: 97, train loss: 8.549592018127441, test loss: 5.701498985290527\n",
      "h: 89 | epoch: 98, train loss: 8.549589157104492, test loss: 5.7013163566589355\n",
      "h: 89 | epoch: 99, train loss: 8.549586296081543, test loss: 5.701147556304932\n",
      "h: 90 | epoch: 0, train loss: 47.633811950683594, test loss: 39.241275787353516\n",
      "h: 90 | epoch: 1, train loss: 42.91543197631836, test loss: 35.35419464111328\n",
      "h: 90 | epoch: 2, train loss: 38.76811599731445, test loss: 31.926227569580078\n",
      "h: 90 | epoch: 3, train loss: 35.120643615722656, test loss: 28.901281356811523\n",
      "h: 90 | epoch: 4, train loss: 31.911474227905273, test loss: 26.23067855834961\n",
      "h: 90 | epoch: 5, train loss: 29.087188720703125, test loss: 23.87199592590332\n",
      "h: 90 | epoch: 6, train loss: 26.601276397705078, test loss: 21.78818702697754\n",
      "h: 90 | epoch: 7, train loss: 24.413101196289062, test loss: 19.9467830657959\n",
      "h: 90 | epoch: 8, train loss: 22.487077713012695, test loss: 18.319297790527344\n",
      "h: 90 | epoch: 9, train loss: 20.791976928710938, test loss: 16.88066864013672\n",
      "h: 90 | epoch: 10, train loss: 19.300357818603516, test loss: 15.6088285446167\n",
      "h: 90 | epoch: 11, train loss: 17.988056182861328, test loss: 14.48432445526123\n",
      "h: 90 | epoch: 12, train loss: 16.83379554748535, test loss: 13.489988327026367\n",
      "h: 90 | epoch: 13, train loss: 15.818801879882812, test loss: 12.610668182373047\n",
      "h: 90 | epoch: 14, train loss: 14.92652416229248, test loss: 11.832971572875977\n",
      "h: 90 | epoch: 15, train loss: 14.142359733581543, test loss: 11.145072937011719\n",
      "h: 90 | epoch: 16, train loss: 13.453410148620605, test loss: 10.536517143249512\n",
      "h: 90 | epoch: 17, train loss: 12.848302841186523, test loss: 9.998064041137695\n",
      "h: 90 | epoch: 18, train loss: 12.31699275970459, test loss: 9.521550178527832\n",
      "h: 90 | epoch: 19, train loss: 11.850619316101074, test loss: 9.099756240844727\n",
      "h: 90 | epoch: 20, train loss: 11.44136905670166, test loss: 8.726303100585938\n",
      "h: 90 | epoch: 21, train loss: 11.082345962524414, test loss: 8.395562171936035\n",
      "h: 90 | epoch: 22, train loss: 10.767474174499512, test loss: 8.102550506591797\n",
      "h: 90 | epoch: 23, train loss: 10.491398811340332, test loss: 7.8428754806518555\n",
      "h: 90 | epoch: 24, train loss: 10.249402046203613, test loss: 7.612648963928223\n",
      "h: 90 | epoch: 25, train loss: 10.037328720092773, test loss: 7.408447265625\n",
      "h: 90 | epoch: 26, train loss: 9.851522445678711, test loss: 7.227243900299072\n",
      "h: 90 | epoch: 27, train loss: 9.688766479492188, test loss: 7.066368103027344\n",
      "h: 90 | epoch: 28, train loss: 9.546228408813477, test loss: 6.923459053039551\n",
      "h: 90 | epoch: 29, train loss: 9.421426773071289, test loss: 6.796438694000244\n",
      "h: 90 | epoch: 30, train loss: 9.312170028686523, test loss: 6.683467864990234\n",
      "h: 90 | epoch: 31, train loss: 9.21653938293457, test loss: 6.582930088043213\n",
      "h: 90 | epoch: 32, train loss: 9.132848739624023, test loss: 6.4933953285217285\n",
      "h: 90 | epoch: 33, train loss: 9.059619903564453, test loss: 6.41359806060791\n",
      "h: 90 | epoch: 34, train loss: 8.995553016662598, test loss: 6.342430114746094\n",
      "h: 90 | epoch: 35, train loss: 8.939508438110352, test loss: 6.278904914855957\n",
      "h: 90 | epoch: 36, train loss: 8.890486717224121, test loss: 6.222158432006836\n",
      "h: 90 | epoch: 37, train loss: 8.847614288330078, test loss: 6.171422004699707\n",
      "h: 90 | epoch: 38, train loss: 8.8101224899292, test loss: 6.126021862030029\n",
      "h: 90 | epoch: 39, train loss: 8.777338981628418, test loss: 6.085358619689941\n",
      "h: 90 | epoch: 40, train loss: 8.748674392700195, test loss: 6.0489068031311035\n",
      "h: 90 | epoch: 41, train loss: 8.723611831665039, test loss: 6.016196250915527\n",
      "h: 90 | epoch: 42, train loss: 8.701703071594238, test loss: 5.986819267272949\n",
      "h: 90 | epoch: 43, train loss: 8.682550430297852, test loss: 5.960408687591553\n",
      "h: 90 | epoch: 44, train loss: 8.665807723999023, test loss: 5.936642646789551\n",
      "h: 90 | epoch: 45, train loss: 8.65117359161377, test loss: 5.9152350425720215\n",
      "h: 90 | epoch: 46, train loss: 8.63838005065918, test loss: 5.895932197570801\n",
      "h: 90 | epoch: 47, train loss: 8.627199172973633, test loss: 5.878510475158691\n",
      "h: 90 | epoch: 48, train loss: 8.617426872253418, test loss: 5.862771987915039\n",
      "h: 90 | epoch: 49, train loss: 8.608884811401367, test loss: 5.848536968231201\n",
      "h: 90 | epoch: 50, train loss: 8.601420402526855, test loss: 5.835653305053711\n",
      "h: 90 | epoch: 51, train loss: 8.594894409179688, test loss: 5.823976993560791\n",
      "h: 90 | epoch: 52, train loss: 8.589192390441895, test loss: 5.813387870788574\n",
      "h: 90 | epoch: 53, train loss: 8.584207534790039, test loss: 5.803773403167725\n",
      "h: 90 | epoch: 54, train loss: 8.579851150512695, test loss: 5.795036315917969\n",
      "h: 90 | epoch: 55, train loss: 8.576044082641602, test loss: 5.787088394165039\n",
      "h: 90 | epoch: 56, train loss: 8.572715759277344, test loss: 5.779853820800781\n",
      "h: 90 | epoch: 57, train loss: 8.569807052612305, test loss: 5.773260593414307\n",
      "h: 90 | epoch: 58, train loss: 8.567264556884766, test loss: 5.767247200012207\n",
      "h: 90 | epoch: 59, train loss: 8.565042495727539, test loss: 5.7617573738098145\n",
      "h: 90 | epoch: 60, train loss: 8.563098907470703, test loss: 5.75674295425415\n",
      "h: 90 | epoch: 61, train loss: 8.5614013671875, test loss: 5.752157211303711\n",
      "h: 90 | epoch: 62, train loss: 8.559915542602539, test loss: 5.747961044311523\n",
      "h: 90 | epoch: 63, train loss: 8.558618545532227, test loss: 5.744117736816406\n",
      "h: 90 | epoch: 64, train loss: 8.557482719421387, test loss: 5.74059534072876\n",
      "h: 90 | epoch: 65, train loss: 8.556492805480957, test loss: 5.737365245819092\n",
      "h: 90 | epoch: 66, train loss: 8.555624008178711, test loss: 5.734399795532227\n",
      "h: 90 | epoch: 67, train loss: 8.554865837097168, test loss: 5.731677532196045\n",
      "h: 90 | epoch: 68, train loss: 8.554203987121582, test loss: 5.7291741371154785\n",
      "h: 90 | epoch: 69, train loss: 8.55362319946289, test loss: 5.726874351501465\n",
      "h: 90 | epoch: 70, train loss: 8.553115844726562, test loss: 5.724756717681885\n",
      "h: 90 | epoch: 71, train loss: 8.55267333984375, test loss: 5.722806930541992\n",
      "h: 90 | epoch: 72, train loss: 8.552286148071289, test loss: 5.721011161804199\n",
      "h: 90 | epoch: 73, train loss: 8.551946640014648, test loss: 5.719357013702393\n",
      "h: 90 | epoch: 74, train loss: 8.551650047302246, test loss: 5.717830181121826\n",
      "h: 90 | epoch: 75, train loss: 8.551389694213867, test loss: 5.7164225578308105\n",
      "h: 90 | epoch: 76, train loss: 8.551163673400879, test loss: 5.715122699737549\n",
      "h: 90 | epoch: 77, train loss: 8.55096435546875, test loss: 5.71392297744751\n",
      "h: 90 | epoch: 78, train loss: 8.550790786743164, test loss: 5.7128143310546875\n",
      "h: 90 | epoch: 79, train loss: 8.550639152526855, test loss: 5.711790084838867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 90 | epoch: 80, train loss: 8.550506591796875, test loss: 5.710843086242676\n",
      "h: 90 | epoch: 81, train loss: 8.550389289855957, test loss: 5.709967613220215\n",
      "h: 90 | epoch: 82, train loss: 8.550288200378418, test loss: 5.709156513214111\n",
      "h: 90 | epoch: 83, train loss: 8.550199508666992, test loss: 5.708407878875732\n",
      "h: 90 | epoch: 84, train loss: 8.550121307373047, test loss: 5.7077131271362305\n",
      "h: 90 | epoch: 85, train loss: 8.550052642822266, test loss: 5.707070350646973\n",
      "h: 90 | epoch: 86, train loss: 8.549993515014648, test loss: 5.706475257873535\n",
      "h: 90 | epoch: 87, train loss: 8.54994010925293, test loss: 5.705923080444336\n",
      "h: 90 | epoch: 88, train loss: 8.549895286560059, test loss: 5.705412864685059\n",
      "h: 90 | epoch: 89, train loss: 8.54985523223877, test loss: 5.704938888549805\n",
      "h: 90 | epoch: 90, train loss: 8.549819946289062, test loss: 5.7044997215271\n",
      "h: 90 | epoch: 91, train loss: 8.549789428710938, test loss: 5.704092979431152\n",
      "h: 90 | epoch: 92, train loss: 8.549762725830078, test loss: 5.703713893890381\n",
      "h: 90 | epoch: 93, train loss: 8.549738883972168, test loss: 5.703364849090576\n",
      "h: 90 | epoch: 94, train loss: 8.549717903137207, test loss: 5.703039169311523\n",
      "h: 90 | epoch: 95, train loss: 8.549699783325195, test loss: 5.702737331390381\n",
      "h: 90 | epoch: 96, train loss: 8.549684524536133, test loss: 5.702458381652832\n",
      "h: 90 | epoch: 97, train loss: 8.549670219421387, test loss: 5.702198505401611\n",
      "h: 90 | epoch: 98, train loss: 8.549657821655273, test loss: 5.701956748962402\n",
      "h: 90 | epoch: 99, train loss: 8.549647331237793, test loss: 5.701732635498047\n",
      "h: 91 | epoch: 0, train loss: 43.47977828979492, test loss: 36.12518310546875\n",
      "h: 91 | epoch: 1, train loss: 39.825714111328125, test loss: 33.07886505126953\n",
      "h: 91 | epoch: 2, train loss: 36.552711486816406, test loss: 30.342092514038086\n",
      "h: 91 | epoch: 3, train loss: 33.61980056762695, test loss: 27.882308959960938\n",
      "h: 91 | epoch: 4, train loss: 30.99088478088379, test loss: 25.670684814453125\n",
      "h: 91 | epoch: 5, train loss: 28.634044647216797, test loss: 23.681676864624023\n",
      "h: 91 | epoch: 6, train loss: 26.52095603942871, test loss: 21.892518997192383\n",
      "h: 91 | epoch: 7, train loss: 24.62640953063965, test loss: 20.28292465209961\n",
      "h: 91 | epoch: 8, train loss: 22.927932739257812, test loss: 18.834739685058594\n",
      "h: 91 | epoch: 9, train loss: 21.405426025390625, test loss: 17.531719207763672\n",
      "h: 91 | epoch: 10, train loss: 20.04090690612793, test loss: 16.35927963256836\n",
      "h: 91 | epoch: 11, train loss: 18.81825065612793, test loss: 15.304344177246094\n",
      "h: 91 | epoch: 12, train loss: 17.722986221313477, test loss: 14.355143547058105\n",
      "h: 91 | epoch: 13, train loss: 16.742109298706055, test loss: 13.501100540161133\n",
      "h: 91 | epoch: 14, train loss: 15.863943099975586, test loss: 12.732687950134277\n",
      "h: 91 | epoch: 15, train loss: 15.077978134155273, test loss: 12.041337966918945\n",
      "h: 91 | epoch: 16, train loss: 14.374765396118164, test loss: 11.419323921203613\n",
      "h: 91 | epoch: 17, train loss: 13.745800971984863, test loss: 10.85969352722168\n",
      "h: 91 | epoch: 18, train loss: 13.183435440063477, test loss: 10.356185913085938\n",
      "h: 91 | epoch: 19, train loss: 12.680790901184082, test loss: 9.903152465820312\n",
      "h: 91 | epoch: 20, train loss: 12.231674194335938, test loss: 9.495522499084473\n",
      "h: 91 | epoch: 21, train loss: 11.830522537231445, test loss: 9.128708839416504\n",
      "h: 91 | epoch: 22, train loss: 11.472331047058105, test loss: 8.79859733581543\n",
      "h: 91 | epoch: 23, train loss: 11.152605056762695, test loss: 8.501481056213379\n",
      "h: 91 | epoch: 24, train loss: 10.867303848266602, test loss: 8.234018325805664\n",
      "h: 91 | epoch: 25, train loss: 10.612799644470215, test loss: 7.993208885192871\n",
      "h: 91 | epoch: 26, train loss: 10.385839462280273, test loss: 7.7763543128967285\n",
      "h: 91 | epoch: 27, train loss: 10.183499336242676, test loss: 7.5810227394104\n",
      "h: 91 | epoch: 28, train loss: 10.00316047668457, test loss: 7.405034065246582\n",
      "h: 91 | epoch: 29, train loss: 9.842473983764648, test loss: 7.246424198150635\n",
      "h: 91 | epoch: 30, train loss: 9.69933795928955, test loss: 7.103430271148682\n",
      "h: 91 | epoch: 31, train loss: 9.571867942810059, test loss: 6.974475860595703\n",
      "h: 91 | epoch: 32, train loss: 9.458374977111816, test loss: 6.858128547668457\n",
      "h: 91 | epoch: 33, train loss: 9.357352256774902, test loss: 6.753118991851807\n",
      "h: 91 | epoch: 34, train loss: 9.267448425292969, test loss: 6.658295631408691\n",
      "h: 91 | epoch: 35, train loss: 9.187456130981445, test loss: 6.5726318359375\n",
      "h: 91 | epoch: 36, train loss: 9.116300582885742, test loss: 6.4952073097229\n",
      "h: 91 | epoch: 37, train loss: 9.053014755249023, test loss: 6.4251861572265625\n",
      "h: 91 | epoch: 38, train loss: 8.996740341186523, test loss: 6.361828804016113\n",
      "h: 91 | epoch: 39, train loss: 8.946709632873535, test loss: 6.304467678070068\n",
      "h: 91 | epoch: 40, train loss: 8.90223503112793, test loss: 6.252501487731934\n",
      "h: 91 | epoch: 41, train loss: 8.86270809173584, test loss: 6.20539665222168\n",
      "h: 91 | epoch: 42, train loss: 8.827585220336914, test loss: 6.162666320800781\n",
      "h: 91 | epoch: 43, train loss: 8.79637622833252, test loss: 6.123879432678223\n",
      "h: 91 | epoch: 44, train loss: 8.76865005493164, test loss: 6.088648796081543\n",
      "h: 91 | epoch: 45, train loss: 8.744024276733398, test loss: 6.056622505187988\n",
      "h: 91 | epoch: 46, train loss: 8.722150802612305, test loss: 6.027490139007568\n",
      "h: 91 | epoch: 47, train loss: 8.702726364135742, test loss: 6.000969886779785\n",
      "h: 91 | epoch: 48, train loss: 8.685479164123535, test loss: 5.976807594299316\n",
      "h: 91 | epoch: 49, train loss: 8.670164108276367, test loss: 5.954777240753174\n",
      "h: 91 | epoch: 50, train loss: 8.656569480895996, test loss: 5.934676647186279\n",
      "h: 91 | epoch: 51, train loss: 8.644501686096191, test loss: 5.916319370269775\n",
      "h: 91 | epoch: 52, train loss: 8.633790016174316, test loss: 5.899540901184082\n",
      "h: 91 | epoch: 53, train loss: 8.624282836914062, test loss: 5.884194850921631\n",
      "h: 91 | epoch: 54, train loss: 8.6158447265625, test loss: 5.870146751403809\n",
      "h: 91 | epoch: 55, train loss: 8.608358383178711, test loss: 5.857275485992432\n",
      "h: 91 | epoch: 56, train loss: 8.601713180541992, test loss: 5.845472812652588\n",
      "h: 91 | epoch: 57, train loss: 8.595819473266602, test loss: 5.83464241027832\n",
      "h: 91 | epoch: 58, train loss: 8.59058952331543, test loss: 5.824695587158203\n",
      "h: 91 | epoch: 59, train loss: 8.585948944091797, test loss: 5.815553188323975\n",
      "h: 91 | epoch: 60, train loss: 8.581832885742188, test loss: 5.807140827178955\n",
      "h: 91 | epoch: 61, train loss: 8.578181266784668, test loss: 5.7993974685668945\n",
      "h: 91 | epoch: 62, train loss: 8.574942588806152, test loss: 5.792263984680176\n",
      "h: 91 | epoch: 63, train loss: 8.57206916809082, test loss: 5.7856855392456055\n",
      "h: 91 | epoch: 64, train loss: 8.569520950317383, test loss: 5.779614448547363\n",
      "h: 91 | epoch: 65, train loss: 8.5672607421875, test loss: 5.774008750915527\n",
      "h: 91 | epoch: 66, train loss: 8.565256118774414, test loss: 5.768828392028809\n",
      "h: 91 | epoch: 67, train loss: 8.56347942352295, test loss: 5.7640380859375\n",
      "h: 91 | epoch: 68, train loss: 8.56190299987793, test loss: 5.759603500366211\n",
      "h: 91 | epoch: 69, train loss: 8.560505867004395, test loss: 5.755496978759766\n",
      "h: 91 | epoch: 70, train loss: 8.55926513671875, test loss: 5.7516913414001465\n",
      "h: 91 | epoch: 71, train loss: 8.558165550231934, test loss: 5.748162269592285\n",
      "h: 91 | epoch: 72, train loss: 8.557191848754883, test loss: 5.744887828826904\n",
      "h: 91 | epoch: 73, train loss: 8.556327819824219, test loss: 5.741847515106201\n",
      "h: 91 | epoch: 74, train loss: 8.555562019348145, test loss: 5.739023208618164\n",
      "h: 91 | epoch: 75, train loss: 8.554880142211914, test loss: 5.736396789550781\n",
      "h: 91 | epoch: 76, train loss: 8.554279327392578, test loss: 5.733954906463623\n",
      "h: 91 | epoch: 77, train loss: 8.553744316101074, test loss: 5.731682300567627\n",
      "h: 91 | epoch: 78, train loss: 8.553272247314453, test loss: 5.729565620422363\n",
      "h: 91 | epoch: 79, train loss: 8.552852630615234, test loss: 5.727594375610352\n",
      "h: 91 | epoch: 80, train loss: 8.55247974395752, test loss: 5.725757122039795\n",
      "h: 91 | epoch: 81, train loss: 8.552148818969727, test loss: 5.724043846130371\n",
      "h: 91 | epoch: 82, train loss: 8.551856994628906, test loss: 5.722446441650391\n",
      "h: 91 | epoch: 83, train loss: 8.551597595214844, test loss: 5.720953941345215\n",
      "h: 91 | epoch: 84, train loss: 8.55136775970459, test loss: 5.719561576843262\n",
      "h: 91 | epoch: 85, train loss: 8.551164627075195, test loss: 5.718261241912842\n",
      "h: 91 | epoch: 86, train loss: 8.550983428955078, test loss: 5.71704626083374\n",
      "h: 91 | epoch: 87, train loss: 8.550823211669922, test loss: 5.715909481048584\n",
      "h: 91 | epoch: 88, train loss: 8.550680160522461, test loss: 5.714848041534424\n",
      "h: 91 | epoch: 89, train loss: 8.550555229187012, test loss: 5.713854789733887\n",
      "h: 91 | epoch: 90, train loss: 8.550443649291992, test loss: 5.712924003601074\n",
      "h: 91 | epoch: 91, train loss: 8.550344467163086, test loss: 5.712054252624512\n",
      "h: 91 | epoch: 92, train loss: 8.550256729125977, test loss: 5.711239814758301\n",
      "h: 91 | epoch: 93, train loss: 8.550179481506348, test loss: 5.710477828979492\n",
      "h: 91 | epoch: 94, train loss: 8.550110816955566, test loss: 5.709762096405029\n",
      "h: 91 | epoch: 95, train loss: 8.550048828125, test loss: 5.709094047546387\n",
      "h: 91 | epoch: 96, train loss: 8.549994468688965, test loss: 5.708466529846191\n",
      "h: 91 | epoch: 97, train loss: 8.549947738647461, test loss: 5.707878112792969\n",
      "h: 91 | epoch: 98, train loss: 8.549903869628906, test loss: 5.707327365875244\n",
      "h: 91 | epoch: 99, train loss: 8.549867630004883, test loss: 5.706809997558594\n",
      "h: 92 | epoch: 0, train loss: 39.08271789550781, test loss: 32.54198455810547\n",
      "h: 92 | epoch: 1, train loss: 35.446815490722656, test loss: 29.533939361572266\n",
      "h: 92 | epoch: 2, train loss: 32.24246597290039, test loss: 26.87009620666504\n",
      "h: 92 | epoch: 3, train loss: 29.417606353759766, test loss: 24.510046005249023\n",
      "h: 92 | epoch: 4, train loss: 26.926849365234375, test loss: 22.418415069580078\n",
      "h: 92 | epoch: 5, train loss: 24.730514526367188, test loss: 20.564146041870117\n",
      "h: 92 | epoch: 6, train loss: 22.793819427490234, test loss: 18.919923782348633\n",
      "h: 92 | epoch: 7, train loss: 21.086193084716797, test loss: 17.461666107177734\n",
      "h: 92 | epoch: 8, train loss: 19.580751419067383, test loss: 16.16812515258789\n",
      "h: 92 | epoch: 9, train loss: 18.253782272338867, test loss: 15.020517349243164\n",
      "h: 92 | epoch: 10, train loss: 17.084381103515625, test loss: 14.002227783203125\n",
      "h: 92 | epoch: 11, train loss: 16.05408477783203, test loss: 13.098550796508789\n",
      "h: 92 | epoch: 12, train loss: 15.146574020385742, test loss: 12.296453475952148\n",
      "h: 92 | epoch: 13, train loss: 14.347442626953125, test loss: 11.584394454956055\n",
      "h: 92 | epoch: 14, train loss: 13.643940925598145, test loss: 10.952142715454102\n",
      "h: 92 | epoch: 15, train loss: 13.024798393249512, test loss: 10.390628814697266\n",
      "h: 92 | epoch: 16, train loss: 12.480059623718262, test loss: 9.891820907592773\n",
      "h: 92 | epoch: 17, train loss: 12.000920295715332, test loss: 9.448586463928223\n",
      "h: 92 | epoch: 18, train loss: 11.579593658447266, test loss: 9.054615020751953\n",
      "h: 92 | epoch: 19, train loss: 11.209207534790039, test loss: 8.704305648803711\n",
      "h: 92 | epoch: 20, train loss: 10.883687973022461, test loss: 8.392702102661133\n",
      "h: 92 | epoch: 21, train loss: 10.597674369812012, test loss: 8.11540699005127\n",
      "h: 92 | epoch: 22, train loss: 10.346430778503418, test loss: 7.868527412414551\n",
      "h: 92 | epoch: 23, train loss: 10.125785827636719, test loss: 7.648619174957275\n",
      "h: 92 | epoch: 24, train loss: 9.932050704956055, test loss: 7.4526238441467285\n",
      "h: 92 | epoch: 25, train loss: 9.761983871459961, test loss: 7.277841091156006\n",
      "h: 92 | epoch: 26, train loss: 9.61271858215332, test loss: 7.121873378753662\n",
      "h: 92 | epoch: 27, train loss: 9.481738090515137, test loss: 6.98260498046875\n",
      "h: 92 | epoch: 28, train loss: 9.366820335388184, test loss: 6.858158111572266\n",
      "h: 92 | epoch: 29, train loss: 9.266012191772461, test loss: 6.746869087219238\n",
      "h: 92 | epoch: 30, train loss: 9.177594184875488, test loss: 6.64727258682251\n",
      "h: 92 | epoch: 31, train loss: 9.100053787231445, test loss: 6.558061122894287\n",
      "h: 92 | epoch: 32, train loss: 9.032061576843262, test loss: 6.478087425231934\n",
      "h: 92 | epoch: 33, train loss: 8.972450256347656, test loss: 6.406327247619629\n",
      "h: 92 | epoch: 34, train loss: 8.920188903808594, test loss: 6.341878414154053\n",
      "h: 92 | epoch: 35, train loss: 8.87437629699707, test loss: 6.28394079208374\n",
      "h: 92 | epoch: 36, train loss: 8.834221839904785, test loss: 6.231804847717285\n",
      "h: 92 | epoch: 37, train loss: 8.799026489257812, test loss: 6.184840679168701\n",
      "h: 92 | epoch: 38, train loss: 8.768182754516602, test loss: 6.142495155334473\n",
      "h: 92 | epoch: 39, train loss: 8.741150856018066, test loss: 6.104269981384277\n",
      "h: 92 | epoch: 40, train loss: 8.717463493347168, test loss: 6.069729804992676\n",
      "h: 92 | epoch: 41, train loss: 8.696706771850586, test loss: 6.038483619689941\n",
      "h: 92 | epoch: 42, train loss: 8.678518295288086, test loss: 6.01018762588501\n",
      "h: 92 | epoch: 43, train loss: 8.662579536437988, test loss: 5.984536170959473\n",
      "h: 92 | epoch: 44, train loss: 8.648614883422852, test loss: 5.961254119873047\n",
      "h: 92 | epoch: 45, train loss: 8.636377334594727, test loss: 5.940099239349365\n",
      "h: 92 | epoch: 46, train loss: 8.625654220581055, test loss: 5.9208574295043945\n",
      "h: 92 | epoch: 47, train loss: 8.61625862121582, test loss: 5.903335094451904\n",
      "h: 92 | epoch: 48, train loss: 8.608027458190918, test loss: 5.887362480163574\n",
      "h: 92 | epoch: 49, train loss: 8.600813865661621, test loss: 5.872784614562988\n",
      "h: 92 | epoch: 50, train loss: 8.594491958618164, test loss: 5.859466552734375\n",
      "h: 92 | epoch: 51, train loss: 8.588953018188477, test loss: 5.847286701202393\n",
      "h: 92 | epoch: 52, train loss: 8.584099769592285, test loss: 5.8361358642578125\n",
      "h: 92 | epoch: 53, train loss: 8.579846382141113, test loss: 5.825916767120361\n",
      "h: 92 | epoch: 54, train loss: 8.576117515563965, test loss: 5.816542625427246\n",
      "h: 92 | epoch: 55, train loss: 8.572850227355957, test loss: 5.8079328536987305\n",
      "h: 92 | epoch: 56, train loss: 8.569986343383789, test loss: 5.80001974105835\n",
      "h: 92 | epoch: 57, train loss: 8.567475318908691, test loss: 5.792741298675537\n",
      "h: 92 | epoch: 58, train loss: 8.565275192260742, test loss: 5.786036491394043\n",
      "h: 92 | epoch: 59, train loss: 8.563345909118652, test loss: 5.779858112335205\n",
      "h: 92 | epoch: 60, train loss: 8.561655044555664, test loss: 5.774158477783203\n",
      "h: 92 | epoch: 61, train loss: 8.560173034667969, test loss: 5.768896102905273\n",
      "h: 92 | epoch: 62, train loss: 8.558874130249023, test loss: 5.764033317565918\n",
      "h: 92 | epoch: 63, train loss: 8.557733535766602, test loss: 5.759536266326904\n",
      "h: 92 | epoch: 64, train loss: 8.556734085083008, test loss: 5.75537633895874\n",
      "h: 92 | epoch: 65, train loss: 8.55585765838623, test loss: 5.751523971557617\n",
      "h: 92 | epoch: 66, train loss: 8.555088996887207, test loss: 5.74795389175415\n",
      "h: 92 | epoch: 67, train loss: 8.554414749145508, test loss: 5.744642734527588\n",
      "h: 92 | epoch: 68, train loss: 8.553823471069336, test loss: 5.741570472717285\n",
      "h: 92 | epoch: 69, train loss: 8.553304672241211, test loss: 5.7387189865112305\n",
      "h: 92 | epoch: 70, train loss: 8.552848815917969, test loss: 5.73607063293457\n",
      "h: 92 | epoch: 71, train loss: 8.552450180053711, test loss: 5.733606338500977\n",
      "h: 92 | epoch: 72, train loss: 8.552099227905273, test loss: 5.731317043304443\n",
      "h: 92 | epoch: 73, train loss: 8.551791191101074, test loss: 5.729185104370117\n",
      "h: 92 | epoch: 74, train loss: 8.551521301269531, test loss: 5.727201461791992\n",
      "h: 92 | epoch: 75, train loss: 8.551284790039062, test loss: 5.7253546714782715\n",
      "h: 92 | epoch: 76, train loss: 8.551076889038086, test loss: 5.723633289337158\n",
      "h: 92 | epoch: 77, train loss: 8.550893783569336, test loss: 5.722029685974121\n",
      "h: 92 | epoch: 78, train loss: 8.55073356628418, test loss: 5.720533847808838\n",
      "h: 92 | epoch: 79, train loss: 8.550592422485352, test loss: 5.719137191772461\n",
      "h: 92 | epoch: 80, train loss: 8.550469398498535, test loss: 5.71783447265625\n",
      "h: 92 | epoch: 81, train loss: 8.550361633300781, test loss: 5.716619491577148\n",
      "h: 92 | epoch: 82, train loss: 8.550265312194824, test loss: 5.715484142303467\n",
      "h: 92 | epoch: 83, train loss: 8.550180435180664, test loss: 5.714425086975098\n",
      "h: 92 | epoch: 84, train loss: 8.550107955932617, test loss: 5.713433265686035\n",
      "h: 92 | epoch: 85, train loss: 8.550043106079102, test loss: 5.712508201599121\n",
      "h: 92 | epoch: 86, train loss: 8.54998779296875, test loss: 5.711642265319824\n",
      "h: 92 | epoch: 87, train loss: 8.549936294555664, test loss: 5.7108330726623535\n",
      "h: 92 | epoch: 88, train loss: 8.549893379211426, test loss: 5.710077285766602\n",
      "h: 92 | epoch: 89, train loss: 8.54985523223877, test loss: 5.70936918258667\n",
      "h: 92 | epoch: 90, train loss: 8.549820899963379, test loss: 5.708706855773926\n",
      "h: 92 | epoch: 91, train loss: 8.54979133605957, test loss: 5.70808744430542\n",
      "h: 92 | epoch: 92, train loss: 8.549764633178711, test loss: 5.707507133483887\n",
      "h: 92 | epoch: 93, train loss: 8.5497407913208, test loss: 5.706963539123535\n",
      "h: 92 | epoch: 94, train loss: 8.549721717834473, test loss: 5.706454277038574\n",
      "h: 92 | epoch: 95, train loss: 8.549703598022461, test loss: 5.705978870391846\n",
      "h: 92 | epoch: 96, train loss: 8.549688339233398, test loss: 5.705532073974609\n",
      "h: 92 | epoch: 97, train loss: 8.549674034118652, test loss: 5.70511531829834\n",
      "h: 92 | epoch: 98, train loss: 8.549661636352539, test loss: 5.704722881317139\n",
      "h: 92 | epoch: 99, train loss: 8.549651145935059, test loss: 5.704357147216797\n",
      "h: 93 | epoch: 0, train loss: 43.60342788696289, test loss: 35.993919372558594\n",
      "h: 93 | epoch: 1, train loss: 39.36378860473633, test loss: 32.51793670654297\n",
      "h: 93 | epoch: 2, train loss: 35.635536193847656, test loss: 29.44791030883789\n",
      "h: 93 | epoch: 3, train loss: 32.355690002441406, test loss: 26.735095977783203\n",
      "h: 93 | epoch: 4, train loss: 29.469594955444336, test loss: 24.337038040161133\n",
      "h: 93 | epoch: 5, train loss: 26.929662704467773, test loss: 22.216594696044922\n",
      "h: 93 | epoch: 6, train loss: 24.694324493408203, test loss: 20.34122085571289\n",
      "h: 93 | epoch: 7, train loss: 22.72718048095703, test loss: 18.682308197021484\n",
      "h: 93 | epoch: 8, train loss: 20.99628257751465, test loss: 17.21467399597168\n",
      "h: 93 | epoch: 9, train loss: 19.473556518554688, test loss: 15.916132926940918\n",
      "h: 93 | epoch: 10, train loss: 18.13427734375, test loss: 14.767087936401367\n",
      "h: 93 | epoch: 11, train loss: 16.956668853759766, test loss: 13.750242233276367\n",
      "h: 93 | epoch: 12, train loss: 15.92152214050293, test loss: 12.850298881530762\n",
      "h: 93 | epoch: 13, train loss: 15.011892318725586, test loss: 12.053743362426758\n",
      "h: 93 | epoch: 14, train loss: 14.212827682495117, test loss: 11.348611831665039\n",
      "h: 93 | epoch: 15, train loss: 13.511125564575195, test loss: 10.724322319030762\n",
      "h: 93 | epoch: 16, train loss: 12.895135879516602, test loss: 10.17151165008545\n",
      "h: 93 | epoch: 17, train loss: 12.35457706451416, test loss: 9.681904792785645\n",
      "h: 93 | epoch: 18, train loss: 11.880372047424316, test loss: 9.2481689453125\n",
      "h: 93 | epoch: 19, train loss: 11.464517593383789, test loss: 8.863832473754883\n",
      "h: 93 | epoch: 20, train loss: 11.099954605102539, test loss: 8.52315902709961\n",
      "h: 93 | epoch: 21, train loss: 10.7804594039917, test loss: 8.221084594726562\n",
      "h: 93 | epoch: 22, train loss: 10.500547409057617, test loss: 7.953134059906006\n",
      "h: 93 | epoch: 23, train loss: 10.255388259887695, test loss: 7.715348243713379\n",
      "h: 93 | epoch: 24, train loss: 10.040731430053711, test loss: 7.504229545593262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 93 | epoch: 25, train loss: 9.852834701538086, test loss: 7.316695213317871\n",
      "h: 93 | epoch: 26, train loss: 9.688404083251953, test loss: 7.1500115394592285\n",
      "h: 93 | epoch: 27, train loss: 9.544547080993652, test loss: 7.001771450042725\n",
      "h: 93 | epoch: 28, train loss: 9.418718338012695, test loss: 6.869847297668457\n",
      "h: 93 | epoch: 29, train loss: 9.308685302734375, test loss: 6.7523627281188965\n",
      "h: 93 | epoch: 30, train loss: 9.21248722076416, test loss: 6.647660255432129\n",
      "h: 93 | epoch: 31, train loss: 9.128399848937988, test loss: 6.554274082183838\n",
      "h: 93 | epoch: 32, train loss: 9.054913520812988, test loss: 6.4709153175354\n",
      "h: 93 | epoch: 33, train loss: 8.990703582763672, test loss: 6.396440505981445\n",
      "h: 93 | epoch: 34, train loss: 8.934609413146973, test loss: 6.3298444747924805\n",
      "h: 93 | epoch: 35, train loss: 8.885612487792969, test loss: 6.27023983001709\n",
      "h: 93 | epoch: 36, train loss: 8.84282112121582, test loss: 6.216839790344238\n",
      "h: 93 | epoch: 37, train loss: 8.805456161499023, test loss: 6.168948650360107\n",
      "h: 93 | epoch: 38, train loss: 8.772832870483398, test loss: 6.125956058502197\n",
      "h: 93 | epoch: 39, train loss: 8.744354248046875, test loss: 6.087321758270264\n",
      "h: 93 | epoch: 40, train loss: 8.71949577331543, test loss: 6.052566051483154\n",
      "h: 93 | epoch: 41, train loss: 8.697799682617188, test loss: 6.021263599395752\n",
      "h: 93 | epoch: 42, train loss: 8.678865432739258, test loss: 5.993043899536133\n",
      "h: 93 | epoch: 43, train loss: 8.66234302520752, test loss: 5.967570781707764\n",
      "h: 93 | epoch: 44, train loss: 8.647926330566406, test loss: 5.944554805755615\n",
      "h: 93 | epoch: 45, train loss: 8.63534927368164, test loss: 5.923732757568359\n",
      "h: 93 | epoch: 46, train loss: 8.62437629699707, test loss: 5.9048752784729\n",
      "h: 93 | epoch: 47, train loss: 8.614805221557617, test loss: 5.887778282165527\n",
      "h: 93 | epoch: 48, train loss: 8.606456756591797, test loss: 5.87225866317749\n",
      "h: 93 | epoch: 49, train loss: 8.599173545837402, test loss: 5.858155250549316\n",
      "h: 93 | epoch: 50, train loss: 8.592823028564453, test loss: 5.8453264236450195\n",
      "h: 93 | epoch: 51, train loss: 8.587285041809082, test loss: 5.83364200592041\n",
      "h: 93 | epoch: 52, train loss: 8.582453727722168, test loss: 5.822988986968994\n",
      "h: 93 | epoch: 53, train loss: 8.578242301940918, test loss: 5.813265800476074\n",
      "h: 93 | epoch: 54, train loss: 8.574567794799805, test loss: 5.804384231567383\n",
      "h: 93 | epoch: 55, train loss: 8.571365356445312, test loss: 5.79625940322876\n",
      "h: 93 | epoch: 56, train loss: 8.568572044372559, test loss: 5.788822650909424\n",
      "h: 93 | epoch: 57, train loss: 8.566137313842773, test loss: 5.78200626373291\n",
      "h: 93 | epoch: 58, train loss: 8.56401252746582, test loss: 5.775754451751709\n",
      "h: 93 | epoch: 59, train loss: 8.562162399291992, test loss: 5.770016193389893\n",
      "h: 93 | epoch: 60, train loss: 8.560548782348633, test loss: 5.76474142074585\n",
      "h: 93 | epoch: 61, train loss: 8.559141159057617, test loss: 5.759890556335449\n",
      "h: 93 | epoch: 62, train loss: 8.557915687561035, test loss: 5.755426406860352\n",
      "h: 93 | epoch: 63, train loss: 8.556844711303711, test loss: 5.75131368637085\n",
      "h: 93 | epoch: 64, train loss: 8.555912971496582, test loss: 5.747522354125977\n",
      "h: 93 | epoch: 65, train loss: 8.555100440979004, test loss: 5.744023323059082\n",
      "h: 93 | epoch: 66, train loss: 8.554390907287598, test loss: 5.7407941818237305\n",
      "h: 93 | epoch: 67, train loss: 8.553772926330566, test loss: 5.737809181213379\n",
      "h: 93 | epoch: 68, train loss: 8.553235054016113, test loss: 5.735051155090332\n",
      "h: 93 | epoch: 69, train loss: 8.552765846252441, test loss: 5.732500076293945\n",
      "h: 93 | epoch: 70, train loss: 8.552355766296387, test loss: 5.730138301849365\n",
      "h: 93 | epoch: 71, train loss: 8.55199909210205, test loss: 5.727950572967529\n",
      "h: 93 | epoch: 72, train loss: 8.551687240600586, test loss: 5.725925922393799\n",
      "h: 93 | epoch: 73, train loss: 8.551417350769043, test loss: 5.724045276641846\n",
      "h: 93 | epoch: 74, train loss: 8.551180839538574, test loss: 5.722302436828613\n",
      "h: 93 | epoch: 75, train loss: 8.55097484588623, test loss: 5.720685958862305\n",
      "h: 93 | epoch: 76, train loss: 8.55079460144043, test loss: 5.719185829162598\n",
      "h: 93 | epoch: 77, train loss: 8.550638198852539, test loss: 5.7177910804748535\n",
      "h: 93 | epoch: 78, train loss: 8.550501823425293, test loss: 5.716495990753174\n",
      "h: 93 | epoch: 79, train loss: 8.550382614135742, test loss: 5.715292453765869\n",
      "h: 93 | epoch: 80, train loss: 8.55027961730957, test loss: 5.714173316955566\n",
      "h: 93 | epoch: 81, train loss: 8.550188064575195, test loss: 5.713131904602051\n",
      "h: 93 | epoch: 82, train loss: 8.55010986328125, test loss: 5.712164878845215\n",
      "h: 93 | epoch: 83, train loss: 8.550041198730469, test loss: 5.711263656616211\n",
      "h: 93 | epoch: 84, train loss: 8.549981117248535, test loss: 5.710424423217773\n",
      "h: 93 | epoch: 85, train loss: 8.549928665161133, test loss: 5.709643840789795\n",
      "h: 93 | epoch: 86, train loss: 8.549882888793945, test loss: 5.708916664123535\n",
      "h: 93 | epoch: 87, train loss: 8.549843788146973, test loss: 5.7082390785217285\n",
      "h: 93 | epoch: 88, train loss: 8.549808502197266, test loss: 5.707608222961426\n",
      "h: 93 | epoch: 89, train loss: 8.54977798461914, test loss: 5.707019329071045\n",
      "h: 93 | epoch: 90, train loss: 8.549752235412598, test loss: 5.7064714431762695\n",
      "h: 93 | epoch: 91, train loss: 8.549729347229004, test loss: 5.705960273742676\n",
      "h: 93 | epoch: 92, train loss: 8.549708366394043, test loss: 5.705483436584473\n",
      "h: 93 | epoch: 93, train loss: 8.549692153930664, test loss: 5.705039024353027\n",
      "h: 93 | epoch: 94, train loss: 8.549676895141602, test loss: 5.704625129699707\n",
      "h: 93 | epoch: 95, train loss: 8.549663543701172, test loss: 5.704238414764404\n",
      "h: 93 | epoch: 96, train loss: 8.549651145935059, test loss: 5.703877925872803\n",
      "h: 93 | epoch: 97, train loss: 8.549641609191895, test loss: 5.703540802001953\n",
      "h: 93 | epoch: 98, train loss: 8.549633026123047, test loss: 5.7032270431518555\n",
      "h: 93 | epoch: 99, train loss: 8.549625396728516, test loss: 5.702934265136719\n",
      "h: 94 | epoch: 0, train loss: 40.024864196777344, test loss: 32.5001220703125\n",
      "h: 94 | epoch: 1, train loss: 35.45121383666992, test loss: 28.86741065979004\n",
      "h: 94 | epoch: 2, train loss: 31.547842025756836, test loss: 25.746353149414062\n",
      "h: 94 | epoch: 3, train loss: 28.214771270751953, test loss: 23.062641143798828\n",
      "h: 94 | epoch: 4, train loss: 25.367584228515625, test loss: 20.753231048583984\n",
      "h: 94 | epoch: 5, train loss: 22.934778213500977, test loss: 18.764507293701172\n",
      "h: 94 | epoch: 6, train loss: 20.855640411376953, test loss: 17.050765991210938\n",
      "h: 94 | epoch: 7, train loss: 19.07851791381836, test loss: 15.57298469543457\n",
      "h: 94 | epoch: 8, train loss: 17.559398651123047, test loss: 14.297795295715332\n",
      "h: 94 | epoch: 9, train loss: 16.260744094848633, test loss: 13.196649551391602\n",
      "h: 94 | epoch: 10, train loss: 15.150494575500488, test loss: 12.245084762573242\n",
      "h: 94 | epoch: 11, train loss: 14.201278686523438, test loss: 11.422136306762695\n",
      "h: 94 | epoch: 12, train loss: 13.389691352844238, test loss: 10.709827423095703\n",
      "h: 94 | epoch: 13, train loss: 12.695737838745117, test loss: 10.092732429504395\n",
      "h: 94 | epoch: 14, train loss: 12.102319717407227, test loss: 9.5576171875\n",
      "h: 94 | epoch: 15, train loss: 11.594817161560059, test loss: 9.093119621276855\n",
      "h: 94 | epoch: 16, train loss: 11.160737991333008, test loss: 8.689485549926758\n",
      "h: 94 | epoch: 17, train loss: 10.789403915405273, test loss: 8.338338851928711\n",
      "h: 94 | epoch: 18, train loss: 10.471681594848633, test loss: 8.03248119354248\n",
      "h: 94 | epoch: 19, train loss: 10.199777603149414, test loss: 7.7657318115234375\n",
      "h: 94 | epoch: 20, train loss: 9.967021942138672, test loss: 7.53277587890625\n",
      "h: 94 | epoch: 21, train loss: 9.767724990844727, test loss: 7.329049587249756\n",
      "h: 94 | epoch: 22, train loss: 9.597020149230957, test loss: 7.1506218910217285\n",
      "h: 94 | epoch: 23, train loss: 9.45075511932373, test loss: 6.99411153793335\n",
      "h: 94 | epoch: 24, train loss: 9.325383186340332, test loss: 6.856616020202637\n",
      "h: 94 | epoch: 25, train loss: 9.217874526977539, test loss: 6.73562479019165\n",
      "h: 94 | epoch: 26, train loss: 9.125643730163574, test loss: 6.628979682922363\n",
      "h: 94 | epoch: 27, train loss: 9.046479225158691, test loss: 6.534820556640625\n",
      "h: 94 | epoch: 28, train loss: 8.978494644165039, test loss: 6.451542854309082\n",
      "h: 94 | epoch: 29, train loss: 8.920080184936523, test loss: 6.377758502960205\n",
      "h: 94 | epoch: 30, train loss: 8.86985969543457, test loss: 6.312269687652588\n",
      "h: 94 | epoch: 31, train loss: 8.826658248901367, test loss: 6.254039764404297\n",
      "h: 94 | epoch: 32, train loss: 8.789470672607422, test loss: 6.202169418334961\n",
      "h: 94 | epoch: 33, train loss: 8.757439613342285, test loss: 6.155881881713867\n",
      "h: 94 | epoch: 34, train loss: 8.729829788208008, test loss: 6.114504814147949\n",
      "h: 94 | epoch: 35, train loss: 8.706014633178711, test loss: 6.077447891235352\n",
      "h: 94 | epoch: 36, train loss: 8.685457229614258, test loss: 6.044203281402588\n",
      "h: 94 | epoch: 37, train loss: 8.667699813842773, test loss: 6.0143256187438965\n",
      "h: 94 | epoch: 38, train loss: 8.652348518371582, test loss: 5.987429618835449\n",
      "h: 94 | epoch: 39, train loss: 8.639066696166992, test loss: 5.963174343109131\n",
      "h: 94 | epoch: 40, train loss: 8.627567291259766, test loss: 5.941266059875488\n",
      "h: 94 | epoch: 41, train loss: 8.61760139465332, test loss: 5.921444892883301\n",
      "h: 94 | epoch: 42, train loss: 8.608960151672363, test loss: 5.903482437133789\n",
      "h: 94 | epoch: 43, train loss: 8.601457595825195, test loss: 5.887181282043457\n",
      "h: 94 | epoch: 44, train loss: 8.594941139221191, test loss: 5.872365951538086\n",
      "h: 94 | epoch: 45, train loss: 8.589274406433105, test loss: 5.858880043029785\n",
      "h: 94 | epoch: 46, train loss: 8.584344863891602, test loss: 5.84658670425415\n",
      "h: 94 | epoch: 47, train loss: 8.580050468444824, test loss: 5.835366249084473\n",
      "h: 94 | epoch: 48, train loss: 8.57630729675293, test loss: 5.8251118659973145\n",
      "h: 94 | epoch: 49, train loss: 8.573042869567871, test loss: 5.8157267570495605\n",
      "h: 94 | epoch: 50, train loss: 8.570192337036133, test loss: 5.80712890625\n",
      "h: 94 | epoch: 51, train loss: 8.56770133972168, test loss: 5.79924201965332\n",
      "h: 94 | epoch: 52, train loss: 8.565521240234375, test loss: 5.791998863220215\n",
      "h: 94 | epoch: 53, train loss: 8.563614845275879, test loss: 5.785341262817383\n",
      "h: 94 | epoch: 54, train loss: 8.561944961547852, test loss: 5.77921199798584\n",
      "h: 94 | epoch: 55, train loss: 8.560480117797852, test loss: 5.7735676765441895\n",
      "h: 94 | epoch: 56, train loss: 8.559194564819336, test loss: 5.768362998962402\n",
      "h: 94 | epoch: 57, train loss: 8.558067321777344, test loss: 5.763558387756348\n",
      "h: 94 | epoch: 58, train loss: 8.557075500488281, test loss: 5.759119987487793\n",
      "h: 94 | epoch: 59, train loss: 8.556203842163086, test loss: 5.755017280578613\n",
      "h: 94 | epoch: 60, train loss: 8.555437088012695, test loss: 5.751221179962158\n",
      "h: 94 | epoch: 61, train loss: 8.554760932922363, test loss: 5.747705459594727\n",
      "h: 94 | epoch: 62, train loss: 8.554166793823242, test loss: 5.744448661804199\n",
      "h: 94 | epoch: 63, train loss: 8.553641319274902, test loss: 5.741427898406982\n",
      "h: 94 | epoch: 64, train loss: 8.553177833557129, test loss: 5.738624572753906\n",
      "h: 94 | epoch: 65, train loss: 8.552769660949707, test loss: 5.736021041870117\n",
      "h: 94 | epoch: 66, train loss: 8.552408218383789, test loss: 5.733602523803711\n",
      "h: 94 | epoch: 67, train loss: 8.552088737487793, test loss: 5.731354236602783\n",
      "h: 94 | epoch: 68, train loss: 8.551806449890137, test loss: 5.729261875152588\n",
      "h: 94 | epoch: 69, train loss: 8.551556587219238, test loss: 5.7273149490356445\n",
      "h: 94 | epoch: 70, train loss: 8.551335334777832, test loss: 5.725500583648682\n",
      "h: 94 | epoch: 71, train loss: 8.551138877868652, test loss: 5.723811149597168\n",
      "h: 94 | epoch: 72, train loss: 8.55096435546875, test loss: 5.722236156463623\n",
      "h: 94 | epoch: 73, train loss: 8.550810813903809, test loss: 5.720766067504883\n",
      "h: 94 | epoch: 74, train loss: 8.550674438476562, test loss: 5.719396114349365\n",
      "h: 94 | epoch: 75, train loss: 8.550554275512695, test loss: 5.718115329742432\n",
      "h: 94 | epoch: 76, train loss: 8.550445556640625, test loss: 5.716920852661133\n",
      "h: 94 | epoch: 77, train loss: 8.550349235534668, test loss: 5.715803623199463\n",
      "h: 94 | epoch: 78, train loss: 8.550265312194824, test loss: 5.7147603034973145\n",
      "h: 94 | epoch: 79, train loss: 8.550189971923828, test loss: 5.713783264160156\n",
      "h: 94 | epoch: 80, train loss: 8.550122261047363, test loss: 5.7128705978393555\n",
      "h: 94 | epoch: 81, train loss: 8.55006217956543, test loss: 5.712016582489014\n",
      "h: 94 | epoch: 82, train loss: 8.550009727478027, test loss: 5.711216926574707\n",
      "h: 94 | epoch: 83, train loss: 8.549962043762207, test loss: 5.71046781539917\n",
      "h: 94 | epoch: 84, train loss: 8.549920082092285, test loss: 5.709766864776611\n",
      "h: 94 | epoch: 85, train loss: 8.549881935119629, test loss: 5.709109783172607\n",
      "h: 94 | epoch: 86, train loss: 8.549848556518555, test loss: 5.708493709564209\n",
      "h: 94 | epoch: 87, train loss: 8.549818992614746, test loss: 5.707916259765625\n",
      "h: 94 | epoch: 88, train loss: 8.549792289733887, test loss: 5.7073750495910645\n",
      "h: 94 | epoch: 89, train loss: 8.549768447875977, test loss: 5.7068681716918945\n",
      "h: 94 | epoch: 90, train loss: 8.549748420715332, test loss: 5.706390857696533\n",
      "h: 94 | epoch: 91, train loss: 8.549729347229004, test loss: 5.7059431076049805\n",
      "h: 94 | epoch: 92, train loss: 8.549712181091309, test loss: 5.7055230140686035\n",
      "h: 94 | epoch: 93, train loss: 8.549697875976562, test loss: 5.705129146575928\n",
      "h: 94 | epoch: 94, train loss: 8.549684524536133, test loss: 5.704758644104004\n",
      "h: 94 | epoch: 95, train loss: 8.549673080444336, test loss: 5.704411506652832\n",
      "h: 94 | epoch: 96, train loss: 8.549661636352539, test loss: 5.7040839195251465\n",
      "h: 94 | epoch: 97, train loss: 8.549652099609375, test loss: 5.703777313232422\n",
      "h: 94 | epoch: 98, train loss: 8.549643516540527, test loss: 5.703487873077393\n",
      "h: 94 | epoch: 99, train loss: 8.54963493347168, test loss: 5.703217506408691\n",
      "h: 95 | epoch: 0, train loss: 44.83661651611328, test loss: 37.51410675048828\n",
      "h: 95 | epoch: 1, train loss: 40.52986145019531, test loss: 33.896244049072266\n",
      "h: 95 | epoch: 2, train loss: 36.73554611206055, test loss: 30.69856834411621\n",
      "h: 95 | epoch: 3, train loss: 33.39088821411133, test loss: 27.870548248291016\n",
      "h: 95 | epoch: 4, train loss: 30.441455841064453, test loss: 25.368206024169922\n",
      "h: 95 | epoch: 5, train loss: 27.83987808227539, test loss: 23.153169631958008\n",
      "h: 95 | epoch: 6, train loss: 25.544803619384766, test loss: 21.191822052001953\n",
      "h: 95 | epoch: 7, train loss: 23.520023345947266, test loss: 19.454660415649414\n",
      "h: 95 | epoch: 8, train loss: 21.733745574951172, test loss: 17.91573715209961\n",
      "h: 95 | epoch: 9, train loss: 20.158023834228516, test loss: 16.552200317382812\n",
      "h: 95 | epoch: 10, train loss: 18.768230438232422, test loss: 15.343866348266602\n",
      "h: 95 | epoch: 11, train loss: 17.542659759521484, test loss: 14.272929191589355\n",
      "h: 95 | epoch: 12, train loss: 16.46213150024414, test loss: 13.323641777038574\n",
      "h: 95 | epoch: 13, train loss: 15.509710311889648, test loss: 12.48206901550293\n",
      "h: 95 | epoch: 14, train loss: 14.670427322387695, test loss: 11.735892295837402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 95 | epoch: 15, train loss: 13.93104362487793, test loss: 11.07419204711914\n",
      "h: 95 | epoch: 16, train loss: 13.279850959777832, test loss: 10.487306594848633\n",
      "h: 95 | epoch: 17, train loss: 12.706489562988281, test loss: 9.966679573059082\n",
      "h: 95 | epoch: 18, train loss: 12.201807022094727, test loss: 9.504728317260742\n",
      "h: 95 | epoch: 19, train loss: 11.757699012756348, test loss: 9.094741821289062\n",
      "h: 95 | epoch: 20, train loss: 11.367006301879883, test loss: 8.730778694152832\n",
      "h: 95 | epoch: 21, train loss: 11.023397445678711, test loss: 8.407572746276855\n",
      "h: 95 | epoch: 22, train loss: 10.72127914428711, test loss: 8.120461463928223\n",
      "h: 95 | epoch: 23, train loss: 10.455711364746094, test loss: 7.865322113037109\n",
      "h: 95 | epoch: 24, train loss: 10.222332000732422, test loss: 7.6385040283203125\n",
      "h: 95 | epoch: 25, train loss: 10.017287254333496, test loss: 7.436770439147949\n",
      "h: 95 | epoch: 26, train loss: 9.837178230285645, test loss: 7.257266044616699\n",
      "h: 95 | epoch: 27, train loss: 9.679006576538086, test loss: 7.0974555015563965\n",
      "h: 95 | epoch: 28, train loss: 9.540133476257324, test loss: 6.95510196685791\n",
      "h: 95 | epoch: 29, train loss: 9.41822338104248, test loss: 6.82822322845459\n",
      "h: 95 | epoch: 30, train loss: 9.311227798461914, test loss: 6.715068817138672\n",
      "h: 95 | epoch: 31, train loss: 9.217336654663086, test loss: 6.614084720611572\n",
      "h: 95 | epoch: 32, train loss: 9.134961128234863, test loss: 6.523905277252197\n",
      "h: 95 | epoch: 33, train loss: 9.062699317932129, test loss: 6.4433112144470215\n",
      "h: 95 | epoch: 34, train loss: 8.999316215515137, test loss: 6.371230602264404\n",
      "h: 95 | epoch: 35, train loss: 8.943732261657715, test loss: 6.30671501159668\n",
      "h: 95 | epoch: 36, train loss: 8.894990921020508, test loss: 6.248923301696777\n",
      "h: 95 | epoch: 37, train loss: 8.852256774902344, test loss: 6.197110176086426\n",
      "h: 95 | epoch: 38, train loss: 8.814794540405273, test loss: 6.1506171226501465\n",
      "h: 95 | epoch: 39, train loss: 8.781953811645508, test loss: 6.108860969543457\n",
      "h: 95 | epoch: 40, train loss: 8.753170013427734, test loss: 6.071326732635498\n",
      "h: 95 | epoch: 41, train loss: 8.727943420410156, test loss: 6.037554740905762\n",
      "h: 95 | epoch: 42, train loss: 8.705836296081543, test loss: 6.007139205932617\n",
      "h: 95 | epoch: 43, train loss: 8.686464309692383, test loss: 5.979720592498779\n",
      "h: 95 | epoch: 44, train loss: 8.669488906860352, test loss: 5.95497989654541\n",
      "h: 95 | epoch: 45, train loss: 8.654616355895996, test loss: 5.932633399963379\n",
      "h: 95 | epoch: 46, train loss: 8.641587257385254, test loss: 5.912430763244629\n",
      "h: 95 | epoch: 47, train loss: 8.630169868469238, test loss: 5.8941450119018555\n",
      "h: 95 | epoch: 48, train loss: 8.620169639587402, test loss: 5.877581596374512\n",
      "h: 95 | epoch: 49, train loss: 8.611410140991211, test loss: 5.8625617027282715\n",
      "h: 95 | epoch: 50, train loss: 8.60373592376709, test loss: 5.848928928375244\n",
      "h: 95 | epoch: 51, train loss: 8.597013473510742, test loss: 5.836541175842285\n",
      "h: 95 | epoch: 52, train loss: 8.591124534606934, test loss: 5.825275421142578\n",
      "h: 95 | epoch: 53, train loss: 8.585968017578125, test loss: 5.815019130706787\n",
      "h: 95 | epoch: 54, train loss: 8.581450462341309, test loss: 5.8056745529174805\n",
      "h: 95 | epoch: 55, train loss: 8.577493667602539, test loss: 5.7971510887146\n",
      "h: 95 | epoch: 56, train loss: 8.574028968811035, test loss: 5.789370536804199\n",
      "h: 95 | epoch: 57, train loss: 8.570993423461914, test loss: 5.782259941101074\n",
      "h: 95 | epoch: 58, train loss: 8.568334579467773, test loss: 5.775757312774658\n",
      "h: 95 | epoch: 59, train loss: 8.56600570678711, test loss: 5.769806385040283\n",
      "h: 95 | epoch: 60, train loss: 8.563966751098633, test loss: 5.764353275299072\n",
      "h: 95 | epoch: 61, train loss: 8.56218147277832, test loss: 5.759353160858154\n",
      "h: 95 | epoch: 62, train loss: 8.560617446899414, test loss: 5.754765510559082\n",
      "h: 95 | epoch: 63, train loss: 8.559246063232422, test loss: 5.750553131103516\n",
      "h: 95 | epoch: 64, train loss: 8.558046340942383, test loss: 5.746681213378906\n",
      "h: 95 | epoch: 65, train loss: 8.556995391845703, test loss: 5.743119716644287\n",
      "h: 95 | epoch: 66, train loss: 8.556074142456055, test loss: 5.739841938018799\n",
      "h: 95 | epoch: 67, train loss: 8.555268287658691, test loss: 5.736824035644531\n",
      "h: 95 | epoch: 68, train loss: 8.55456256866455, test loss: 5.734042167663574\n",
      "h: 95 | epoch: 69, train loss: 8.55394458770752, test loss: 5.731475830078125\n",
      "h: 95 | epoch: 70, train loss: 8.5534029006958, test loss: 5.729109287261963\n",
      "h: 95 | epoch: 71, train loss: 8.552927017211914, test loss: 5.726922512054443\n",
      "h: 95 | epoch: 72, train loss: 8.552511215209961, test loss: 5.7249040603637695\n",
      "h: 95 | epoch: 73, train loss: 8.55214786529541, test loss: 5.723037242889404\n",
      "h: 95 | epoch: 74, train loss: 8.551828384399414, test loss: 5.721311092376709\n",
      "h: 95 | epoch: 75, train loss: 8.551548957824707, test loss: 5.719714641571045\n",
      "h: 95 | epoch: 76, train loss: 8.55130386352539, test loss: 5.718235969543457\n",
      "h: 95 | epoch: 77, train loss: 8.5510892868042, test loss: 5.716867446899414\n",
      "h: 95 | epoch: 78, train loss: 8.550901412963867, test loss: 5.7155985832214355\n",
      "h: 95 | epoch: 79, train loss: 8.550737380981445, test loss: 5.714422225952148\n",
      "h: 95 | epoch: 80, train loss: 8.550592422485352, test loss: 5.713332653045654\n",
      "h: 95 | epoch: 81, train loss: 8.550466537475586, test loss: 5.712320804595947\n",
      "h: 95 | epoch: 82, train loss: 8.5503568649292, test loss: 5.711382865905762\n",
      "h: 95 | epoch: 83, train loss: 8.55025863647461, test loss: 5.710512161254883\n",
      "h: 95 | epoch: 84, train loss: 8.55017375946045, test loss: 5.709702014923096\n",
      "h: 95 | epoch: 85, train loss: 8.550100326538086, test loss: 5.708950996398926\n",
      "h: 95 | epoch: 86, train loss: 8.55003547668457, test loss: 5.708252906799316\n",
      "h: 95 | epoch: 87, train loss: 8.549978256225586, test loss: 5.707603931427002\n",
      "h: 95 | epoch: 88, train loss: 8.549928665161133, test loss: 5.707001209259033\n",
      "h: 95 | epoch: 89, train loss: 8.549883842468262, test loss: 5.706440448760986\n",
      "h: 95 | epoch: 90, train loss: 8.549844741821289, test loss: 5.705918788909912\n",
      "h: 95 | epoch: 91, train loss: 8.549811363220215, test loss: 5.7054338455200195\n",
      "h: 95 | epoch: 92, train loss: 8.549781799316406, test loss: 5.704981803894043\n",
      "h: 95 | epoch: 93, train loss: 8.54975700378418, test loss: 5.704561710357666\n",
      "h: 95 | epoch: 94, train loss: 8.549734115600586, test loss: 5.704172134399414\n",
      "h: 95 | epoch: 95, train loss: 8.549713134765625, test loss: 5.703807830810547\n",
      "h: 95 | epoch: 96, train loss: 8.54969596862793, test loss: 5.7034687995910645\n",
      "h: 95 | epoch: 97, train loss: 8.549681663513184, test loss: 5.703152656555176\n",
      "h: 95 | epoch: 98, train loss: 8.549667358398438, test loss: 5.702858924865723\n",
      "h: 95 | epoch: 99, train loss: 8.54965591430664, test loss: 5.702585220336914\n",
      "h: 96 | epoch: 0, train loss: 44.9865608215332, test loss: 36.40300750732422\n",
      "h: 96 | epoch: 1, train loss: 39.54755401611328, test loss: 32.058494567871094\n",
      "h: 96 | epoch: 2, train loss: 34.92018508911133, test loss: 28.34194564819336\n",
      "h: 96 | epoch: 3, train loss: 30.981571197509766, test loss: 25.16053581237793\n",
      "h: 96 | epoch: 4, train loss: 27.628320693969727, test loss: 22.435758590698242\n",
      "h: 96 | epoch: 5, train loss: 24.773122787475586, test loss: 20.10105323791504\n",
      "h: 96 | epoch: 6, train loss: 22.342029571533203, test loss: 18.099811553955078\n",
      "h: 96 | epoch: 7, train loss: 20.272266387939453, test loss: 16.38381004333496\n",
      "h: 96 | epoch: 8, train loss: 18.510440826416016, test loss: 14.911905288696289\n",
      "h: 96 | epoch: 9, train loss: 17.01108169555664, test loss: 13.648948669433594\n",
      "h: 96 | epoch: 10, train loss: 15.735429763793945, test loss: 12.564901351928711\n",
      "h: 96 | epoch: 11, train loss: 14.650433540344238, test loss: 11.634065628051758\n",
      "h: 96 | epoch: 12, train loss: 13.727880477905273, test loss: 10.834451675415039\n",
      "h: 96 | epoch: 13, train loss: 12.943705558776855, test loss: 10.147239685058594\n",
      "h: 96 | epoch: 14, train loss: 12.277368545532227, test loss: 9.556319236755371\n",
      "h: 96 | epoch: 15, train loss: 11.711338996887207, test loss: 9.047906875610352\n",
      "h: 96 | epoch: 16, train loss: 11.230670928955078, test loss: 8.610198974609375\n",
      "h: 96 | epoch: 17, train loss: 10.822614669799805, test loss: 8.233088493347168\n",
      "h: 96 | epoch: 18, train loss: 10.476302146911621, test loss: 7.90793514251709\n",
      "h: 96 | epoch: 19, train loss: 10.182470321655273, test loss: 7.627333641052246\n",
      "h: 96 | epoch: 20, train loss: 9.933231353759766, test loss: 7.384957313537598\n",
      "h: 96 | epoch: 21, train loss: 9.721869468688965, test loss: 7.175384521484375\n",
      "h: 96 | epoch: 22, train loss: 9.542671203613281, test loss: 6.993976593017578\n",
      "h: 96 | epoch: 23, train loss: 9.390771865844727, test loss: 6.836767673492432\n",
      "h: 96 | epoch: 24, train loss: 9.262039184570312, test loss: 6.700361728668213\n",
      "h: 96 | epoch: 25, train loss: 9.152957916259766, test loss: 6.581847190856934\n",
      "h: 96 | epoch: 26, train loss: 9.060544967651367, test loss: 6.478735446929932\n",
      "h: 96 | epoch: 27, train loss: 8.982261657714844, test loss: 6.388898849487305\n",
      "h: 96 | epoch: 28, train loss: 8.9159574508667, test loss: 6.310507774353027\n",
      "h: 96 | epoch: 29, train loss: 8.859807014465332, test loss: 6.241995811462402\n",
      "h: 96 | epoch: 30, train loss: 8.812257766723633, test loss: 6.182023048400879\n",
      "h: 96 | epoch: 31, train loss: 8.771997451782227, test loss: 6.129436492919922\n",
      "h: 96 | epoch: 32, train loss: 8.737909317016602, test loss: 6.083248138427734\n",
      "h: 96 | epoch: 33, train loss: 8.7090482711792, test loss: 6.0426106452941895\n",
      "h: 96 | epoch: 34, train loss: 8.684615135192871, test loss: 6.006792068481445\n",
      "h: 96 | epoch: 35, train loss: 8.663931846618652, test loss: 5.975164890289307\n",
      "h: 96 | epoch: 36, train loss: 8.646422386169434, test loss: 5.9471893310546875\n",
      "h: 96 | epoch: 37, train loss: 8.631599426269531, test loss: 5.922398567199707\n",
      "h: 96 | epoch: 38, train loss: 8.619050025939941, test loss: 5.900390625\n",
      "h: 96 | epoch: 39, train loss: 8.60842514038086, test loss: 5.880817890167236\n",
      "h: 96 | epoch: 40, train loss: 8.599432945251465, test loss: 5.86337947845459\n",
      "h: 96 | epoch: 41, train loss: 8.591817855834961, test loss: 5.847817420959473\n",
      "h: 96 | epoch: 42, train loss: 8.585371017456055, test loss: 5.833903789520264\n",
      "h: 96 | epoch: 43, train loss: 8.579912185668945, test loss: 5.821444034576416\n",
      "h: 96 | epoch: 44, train loss: 8.575288772583008, test loss: 5.810267448425293\n",
      "h: 96 | epoch: 45, train loss: 8.571372985839844, test loss: 5.800226211547852\n",
      "h: 96 | epoch: 46, train loss: 8.568058013916016, test loss: 5.791190147399902\n",
      "h: 96 | epoch: 47, train loss: 8.565248489379883, test loss: 5.783045768737793\n",
      "h: 96 | epoch: 48, train loss: 8.56286907196045, test loss: 5.775696754455566\n",
      "h: 96 | epoch: 49, train loss: 8.560853958129883, test loss: 5.769052505493164\n",
      "h: 96 | epoch: 50, train loss: 8.559144973754883, test loss: 5.7630414962768555\n",
      "h: 96 | epoch: 51, train loss: 8.557698249816895, test loss: 5.757593154907227\n",
      "h: 96 | epoch: 52, train loss: 8.55647087097168, test loss: 5.75264835357666\n",
      "h: 96 | epoch: 53, train loss: 8.555429458618164, test loss: 5.7481584548950195\n",
      "h: 96 | epoch: 54, train loss: 8.554548263549805, test loss: 5.744073390960693\n",
      "h: 96 | epoch: 55, train loss: 8.553799629211426, test loss: 5.740355968475342\n",
      "h: 96 | epoch: 56, train loss: 8.5531644821167, test loss: 5.736966133117676\n",
      "h: 96 | epoch: 57, train loss: 8.552626609802246, test loss: 5.733874797821045\n",
      "h: 96 | epoch: 58, train loss: 8.552169799804688, test loss: 5.731051445007324\n",
      "h: 96 | epoch: 59, train loss: 8.55178165435791, test loss: 5.728471279144287\n",
      "h: 96 | epoch: 60, train loss: 8.55145263671875, test loss: 5.726111888885498\n",
      "h: 96 | epoch: 61, train loss: 8.551173210144043, test loss: 5.723951816558838\n",
      "h: 96 | epoch: 62, train loss: 8.550935745239258, test loss: 5.721973419189453\n",
      "h: 96 | epoch: 63, train loss: 8.55073356628418, test loss: 5.720160007476807\n",
      "h: 96 | epoch: 64, train loss: 8.550561904907227, test loss: 5.7184953689575195\n",
      "h: 96 | epoch: 65, train loss: 8.550416946411133, test loss: 5.7169694900512695\n",
      "h: 96 | epoch: 66, train loss: 8.550292015075684, test loss: 5.715566635131836\n",
      "h: 96 | epoch: 67, train loss: 8.550187110900879, test loss: 5.714278221130371\n",
      "h: 96 | epoch: 68, train loss: 8.55009651184082, test loss: 5.713092803955078\n",
      "h: 96 | epoch: 69, train loss: 8.550020217895508, test loss: 5.712003231048584\n",
      "h: 96 | epoch: 70, train loss: 8.549955368041992, test loss: 5.711001396179199\n",
      "h: 96 | epoch: 71, train loss: 8.54990005493164, test loss: 5.710076808929443\n",
      "h: 96 | epoch: 72, train loss: 8.54985237121582, test loss: 5.709225654602051\n",
      "h: 96 | epoch: 73, train loss: 8.549812316894531, test loss: 5.708442687988281\n",
      "h: 96 | epoch: 74, train loss: 8.549777030944824, test loss: 5.707719802856445\n",
      "h: 96 | epoch: 75, train loss: 8.549748420715332, test loss: 5.707054138183594\n",
      "h: 96 | epoch: 76, train loss: 8.549722671508789, test loss: 5.706439971923828\n",
      "h: 96 | epoch: 77, train loss: 8.549701690673828, test loss: 5.70587158203125\n",
      "h: 96 | epoch: 78, train loss: 8.549683570861816, test loss: 5.705348014831543\n",
      "h: 96 | epoch: 79, train loss: 8.549667358398438, test loss: 5.704865455627441\n",
      "h: 96 | epoch: 80, train loss: 8.549654960632324, test loss: 5.704418182373047\n",
      "h: 96 | epoch: 81, train loss: 8.549642562866211, test loss: 5.704005718231201\n",
      "h: 96 | epoch: 82, train loss: 8.549633026123047, test loss: 5.703624248504639\n",
      "h: 96 | epoch: 83, train loss: 8.5496244430542, test loss: 5.703272819519043\n",
      "h: 96 | epoch: 84, train loss: 8.549617767333984, test loss: 5.702946186065674\n",
      "h: 96 | epoch: 85, train loss: 8.549612045288086, test loss: 5.7026448249816895\n",
      "h: 96 | epoch: 86, train loss: 8.549605369567871, test loss: 5.702366828918457\n",
      "h: 96 | epoch: 87, train loss: 8.549601554870605, test loss: 5.702108860015869\n",
      "h: 96 | epoch: 88, train loss: 8.549596786499023, test loss: 5.701869487762451\n",
      "h: 96 | epoch: 89, train loss: 8.549593925476074, test loss: 5.7016496658325195\n",
      "h: 96 | epoch: 90, train loss: 8.549591064453125, test loss: 5.701444625854492\n",
      "h: 96 | epoch: 91, train loss: 8.549589157104492, test loss: 5.701254367828369\n",
      "h: 96 | epoch: 92, train loss: 8.549586296081543, test loss: 5.701078414916992\n",
      "h: 96 | epoch: 93, train loss: 8.549585342407227, test loss: 5.700915813446045\n",
      "h: 96 | epoch: 94, train loss: 8.549583435058594, test loss: 5.700766563415527\n",
      "h: 96 | epoch: 95, train loss: 8.549581527709961, test loss: 5.700625419616699\n",
      "h: 96 | epoch: 96, train loss: 8.549580574035645, test loss: 5.700496196746826\n",
      "h: 96 | epoch: 97, train loss: 8.549579620361328, test loss: 5.700376033782959\n",
      "h: 96 | epoch: 98, train loss: 8.549578666687012, test loss: 5.7002644538879395\n",
      "h: 96 | epoch: 99, train loss: 8.549577713012695, test loss: 5.700161457061768\n",
      "h: 97 | epoch: 0, train loss: 44.271484375, test loss: 35.69220733642578\n",
      "h: 97 | epoch: 1, train loss: 39.57145690917969, test loss: 31.926891326904297\n",
      "h: 97 | epoch: 2, train loss: 35.48880386352539, test loss: 28.642318725585938\n",
      "h: 97 | epoch: 3, train loss: 31.940921783447266, test loss: 25.775592803955078\n",
      "h: 97 | epoch: 4, train loss: 28.856952667236328, test loss: 23.272537231445312\n",
      "h: 97 | epoch: 5, train loss: 26.175899505615234, test loss: 21.08633804321289\n",
      "h: 97 | epoch: 6, train loss: 23.84510040283203, test loss: 19.176410675048828\n",
      "h: 97 | epoch: 7, train loss: 21.818960189819336, test loss: 17.507524490356445\n",
      "h: 97 | epoch: 8, train loss: 20.057926177978516, test loss: 16.04902458190918\n",
      "h: 97 | epoch: 9, train loss: 18.52763557434082, test loss: 14.774210929870605\n",
      "h: 97 | epoch: 10, train loss: 17.198192596435547, test loss: 13.65980052947998\n",
      "h: 97 | epoch: 11, train loss: 16.043575286865234, test loss: 12.685480117797852\n",
      "h: 97 | epoch: 12, train loss: 15.041110038757324, test loss: 11.833517074584961\n",
      "h: 97 | epoch: 13, train loss: 14.17103385925293, test loss: 11.088421821594238\n",
      "h: 97 | epoch: 14, train loss: 13.416122436523438, test loss: 10.436661720275879\n",
      "h: 97 | epoch: 15, train loss: 12.761362075805664, test loss: 9.866418838500977\n",
      "h: 97 | epoch: 16, train loss: 12.19366455078125, test loss: 9.367369651794434\n",
      "h: 97 | epoch: 17, train loss: 11.701624870300293, test loss: 8.930493354797363\n",
      "h: 97 | epoch: 18, train loss: 11.275304794311523, test loss: 8.547914505004883\n",
      "h: 97 | epoch: 19, train loss: 10.906050682067871, test loss: 8.21275520324707\n",
      "h: 97 | epoch: 20, train loss: 10.586328506469727, test loss: 7.919007301330566\n",
      "h: 97 | epoch: 21, train loss: 10.309579849243164, test loss: 7.661430358886719\n",
      "h: 97 | epoch: 22, train loss: 10.07010269165039, test loss: 7.435450077056885\n",
      "h: 97 | epoch: 23, train loss: 9.862937927246094, test loss: 7.237069606781006\n",
      "h: 97 | epoch: 24, train loss: 9.6837739944458, test loss: 7.062809944152832\n",
      "h: 97 | epoch: 25, train loss: 9.52886962890625, test loss: 6.909631252288818\n",
      "h: 97 | epoch: 26, train loss: 9.39497184753418, test loss: 6.774880409240723\n",
      "h: 97 | epoch: 27, train loss: 9.27926254272461, test loss: 6.656247615814209\n",
      "h: 97 | epoch: 28, train loss: 9.179292678833008, test loss: 6.5517144203186035\n",
      "h: 97 | epoch: 29, train loss: 9.092938423156738, test loss: 6.4595232009887695\n",
      "h: 97 | epoch: 30, train loss: 9.01836109161377, test loss: 6.378138542175293\n",
      "h: 97 | epoch: 31, train loss: 8.953968048095703, test loss: 6.306224346160889\n",
      "h: 97 | epoch: 32, train loss: 8.898377418518066, test loss: 6.242609977722168\n",
      "h: 97 | epoch: 33, train loss: 8.850395202636719, test loss: 6.186279773712158\n",
      "h: 97 | epoch: 34, train loss: 8.808984756469727, test loss: 6.136340618133545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 97 | epoch: 35, train loss: 8.773253440856934, test loss: 6.0920186042785645\n",
      "h: 97 | epoch: 36, train loss: 8.742426872253418, test loss: 6.052632808685303\n",
      "h: 97 | epoch: 37, train loss: 8.715832710266113, test loss: 6.017594337463379\n",
      "h: 97 | epoch: 38, train loss: 8.692895889282227, test loss: 5.9863810539245605\n",
      "h: 97 | epoch: 39, train loss: 8.673112869262695, test loss: 5.958542823791504\n",
      "h: 97 | epoch: 40, train loss: 8.65605354309082, test loss: 5.933682441711426\n",
      "h: 97 | epoch: 41, train loss: 8.64134407043457, test loss: 5.911452770233154\n",
      "h: 97 | epoch: 42, train loss: 8.628662109375, test loss: 5.891549110412598\n",
      "h: 97 | epoch: 43, train loss: 8.617727279663086, test loss: 5.873705863952637\n",
      "h: 97 | epoch: 44, train loss: 8.608302116394043, test loss: 5.857687950134277\n",
      "h: 97 | epoch: 45, train loss: 8.600177764892578, test loss: 5.84329080581665\n",
      "h: 97 | epoch: 46, train loss: 8.593175888061523, test loss: 5.8303327560424805\n",
      "h: 97 | epoch: 47, train loss: 8.587141036987305, test loss: 5.818657398223877\n",
      "h: 97 | epoch: 48, train loss: 8.581938743591309, test loss: 5.8081207275390625\n",
      "h: 97 | epoch: 49, train loss: 8.577457427978516, test loss: 5.7986040115356445\n",
      "h: 97 | epoch: 50, train loss: 8.57359504699707, test loss: 5.7899956703186035\n",
      "h: 97 | epoch: 51, train loss: 8.570268630981445, test loss: 5.782199859619141\n",
      "h: 97 | epoch: 52, train loss: 8.567399978637695, test loss: 5.775132179260254\n",
      "h: 97 | epoch: 53, train loss: 8.564929962158203, test loss: 5.768715858459473\n",
      "h: 97 | epoch: 54, train loss: 8.562801361083984, test loss: 5.762887954711914\n",
      "h: 97 | epoch: 55, train loss: 8.560968399047852, test loss: 5.757585525512695\n",
      "h: 97 | epoch: 56, train loss: 8.559388160705566, test loss: 5.752758026123047\n",
      "h: 97 | epoch: 57, train loss: 8.558027267456055, test loss: 5.748357772827148\n",
      "h: 97 | epoch: 58, train loss: 8.556855201721191, test loss: 5.744342803955078\n",
      "h: 97 | epoch: 59, train loss: 8.5558443069458, test loss: 5.740677833557129\n",
      "h: 97 | epoch: 60, train loss: 8.554975509643555, test loss: 5.737326145172119\n",
      "h: 97 | epoch: 61, train loss: 8.554224967956543, test loss: 5.734262466430664\n",
      "h: 97 | epoch: 62, train loss: 8.553580284118652, test loss: 5.7314558029174805\n",
      "h: 97 | epoch: 63, train loss: 8.553025245666504, test loss: 5.728885173797607\n",
      "h: 97 | epoch: 64, train loss: 8.552545547485352, test loss: 5.726528167724609\n",
      "h: 97 | epoch: 65, train loss: 8.552133560180664, test loss: 5.724366188049316\n",
      "h: 97 | epoch: 66, train loss: 8.551777839660645, test loss: 5.722380638122559\n",
      "h: 97 | epoch: 67, train loss: 8.551472663879395, test loss: 5.72055721282959\n",
      "h: 97 | epoch: 68, train loss: 8.55120849609375, test loss: 5.718880653381348\n",
      "h: 97 | epoch: 69, train loss: 8.550981521606445, test loss: 5.7173380851745605\n",
      "h: 97 | epoch: 70, train loss: 8.550786018371582, test loss: 5.715920448303223\n",
      "h: 97 | epoch: 71, train loss: 8.550617218017578, test loss: 5.714613437652588\n",
      "h: 97 | epoch: 72, train loss: 8.5504732131958, test loss: 5.713409900665283\n",
      "h: 97 | epoch: 73, train loss: 8.550348281860352, test loss: 5.712302207946777\n",
      "h: 97 | epoch: 74, train loss: 8.550240516662598, test loss: 5.71127986907959\n",
      "h: 97 | epoch: 75, train loss: 8.550148010253906, test loss: 5.710337162017822\n",
      "h: 97 | epoch: 76, train loss: 8.550066947937012, test loss: 5.709467887878418\n",
      "h: 97 | epoch: 77, train loss: 8.54999828338623, test loss: 5.708665370941162\n",
      "h: 97 | epoch: 78, train loss: 8.54994010925293, test loss: 5.707924842834473\n",
      "h: 97 | epoch: 79, train loss: 8.549888610839844, test loss: 5.707242012023926\n",
      "h: 97 | epoch: 80, train loss: 8.549844741821289, test loss: 5.70660924911499\n",
      "h: 97 | epoch: 81, train loss: 8.54980754852295, test loss: 5.706025123596191\n",
      "h: 97 | epoch: 82, train loss: 8.549775123596191, test loss: 5.70548677444458\n",
      "h: 97 | epoch: 83, train loss: 8.5497465133667, test loss: 5.704987525939941\n",
      "h: 97 | epoch: 84, train loss: 8.549722671508789, test loss: 5.704526424407959\n",
      "h: 97 | epoch: 85, train loss: 8.549701690673828, test loss: 5.704100608825684\n",
      "h: 97 | epoch: 86, train loss: 8.549684524536133, test loss: 5.703707695007324\n",
      "h: 97 | epoch: 87, train loss: 8.549668312072754, test loss: 5.703341960906982\n",
      "h: 97 | epoch: 88, train loss: 8.549654006958008, test loss: 5.703004837036133\n",
      "h: 97 | epoch: 89, train loss: 8.549643516540527, test loss: 5.702692985534668\n",
      "h: 97 | epoch: 90, train loss: 8.549633979797363, test loss: 5.702403545379639\n",
      "h: 97 | epoch: 91, train loss: 8.549625396728516, test loss: 5.702136039733887\n",
      "h: 97 | epoch: 92, train loss: 8.5496187210083, test loss: 5.701888561248779\n",
      "h: 97 | epoch: 93, train loss: 8.549612045288086, test loss: 5.701659202575684\n",
      "h: 97 | epoch: 94, train loss: 8.549605369567871, test loss: 5.701447486877441\n",
      "h: 97 | epoch: 95, train loss: 8.549601554870605, test loss: 5.701251983642578\n",
      "h: 97 | epoch: 96, train loss: 8.54959774017334, test loss: 5.701070308685303\n",
      "h: 97 | epoch: 97, train loss: 8.549593925476074, test loss: 5.700901985168457\n",
      "h: 97 | epoch: 98, train loss: 8.549591064453125, test loss: 5.700746059417725\n",
      "h: 97 | epoch: 99, train loss: 8.549589157104492, test loss: 5.700601577758789\n",
      "h: 98 | epoch: 0, train loss: 45.74272918701172, test loss: 36.861141204833984\n",
      "h: 98 | epoch: 1, train loss: 40.58185577392578, test loss: 32.72343063354492\n",
      "h: 98 | epoch: 2, train loss: 36.137351989746094, test loss: 29.14499855041504\n",
      "h: 98 | epoch: 3, train loss: 32.307884216308594, test loss: 26.048381805419922\n",
      "h: 98 | epoch: 4, train loss: 29.007312774658203, test loss: 23.367450714111328\n",
      "h: 98 | epoch: 5, train loss: 26.16213607788086, test loss: 21.045528411865234\n",
      "h: 98 | epoch: 6, train loss: 23.709423065185547, test loss: 19.0339298248291\n",
      "h: 98 | epoch: 7, train loss: 21.595157623291016, test loss: 17.290748596191406\n",
      "h: 98 | epoch: 8, train loss: 19.772876739501953, test loss: 15.779825210571289\n",
      "h: 98 | epoch: 9, train loss: 18.202560424804688, test loss: 14.469958305358887\n",
      "h: 98 | epoch: 10, train loss: 16.84969139099121, test loss: 13.334161758422852\n",
      "h: 98 | epoch: 11, train loss: 15.684475898742676, test loss: 12.349101066589355\n",
      "h: 98 | epoch: 12, train loss: 14.681180000305176, test loss: 11.49458122253418\n",
      "h: 98 | epoch: 13, train loss: 13.817567825317383, test loss: 10.753114700317383\n",
      "h: 98 | epoch: 14, train loss: 13.074427604675293, test loss: 10.109564781188965\n",
      "h: 98 | epoch: 15, train loss: 12.435159683227539, test loss: 9.550824165344238\n",
      "h: 98 | epoch: 16, train loss: 11.885418891906738, test loss: 9.065542221069336\n",
      "h: 98 | epoch: 17, train loss: 11.412816047668457, test loss: 8.643882751464844\n",
      "h: 98 | epoch: 18, train loss: 11.006650924682617, test loss: 8.277344703674316\n",
      "h: 98 | epoch: 19, train loss: 10.657683372497559, test loss: 7.958558559417725\n",
      "h: 98 | epoch: 20, train loss: 10.357948303222656, test loss: 7.681143760681152\n",
      "h: 98 | epoch: 21, train loss: 10.100564956665039, test loss: 7.4395880699157715\n",
      "h: 98 | epoch: 22, train loss: 9.879608154296875, test loss: 7.2291131019592285\n",
      "h: 98 | epoch: 23, train loss: 9.68996524810791, test loss: 7.045587062835693\n",
      "h: 98 | epoch: 24, train loss: 9.527238845825195, test loss: 6.885434150695801\n",
      "h: 98 | epoch: 25, train loss: 9.387632369995117, test loss: 6.745561122894287\n",
      "h: 98 | epoch: 26, train loss: 9.267889022827148, test loss: 6.623290061950684\n",
      "h: 98 | epoch: 27, train loss: 9.16519832611084, test loss: 6.516308784484863\n",
      "h: 98 | epoch: 28, train loss: 9.0771484375, test loss: 6.422610282897949\n",
      "h: 98 | epoch: 29, train loss: 9.00166130065918, test loss: 6.340460777282715\n",
      "h: 98 | epoch: 30, train loss: 8.936955451965332, test loss: 6.268359184265137\n",
      "h: 98 | epoch: 31, train loss: 8.881494522094727, test loss: 6.2050042152404785\n",
      "h: 98 | epoch: 32, train loss: 8.833966255187988, test loss: 6.149271011352539\n",
      "h: 98 | epoch: 33, train loss: 8.793237686157227, test loss: 6.100184917449951\n",
      "h: 98 | epoch: 34, train loss: 8.758338928222656, test loss: 6.056898593902588\n",
      "h: 98 | epoch: 35, train loss: 8.728439331054688, test loss: 6.018678665161133\n",
      "h: 98 | epoch: 36, train loss: 8.7028226852417, test loss: 5.984890460968018\n",
      "h: 98 | epoch: 37, train loss: 8.680877685546875, test loss: 5.95497989654541\n",
      "h: 98 | epoch: 38, train loss: 8.662076950073242, test loss: 5.928469657897949\n",
      "h: 98 | epoch: 39, train loss: 8.64597225189209, test loss: 5.9049391746521\n",
      "h: 98 | epoch: 40, train loss: 8.632177352905273, test loss: 5.884028434753418\n",
      "h: 98 | epoch: 41, train loss: 8.62035846710205, test loss: 5.865420341491699\n",
      "h: 98 | epoch: 42, train loss: 8.610235214233398, test loss: 5.848839282989502\n",
      "h: 98 | epoch: 43, train loss: 8.601563453674316, test loss: 5.834045886993408\n",
      "h: 98 | epoch: 44, train loss: 8.594134330749512, test loss: 5.820829391479492\n",
      "h: 98 | epoch: 45, train loss: 8.587770462036133, test loss: 5.80900764465332\n",
      "h: 98 | epoch: 46, train loss: 8.582318305969238, test loss: 5.798418998718262\n",
      "h: 98 | epoch: 47, train loss: 8.57764720916748, test loss: 5.7889251708984375\n",
      "h: 98 | epoch: 48, train loss: 8.573644638061523, test loss: 5.78040075302124\n",
      "h: 98 | epoch: 49, train loss: 8.570215225219727, test loss: 5.772739887237549\n",
      "h: 98 | epoch: 50, train loss: 8.567276954650879, test loss: 5.76584529876709\n",
      "h: 98 | epoch: 51, train loss: 8.564757347106934, test loss: 5.7596354484558105\n",
      "h: 98 | epoch: 52, train loss: 8.562601089477539, test loss: 5.754035472869873\n",
      "h: 98 | epoch: 53, train loss: 8.560750961303711, test loss: 5.748979091644287\n",
      "h: 98 | epoch: 54, train loss: 8.559164047241211, test loss: 5.744412422180176\n",
      "h: 98 | epoch: 55, train loss: 8.557805061340332, test loss: 5.740281581878662\n",
      "h: 98 | epoch: 56, train loss: 8.556638717651367, test loss: 5.736542224884033\n",
      "h: 98 | epoch: 57, train loss: 8.55564022064209, test loss: 5.733155250549316\n",
      "h: 98 | epoch: 58, train loss: 8.55478286743164, test loss: 5.730083465576172\n",
      "h: 98 | epoch: 59, train loss: 8.554047584533691, test loss: 5.727295875549316\n",
      "h: 98 | epoch: 60, train loss: 8.55341625213623, test loss: 5.724766731262207\n",
      "h: 98 | epoch: 61, train loss: 8.552876472473145, test loss: 5.722466468811035\n",
      "h: 98 | epoch: 62, train loss: 8.552412033081055, test loss: 5.720376014709473\n",
      "h: 98 | epoch: 63, train loss: 8.552013397216797, test loss: 5.718474388122559\n",
      "h: 98 | epoch: 64, train loss: 8.55167007446289, test loss: 5.716742992401123\n",
      "h: 98 | epoch: 65, train loss: 8.55137825012207, test loss: 5.715167999267578\n",
      "h: 98 | epoch: 66, train loss: 8.55112361907959, test loss: 5.71373176574707\n",
      "h: 98 | epoch: 67, train loss: 8.550908088684082, test loss: 5.7124223709106445\n",
      "h: 98 | epoch: 68, train loss: 8.550722122192383, test loss: 5.711228847503662\n",
      "h: 98 | epoch: 69, train loss: 8.550561904907227, test loss: 5.710141181945801\n",
      "h: 98 | epoch: 70, train loss: 8.550424575805664, test loss: 5.7091474533081055\n",
      "h: 98 | epoch: 71, train loss: 8.55030632019043, test loss: 5.708240985870361\n",
      "h: 98 | epoch: 72, train loss: 8.550204277038574, test loss: 5.7074127197265625\n",
      "h: 98 | epoch: 73, train loss: 8.550116539001465, test loss: 5.706658363342285\n",
      "h: 98 | epoch: 74, train loss: 8.550042152404785, test loss: 5.705966949462891\n",
      "h: 98 | epoch: 75, train loss: 8.549978256225586, test loss: 5.7053375244140625\n",
      "h: 98 | epoch: 76, train loss: 8.549922943115234, test loss: 5.704760551452637\n",
      "h: 98 | epoch: 77, train loss: 8.549873352050781, test loss: 5.704234600067139\n",
      "h: 98 | epoch: 78, train loss: 8.549833297729492, test loss: 5.70375394821167\n",
      "h: 98 | epoch: 79, train loss: 8.549796104431152, test loss: 5.703314781188965\n",
      "h: 98 | epoch: 80, train loss: 8.549766540527344, test loss: 5.7029128074646\n",
      "h: 98 | epoch: 81, train loss: 8.549739837646484, test loss: 5.702548027038574\n",
      "h: 98 | epoch: 82, train loss: 8.549717903137207, test loss: 5.702210426330566\n",
      "h: 98 | epoch: 83, train loss: 8.549697875976562, test loss: 5.701905250549316\n",
      "h: 98 | epoch: 84, train loss: 8.549680709838867, test loss: 5.701624393463135\n",
      "h: 98 | epoch: 85, train loss: 8.549666404724121, test loss: 5.701368808746338\n",
      "h: 98 | epoch: 86, train loss: 8.549654006958008, test loss: 5.70113468170166\n",
      "h: 98 | epoch: 87, train loss: 8.549642562866211, test loss: 5.700921058654785\n",
      "h: 98 | epoch: 88, train loss: 8.54963493347168, test loss: 5.700726509094238\n",
      "h: 98 | epoch: 89, train loss: 8.549625396728516, test loss: 5.700547695159912\n",
      "h: 98 | epoch: 90, train loss: 8.5496187210083, test loss: 5.700386047363281\n",
      "h: 98 | epoch: 91, train loss: 8.549612998962402, test loss: 5.700236797332764\n",
      "h: 98 | epoch: 92, train loss: 8.54960823059082, test loss: 5.700101375579834\n",
      "h: 98 | epoch: 93, train loss: 8.549602508544922, test loss: 5.699976921081543\n",
      "h: 98 | epoch: 94, train loss: 8.549599647521973, test loss: 5.699864387512207\n",
      "h: 98 | epoch: 95, train loss: 8.54959487915039, test loss: 5.699762344360352\n",
      "h: 98 | epoch: 96, train loss: 8.549592971801758, test loss: 5.6996684074401855\n",
      "h: 98 | epoch: 97, train loss: 8.549589157104492, test loss: 5.699582099914551\n",
      "h: 98 | epoch: 98, train loss: 8.549588203430176, test loss: 5.699505805969238\n",
      "h: 98 | epoch: 99, train loss: 8.549585342407227, test loss: 5.699434757232666\n",
      "h: 99 | epoch: 0, train loss: 43.492347717285156, test loss: 36.1239013671875\n",
      "h: 99 | epoch: 1, train loss: 39.116416931152344, test loss: 32.51194763183594\n",
      "h: 99 | epoch: 2, train loss: 35.28693389892578, test loss: 29.336673736572266\n",
      "h: 99 | epoch: 3, train loss: 31.934371948242188, test loss: 26.543903350830078\n",
      "h: 99 | epoch: 4, train loss: 28.998666763305664, test loss: 24.086627960205078\n",
      "h: 99 | epoch: 5, train loss: 26.427709579467773, test loss: 21.923885345458984\n",
      "h: 99 | epoch: 6, train loss: 24.176189422607422, test loss: 20.019943237304688\n",
      "h: 99 | epoch: 7, train loss: 22.20458221435547, test loss: 18.343505859375\n",
      "h: 99 | epoch: 8, train loss: 20.47835922241211, test loss: 16.8671817779541\n",
      "h: 99 | epoch: 9, train loss: 18.967294692993164, test loss: 15.566904067993164\n",
      "h: 99 | epoch: 10, train loss: 17.644908905029297, test loss: 14.421539306640625\n",
      "h: 99 | epoch: 11, train loss: 16.48796844482422, test loss: 13.412513732910156\n",
      "h: 99 | epoch: 12, train loss: 15.476097106933594, test loss: 12.523484230041504\n",
      "h: 99 | epoch: 13, train loss: 14.591390609741211, test loss: 11.740060806274414\n",
      "h: 99 | epoch: 14, train loss: 13.818132400512695, test loss: 11.049580574035645\n",
      "h: 99 | epoch: 15, train loss: 13.142515182495117, test loss: 10.440901756286621\n",
      "h: 99 | epoch: 16, train loss: 12.552416801452637, test loss: 9.904206275939941\n",
      "h: 99 | epoch: 17, train loss: 12.037191390991211, test loss: 9.430858612060547\n",
      "h: 99 | epoch: 18, train loss: 11.587489128112793, test loss: 9.013257026672363\n",
      "h: 99 | epoch: 19, train loss: 11.195114135742188, test loss: 8.644704818725586\n",
      "h: 99 | epoch: 20, train loss: 10.852866172790527, test loss: 8.319317817687988\n",
      "h: 99 | epoch: 21, train loss: 10.554436683654785, test loss: 8.031912803649902\n",
      "h: 99 | epoch: 22, train loss: 10.294296264648438, test loss: 7.777937889099121\n",
      "h: 99 | epoch: 23, train loss: 10.067598342895508, test loss: 7.553384304046631\n",
      "h: 99 | epoch: 24, train loss: 9.870099067687988, test loss: 7.354726314544678\n",
      "h: 99 | epoch: 25, train loss: 9.698083877563477, test loss: 7.178872585296631\n",
      "h: 99 | epoch: 26, train loss: 9.548303604125977, test loss: 7.023097038269043\n",
      "h: 99 | epoch: 27, train loss: 9.417915344238281, test loss: 6.885008811950684\n",
      "h: 99 | epoch: 28, train loss: 9.304434776306152, test loss: 6.762506008148193\n",
      "h: 99 | epoch: 29, train loss: 9.205690383911133, test loss: 6.653738498687744\n",
      "h: 99 | epoch: 30, train loss: 9.11978816986084, test loss: 6.5570855140686035\n",
      "h: 99 | epoch: 31, train loss: 9.045069694519043, test loss: 6.471121311187744\n",
      "h: 99 | epoch: 32, train loss: 8.980093002319336, test loss: 6.394586563110352\n",
      "h: 99 | epoch: 33, train loss: 8.92359733581543, test loss: 6.326384544372559\n",
      "h: 99 | epoch: 34, train loss: 8.874482154846191, test loss: 6.265544891357422\n",
      "h: 99 | epoch: 35, train loss: 8.831790924072266, test loss: 6.211215019226074\n",
      "h: 99 | epoch: 36, train loss: 8.79468822479248, test loss: 6.162644863128662\n",
      "h: 99 | epoch: 37, train loss: 8.762447357177734, test loss: 6.119176864624023\n",
      "h: 99 | epoch: 38, train loss: 8.7344331741333, test loss: 6.080229759216309\n",
      "h: 99 | epoch: 39, train loss: 8.710095405578613, test loss: 6.04529333114624\n",
      "h: 99 | epoch: 40, train loss: 8.688953399658203, test loss: 6.013916492462158\n",
      "h: 99 | epoch: 41, train loss: 8.670588493347168, test loss: 5.98570442199707\n",
      "h: 99 | epoch: 42, train loss: 8.654638290405273, test loss: 5.9603071212768555\n",
      "h: 99 | epoch: 43, train loss: 8.640788078308105, test loss: 5.937414169311523\n",
      "h: 99 | epoch: 44, train loss: 8.628759384155273, test loss: 5.9167561531066895\n",
      "h: 99 | epoch: 45, train loss: 8.618314743041992, test loss: 5.898089408874512\n",
      "h: 99 | epoch: 46, train loss: 8.609245300292969, test loss: 5.881203651428223\n",
      "h: 99 | epoch: 47, train loss: 8.601370811462402, test loss: 5.865910053253174\n",
      "h: 99 | epoch: 48, train loss: 8.594535827636719, test loss: 5.852042198181152\n",
      "h: 99 | epoch: 49, train loss: 8.588601112365723, test loss: 5.839451789855957\n",
      "h: 99 | epoch: 50, train loss: 8.58344841003418, test loss: 5.82800817489624\n",
      "h: 99 | epoch: 51, train loss: 8.57897663116455, test loss: 5.817594051361084\n",
      "h: 99 | epoch: 52, train loss: 8.575094223022461, test loss: 5.808107376098633\n",
      "h: 99 | epoch: 53, train loss: 8.571723937988281, test loss: 5.799454689025879\n",
      "h: 99 | epoch: 54, train loss: 8.56879997253418, test loss: 5.791557788848877\n",
      "h: 99 | epoch: 55, train loss: 8.566261291503906, test loss: 5.78433895111084\n",
      "h: 99 | epoch: 56, train loss: 8.564057350158691, test loss: 5.777733325958252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 99 | epoch: 57, train loss: 8.56214427947998, test loss: 5.771686553955078\n",
      "h: 99 | epoch: 58, train loss: 8.560484886169434, test loss: 5.766142845153809\n",
      "h: 99 | epoch: 59, train loss: 8.559043884277344, test loss: 5.761057376861572\n",
      "h: 99 | epoch: 60, train loss: 8.557792663574219, test loss: 5.756386756896973\n",
      "h: 99 | epoch: 61, train loss: 8.556708335876465, test loss: 5.7520952224731445\n",
      "h: 99 | epoch: 62, train loss: 8.555766105651855, test loss: 5.748147010803223\n",
      "h: 99 | epoch: 63, train loss: 8.554948806762695, test loss: 5.744512557983398\n",
      "h: 99 | epoch: 64, train loss: 8.554239273071289, test loss: 5.741164207458496\n",
      "h: 99 | epoch: 65, train loss: 8.55362319946289, test loss: 5.7380781173706055\n",
      "h: 99 | epoch: 66, train loss: 8.553089141845703, test loss: 5.735231876373291\n",
      "h: 99 | epoch: 67, train loss: 8.55262565612793, test loss: 5.732603549957275\n",
      "h: 99 | epoch: 68, train loss: 8.55222225189209, test loss: 5.730175971984863\n",
      "h: 99 | epoch: 69, train loss: 8.551873207092285, test loss: 5.727932453155518\n",
      "h: 99 | epoch: 70, train loss: 8.551568984985352, test loss: 5.725857257843018\n",
      "h: 99 | epoch: 71, train loss: 8.551305770874023, test loss: 5.72393798828125\n",
      "h: 99 | epoch: 72, train loss: 8.551077842712402, test loss: 5.722160816192627\n",
      "h: 99 | epoch: 73, train loss: 8.55087947845459, test loss: 5.720515251159668\n",
      "h: 99 | epoch: 74, train loss: 8.55070686340332, test loss: 5.718990802764893\n",
      "h: 99 | epoch: 75, train loss: 8.550558090209961, test loss: 5.717576503753662\n",
      "h: 99 | epoch: 76, train loss: 8.55042839050293, test loss: 5.716265678405762\n",
      "h: 99 | epoch: 77, train loss: 8.550315856933594, test loss: 5.715048789978027\n",
      "h: 99 | epoch: 78, train loss: 8.550217628479004, test loss: 5.713921070098877\n",
      "h: 99 | epoch: 79, train loss: 8.550132751464844, test loss: 5.712873935699463\n",
      "h: 99 | epoch: 80, train loss: 8.550058364868164, test loss: 5.711901664733887\n",
      "h: 99 | epoch: 81, train loss: 8.549994468688965, test loss: 5.710997581481934\n",
      "h: 99 | epoch: 82, train loss: 8.549939155578613, test loss: 5.710158824920654\n",
      "h: 99 | epoch: 83, train loss: 8.549890518188477, test loss: 5.709377765655518\n",
      "h: 99 | epoch: 84, train loss: 8.549849510192871, test loss: 5.708652019500732\n",
      "h: 99 | epoch: 85, train loss: 8.549813270568848, test loss: 5.707976341247559\n",
      "h: 99 | epoch: 86, train loss: 8.54978084564209, test loss: 5.707350254058838\n",
      "h: 99 | epoch: 87, train loss: 8.549753189086914, test loss: 5.706766605377197\n",
      "h: 99 | epoch: 88, train loss: 8.54973030090332, test loss: 5.706223487854004\n",
      "h: 99 | epoch: 89, train loss: 8.54970932006836, test loss: 5.705718040466309\n",
      "h: 99 | epoch: 90, train loss: 8.549691200256348, test loss: 5.705247402191162\n",
      "h: 99 | epoch: 91, train loss: 8.549675941467285, test loss: 5.70481014251709\n",
      "h: 99 | epoch: 92, train loss: 8.549662590026855, test loss: 5.70440149307251\n",
      "h: 99 | epoch: 93, train loss: 8.549650192260742, test loss: 5.704022407531738\n",
      "h: 99 | epoch: 94, train loss: 8.549640655517578, test loss: 5.703668117523193\n",
      "h: 99 | epoch: 95, train loss: 8.549631118774414, test loss: 5.703339576721191\n",
      "h: 99 | epoch: 96, train loss: 8.549623489379883, test loss: 5.70303201675415\n",
      "h: 99 | epoch: 97, train loss: 8.549616813659668, test loss: 5.702746391296387\n",
      "h: 99 | epoch: 98, train loss: 8.549612045288086, test loss: 5.702478885650635\n",
      "h: 99 | epoch: 99, train loss: 8.549605369567871, test loss: 5.702231407165527\n",
      "h: 100 | epoch: 0, train loss: 43.53803253173828, test loss: 35.187686920166016\n",
      "h: 100 | epoch: 1, train loss: 38.613258361816406, test loss: 31.220754623413086\n",
      "h: 100 | epoch: 2, train loss: 34.380836486816406, test loss: 27.796878814697266\n",
      "h: 100 | epoch: 3, train loss: 30.741947174072266, test loss: 24.840105056762695\n",
      "h: 100 | epoch: 4, train loss: 27.612646102905273, test loss: 22.285598754882812\n",
      "h: 100 | epoch: 5, train loss: 24.921310424804688, test loss: 20.077869415283203\n",
      "h: 100 | epoch: 6, train loss: 22.606678009033203, test loss: 18.169292449951172\n",
      "h: 100 | epoch: 7, train loss: 20.61623191833496, test loss: 16.51892852783203\n",
      "h: 100 | epoch: 8, train loss: 18.904857635498047, test loss: 15.091517448425293\n",
      "h: 100 | epoch: 9, train loss: 17.433757781982422, test loss: 13.856683731079102\n",
      "h: 100 | epoch: 10, train loss: 16.16952896118164, test loss: 12.788204193115234\n",
      "h: 100 | epoch: 11, train loss: 15.083392143249512, test loss: 11.863454818725586\n",
      "h: 100 | epoch: 12, train loss: 14.150550842285156, test loss: 11.062887191772461\n",
      "h: 100 | epoch: 13, train loss: 13.349618911743164, test loss: 10.369629859924316\n",
      "h: 100 | epoch: 14, train loss: 12.66217041015625, test loss: 9.769098281860352\n",
      "h: 100 | epoch: 15, train loss: 12.07231330871582, test loss: 9.248701095581055\n",
      "h: 100 | epoch: 16, train loss: 11.566354751586914, test loss: 8.797553062438965\n",
      "h: 100 | epoch: 17, train loss: 11.13249683380127, test loss: 8.406255722045898\n",
      "h: 100 | epoch: 18, train loss: 10.760576248168945, test loss: 8.066693305969238\n",
      "h: 100 | epoch: 19, train loss: 10.441844940185547, test loss: 7.77185583114624\n",
      "h: 100 | epoch: 20, train loss: 10.168767929077148, test loss: 7.515688896179199\n",
      "h: 100 | epoch: 21, train loss: 9.934869766235352, test loss: 7.29296875\n",
      "h: 100 | epoch: 22, train loss: 9.734579086303711, test loss: 7.099179744720459\n",
      "h: 100 | epoch: 23, train loss: 9.563103675842285, test loss: 6.930431365966797\n",
      "h: 100 | epoch: 24, train loss: 9.416333198547363, test loss: 6.783358573913574\n",
      "h: 100 | epoch: 25, train loss: 9.290731430053711, test loss: 6.655060768127441\n",
      "h: 100 | epoch: 26, train loss: 9.183266639709473, test loss: 6.543031215667725\n",
      "h: 100 | epoch: 27, train loss: 9.091336250305176, test loss: 6.445106506347656\n",
      "h: 100 | epoch: 28, train loss: 9.012702941894531, test loss: 6.359421253204346\n",
      "h: 100 | epoch: 29, train loss: 8.945459365844727, test loss: 6.284356594085693\n",
      "h: 100 | epoch: 30, train loss: 8.887956619262695, test loss: 6.218523025512695\n",
      "h: 100 | epoch: 31, train loss: 8.838793754577637, test loss: 6.160715103149414\n",
      "h: 100 | epoch: 32, train loss: 8.796764373779297, test loss: 6.109888553619385\n",
      "h: 100 | epoch: 33, train loss: 8.76083755493164, test loss: 6.065145492553711\n",
      "h: 100 | epoch: 34, train loss: 8.730127334594727, test loss: 6.025705337524414\n",
      "h: 100 | epoch: 35, train loss: 8.703880310058594, test loss: 5.990893840789795\n",
      "h: 100 | epoch: 36, train loss: 8.681447982788086, test loss: 5.960124969482422\n",
      "h: 100 | epoch: 37, train loss: 8.662277221679688, test loss: 5.9328932762146\n",
      "h: 100 | epoch: 38, train loss: 8.645895004272461, test loss: 5.908759117126465\n",
      "h: 100 | epoch: 39, train loss: 8.631895065307617, test loss: 5.887338638305664\n",
      "h: 100 | epoch: 40, train loss: 8.619932174682617, test loss: 5.868301868438721\n",
      "h: 100 | epoch: 41, train loss: 8.609708786010742, test loss: 5.851360321044922\n",
      "h: 100 | epoch: 42, train loss: 8.600974082946777, test loss: 5.8362627029418945\n",
      "h: 100 | epoch: 43, train loss: 8.593509674072266, test loss: 5.822789192199707\n",
      "h: 100 | epoch: 44, train loss: 8.58713150024414, test loss: 5.810749530792236\n",
      "h: 100 | epoch: 45, train loss: 8.581679344177246, test loss: 5.799978256225586\n",
      "h: 100 | epoch: 46, train loss: 8.577022552490234, test loss: 5.790328025817871\n",
      "h: 100 | epoch: 47, train loss: 8.573041915893555, test loss: 5.781671524047852\n",
      "h: 100 | epoch: 48, train loss: 8.569639205932617, test loss: 5.77389669418335\n",
      "h: 100 | epoch: 49, train loss: 8.566732406616211, test loss: 5.766905784606934\n",
      "h: 100 | epoch: 50, train loss: 8.56424617767334, test loss: 5.760611534118652\n",
      "h: 100 | epoch: 51, train loss: 8.56212329864502, test loss: 5.754939079284668\n",
      "h: 100 | epoch: 52, train loss: 8.560308456420898, test loss: 5.74982213973999\n",
      "h: 100 | epoch: 53, train loss: 8.558754920959473, test loss: 5.745200157165527\n",
      "h: 100 | epoch: 54, train loss: 8.557429313659668, test loss: 5.741022109985352\n",
      "h: 100 | epoch: 55, train loss: 8.556294441223145, test loss: 5.737241744995117\n",
      "h: 100 | epoch: 56, train loss: 8.555323600769043, test loss: 5.7338175773620605\n",
      "h: 100 | epoch: 57, train loss: 8.554494857788086, test loss: 5.730713844299316\n",
      "h: 100 | epoch: 58, train loss: 8.55378532409668, test loss: 5.727898597717285\n",
      "h: 100 | epoch: 59, train loss: 8.553178787231445, test loss: 5.725342273712158\n",
      "h: 100 | epoch: 60, train loss: 8.55265998840332, test loss: 5.723019123077393\n",
      "h: 100 | epoch: 61, train loss: 8.552216529846191, test loss: 5.720908164978027\n",
      "h: 100 | epoch: 62, train loss: 8.551835060119629, test loss: 5.7189860343933105\n",
      "h: 100 | epoch: 63, train loss: 8.551511764526367, test loss: 5.71723747253418\n",
      "h: 100 | epoch: 64, train loss: 8.55123233795166, test loss: 5.715644359588623\n",
      "h: 100 | epoch: 65, train loss: 8.550993919372559, test loss: 5.7141923904418945\n",
      "h: 100 | epoch: 66, train loss: 8.55079174041748, test loss: 5.712868690490723\n",
      "h: 100 | epoch: 67, train loss: 8.550616264343262, test loss: 5.711661338806152\n",
      "h: 100 | epoch: 68, train loss: 8.550467491149902, test loss: 5.710558891296387\n",
      "h: 100 | epoch: 69, train loss: 8.55034065246582, test loss: 5.709553241729736\n",
      "h: 100 | epoch: 70, train loss: 8.550230026245117, test loss: 5.708634853363037\n",
      "h: 100 | epoch: 71, train loss: 8.550137519836426, test loss: 5.707795143127441\n",
      "h: 100 | epoch: 72, train loss: 8.550057411193848, test loss: 5.707028865814209\n",
      "h: 100 | epoch: 73, train loss: 8.54998779296875, test loss: 5.706327438354492\n",
      "h: 100 | epoch: 74, train loss: 8.549928665161133, test loss: 5.705686569213867\n",
      "h: 100 | epoch: 75, train loss: 8.54987907409668, test loss: 5.705100059509277\n",
      "h: 100 | epoch: 76, train loss: 8.549834251403809, test loss: 5.704564094543457\n",
      "h: 100 | epoch: 77, train loss: 8.549798011779785, test loss: 5.704072952270508\n",
      "h: 100 | epoch: 78, train loss: 8.549764633178711, test loss: 5.703624248504639\n",
      "h: 100 | epoch: 79, train loss: 8.549738883972168, test loss: 5.703213214874268\n",
      "h: 100 | epoch: 80, train loss: 8.549715995788574, test loss: 5.702836513519287\n",
      "h: 100 | epoch: 81, train loss: 8.549695014953613, test loss: 5.702493190765381\n",
      "h: 100 | epoch: 82, train loss: 8.549677848815918, test loss: 5.702178001403809\n",
      "h: 100 | epoch: 83, train loss: 8.549661636352539, test loss: 5.701888084411621\n",
      "h: 100 | epoch: 84, train loss: 8.549650192260742, test loss: 5.701624393463135\n",
      "h: 100 | epoch: 85, train loss: 8.549639701843262, test loss: 5.701382637023926\n",
      "h: 100 | epoch: 86, train loss: 8.549631118774414, test loss: 5.701160907745361\n",
      "h: 100 | epoch: 87, train loss: 8.549622535705566, test loss: 5.700958251953125\n",
      "h: 100 | epoch: 88, train loss: 8.549615859985352, test loss: 5.700772285461426\n",
      "h: 100 | epoch: 89, train loss: 8.549609184265137, test loss: 5.700602054595947\n",
      "h: 100 | epoch: 90, train loss: 8.549604415893555, test loss: 5.700446605682373\n",
      "h: 100 | epoch: 91, train loss: 8.549600601196289, test loss: 5.7003045082092285\n",
      "h: 100 | epoch: 92, train loss: 8.549595832824707, test loss: 5.700173377990723\n",
      "h: 100 | epoch: 93, train loss: 8.549592971801758, test loss: 5.70005464553833\n",
      "h: 100 | epoch: 94, train loss: 8.549590110778809, test loss: 5.699944019317627\n",
      "h: 100 | epoch: 95, train loss: 8.549588203430176, test loss: 5.699844837188721\n",
      "h: 100 | epoch: 96, train loss: 8.549585342407227, test loss: 5.6997528076171875\n",
      "h: 100 | epoch: 97, train loss: 8.54958438873291, test loss: 5.699667930603027\n",
      "h: 100 | epoch: 98, train loss: 8.549582481384277, test loss: 5.699591159820557\n",
      "h: 100 | epoch: 99, train loss: 8.549581527709961, test loss: 5.699521064758301\n",
      "h: 101 | epoch: 0, train loss: 47.07051086425781, test loss: 37.944419860839844\n",
      "h: 101 | epoch: 1, train loss: 42.43500518798828, test loss: 34.308956146240234\n",
      "h: 101 | epoch: 2, train loss: 38.37516784667969, test loss: 31.108627319335938\n",
      "h: 101 | epoch: 3, train loss: 34.815345764160156, test loss: 28.287609100341797\n",
      "h: 101 | epoch: 4, train loss: 31.690832138061523, test loss: 25.797985076904297\n",
      "h: 101 | epoch: 5, train loss: 28.946069717407227, test loss: 23.598453521728516\n",
      "h: 101 | epoch: 6, train loss: 26.53314208984375, test loss: 21.653295516967773\n",
      "h: 101 | epoch: 7, train loss: 24.410625457763672, test loss: 19.931529998779297\n",
      "h: 101 | epoch: 8, train loss: 22.542566299438477, test loss: 18.406200408935547\n",
      "h: 101 | epoch: 9, train loss: 20.897701263427734, test loss: 17.053815841674805\n",
      "h: 101 | epoch: 10, train loss: 19.44878387451172, test loss: 15.853846549987793\n",
      "h: 101 | epoch: 11, train loss: 18.17202377319336, test loss: 14.788330078125\n",
      "h: 101 | epoch: 12, train loss: 17.04661750793457, test loss: 13.841520309448242\n",
      "h: 101 | epoch: 13, train loss: 16.05434799194336, test loss: 12.999597549438477\n",
      "h: 101 | epoch: 14, train loss: 15.179243087768555, test loss: 12.250419616699219\n",
      "h: 101 | epoch: 15, train loss: 14.4072904586792, test loss: 11.58330249786377\n",
      "h: 101 | epoch: 16, train loss: 13.726188659667969, test loss: 10.988842010498047\n",
      "h: 101 | epoch: 17, train loss: 13.125123977661133, test loss: 10.458749771118164\n",
      "h: 101 | epoch: 18, train loss: 12.594586372375488, test loss: 9.985723495483398\n",
      "h: 101 | epoch: 19, train loss: 12.126214981079102, test loss: 9.563302993774414\n",
      "h: 101 | epoch: 20, train loss: 11.712651252746582, test loss: 9.18580436706543\n",
      "h: 101 | epoch: 21, train loss: 11.347414016723633, test loss: 8.848194122314453\n",
      "h: 101 | epoch: 22, train loss: 11.024799346923828, test loss: 8.546027183532715\n",
      "h: 101 | epoch: 23, train loss: 10.739787101745605, test loss: 8.275371551513672\n",
      "h: 101 | epoch: 24, train loss: 10.487945556640625, test loss: 8.032746315002441\n",
      "h: 101 | epoch: 25, train loss: 10.265375137329102, test loss: 7.815071105957031\n",
      "h: 101 | epoch: 26, train loss: 10.06864070892334, test loss: 7.619620323181152\n",
      "h: 101 | epoch: 27, train loss: 9.894710540771484, test loss: 7.44397497177124\n",
      "h: 101 | epoch: 28, train loss: 9.740911483764648, test loss: 7.285991668701172\n",
      "h: 101 | epoch: 29, train loss: 9.604890823364258, test loss: 7.143772125244141\n",
      "h: 101 | epoch: 30, train loss: 9.484572410583496, test loss: 7.015627384185791\n",
      "h: 101 | epoch: 31, train loss: 9.378121376037598, test loss: 6.900059700012207\n",
      "h: 101 | epoch: 32, train loss: 9.28392219543457, test loss: 6.795741081237793\n",
      "h: 101 | epoch: 33, train loss: 9.200549125671387, test loss: 6.701488494873047\n",
      "h: 101 | epoch: 34, train loss: 9.126745223999023, test loss: 6.61624813079834\n",
      "h: 101 | epoch: 35, train loss: 9.061397552490234, test loss: 6.539087772369385\n",
      "h: 101 | epoch: 36, train loss: 9.00352668762207, test loss: 6.469174385070801\n",
      "h: 101 | epoch: 37, train loss: 8.9522705078125, test loss: 6.405764579772949\n",
      "h: 101 | epoch: 38, train loss: 8.9068603515625, test loss: 6.348198890686035\n",
      "h: 101 | epoch: 39, train loss: 8.86662483215332, test loss: 6.295886516571045\n",
      "h: 101 | epoch: 40, train loss: 8.830965995788574, test loss: 6.2483015060424805\n",
      "h: 101 | epoch: 41, train loss: 8.799358367919922, test loss: 6.2049760818481445\n",
      "h: 101 | epoch: 42, train loss: 8.771336555480957, test loss: 6.165487289428711\n",
      "h: 101 | epoch: 43, train loss: 8.746488571166992, test loss: 6.129462242126465\n",
      "h: 101 | epoch: 44, train loss: 8.724451065063477, test loss: 6.096564292907715\n",
      "h: 101 | epoch: 45, train loss: 8.704903602600098, test loss: 6.066492557525635\n",
      "h: 101 | epoch: 46, train loss: 8.68756103515625, test loss: 6.038976192474365\n",
      "h: 101 | epoch: 47, train loss: 8.672171592712402, test loss: 6.013775825500488\n",
      "h: 101 | epoch: 48, train loss: 8.658514022827148, test loss: 5.990673065185547\n",
      "h: 101 | epoch: 49, train loss: 8.646390914916992, test loss: 5.969473361968994\n",
      "h: 101 | epoch: 50, train loss: 8.635629653930664, test loss: 5.950003623962402\n",
      "h: 101 | epoch: 51, train loss: 8.62607192993164, test loss: 5.932101249694824\n",
      "h: 101 | epoch: 52, train loss: 8.617585182189941, test loss: 5.915629863739014\n",
      "h: 101 | epoch: 53, train loss: 8.610048294067383, test loss: 5.9004597663879395\n",
      "h: 101 | epoch: 54, train loss: 8.603352546691895, test loss: 5.886475563049316\n",
      "h: 101 | epoch: 55, train loss: 8.597402572631836, test loss: 5.873573303222656\n",
      "h: 101 | epoch: 56, train loss: 8.59211540222168, test loss: 5.861659049987793\n",
      "h: 101 | epoch: 57, train loss: 8.587418556213379, test loss: 5.850647926330566\n",
      "h: 101 | epoch: 58, train loss: 8.583242416381836, test loss: 5.840462684631348\n",
      "h: 101 | epoch: 59, train loss: 8.579530715942383, test loss: 5.8310346603393555\n",
      "h: 101 | epoch: 60, train loss: 8.576231002807617, test loss: 5.822300434112549\n",
      "h: 101 | epoch: 61, train loss: 8.573295593261719, test loss: 5.814201831817627\n",
      "h: 101 | epoch: 62, train loss: 8.570687294006348, test loss: 5.806687831878662\n",
      "h: 101 | epoch: 63, train loss: 8.568366050720215, test loss: 5.799712181091309\n",
      "h: 101 | epoch: 64, train loss: 8.566301345825195, test loss: 5.793229579925537\n",
      "h: 101 | epoch: 65, train loss: 8.56446647644043, test loss: 5.787203311920166\n",
      "h: 101 | epoch: 66, train loss: 8.562832832336426, test loss: 5.7815961837768555\n",
      "h: 101 | epoch: 67, train loss: 8.561379432678223, test loss: 5.7763752937316895\n",
      "h: 101 | epoch: 68, train loss: 8.56008529663086, test loss: 5.771512031555176\n",
      "h: 101 | epoch: 69, train loss: 8.558934211730957, test loss: 5.7669782638549805\n",
      "h: 101 | epoch: 70, train loss: 8.55790901184082, test loss: 5.762750148773193\n",
      "h: 101 | epoch: 71, train loss: 8.556997299194336, test loss: 5.758803844451904\n",
      "h: 101 | epoch: 72, train loss: 8.55618667602539, test loss: 5.755120277404785\n",
      "h: 101 | epoch: 73, train loss: 8.555463790893555, test loss: 5.751678943634033\n",
      "h: 101 | epoch: 74, train loss: 8.554821014404297, test loss: 5.748461723327637\n",
      "h: 101 | epoch: 75, train loss: 8.554247856140137, test loss: 5.745453834533691\n",
      "h: 101 | epoch: 76, train loss: 8.55373764038086, test loss: 5.742639541625977\n",
      "h: 101 | epoch: 77, train loss: 8.553282737731934, test loss: 5.740004539489746\n",
      "h: 101 | epoch: 78, train loss: 8.552879333496094, test loss: 5.7375383377075195\n",
      "h: 101 | epoch: 79, train loss: 8.552518844604492, test loss: 5.735228061676025\n",
      "h: 101 | epoch: 80, train loss: 8.55219841003418, test loss: 5.733062744140625\n",
      "h: 101 | epoch: 81, train loss: 8.551912307739258, test loss: 5.7310333251953125\n",
      "h: 101 | epoch: 82, train loss: 8.551656723022461, test loss: 5.729128837585449\n",
      "h: 101 | epoch: 83, train loss: 8.551430702209473, test loss: 5.727344989776611\n",
      "h: 101 | epoch: 84, train loss: 8.551228523254395, test loss: 5.725668430328369\n",
      "h: 101 | epoch: 85, train loss: 8.551048278808594, test loss: 5.724094867706299\n",
      "h: 101 | epoch: 86, train loss: 8.550887107849121, test loss: 5.7226176261901855\n",
      "h: 101 | epoch: 87, train loss: 8.55074405670166, test loss: 5.721230506896973\n",
      "h: 101 | epoch: 88, train loss: 8.550617218017578, test loss: 5.719926834106445\n",
      "h: 101 | epoch: 89, train loss: 8.550503730773926, test loss: 5.718702793121338\n",
      "h: 101 | epoch: 90, train loss: 8.55040168762207, test loss: 5.717550277709961\n",
      "h: 101 | epoch: 91, train loss: 8.550312995910645, test loss: 5.716466903686523\n",
      "h: 101 | epoch: 92, train loss: 8.550232887268066, test loss: 5.71544885635376\n",
      "h: 101 | epoch: 93, train loss: 8.550161361694336, test loss: 5.714491844177246\n",
      "h: 101 | epoch: 94, train loss: 8.550097465515137, test loss: 5.713590145111084\n",
      "h: 101 | epoch: 95, train loss: 8.550040245056152, test loss: 5.712741851806641\n",
      "h: 101 | epoch: 96, train loss: 8.549989700317383, test loss: 5.711943626403809\n",
      "h: 101 | epoch: 97, train loss: 8.549944877624512, test loss: 5.7111921310424805\n",
      "h: 101 | epoch: 98, train loss: 8.549904823303223, test loss: 5.710485458374023\n",
      "h: 101 | epoch: 99, train loss: 8.549867630004883, test loss: 5.7098188400268555\n",
      "h: 102 | epoch: 0, train loss: 41.417171478271484, test loss: 34.392677307128906\n",
      "h: 102 | epoch: 1, train loss: 37.20704650878906, test loss: 30.902660369873047\n",
      "h: 102 | epoch: 2, train loss: 33.53422164916992, test loss: 27.844655990600586\n",
      "h: 102 | epoch: 3, train loss: 30.32908058166504, test loss: 25.163959503173828\n",
      "h: 102 | epoch: 4, train loss: 27.53154945373535, test loss: 22.81315803527832\n",
      "h: 102 | epoch: 5, train loss: 25.089635848999023, test loss: 20.751068115234375\n",
      "h: 102 | epoch: 6, train loss: 22.958209991455078, test loss: 18.941816329956055\n",
      "h: 102 | epoch: 7, train loss: 21.097999572753906, test loss: 17.354122161865234\n",
      "h: 102 | epoch: 8, train loss: 19.47479248046875, test loss: 15.960627555847168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 102 | epoch: 9, train loss: 18.058712005615234, test loss: 14.737406730651855\n",
      "h: 102 | epoch: 10, train loss: 16.823659896850586, test loss: 13.663497924804688\n",
      "h: 102 | epoch: 11, train loss: 15.746813774108887, test loss: 12.720548629760742\n",
      "h: 102 | epoch: 12, train loss: 14.808209419250488, test loss: 11.89245319366455\n",
      "h: 102 | epoch: 13, train loss: 13.990373611450195, test loss: 11.165094375610352\n",
      "h: 102 | epoch: 14, train loss: 13.278010368347168, test loss: 10.526086807250977\n",
      "h: 102 | epoch: 15, train loss: 12.657734870910645, test loss: 9.964569091796875\n",
      "h: 102 | epoch: 16, train loss: 12.117830276489258, test loss: 9.471006393432617\n",
      "h: 102 | epoch: 17, train loss: 11.648041725158691, test loss: 9.037041664123535\n",
      "h: 102 | epoch: 18, train loss: 11.239404678344727, test loss: 8.655345916748047\n",
      "h: 102 | epoch: 19, train loss: 10.884077072143555, test loss: 8.319489479064941\n",
      "h: 102 | epoch: 20, train loss: 10.575203895568848, test loss: 8.023837089538574\n",
      "h: 102 | epoch: 21, train loss: 10.306795120239258, test loss: 7.763449192047119\n",
      "h: 102 | epoch: 22, train loss: 10.073620796203613, test loss: 7.533995628356934\n",
      "h: 102 | epoch: 23, train loss: 9.871114730834961, test loss: 7.331681728363037\n",
      "h: 102 | epoch: 24, train loss: 9.695291519165039, test loss: 7.153186798095703\n",
      "h: 102 | epoch: 25, train loss: 9.54267406463623, test loss: 6.9955949783325195\n",
      "h: 102 | epoch: 26, train loss: 9.410238265991211, test loss: 6.8563551902771\n",
      "h: 102 | epoch: 27, train loss: 9.295336723327637, test loss: 6.733239650726318\n",
      "h: 102 | epoch: 28, train loss: 9.19567584991455, test loss: 6.624284267425537\n",
      "h: 102 | epoch: 29, train loss: 9.109248161315918, test loss: 6.527779579162598\n",
      "h: 102 | epoch: 30, train loss: 9.034316062927246, test loss: 6.442221164703369\n",
      "h: 102 | epoch: 31, train loss: 8.969362258911133, test loss: 6.366296768188477\n",
      "h: 102 | epoch: 32, train loss: 8.913065910339355, test loss: 6.298851013183594\n",
      "h: 102 | epoch: 33, train loss: 8.864282608032227, test loss: 6.238875865936279\n",
      "h: 102 | epoch: 34, train loss: 8.822016716003418, test loss: 6.185486793518066\n",
      "h: 102 | epoch: 35, train loss: 8.785405158996582, test loss: 6.137909889221191\n",
      "h: 102 | epoch: 36, train loss: 8.753692626953125, test loss: 6.095457553863525\n",
      "h: 102 | epoch: 37, train loss: 8.726228713989258, test loss: 6.057539463043213\n",
      "h: 102 | epoch: 38, train loss: 8.702448844909668, test loss: 6.023628234863281\n",
      "h: 102 | epoch: 39, train loss: 8.681857109069824, test loss: 5.9932661056518555\n",
      "h: 102 | epoch: 40, train loss: 8.664031982421875, test loss: 5.9660468101501465\n",
      "h: 102 | epoch: 41, train loss: 8.648602485656738, test loss: 5.941616058349609\n",
      "h: 102 | epoch: 42, train loss: 8.635247230529785, test loss: 5.919659614562988\n",
      "h: 102 | epoch: 43, train loss: 8.623688697814941, test loss: 5.899903774261475\n",
      "h: 102 | epoch: 44, train loss: 8.613686561584473, test loss: 5.882105350494385\n",
      "h: 102 | epoch: 45, train loss: 8.60503101348877, test loss: 5.866049766540527\n",
      "h: 102 | epoch: 46, train loss: 8.597540855407715, test loss: 5.851551055908203\n",
      "h: 102 | epoch: 47, train loss: 8.591060638427734, test loss: 5.838439464569092\n",
      "h: 102 | epoch: 48, train loss: 8.585454940795898, test loss: 5.826570987701416\n",
      "h: 102 | epoch: 49, train loss: 8.580606460571289, test loss: 5.815812110900879\n",
      "h: 102 | epoch: 50, train loss: 8.576410293579102, test loss: 5.8060503005981445\n",
      "h: 102 | epoch: 51, train loss: 8.572781562805176, test loss: 5.797182083129883\n",
      "h: 102 | epoch: 52, train loss: 8.569642066955566, test loss: 5.7891154289245605\n",
      "h: 102 | epoch: 53, train loss: 8.566926956176758, test loss: 5.781771659851074\n",
      "h: 102 | epoch: 54, train loss: 8.564579010009766, test loss: 5.775079727172852\n",
      "h: 102 | epoch: 55, train loss: 8.56254768371582, test loss: 5.768972873687744\n",
      "h: 102 | epoch: 56, train loss: 8.560792922973633, test loss: 5.763397216796875\n",
      "h: 102 | epoch: 57, train loss: 8.559274673461914, test loss: 5.758299827575684\n",
      "h: 102 | epoch: 58, train loss: 8.557961463928223, test loss: 5.753636360168457\n",
      "h: 102 | epoch: 59, train loss: 8.556825637817383, test loss: 5.74936580657959\n",
      "h: 102 | epoch: 60, train loss: 8.555842399597168, test loss: 5.745450019836426\n",
      "h: 102 | epoch: 61, train loss: 8.554993629455566, test loss: 5.741860389709473\n",
      "h: 102 | epoch: 62, train loss: 8.55426025390625, test loss: 5.738564491271973\n",
      "h: 102 | epoch: 63, train loss: 8.553625106811523, test loss: 5.735535621643066\n",
      "h: 102 | epoch: 64, train loss: 8.553075790405273, test loss: 5.732753276824951\n",
      "h: 102 | epoch: 65, train loss: 8.55260181427002, test loss: 5.730192184448242\n",
      "h: 102 | epoch: 66, train loss: 8.552190780639648, test loss: 5.727835178375244\n",
      "h: 102 | epoch: 67, train loss: 8.551836013793945, test loss: 5.725665092468262\n",
      "h: 102 | epoch: 68, train loss: 8.551528930664062, test loss: 5.723662853240967\n",
      "h: 102 | epoch: 69, train loss: 8.551263809204102, test loss: 5.721817970275879\n",
      "h: 102 | epoch: 70, train loss: 8.551034927368164, test loss: 5.7201151847839355\n",
      "h: 102 | epoch: 71, train loss: 8.550836563110352, test loss: 5.718544006347656\n",
      "h: 102 | epoch: 72, train loss: 8.550665855407715, test loss: 5.7170939445495605\n",
      "h: 102 | epoch: 73, train loss: 8.550518035888672, test loss: 5.715754508972168\n",
      "h: 102 | epoch: 74, train loss: 8.550389289855957, test loss: 5.71451473236084\n",
      "h: 102 | epoch: 75, train loss: 8.550278663635254, test loss: 5.7133684158325195\n",
      "h: 102 | epoch: 76, train loss: 8.550182342529297, test loss: 5.71230936050415\n",
      "h: 102 | epoch: 77, train loss: 8.550100326538086, test loss: 5.711329460144043\n",
      "h: 102 | epoch: 78, train loss: 8.550028800964355, test loss: 5.710421562194824\n",
      "h: 102 | epoch: 79, train loss: 8.549966812133789, test loss: 5.709582328796387\n",
      "h: 102 | epoch: 80, train loss: 8.54991340637207, test loss: 5.708804130554199\n",
      "h: 102 | epoch: 81, train loss: 8.549866676330566, test loss: 5.708083152770996\n",
      "h: 102 | epoch: 82, train loss: 8.549827575683594, test loss: 5.707415580749512\n",
      "h: 102 | epoch: 83, train loss: 8.549792289733887, test loss: 5.706797122955322\n",
      "h: 102 | epoch: 84, train loss: 8.549763679504395, test loss: 5.706223487854004\n",
      "h: 102 | epoch: 85, train loss: 8.549737930297852, test loss: 5.705692291259766\n",
      "h: 102 | epoch: 86, train loss: 8.549715042114258, test loss: 5.705198764801025\n",
      "h: 102 | epoch: 87, train loss: 8.54969596862793, test loss: 5.704741477966309\n",
      "h: 102 | epoch: 88, train loss: 8.54967975616455, test loss: 5.704317092895508\n",
      "h: 102 | epoch: 89, train loss: 8.549664497375488, test loss: 5.703923225402832\n",
      "h: 102 | epoch: 90, train loss: 8.549652099609375, test loss: 5.703557968139648\n",
      "h: 102 | epoch: 91, train loss: 8.549641609191895, test loss: 5.703219413757324\n",
      "h: 102 | epoch: 92, train loss: 8.54963207244873, test loss: 5.70290470123291\n",
      "h: 102 | epoch: 93, train loss: 8.549623489379883, test loss: 5.702611923217773\n",
      "h: 102 | epoch: 94, train loss: 8.549616813659668, test loss: 5.702340126037598\n",
      "h: 102 | epoch: 95, train loss: 8.54961109161377, test loss: 5.702088832855225\n",
      "h: 102 | epoch: 96, train loss: 8.549606323242188, test loss: 5.701855182647705\n",
      "h: 102 | epoch: 97, train loss: 8.549601554870605, test loss: 5.701638221740723\n",
      "h: 102 | epoch: 98, train loss: 8.54959774017334, test loss: 5.701437473297119\n",
      "h: 102 | epoch: 99, train loss: 8.54959487915039, test loss: 5.701248645782471\n",
      "h: 103 | epoch: 0, train loss: 42.522926330566406, test loss: 34.425167083740234\n",
      "h: 103 | epoch: 1, train loss: 38.10114288330078, test loss: 30.864065170288086\n",
      "h: 103 | epoch: 2, train loss: 34.252708435058594, test loss: 27.7514591217041\n",
      "h: 103 | epoch: 3, train loss: 30.902149200439453, test loss: 25.02961540222168\n",
      "h: 103 | epoch: 4, train loss: 27.984516143798828, test loss: 22.648662567138672\n",
      "h: 103 | epoch: 5, train loss: 25.4437198638916, test loss: 20.565357208251953\n",
      "h: 103 | epoch: 6, train loss: 23.23118782043457, test loss: 18.742124557495117\n",
      "h: 103 | epoch: 7, train loss: 21.30474281311035, test loss: 17.146244049072266\n",
      "h: 103 | epoch: 8, train loss: 19.627721786499023, test loss: 15.749176025390625\n",
      "h: 103 | epoch: 9, train loss: 18.168188095092773, test loss: 14.526018142700195\n",
      "h: 103 | epoch: 10, train loss: 16.898296356201172, test loss: 13.454994201660156\n",
      "h: 103 | epoch: 11, train loss: 15.793756484985352, test loss: 12.517077445983887\n",
      "h: 103 | epoch: 12, train loss: 14.833355903625488, test loss: 11.695613861083984\n",
      "h: 103 | epoch: 13, train loss: 13.998579025268555, test loss: 10.976032257080078\n",
      "h: 103 | epoch: 14, train loss: 13.2732572555542, test loss: 10.3455810546875\n",
      "h: 103 | epoch: 15, train loss: 12.64326286315918, test loss: 9.793105125427246\n",
      "h: 103 | epoch: 16, train loss: 12.096267700195312, test loss: 9.308833122253418\n",
      "h: 103 | epoch: 17, train loss: 11.621506690979004, test loss: 8.884225845336914\n",
      "h: 103 | epoch: 18, train loss: 11.209583282470703, test loss: 8.511800765991211\n",
      "h: 103 | epoch: 19, train loss: 10.852304458618164, test loss: 8.185023307800293\n",
      "h: 103 | epoch: 20, train loss: 10.542526245117188, test loss: 7.898172855377197\n",
      "h: 103 | epoch: 21, train loss: 10.274015426635742, test loss: 7.646247863769531\n",
      "h: 103 | epoch: 22, train loss: 10.041352272033691, test loss: 7.424880027770996\n",
      "h: 103 | epoch: 23, train loss: 9.839807510375977, test loss: 7.230246067047119\n",
      "h: 103 | epoch: 24, train loss: 9.665267944335938, test loss: 7.0590105056762695\n",
      "h: 103 | epoch: 25, train loss: 9.514156341552734, test loss: 6.908255100250244\n",
      "h: 103 | epoch: 26, train loss: 9.383363723754883, test loss: 6.775429725646973\n",
      "h: 103 | epoch: 27, train loss: 9.270185470581055, test loss: 6.6583123207092285\n",
      "h: 103 | epoch: 28, train loss: 9.172269821166992, test loss: 6.554954528808594\n",
      "h: 103 | epoch: 29, train loss: 9.087579727172852, test loss: 6.463658809661865\n",
      "h: 103 | epoch: 30, train loss: 9.014341354370117, test loss: 6.382943630218506\n",
      "h: 103 | epoch: 31, train loss: 8.951021194458008, test loss: 6.311509132385254\n",
      "h: 103 | epoch: 32, train loss: 8.896284103393555, test loss: 6.248225212097168\n",
      "h: 103 | epoch: 33, train loss: 8.848976135253906, test loss: 6.192100524902344\n",
      "h: 103 | epoch: 34, train loss: 8.80809497833252, test loss: 6.142270565032959\n",
      "h: 103 | epoch: 35, train loss: 8.772772789001465, test loss: 6.0979766845703125\n",
      "h: 103 | epoch: 36, train loss: 8.74225902557373, test loss: 6.058559417724609\n",
      "h: 103 | epoch: 37, train loss: 8.715902328491211, test loss: 6.023438453674316\n",
      "h: 103 | epoch: 38, train loss: 8.693138122558594, test loss: 5.992108345031738\n",
      "h: 103 | epoch: 39, train loss: 8.673479080200195, test loss: 5.9641242027282715\n",
      "h: 103 | epoch: 40, train loss: 8.656506538391113, test loss: 5.939096450805664\n",
      "h: 103 | epoch: 41, train loss: 8.641851425170898, test loss: 5.916684627532959\n",
      "h: 103 | epoch: 42, train loss: 8.629199028015137, test loss: 5.896590232849121\n",
      "h: 103 | epoch: 43, train loss: 8.618278503417969, test loss: 5.878549575805664\n",
      "h: 103 | epoch: 44, train loss: 8.608851432800293, test loss: 5.862330913543701\n",
      "h: 103 | epoch: 45, train loss: 8.600715637207031, test loss: 5.847733497619629\n",
      "h: 103 | epoch: 46, train loss: 8.593694686889648, test loss: 5.834577560424805\n",
      "h: 103 | epoch: 47, train loss: 8.58763599395752, test loss: 5.822705268859863\n",
      "h: 103 | epoch: 48, train loss: 8.582406997680664, test loss: 5.8119797706604\n",
      "h: 103 | epoch: 49, train loss: 8.577897071838379, test loss: 5.802276134490967\n",
      "h: 103 | epoch: 50, train loss: 8.574003219604492, test loss: 5.793488502502441\n",
      "h: 103 | epoch: 51, train loss: 8.570646286010742, test loss: 5.785519123077393\n",
      "h: 103 | epoch: 52, train loss: 8.5677490234375, test loss: 5.778284072875977\n",
      "h: 103 | epoch: 53, train loss: 8.565248489379883, test loss: 5.771708965301514\n",
      "h: 103 | epoch: 54, train loss: 8.563093185424805, test loss: 5.7657270431518555\n",
      "h: 103 | epoch: 55, train loss: 8.561233520507812, test loss: 5.76027774810791\n",
      "h: 103 | epoch: 56, train loss: 8.559629440307617, test loss: 5.75531005859375\n",
      "h: 103 | epoch: 57, train loss: 8.558245658874512, test loss: 5.750775337219238\n",
      "h: 103 | epoch: 58, train loss: 8.557051658630371, test loss: 5.746634006500244\n",
      "h: 103 | epoch: 59, train loss: 8.556022644042969, test loss: 5.742846965789795\n",
      "h: 103 | epoch: 60, train loss: 8.555135726928711, test loss: 5.739380836486816\n",
      "h: 103 | epoch: 61, train loss: 8.554369926452637, test loss: 5.736207008361816\n",
      "h: 103 | epoch: 62, train loss: 8.553709030151367, test loss: 5.733295917510986\n",
      "h: 103 | epoch: 63, train loss: 8.553140640258789, test loss: 5.730627536773682\n",
      "h: 103 | epoch: 64, train loss: 8.552648544311523, test loss: 5.728177070617676\n",
      "h: 103 | epoch: 65, train loss: 8.552225112915039, test loss: 5.725924968719482\n",
      "h: 103 | epoch: 66, train loss: 8.551859855651855, test loss: 5.723855972290039\n",
      "h: 103 | epoch: 67, train loss: 8.551545143127441, test loss: 5.721951484680176\n",
      "h: 103 | epoch: 68, train loss: 8.551273345947266, test loss: 5.7201995849609375\n",
      "h: 103 | epoch: 69, train loss: 8.55103874206543, test loss: 5.718586444854736\n",
      "h: 103 | epoch: 70, train loss: 8.550836563110352, test loss: 5.717099189758301\n",
      "h: 103 | epoch: 71, train loss: 8.550662994384766, test loss: 5.715729236602783\n",
      "h: 103 | epoch: 72, train loss: 8.550512313842773, test loss: 5.714465141296387\n",
      "h: 103 | epoch: 73, train loss: 8.550382614135742, test loss: 5.713298797607422\n",
      "h: 103 | epoch: 74, train loss: 8.550271987915039, test loss: 5.712222099304199\n",
      "h: 103 | epoch: 75, train loss: 8.550175666809082, test loss: 5.711227893829346\n",
      "h: 103 | epoch: 76, train loss: 8.550091743469238, test loss: 5.7103095054626465\n",
      "h: 103 | epoch: 77, train loss: 8.550020217895508, test loss: 5.709460735321045\n",
      "h: 103 | epoch: 78, train loss: 8.549959182739258, test loss: 5.708676338195801\n",
      "h: 103 | epoch: 79, train loss: 8.549905776977539, test loss: 5.707950592041016\n",
      "h: 103 | epoch: 80, train loss: 8.549860000610352, test loss: 5.707280158996582\n",
      "h: 103 | epoch: 81, train loss: 8.549820899963379, test loss: 5.706658840179443\n",
      "h: 103 | epoch: 82, train loss: 8.549785614013672, test loss: 5.706085205078125\n",
      "h: 103 | epoch: 83, train loss: 8.54975700378418, test loss: 5.70555305480957\n",
      "h: 103 | epoch: 84, train loss: 8.549731254577637, test loss: 5.705061435699463\n",
      "h: 103 | epoch: 85, train loss: 8.549710273742676, test loss: 5.704604625701904\n",
      "h: 103 | epoch: 86, train loss: 8.549691200256348, test loss: 5.7041826248168945\n",
      "h: 103 | epoch: 87, train loss: 8.549674034118652, test loss: 5.703791618347168\n",
      "h: 103 | epoch: 88, train loss: 8.549660682678223, test loss: 5.703429222106934\n",
      "h: 103 | epoch: 89, train loss: 8.549649238586426, test loss: 5.7030930519104\n",
      "h: 103 | epoch: 90, train loss: 8.549638748168945, test loss: 5.70278263092041\n",
      "h: 103 | epoch: 91, train loss: 8.549628257751465, test loss: 5.702494144439697\n",
      "h: 103 | epoch: 92, train loss: 8.549620628356934, test loss: 5.702226161956787\n",
      "h: 103 | epoch: 93, train loss: 8.549613952636719, test loss: 5.70197868347168\n",
      "h: 103 | epoch: 94, train loss: 8.549609184265137, test loss: 5.701747417449951\n",
      "h: 103 | epoch: 95, train loss: 8.549603462219238, test loss: 5.701535701751709\n",
      "h: 103 | epoch: 96, train loss: 8.549599647521973, test loss: 5.701337814331055\n",
      "h: 103 | epoch: 97, train loss: 8.549596786499023, test loss: 5.7011542320251465\n",
      "h: 103 | epoch: 98, train loss: 8.549592971801758, test loss: 5.700984001159668\n",
      "h: 103 | epoch: 99, train loss: 8.549589157104492, test loss: 5.700826168060303\n",
      "h: 104 | epoch: 0, train loss: 39.74814987182617, test loss: 31.923629760742188\n",
      "h: 104 | epoch: 1, train loss: 35.44110870361328, test loss: 28.467090606689453\n",
      "h: 104 | epoch: 2, train loss: 31.72677993774414, test loss: 25.47365379333496\n",
      "h: 104 | epoch: 3, train loss: 28.522695541381836, test loss: 22.880107879638672\n",
      "h: 104 | epoch: 4, train loss: 25.758371353149414, test loss: 20.63222885131836\n",
      "h: 104 | epoch: 5, train loss: 23.37339210510254, test loss: 18.683391571044922\n",
      "h: 104 | epoch: 6, train loss: 21.315841674804688, test loss: 16.993398666381836\n",
      "h: 104 | epoch: 7, train loss: 19.541019439697266, test loss: 15.527572631835938\n",
      "h: 104 | epoch: 8, train loss: 18.0103816986084, test loss: 14.255929946899414\n",
      "h: 104 | epoch: 9, train loss: 16.690649032592773, test loss: 13.152532577514648\n",
      "h: 104 | epoch: 10, train loss: 15.553072929382324, test loss: 12.19493579864502\n",
      "h: 104 | epoch: 11, train loss: 14.572799682617188, test loss: 11.363691329956055\n",
      "h: 104 | epoch: 12, train loss: 13.728340148925781, test loss: 10.641953468322754\n",
      "h: 104 | epoch: 13, train loss: 13.001108169555664, test loss: 10.015128135681152\n",
      "h: 104 | epoch: 14, train loss: 12.375027656555176, test loss: 9.470564842224121\n",
      "h: 104 | epoch: 15, train loss: 11.83620548248291, test loss: 8.99730110168457\n",
      "h: 104 | epoch: 16, train loss: 11.3726224899292, test loss: 8.585844039916992\n",
      "h: 104 | epoch: 17, train loss: 10.973896026611328, test loss: 8.227956771850586\n",
      "h: 104 | epoch: 18, train loss: 10.631051063537598, test loss: 7.916520118713379\n",
      "h: 104 | epoch: 19, train loss: 10.33634090423584, test loss: 7.645352840423584\n",
      "h: 104 | epoch: 20, train loss: 10.083078384399414, test loss: 7.4091033935546875\n",
      "h: 104 | epoch: 21, train loss: 9.865488052368164, test loss: 7.203150272369385\n",
      "h: 104 | epoch: 22, train loss: 9.67859172821045, test loss: 7.023473262786865\n",
      "h: 104 | epoch: 23, train loss: 9.518098831176758, test loss: 6.86660623550415\n",
      "h: 104 | epoch: 24, train loss: 9.380306243896484, test loss: 6.7295403480529785\n",
      "h: 104 | epoch: 25, train loss: 9.262028694152832, test loss: 6.609671592712402\n",
      "h: 104 | epoch: 26, train loss: 9.160520553588867, test loss: 6.5047478675842285\n",
      "h: 104 | epoch: 27, train loss: 9.073420524597168, test loss: 6.412814140319824\n",
      "h: 104 | epoch: 28, train loss: 8.998697280883789, test loss: 6.332183361053467\n",
      "h: 104 | epoch: 29, train loss: 8.93459701538086, test loss: 6.26138973236084\n",
      "h: 104 | epoch: 30, train loss: 8.879621505737305, test loss: 6.19916296005249\n",
      "h: 104 | epoch: 31, train loss: 8.832475662231445, test loss: 6.144405841827393\n",
      "h: 104 | epoch: 32, train loss: 8.792051315307617, test loss: 6.096162796020508\n",
      "h: 104 | epoch: 33, train loss: 8.757390975952148, test loss: 6.0536088943481445\n",
      "h: 104 | epoch: 34, train loss: 8.727677345275879, test loss: 6.016023635864258\n",
      "h: 104 | epoch: 35, train loss: 8.702205657958984, test loss: 5.982788562774658\n",
      "h: 104 | epoch: 36, train loss: 8.68037223815918, test loss: 5.953359127044678\n",
      "h: 104 | epoch: 37, train loss: 8.661660194396973, test loss: 5.9272661209106445\n",
      "h: 104 | epoch: 38, train loss: 8.645623207092285, test loss: 5.904101848602295\n",
      "h: 104 | epoch: 39, train loss: 8.631877899169922, test loss: 5.883508682250977\n",
      "h: 104 | epoch: 40, train loss: 8.620099067687988, test loss: 5.865177154541016\n",
      "h: 104 | epoch: 41, train loss: 8.610006332397461, test loss: 5.848836898803711\n",
      "h: 104 | epoch: 42, train loss: 8.601357460021973, test loss: 5.83425235748291\n",
      "h: 104 | epoch: 43, train loss: 8.59394645690918, test loss: 5.821218967437744\n",
      "h: 104 | epoch: 44, train loss: 8.58759593963623, test loss: 5.809554100036621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 104 | epoch: 45, train loss: 8.582155227661133, test loss: 5.799101829528809\n",
      "h: 104 | epoch: 46, train loss: 8.577492713928223, test loss: 5.789723873138428\n",
      "h: 104 | epoch: 47, train loss: 8.573497772216797, test loss: 5.781299114227295\n",
      "h: 104 | epoch: 48, train loss: 8.570075035095215, test loss: 5.7737226486206055\n",
      "h: 104 | epoch: 49, train loss: 8.567142486572266, test loss: 5.766899108886719\n",
      "h: 104 | epoch: 50, train loss: 8.564630508422852, test loss: 5.760746955871582\n",
      "h: 104 | epoch: 51, train loss: 8.562477111816406, test loss: 5.755194664001465\n",
      "h: 104 | epoch: 52, train loss: 8.56063175201416, test loss: 5.75017786026001\n",
      "h: 104 | epoch: 53, train loss: 8.559050559997559, test loss: 5.745639324188232\n",
      "h: 104 | epoch: 54, train loss: 8.557695388793945, test loss: 5.741529941558838\n",
      "h: 104 | epoch: 55, train loss: 8.556535720825195, test loss: 5.737806797027588\n",
      "h: 104 | epoch: 56, train loss: 8.555540084838867, test loss: 5.7344279289245605\n",
      "h: 104 | epoch: 57, train loss: 8.5546875, test loss: 5.73136043548584\n",
      "h: 104 | epoch: 58, train loss: 8.553956985473633, test loss: 5.728572368621826\n",
      "h: 104 | epoch: 59, train loss: 8.55333137512207, test loss: 5.726035118103027\n",
      "h: 104 | epoch: 60, train loss: 8.55279541015625, test loss: 5.723726749420166\n",
      "h: 104 | epoch: 61, train loss: 8.552335739135742, test loss: 5.721624374389648\n",
      "h: 104 | epoch: 62, train loss: 8.551941871643066, test loss: 5.719707489013672\n",
      "h: 104 | epoch: 63, train loss: 8.551603317260742, test loss: 5.717958450317383\n",
      "h: 104 | epoch: 64, train loss: 8.551313400268555, test loss: 5.716362953186035\n",
      "h: 104 | epoch: 65, train loss: 8.551065444946289, test loss: 5.714905261993408\n",
      "h: 104 | epoch: 66, train loss: 8.550853729248047, test loss: 5.713572978973389\n",
      "h: 104 | epoch: 67, train loss: 8.550671577453613, test loss: 5.712353706359863\n",
      "h: 104 | epoch: 68, train loss: 8.550515174865723, test loss: 5.711239337921143\n",
      "h: 104 | epoch: 69, train loss: 8.550379753112793, test loss: 5.710219383239746\n",
      "h: 104 | epoch: 70, train loss: 8.550265312194824, test loss: 5.709285259246826\n",
      "h: 104 | epoch: 71, train loss: 8.550167083740234, test loss: 5.708430290222168\n",
      "h: 104 | epoch: 72, train loss: 8.550081253051758, test loss: 5.707645416259766\n",
      "h: 104 | epoch: 73, train loss: 8.550009727478027, test loss: 5.706926345825195\n",
      "h: 104 | epoch: 74, train loss: 8.549947738647461, test loss: 5.706267356872559\n",
      "h: 104 | epoch: 75, train loss: 8.549894332885742, test loss: 5.705663204193115\n",
      "h: 104 | epoch: 76, train loss: 8.549848556518555, test loss: 5.70510721206665\n",
      "h: 104 | epoch: 77, train loss: 8.549809455871582, test loss: 5.704599857330322\n",
      "h: 104 | epoch: 78, train loss: 8.549776077270508, test loss: 5.704132080078125\n",
      "h: 104 | epoch: 79, train loss: 8.5497465133667, test loss: 5.703701972961426\n",
      "h: 104 | epoch: 80, train loss: 8.549722671508789, test loss: 5.703307628631592\n",
      "h: 104 | epoch: 81, train loss: 8.549700736999512, test loss: 5.702945232391357\n",
      "h: 104 | epoch: 82, train loss: 8.549683570861816, test loss: 5.702611446380615\n",
      "h: 104 | epoch: 83, train loss: 8.549666404724121, test loss: 5.702305793762207\n",
      "h: 104 | epoch: 84, train loss: 8.549654006958008, test loss: 5.702025413513184\n",
      "h: 104 | epoch: 85, train loss: 8.549642562866211, test loss: 5.701766014099121\n",
      "h: 104 | epoch: 86, train loss: 8.54963207244873, test loss: 5.7015275955200195\n",
      "h: 104 | epoch: 87, train loss: 8.549623489379883, test loss: 5.701309680938721\n",
      "h: 104 | epoch: 88, train loss: 8.549616813659668, test loss: 5.701108455657959\n",
      "h: 104 | epoch: 89, train loss: 8.54961109161377, test loss: 5.700923442840576\n",
      "h: 104 | epoch: 90, train loss: 8.549605369567871, test loss: 5.700753688812256\n",
      "h: 104 | epoch: 91, train loss: 8.549600601196289, test loss: 5.700596809387207\n",
      "h: 104 | epoch: 92, train loss: 8.549596786499023, test loss: 5.700453758239746\n",
      "h: 104 | epoch: 93, train loss: 8.549592971801758, test loss: 5.700319290161133\n",
      "h: 104 | epoch: 94, train loss: 8.549590110778809, test loss: 5.700198173522949\n",
      "h: 104 | epoch: 95, train loss: 8.549588203430176, test loss: 5.70008659362793\n",
      "h: 104 | epoch: 96, train loss: 8.549585342407227, test loss: 5.699982643127441\n",
      "h: 104 | epoch: 97, train loss: 8.54958438873291, test loss: 5.699886798858643\n",
      "h: 104 | epoch: 98, train loss: 8.549581527709961, test loss: 5.699799537658691\n",
      "h: 104 | epoch: 99, train loss: 8.549581527709961, test loss: 5.6997199058532715\n",
      "h: 105 | epoch: 0, train loss: 44.955955505371094, test loss: 36.856544494628906\n",
      "h: 105 | epoch: 1, train loss: 39.968597412109375, test loss: 32.78117370605469\n",
      "h: 105 | epoch: 2, train loss: 35.66351318359375, test loss: 29.248416900634766\n",
      "h: 105 | epoch: 3, train loss: 31.945764541625977, test loss: 26.18430519104004\n",
      "h: 105 | epoch: 4, train loss: 28.73441505432129, test loss: 23.525516510009766\n",
      "h: 105 | epoch: 5, train loss: 25.96016502380371, test loss: 21.217639923095703\n",
      "h: 105 | epoch: 6, train loss: 23.56353187561035, test loss: 19.21379852294922\n",
      "h: 105 | epoch: 7, train loss: 21.493316650390625, test loss: 17.473539352416992\n",
      "h: 105 | epoch: 8, train loss: 19.705360412597656, test loss: 15.961893081665039\n",
      "h: 105 | epoch: 9, train loss: 18.161523818969727, test loss: 14.648571968078613\n",
      "h: 105 | epoch: 10, train loss: 16.828838348388672, test loss: 13.507349967956543\n",
      "h: 105 | epoch: 11, train loss: 15.6787691116333, test loss: 12.515478134155273\n",
      "h: 105 | epoch: 12, train loss: 14.686610221862793, test loss: 11.653226852416992\n",
      "h: 105 | epoch: 13, train loss: 13.830968856811523, test loss: 10.903480529785156\n",
      "h: 105 | epoch: 14, train loss: 13.093317031860352, test loss: 10.251378059387207\n",
      "h: 105 | epoch: 15, train loss: 12.457601547241211, test loss: 9.684025764465332\n",
      "h: 105 | epoch: 16, train loss: 11.909927368164062, test loss: 9.190231323242188\n",
      "h: 105 | epoch: 17, train loss: 11.438260078430176, test loss: 8.76029109954834\n",
      "h: 105 | epoch: 18, train loss: 11.032190322875977, test loss: 8.385770797729492\n",
      "h: 105 | epoch: 19, train loss: 10.682703018188477, test loss: 8.059370040893555\n",
      "h: 105 | epoch: 20, train loss: 10.382011413574219, test loss: 7.774741172790527\n",
      "h: 105 | epoch: 21, train loss: 10.12337589263916, test loss: 7.5263872146606445\n",
      "h: 105 | epoch: 22, train loss: 9.900979995727539, test loss: 7.309542179107666\n",
      "h: 105 | epoch: 23, train loss: 9.709798812866211, test loss: 7.120068550109863\n",
      "h: 105 | epoch: 24, train loss: 9.545492172241211, test loss: 6.954380035400391\n",
      "h: 105 | epoch: 25, train loss: 9.404316902160645, test loss: 6.809370517730713\n",
      "h: 105 | epoch: 26, train loss: 9.28304386138916, test loss: 6.682347774505615\n",
      "h: 105 | epoch: 27, train loss: 9.1788911819458, test loss: 6.5709710121154785\n",
      "h: 105 | epoch: 28, train loss: 9.089456558227539, test loss: 6.4732160568237305\n",
      "h: 105 | epoch: 29, train loss: 9.012678146362305, test loss: 6.387330055236816\n",
      "h: 105 | epoch: 30, train loss: 8.946775436401367, test loss: 6.3117852210998535\n",
      "h: 105 | epoch: 31, train loss: 8.890218734741211, test loss: 6.245262622833252\n",
      "h: 105 | epoch: 32, train loss: 8.84168529510498, test loss: 6.186612606048584\n",
      "h: 105 | epoch: 33, train loss: 8.800046920776367, test loss: 6.1348443031311035\n",
      "h: 105 | epoch: 34, train loss: 8.764327049255371, test loss: 6.089090347290039\n",
      "h: 105 | epoch: 35, train loss: 8.733689308166504, test loss: 6.048601150512695\n",
      "h: 105 | epoch: 36, train loss: 8.707412719726562, test loss: 6.012722492218018\n",
      "h: 105 | epoch: 37, train loss: 8.684877395629883, test loss: 5.980887413024902\n",
      "h: 105 | epoch: 38, train loss: 8.665555953979492, test loss: 5.952603340148926\n",
      "h: 105 | epoch: 39, train loss: 8.648988723754883, test loss: 5.9274396896362305\n",
      "h: 105 | epoch: 40, train loss: 8.634782791137695, test loss: 5.905019760131836\n",
      "h: 105 | epoch: 41, train loss: 8.62260627746582, test loss: 5.885018348693848\n",
      "h: 105 | epoch: 42, train loss: 8.612168312072754, test loss: 5.86715030670166\n",
      "h: 105 | epoch: 43, train loss: 8.603219985961914, test loss: 5.851166248321533\n",
      "h: 105 | epoch: 44, train loss: 8.595549583435059, test loss: 5.83684778213501\n",
      "h: 105 | epoch: 45, train loss: 8.588974952697754, test loss: 5.824005603790283\n",
      "h: 105 | epoch: 46, train loss: 8.583341598510742, test loss: 5.812470436096191\n",
      "h: 105 | epoch: 47, train loss: 8.578512191772461, test loss: 5.802095890045166\n",
      "h: 105 | epoch: 48, train loss: 8.574373245239258, test loss: 5.792755126953125\n",
      "h: 105 | epoch: 49, train loss: 8.570826530456543, test loss: 5.784333229064941\n",
      "h: 105 | epoch: 50, train loss: 8.567789077758789, test loss: 5.776731967926025\n",
      "h: 105 | epoch: 51, train loss: 8.565183639526367, test loss: 5.769862174987793\n",
      "h: 105 | epoch: 52, train loss: 8.562950134277344, test loss: 5.763646602630615\n",
      "h: 105 | epoch: 53, train loss: 8.56103801727295, test loss: 5.758017063140869\n",
      "h: 105 | epoch: 54, train loss: 8.559398651123047, test loss: 5.752913475036621\n",
      "h: 105 | epoch: 55, train loss: 8.557992935180664, test loss: 5.748281478881836\n",
      "h: 105 | epoch: 56, train loss: 8.556790351867676, test loss: 5.744072437286377\n",
      "h: 105 | epoch: 57, train loss: 8.555757522583008, test loss: 5.740245342254639\n",
      "h: 105 | epoch: 58, train loss: 8.5548734664917, test loss: 5.736761569976807\n",
      "h: 105 | epoch: 59, train loss: 8.554117202758789, test loss: 5.733589172363281\n",
      "h: 105 | epoch: 60, train loss: 8.553467750549316, test loss: 5.730696678161621\n",
      "h: 105 | epoch: 61, train loss: 8.552910804748535, test loss: 5.728056907653809\n",
      "h: 105 | epoch: 62, train loss: 8.552433967590332, test loss: 5.725647449493408\n",
      "h: 105 | epoch: 63, train loss: 8.55202579498291, test loss: 5.723443984985352\n",
      "h: 105 | epoch: 64, train loss: 8.551675796508789, test loss: 5.721431255340576\n",
      "h: 105 | epoch: 65, train loss: 8.551375389099121, test loss: 5.719590187072754\n",
      "h: 105 | epoch: 66, train loss: 8.551116943359375, test loss: 5.717904567718506\n",
      "h: 105 | epoch: 67, train loss: 8.550897598266602, test loss: 5.716360569000244\n",
      "h: 105 | epoch: 68, train loss: 8.550707817077637, test loss: 5.714945316314697\n",
      "h: 105 | epoch: 69, train loss: 8.550546646118164, test loss: 5.713647842407227\n",
      "h: 105 | epoch: 70, train loss: 8.550407409667969, test loss: 5.712458610534668\n",
      "h: 105 | epoch: 71, train loss: 8.550288200378418, test loss: 5.711366653442383\n",
      "h: 105 | epoch: 72, train loss: 8.550186157226562, test loss: 5.710364818572998\n",
      "h: 105 | epoch: 73, train loss: 8.550098419189453, test loss: 5.709444999694824\n",
      "h: 105 | epoch: 74, train loss: 8.550024032592773, test loss: 5.708598613739014\n",
      "h: 105 | epoch: 75, train loss: 8.549960136413574, test loss: 5.707821846008301\n",
      "h: 105 | epoch: 76, train loss: 8.549904823303223, test loss: 5.707107067108154\n",
      "h: 105 | epoch: 77, train loss: 8.549857139587402, test loss: 5.706450939178467\n",
      "h: 105 | epoch: 78, train loss: 8.549817085266113, test loss: 5.705846309661865\n",
      "h: 105 | epoch: 79, train loss: 8.549781799316406, test loss: 5.705291271209717\n",
      "h: 105 | epoch: 80, train loss: 8.549751281738281, test loss: 5.704779148101807\n",
      "h: 105 | epoch: 81, train loss: 8.549726486206055, test loss: 5.704309940338135\n",
      "h: 105 | epoch: 82, train loss: 8.549705505371094, test loss: 5.703876495361328\n",
      "h: 105 | epoch: 83, train loss: 8.549686431884766, test loss: 5.70347785949707\n",
      "h: 105 | epoch: 84, train loss: 8.549670219421387, test loss: 5.703110218048096\n",
      "h: 105 | epoch: 85, train loss: 8.549656867980957, test loss: 5.7027716636657715\n",
      "h: 105 | epoch: 86, train loss: 8.549644470214844, test loss: 5.702460765838623\n",
      "h: 105 | epoch: 87, train loss: 8.54963493347168, test loss: 5.702174186706543\n",
      "h: 105 | epoch: 88, train loss: 8.549625396728516, test loss: 5.701909065246582\n",
      "h: 105 | epoch: 89, train loss: 8.549617767333984, test loss: 5.701665878295898\n",
      "h: 105 | epoch: 90, train loss: 8.549612045288086, test loss: 5.701441764831543\n",
      "h: 105 | epoch: 91, train loss: 8.549606323242188, test loss: 5.701233863830566\n",
      "h: 105 | epoch: 92, train loss: 8.549601554870605, test loss: 5.701043128967285\n",
      "h: 105 | epoch: 93, train loss: 8.549596786499023, test loss: 5.700867652893066\n",
      "h: 105 | epoch: 94, train loss: 8.549593925476074, test loss: 5.700705528259277\n",
      "h: 105 | epoch: 95, train loss: 8.549590110778809, test loss: 5.700556755065918\n",
      "h: 105 | epoch: 96, train loss: 8.549588203430176, test loss: 5.700418472290039\n",
      "h: 105 | epoch: 97, train loss: 8.549586296081543, test loss: 5.700292587280273\n",
      "h: 105 | epoch: 98, train loss: 8.549585342407227, test loss: 5.700173854827881\n",
      "h: 105 | epoch: 99, train loss: 8.549582481384277, test loss: 5.700066566467285\n",
      "h: 106 | epoch: 0, train loss: 51.28649139404297, test loss: 42.05592727661133\n",
      "h: 106 | epoch: 1, train loss: 46.091190338134766, test loss: 37.77753448486328\n",
      "h: 106 | epoch: 2, train loss: 41.54119873046875, test loss: 34.02142333984375\n",
      "h: 106 | epoch: 3, train loss: 37.552040100097656, test loss: 30.720144271850586\n",
      "h: 106 | epoch: 4, train loss: 34.051475524902344, test loss: 27.81582260131836\n",
      "h: 106 | epoch: 5, train loss: 30.977474212646484, test loss: 25.258609771728516\n",
      "h: 106 | epoch: 6, train loss: 28.276519775390625, test loss: 23.005420684814453\n",
      "h: 106 | epoch: 7, train loss: 25.902273178100586, test loss: 21.018856048583984\n",
      "h: 106 | epoch: 8, train loss: 23.81450080871582, test loss: 19.26641082763672\n",
      "h: 106 | epoch: 9, train loss: 21.978158950805664, test loss: 17.719730377197266\n",
      "h: 106 | epoch: 10, train loss: 20.362667083740234, test loss: 16.354053497314453\n",
      "h: 106 | epoch: 11, train loss: 18.941287994384766, test loss: 15.14771556854248\n",
      "h: 106 | epoch: 12, train loss: 17.690601348876953, test loss: 14.081713676452637\n",
      "h: 106 | epoch: 13, train loss: 16.59006118774414, test loss: 13.139387130737305\n",
      "h: 106 | epoch: 14, train loss: 15.62164306640625, test loss: 12.30610179901123\n",
      "h: 106 | epoch: 15, train loss: 14.769495964050293, test loss: 11.568988800048828\n",
      "h: 106 | epoch: 16, train loss: 14.019694328308105, test loss: 10.916728019714355\n",
      "h: 106 | epoch: 17, train loss: 13.359979629516602, test loss: 10.339356422424316\n",
      "h: 106 | epoch: 18, train loss: 12.779565811157227, test loss: 9.828096389770508\n",
      "h: 106 | epoch: 19, train loss: 12.268957138061523, test loss: 9.375216484069824\n",
      "h: 106 | epoch: 20, train loss: 11.819792747497559, test loss: 8.973902702331543\n",
      "h: 106 | epoch: 21, train loss: 11.424714088439941, test loss: 8.61815071105957\n",
      "h: 106 | epoch: 22, train loss: 11.077232360839844, test loss: 8.302656173706055\n",
      "h: 106 | epoch: 23, train loss: 10.77164077758789, test loss: 8.022748947143555\n",
      "h: 106 | epoch: 24, train loss: 10.502908706665039, test loss: 7.774308681488037\n",
      "h: 106 | epoch: 25, train loss: 10.266608238220215, test loss: 7.553694725036621\n",
      "h: 106 | epoch: 26, train loss: 10.058839797973633, test loss: 7.3576979637146\n",
      "h: 106 | epoch: 27, train loss: 9.876172065734863, test loss: 7.1834845542907715\n",
      "h: 106 | epoch: 28, train loss: 9.715581893920898, test loss: 7.028552055358887\n",
      "h: 106 | epoch: 29, train loss: 9.574407577514648, test loss: 6.890695095062256\n",
      "h: 106 | epoch: 30, train loss: 9.450308799743652, test loss: 6.767960548400879\n",
      "h: 106 | epoch: 31, train loss: 9.341226577758789, test loss: 6.658622741699219\n",
      "h: 106 | epoch: 32, train loss: 9.245347023010254, test loss: 6.561167240142822\n",
      "h: 106 | epoch: 33, train loss: 9.1610746383667, test loss: 6.474240779876709\n",
      "h: 106 | epoch: 34, train loss: 9.087006568908691, test loss: 6.396659851074219\n",
      "h: 106 | epoch: 35, train loss: 9.021909713745117, test loss: 6.3273725509643555\n",
      "h: 106 | epoch: 36, train loss: 8.964700698852539, test loss: 6.265449047088623\n",
      "h: 106 | epoch: 37, train loss: 8.914421081542969, test loss: 6.210068702697754\n",
      "h: 106 | epoch: 38, train loss: 8.870232582092285, test loss: 6.1605024337768555\n",
      "h: 106 | epoch: 39, train loss: 8.831399917602539, test loss: 6.116107940673828\n",
      "h: 106 | epoch: 40, train loss: 8.797272682189941, test loss: 6.076314449310303\n",
      "h: 106 | epoch: 41, train loss: 8.767279624938965, test loss: 6.040618419647217\n",
      "h: 106 | epoch: 42, train loss: 8.740922927856445, test loss: 6.008571147918701\n",
      "h: 106 | epoch: 43, train loss: 8.717758178710938, test loss: 5.979778289794922\n",
      "h: 106 | epoch: 44, train loss: 8.69740104675293, test loss: 5.953886032104492\n",
      "h: 106 | epoch: 45, train loss: 8.679510116577148, test loss: 5.930584907531738\n",
      "h: 106 | epoch: 46, train loss: 8.663787841796875, test loss: 5.9095964431762695\n",
      "h: 106 | epoch: 47, train loss: 8.649969100952148, test loss: 5.8906755447387695\n",
      "h: 106 | epoch: 48, train loss: 8.637824058532715, test loss: 5.873603343963623\n",
      "h: 106 | epoch: 49, train loss: 8.627150535583496, test loss: 5.858187675476074\n",
      "h: 106 | epoch: 50, train loss: 8.617769241333008, test loss: 5.844252586364746\n",
      "h: 106 | epoch: 51, train loss: 8.609524726867676, test loss: 5.831647872924805\n",
      "h: 106 | epoch: 52, train loss: 8.602277755737305, test loss: 5.820235252380371\n",
      "h: 106 | epoch: 53, train loss: 8.595907211303711, test loss: 5.8098931312561035\n",
      "h: 106 | epoch: 54, train loss: 8.59030818939209, test loss: 5.800511837005615\n",
      "h: 106 | epoch: 55, train loss: 8.585386276245117, test loss: 5.791995525360107\n",
      "h: 106 | epoch: 56, train loss: 8.581060409545898, test loss: 5.784260272979736\n",
      "h: 106 | epoch: 57, train loss: 8.57725715637207, test loss: 5.777223110198975\n",
      "h: 106 | epoch: 58, train loss: 8.573914527893066, test loss: 5.770819187164307\n",
      "h: 106 | epoch: 59, train loss: 8.570975303649902, test loss: 5.764985084533691\n",
      "h: 106 | epoch: 60, train loss: 8.568391799926758, test loss: 5.75966739654541\n",
      "h: 106 | epoch: 61, train loss: 8.566120147705078, test loss: 5.754814147949219\n",
      "h: 106 | epoch: 62, train loss: 8.56412410736084, test loss: 5.750382423400879\n",
      "h: 106 | epoch: 63, train loss: 8.562368392944336, test loss: 5.746331691741943\n",
      "h: 106 | epoch: 64, train loss: 8.560824394226074, test loss: 5.742626190185547\n",
      "h: 106 | epoch: 65, train loss: 8.559467315673828, test loss: 5.739234447479248\n",
      "h: 106 | epoch: 66, train loss: 8.55827522277832, test loss: 5.736128330230713\n",
      "h: 106 | epoch: 67, train loss: 8.557225227355957, test loss: 5.733278274536133\n",
      "h: 106 | epoch: 68, train loss: 8.556303024291992, test loss: 5.730665683746338\n",
      "h: 106 | epoch: 69, train loss: 8.555490493774414, test loss: 5.728264808654785\n",
      "h: 106 | epoch: 70, train loss: 8.554779052734375, test loss: 5.72606086730957\n",
      "h: 106 | epoch: 71, train loss: 8.55415153503418, test loss: 5.724033355712891\n",
      "h: 106 | epoch: 72, train loss: 8.55359935760498, test loss: 5.722167015075684\n",
      "h: 106 | epoch: 73, train loss: 8.55311393737793, test loss: 5.720450401306152\n",
      "h: 106 | epoch: 74, train loss: 8.552688598632812, test loss: 5.718867301940918\n",
      "h: 106 | epoch: 75, train loss: 8.552312850952148, test loss: 5.717410087585449\n",
      "h: 106 | epoch: 76, train loss: 8.551982879638672, test loss: 5.716063022613525\n",
      "h: 106 | epoch: 77, train loss: 8.551692962646484, test loss: 5.7148213386535645\n",
      "h: 106 | epoch: 78, train loss: 8.551436424255371, test loss: 5.713675498962402\n",
      "h: 106 | epoch: 79, train loss: 8.551213264465332, test loss: 5.712615489959717\n",
      "h: 106 | epoch: 80, train loss: 8.55101490020752, test loss: 5.711635112762451\n",
      "h: 106 | epoch: 81, train loss: 8.550841331481934, test loss: 5.710729122161865\n",
      "h: 106 | epoch: 82, train loss: 8.550689697265625, test loss: 5.709890365600586\n",
      "h: 106 | epoch: 83, train loss: 8.550555229187012, test loss: 5.7091145515441895\n",
      "h: 106 | epoch: 84, train loss: 8.550436973571777, test loss: 5.7083940505981445\n",
      "h: 106 | epoch: 85, train loss: 8.550332069396973, test loss: 5.707727909088135\n",
      "h: 106 | epoch: 86, train loss: 8.550241470336914, test loss: 5.707110404968262\n",
      "h: 106 | epoch: 87, train loss: 8.550161361694336, test loss: 5.706537246704102\n",
      "h: 106 | epoch: 88, train loss: 8.550089836120605, test loss: 5.706006050109863\n",
      "h: 106 | epoch: 89, train loss: 8.550027847290039, test loss: 5.705512046813965\n",
      "h: 106 | epoch: 90, train loss: 8.549973487854004, test loss: 5.705053806304932\n",
      "h: 106 | epoch: 91, train loss: 8.549924850463867, test loss: 5.70462703704834\n",
      "h: 106 | epoch: 92, train loss: 8.549882888793945, test loss: 5.704232215881348\n",
      "h: 106 | epoch: 93, train loss: 8.549844741821289, test loss: 5.703863143920898\n",
      "h: 106 | epoch: 94, train loss: 8.549813270568848, test loss: 5.703522205352783\n",
      "h: 106 | epoch: 95, train loss: 8.549783706665039, test loss: 5.703203201293945\n",
      "h: 106 | epoch: 96, train loss: 8.549757957458496, test loss: 5.702907085418701\n",
      "h: 106 | epoch: 97, train loss: 8.549736022949219, test loss: 5.702631950378418\n",
      "h: 106 | epoch: 98, train loss: 8.549715995788574, test loss: 5.702376365661621\n",
      "h: 106 | epoch: 99, train loss: 8.549699783325195, test loss: 5.702137470245361\n",
      "h: 107 | epoch: 0, train loss: 40.282676696777344, test loss: 32.11416244506836\n",
      "h: 107 | epoch: 1, train loss: 35.691200256347656, test loss: 28.448787689208984\n",
      "h: 107 | epoch: 2, train loss: 31.76605224609375, test loss: 25.303070068359375\n",
      "h: 107 | epoch: 3, train loss: 28.408966064453125, test loss: 22.601545333862305\n",
      "h: 107 | epoch: 4, train loss: 25.536813735961914, test loss: 20.28018569946289\n",
      "h: 107 | epoch: 5, train loss: 23.07907485961914, test loss: 18.28451919555664\n",
      "h: 107 | epoch: 6, train loss: 20.97576141357422, test loss: 16.56810760498047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 107 | epoch: 7, train loss: 19.175729751586914, test loss: 15.091279983520508\n",
      "h: 107 | epoch: 8, train loss: 17.635316848754883, test loss: 13.820103645324707\n",
      "h: 107 | epoch: 9, train loss: 16.317188262939453, test loss: 12.725528717041016\n",
      "h: 107 | epoch: 10, train loss: 15.189410209655762, test loss: 11.782654762268066\n",
      "h: 107 | epoch: 11, train loss: 14.224626541137695, test loss: 10.970131874084473\n",
      "h: 107 | epoch: 12, train loss: 13.39941120147705, test loss: 10.269643783569336\n",
      "h: 107 | epoch: 13, train loss: 12.69369125366211, test loss: 9.665467262268066\n",
      "h: 107 | epoch: 14, train loss: 12.090263366699219, test loss: 9.144110679626465\n",
      "h: 107 | epoch: 15, train loss: 11.574386596679688, test loss: 8.69398307800293\n",
      "h: 107 | epoch: 16, train loss: 11.133432388305664, test loss: 8.305131912231445\n",
      "h: 107 | epoch: 17, train loss: 10.756572723388672, test loss: 7.9690141677856445\n",
      "h: 107 | epoch: 18, train loss: 10.434542655944824, test loss: 7.678286552429199\n",
      "h: 107 | epoch: 19, train loss: 10.159400939941406, test loss: 7.426645755767822\n",
      "h: 107 | epoch: 20, train loss: 9.924348831176758, test loss: 7.20867395401001\n",
      "h: 107 | epoch: 21, train loss: 9.723567008972168, test loss: 7.019715785980225\n",
      "h: 107 | epoch: 22, train loss: 9.552079200744629, test loss: 6.855771541595459\n",
      "h: 107 | epoch: 23, train loss: 9.405620574951172, test loss: 6.713401794433594\n",
      "h: 107 | epoch: 24, train loss: 9.280550003051758, test loss: 6.589653968811035\n",
      "h: 107 | epoch: 25, train loss: 9.173749923706055, test loss: 6.481988430023193\n",
      "h: 107 | epoch: 26, train loss: 9.08255672454834, test loss: 6.388219833374023\n",
      "h: 107 | epoch: 27, train loss: 9.004693031311035, test loss: 6.306464672088623\n",
      "h: 107 | epoch: 28, train loss: 8.938211441040039, test loss: 6.235106945037842\n",
      "h: 107 | epoch: 29, train loss: 8.881448745727539, test loss: 6.172753810882568\n",
      "h: 107 | epoch: 30, train loss: 8.832987785339355, test loss: 6.1182050704956055\n",
      "h: 107 | epoch: 31, train loss: 8.791609764099121, test loss: 6.070426940917969\n",
      "h: 107 | epoch: 32, train loss: 8.7562837600708, test loss: 6.028526782989502\n",
      "h: 107 | epoch: 33, train loss: 8.72612190246582, test loss: 5.991737365722656\n",
      "h: 107 | epoch: 34, train loss: 8.700368881225586, test loss: 5.959391117095947\n",
      "h: 107 | epoch: 35, train loss: 8.678380012512207, test loss: 5.9309163093566895\n",
      "h: 107 | epoch: 36, train loss: 8.659603118896484, test loss: 5.905817985534668\n",
      "h: 107 | epoch: 37, train loss: 8.643569946289062, test loss: 5.883664608001709\n",
      "h: 107 | epoch: 38, train loss: 8.629878997802734, test loss: 5.864085674285889\n",
      "h: 107 | epoch: 39, train loss: 8.618185997009277, test loss: 5.84675931930542\n",
      "h: 107 | epoch: 40, train loss: 8.608200073242188, test loss: 5.8314056396484375\n",
      "h: 107 | epoch: 41, train loss: 8.599671363830566, test loss: 5.817782402038574\n",
      "h: 107 | epoch: 42, train loss: 8.592386245727539, test loss: 5.805678844451904\n",
      "h: 107 | epoch: 43, train loss: 8.586163520812988, test loss: 5.794910430908203\n",
      "h: 107 | epoch: 44, train loss: 8.580846786499023, test loss: 5.785318374633789\n",
      "h: 107 | epoch: 45, train loss: 8.576305389404297, test loss: 5.776764869689941\n",
      "h: 107 | epoch: 46, train loss: 8.57242488861084, test loss: 5.7691240310668945\n",
      "h: 107 | epoch: 47, train loss: 8.569109916687012, test loss: 5.762293815612793\n",
      "h: 107 | epoch: 48, train loss: 8.566276550292969, test loss: 5.756178855895996\n",
      "h: 107 | epoch: 49, train loss: 8.56385612487793, test loss: 5.750699043273926\n",
      "h: 107 | epoch: 50, train loss: 8.561785697937012, test loss: 5.745781421661377\n",
      "h: 107 | epoch: 51, train loss: 8.560017585754395, test loss: 5.741364002227783\n",
      "h: 107 | epoch: 52, train loss: 8.55850601196289, test loss: 5.737391948699951\n",
      "h: 107 | epoch: 53, train loss: 8.55721378326416, test loss: 5.733817100524902\n",
      "h: 107 | epoch: 54, train loss: 8.556108474731445, test loss: 5.730595588684082\n",
      "h: 107 | epoch: 55, train loss: 8.55516529083252, test loss: 5.727688789367676\n",
      "h: 107 | epoch: 56, train loss: 8.554356575012207, test loss: 5.7250657081604\n",
      "h: 107 | epoch: 57, train loss: 8.553665161132812, test loss: 5.722695350646973\n",
      "h: 107 | epoch: 58, train loss: 8.553074836730957, test loss: 5.720551490783691\n",
      "h: 107 | epoch: 59, train loss: 8.552569389343262, test loss: 5.7186102867126465\n",
      "h: 107 | epoch: 60, train loss: 8.55213737487793, test loss: 5.716851234436035\n",
      "h: 107 | epoch: 61, train loss: 8.551767349243164, test loss: 5.71525764465332\n",
      "h: 107 | epoch: 62, train loss: 8.551451683044434, test loss: 5.713810920715332\n",
      "h: 107 | epoch: 63, train loss: 8.551180839538574, test loss: 5.712498664855957\n",
      "h: 107 | epoch: 64, train loss: 8.550949096679688, test loss: 5.711305141448975\n",
      "h: 107 | epoch: 65, train loss: 8.550750732421875, test loss: 5.710220813751221\n",
      "h: 107 | epoch: 66, train loss: 8.550580978393555, test loss: 5.709234237670898\n",
      "h: 107 | epoch: 67, train loss: 8.550436973571777, test loss: 5.70833683013916\n",
      "h: 107 | epoch: 68, train loss: 8.550313949584961, test loss: 5.707519054412842\n",
      "h: 107 | epoch: 69, train loss: 8.550206184387207, test loss: 5.706774711608887\n",
      "h: 107 | epoch: 70, train loss: 8.550114631652832, test loss: 5.706094741821289\n",
      "h: 107 | epoch: 71, train loss: 8.550037384033203, test loss: 5.705475330352783\n",
      "h: 107 | epoch: 72, train loss: 8.549970626831055, test loss: 5.704909324645996\n",
      "h: 107 | epoch: 73, train loss: 8.54991340637207, test loss: 5.7043938636779785\n",
      "h: 107 | epoch: 74, train loss: 8.549863815307617, test loss: 5.703923225402832\n",
      "h: 107 | epoch: 75, train loss: 8.549821853637695, test loss: 5.703492164611816\n",
      "h: 107 | epoch: 76, train loss: 8.549787521362305, test loss: 5.703098773956299\n",
      "h: 107 | epoch: 77, train loss: 8.54975700378418, test loss: 5.702739238739014\n",
      "h: 107 | epoch: 78, train loss: 8.54973030090332, test loss: 5.702410697937012\n",
      "h: 107 | epoch: 79, train loss: 8.549707412719727, test loss: 5.702110767364502\n",
      "h: 107 | epoch: 80, train loss: 8.549688339233398, test loss: 5.701834678649902\n",
      "h: 107 | epoch: 81, train loss: 8.54967212677002, test loss: 5.701581954956055\n",
      "h: 107 | epoch: 82, train loss: 8.549657821655273, test loss: 5.701350688934326\n",
      "h: 107 | epoch: 83, train loss: 8.54964542388916, test loss: 5.701139450073242\n",
      "h: 107 | epoch: 84, train loss: 8.54963493347168, test loss: 5.700946807861328\n",
      "h: 107 | epoch: 85, train loss: 8.549625396728516, test loss: 5.700768947601318\n",
      "h: 107 | epoch: 86, train loss: 8.5496187210083, test loss: 5.700606346130371\n",
      "h: 107 | epoch: 87, train loss: 8.549612045288086, test loss: 5.700457572937012\n",
      "h: 107 | epoch: 88, train loss: 8.549606323242188, test loss: 5.700322151184082\n",
      "h: 107 | epoch: 89, train loss: 8.549601554870605, test loss: 5.700195789337158\n",
      "h: 107 | epoch: 90, train loss: 8.54959774017334, test loss: 5.7000813484191895\n",
      "h: 107 | epoch: 91, train loss: 8.549593925476074, test loss: 5.699975967407227\n",
      "h: 107 | epoch: 92, train loss: 8.549591064453125, test loss: 5.699878692626953\n",
      "h: 107 | epoch: 93, train loss: 8.549589157104492, test loss: 5.699790954589844\n",
      "h: 107 | epoch: 94, train loss: 8.549585342407227, test loss: 5.699709892272949\n",
      "h: 107 | epoch: 95, train loss: 8.549583435058594, test loss: 5.699633598327637\n",
      "h: 107 | epoch: 96, train loss: 8.549581527709961, test loss: 5.6995649337768555\n",
      "h: 107 | epoch: 97, train loss: 8.549581527709961, test loss: 5.699502944946289\n",
      "h: 107 | epoch: 98, train loss: 8.549579620361328, test loss: 5.699444770812988\n",
      "h: 107 | epoch: 99, train loss: 8.549578666687012, test loss: 5.699392795562744\n",
      "h: 108 | epoch: 0, train loss: 48.50336456298828, test loss: 39.461402893066406\n",
      "h: 108 | epoch: 1, train loss: 42.88379669189453, test loss: 34.8931999206543\n",
      "h: 108 | epoch: 2, train loss: 38.06018829345703, test loss: 30.95822525024414\n",
      "h: 108 | epoch: 3, train loss: 33.916717529296875, test loss: 27.565799713134766\n",
      "h: 108 | epoch: 4, train loss: 30.35556411743164, test loss: 24.639068603515625\n",
      "h: 108 | epoch: 5, train loss: 27.293766021728516, test loss: 22.112613677978516\n",
      "h: 108 | epoch: 6, train loss: 24.66067886352539, test loss: 19.930599212646484\n",
      "h: 108 | epoch: 7, train loss: 22.39600372314453, test loss: 18.045238494873047\n",
      "h: 108 | epoch: 8, train loss: 20.448116302490234, test loss: 16.41555404663086\n",
      "h: 108 | epoch: 9, train loss: 18.772762298583984, test loss: 15.006357192993164\n",
      "h: 108 | epoch: 10, train loss: 17.33193588256836, test loss: 13.787385940551758\n",
      "h: 108 | epoch: 11, train loss: 16.09296417236328, test loss: 12.732583999633789\n",
      "h: 108 | epoch: 12, train loss: 15.027728080749512, test loss: 11.819515228271484\n",
      "h: 108 | epoch: 13, train loss: 14.112032890319824, test loss: 11.028831481933594\n",
      "h: 108 | epoch: 14, train loss: 13.325032234191895, test loss: 10.343852043151855\n",
      "h: 108 | epoch: 15, train loss: 12.648767471313477, test loss: 9.750187873840332\n",
      "h: 108 | epoch: 16, train loss: 12.067777633666992, test loss: 9.23542594909668\n",
      "h: 108 | epoch: 17, train loss: 11.568735122680664, test loss: 8.788851737976074\n",
      "h: 108 | epoch: 18, train loss: 11.140164375305176, test loss: 8.401219367980957\n",
      "h: 108 | epoch: 19, train loss: 10.772180557250977, test loss: 8.064552307128906\n",
      "h: 108 | epoch: 20, train loss: 10.456274032592773, test loss: 7.771960258483887\n",
      "h: 108 | epoch: 21, train loss: 10.185123443603516, test loss: 7.517502784729004\n",
      "h: 108 | epoch: 22, train loss: 9.95241928100586, test loss: 7.296044826507568\n",
      "h: 108 | epoch: 23, train loss: 9.752739906311035, test loss: 7.103157997131348\n",
      "h: 108 | epoch: 24, train loss: 9.581422805786133, test loss: 6.9350175857543945\n",
      "h: 108 | epoch: 25, train loss: 9.434453964233398, test loss: 6.788318634033203\n",
      "h: 108 | epoch: 26, train loss: 9.308387756347656, test loss: 6.660210609436035\n",
      "h: 108 | epoch: 27, train loss: 9.200262069702148, test loss: 6.5482306480407715\n",
      "h: 108 | epoch: 28, train loss: 9.107531547546387, test loss: 6.450247287750244\n",
      "h: 108 | epoch: 29, train loss: 9.028007507324219, test loss: 6.364423751831055\n",
      "h: 108 | epoch: 30, train loss: 8.95981502532959, test loss: 6.289168357849121\n",
      "h: 108 | epoch: 31, train loss: 8.901342391967773, test loss: 6.2231035232543945\n",
      "h: 108 | epoch: 32, train loss: 8.851205825805664, test loss: 6.165042877197266\n",
      "h: 108 | epoch: 33, train loss: 8.808218955993652, test loss: 6.113954544067383\n",
      "h: 108 | epoch: 34, train loss: 8.771361351013184, test loss: 6.068946361541748\n",
      "h: 108 | epoch: 35, train loss: 8.739760398864746, test loss: 6.029244422912598\n",
      "h: 108 | epoch: 36, train loss: 8.712667465209961, test loss: 5.9941816329956055\n",
      "h: 108 | epoch: 37, train loss: 8.689438819885254, test loss: 5.963173866271973\n",
      "h: 108 | epoch: 38, train loss: 8.669522285461426, test loss: 5.935717582702637\n",
      "h: 108 | epoch: 39, train loss: 8.652446746826172, test loss: 5.911375522613525\n",
      "h: 108 | epoch: 40, train loss: 8.637804985046387, test loss: 5.889764308929443\n",
      "h: 108 | epoch: 41, train loss: 8.625250816345215, test loss: 5.870552062988281\n",
      "h: 108 | epoch: 42, train loss: 8.614486694335938, test loss: 5.853451251983643\n",
      "h: 108 | epoch: 43, train loss: 8.605257034301758, test loss: 5.8382110595703125\n",
      "h: 108 | epoch: 44, train loss: 8.597341537475586, test loss: 5.82460880279541\n",
      "h: 108 | epoch: 45, train loss: 8.590553283691406, test loss: 5.812454700469971\n",
      "h: 108 | epoch: 46, train loss: 8.584732055664062, test loss: 5.801579475402832\n",
      "h: 108 | epoch: 47, train loss: 8.57973861694336, test loss: 5.791837692260742\n",
      "h: 108 | epoch: 48, train loss: 8.575457572937012, test loss: 5.783100128173828\n",
      "h: 108 | epoch: 49, train loss: 8.571784973144531, test loss: 5.775252819061279\n",
      "h: 108 | epoch: 50, train loss: 8.568634033203125, test loss: 5.768197536468506\n",
      "h: 108 | epoch: 51, train loss: 8.56593132019043, test loss: 5.761846542358398\n",
      "h: 108 | epoch: 52, train loss: 8.563612937927246, test loss: 5.756124496459961\n",
      "h: 108 | epoch: 53, train loss: 8.561623573303223, test loss: 5.750962257385254\n",
      "h: 108 | epoch: 54, train loss: 8.559917449951172, test loss: 5.746298789978027\n",
      "h: 108 | epoch: 55, train loss: 8.558452606201172, test loss: 5.742084980010986\n",
      "h: 108 | epoch: 56, train loss: 8.557195663452148, test loss: 5.738269805908203\n",
      "h: 108 | epoch: 57, train loss: 8.556117057800293, test loss: 5.734814643859863\n",
      "h: 108 | epoch: 58, train loss: 8.555192947387695, test loss: 5.731682777404785\n",
      "h: 108 | epoch: 59, train loss: 8.554399490356445, test loss: 5.728840351104736\n",
      "h: 108 | epoch: 60, train loss: 8.553716659545898, test loss: 5.726258754730225\n",
      "h: 108 | epoch: 61, train loss: 8.553130149841309, test loss: 5.723912715911865\n",
      "h: 108 | epoch: 62, train loss: 8.552628517150879, test loss: 5.72177791595459\n",
      "h: 108 | epoch: 63, train loss: 8.552197456359863, test loss: 5.719834804534912\n",
      "h: 108 | epoch: 64, train loss: 8.551827430725098, test loss: 5.718064785003662\n",
      "h: 108 | epoch: 65, train loss: 8.551508903503418, test loss: 5.716451168060303\n",
      "h: 108 | epoch: 66, train loss: 8.551237106323242, test loss: 5.71497917175293\n",
      "h: 108 | epoch: 67, train loss: 8.55100154876709, test loss: 5.713635444641113\n",
      "h: 108 | epoch: 68, train loss: 8.550800323486328, test loss: 5.71240758895874\n",
      "h: 108 | epoch: 69, train loss: 8.550627708435059, test loss: 5.711287021636963\n",
      "h: 108 | epoch: 70, train loss: 8.550479888916016, test loss: 5.710261344909668\n",
      "h: 108 | epoch: 71, train loss: 8.550352096557617, test loss: 5.709322929382324\n",
      "h: 108 | epoch: 72, train loss: 8.550241470336914, test loss: 5.708465099334717\n",
      "h: 108 | epoch: 73, train loss: 8.550148010253906, test loss: 5.70767879486084\n",
      "h: 108 | epoch: 74, train loss: 8.550066947937012, test loss: 5.706958293914795\n",
      "h: 108 | epoch: 75, train loss: 8.549997329711914, test loss: 5.706298828125\n",
      "h: 108 | epoch: 76, train loss: 8.54993724822998, test loss: 5.705694675445557\n",
      "h: 108 | epoch: 77, train loss: 8.549886703491211, test loss: 5.705139636993408\n",
      "h: 108 | epoch: 78, train loss: 8.549842834472656, test loss: 5.704630374908447\n",
      "h: 108 | epoch: 79, train loss: 8.5498046875, test loss: 5.70416259765625\n",
      "h: 108 | epoch: 80, train loss: 8.549772262573242, test loss: 5.703733921051025\n",
      "h: 108 | epoch: 81, train loss: 8.549744606018066, test loss: 5.703340530395508\n",
      "h: 108 | epoch: 82, train loss: 8.549720764160156, test loss: 5.70297908782959\n",
      "h: 108 | epoch: 83, train loss: 8.549699783325195, test loss: 5.702646732330322\n",
      "h: 108 | epoch: 84, train loss: 8.549680709838867, test loss: 5.702340602874756\n",
      "h: 108 | epoch: 85, train loss: 8.549665451049805, test loss: 5.702059745788574\n",
      "h: 108 | epoch: 86, train loss: 8.549654960632324, test loss: 5.7018022537231445\n",
      "h: 108 | epoch: 87, train loss: 8.549641609191895, test loss: 5.701564311981201\n",
      "h: 108 | epoch: 88, train loss: 8.54963207244873, test loss: 5.701345920562744\n",
      "h: 108 | epoch: 89, train loss: 8.549623489379883, test loss: 5.701144695281982\n",
      "h: 108 | epoch: 90, train loss: 8.549616813659668, test loss: 5.700960636138916\n",
      "h: 108 | epoch: 91, train loss: 8.549610137939453, test loss: 5.700789451599121\n",
      "h: 108 | epoch: 92, train loss: 8.549605369567871, test loss: 5.700633525848389\n",
      "h: 108 | epoch: 93, train loss: 8.549600601196289, test loss: 5.7004899978637695\n",
      "h: 108 | epoch: 94, train loss: 8.549596786499023, test loss: 5.700356483459473\n",
      "h: 108 | epoch: 95, train loss: 8.549592971801758, test loss: 5.700234413146973\n",
      "h: 108 | epoch: 96, train loss: 8.549590110778809, test loss: 5.700121879577637\n",
      "h: 108 | epoch: 97, train loss: 8.549588203430176, test loss: 5.700016975402832\n",
      "h: 108 | epoch: 98, train loss: 8.549585342407227, test loss: 5.699922561645508\n",
      "h: 108 | epoch: 99, train loss: 8.549585342407227, test loss: 5.699833869934082\n",
      "h: 109 | epoch: 0, train loss: 39.88477325439453, test loss: 32.313697814941406\n",
      "h: 109 | epoch: 1, train loss: 35.61145782470703, test loss: 28.83078956604004\n",
      "h: 109 | epoch: 2, train loss: 31.92330551147461, test loss: 25.814342498779297\n",
      "h: 109 | epoch: 3, train loss: 28.738683700561523, test loss: 23.200193405151367\n",
      "h: 109 | epoch: 4, train loss: 25.987930297851562, test loss: 20.933435440063477\n",
      "h: 109 | epoch: 5, train loss: 23.61141586303711, test loss: 18.966962814331055\n",
      "h: 109 | epoch: 6, train loss: 21.557968139648438, test loss: 17.260271072387695\n",
      "h: 109 | epoch: 7, train loss: 19.783580780029297, test loss: 15.778462409973145\n",
      "h: 109 | epoch: 8, train loss: 18.25033950805664, test loss: 14.491435050964355\n",
      "h: 109 | epoch: 9, train loss: 16.925540924072266, test loss: 13.373181343078613\n",
      "h: 109 | epoch: 10, train loss: 15.780940055847168, test loss: 12.40122127532959\n",
      "h: 109 | epoch: 11, train loss: 14.792132377624512, test loss: 11.556107521057129\n",
      "h: 109 | epoch: 12, train loss: 13.938021659851074, test loss: 10.821001052856445\n",
      "h: 109 | epoch: 13, train loss: 13.200355529785156, test loss: 10.181333541870117\n",
      "h: 109 | epoch: 14, train loss: 12.563342094421387, test loss: 9.624467849731445\n",
      "h: 109 | epoch: 15, train loss: 12.013326644897461, test loss: 9.139467239379883\n",
      "h: 109 | epoch: 16, train loss: 11.53848934173584, test loss: 8.716845512390137\n",
      "h: 109 | epoch: 17, train loss: 11.128607749938965, test loss: 8.348394393920898\n",
      "h: 109 | epoch: 18, train loss: 10.774839401245117, test loss: 8.026983261108398\n",
      "h: 109 | epoch: 19, train loss: 10.469533920288086, test loss: 7.746439456939697\n",
      "h: 109 | epoch: 20, train loss: 10.206077575683594, test loss: 7.501410484313965\n",
      "h: 109 | epoch: 21, train loss: 9.978757858276367, test loss: 7.287259101867676\n",
      "h: 109 | epoch: 22, train loss: 9.782630920410156, test loss: 7.099956512451172\n",
      "h: 109 | epoch: 23, train loss: 9.61342716217041, test loss: 6.9360151290893555\n",
      "h: 109 | epoch: 24, train loss: 9.46745777130127, test loss: 6.7924065589904785\n",
      "h: 109 | epoch: 25, train loss: 9.34153938293457, test loss: 6.666506290435791\n",
      "h: 109 | epoch: 26, train loss: 9.232915878295898, test loss: 6.556034088134766\n",
      "h: 109 | epoch: 27, train loss: 9.139219284057617, test loss: 6.459012508392334\n",
      "h: 109 | epoch: 28, train loss: 9.058395385742188, test loss: 6.3737263679504395\n",
      "h: 109 | epoch: 29, train loss: 8.988676071166992, test loss: 6.298682689666748\n",
      "h: 109 | epoch: 30, train loss: 8.928534507751465, test loss: 6.232585906982422\n",
      "h: 109 | epoch: 31, train loss: 8.876653671264648, test loss: 6.174312114715576\n",
      "h: 109 | epoch: 32, train loss: 8.831897735595703, test loss: 6.1228814125061035\n",
      "h: 109 | epoch: 33, train loss: 8.793286323547363, test loss: 6.077442169189453\n",
      "h: 109 | epoch: 34, train loss: 8.759973526000977, test loss: 6.037250518798828\n",
      "h: 109 | epoch: 35, train loss: 8.731231689453125, test loss: 6.001665115356445\n",
      "h: 109 | epoch: 36, train loss: 8.706430435180664, test loss: 5.970121383666992\n",
      "h: 109 | epoch: 37, train loss: 8.685028076171875, test loss: 5.942130088806152\n",
      "h: 109 | epoch: 38, train loss: 8.666559219360352, test loss: 5.917262077331543\n",
      "h: 109 | epoch: 39, train loss: 8.650618553161621, test loss: 5.895143508911133\n",
      "h: 109 | epoch: 40, train loss: 8.636859893798828, test loss: 5.875448703765869\n",
      "h: 109 | epoch: 41, train loss: 8.624982833862305, test loss: 5.857892990112305\n",
      "h: 109 | epoch: 42, train loss: 8.614728927612305, test loss: 5.842225074768066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 109 | epoch: 43, train loss: 8.605875968933105, test loss: 5.828227519989014\n",
      "h: 109 | epoch: 44, train loss: 8.598230361938477, test loss: 5.815707206726074\n",
      "h: 109 | epoch: 45, train loss: 8.591629981994629, test loss: 5.804495811462402\n",
      "h: 109 | epoch: 46, train loss: 8.585927963256836, test loss: 5.7944464683532715\n",
      "h: 109 | epoch: 47, train loss: 8.581003189086914, test loss: 5.785427093505859\n",
      "h: 109 | epoch: 48, train loss: 8.576748847961426, test loss: 5.777325630187988\n",
      "h: 109 | epoch: 49, train loss: 8.57307243347168, test loss: 5.770040035247803\n",
      "h: 109 | epoch: 50, train loss: 8.56989574432373, test loss: 5.763482093811035\n",
      "h: 109 | epoch: 51, train loss: 8.56715202331543, test loss: 5.757572650909424\n",
      "h: 109 | epoch: 52, train loss: 8.564778327941895, test loss: 5.752242088317871\n",
      "h: 109 | epoch: 53, train loss: 8.562727928161621, test loss: 5.7474284172058105\n",
      "h: 109 | epoch: 54, train loss: 8.560956001281738, test loss: 5.743080139160156\n",
      "h: 109 | epoch: 55, train loss: 8.559422492980957, test loss: 5.7391462326049805\n",
      "h: 109 | epoch: 56, train loss: 8.558096885681152, test loss: 5.735585689544678\n",
      "h: 109 | epoch: 57, train loss: 8.556951522827148, test loss: 5.732357978820801\n",
      "h: 109 | epoch: 58, train loss: 8.555959701538086, test loss: 5.729432106018066\n",
      "h: 109 | epoch: 59, train loss: 8.555102348327637, test loss: 5.726776599884033\n",
      "h: 109 | epoch: 60, train loss: 8.554359436035156, test loss: 5.724364757537842\n",
      "h: 109 | epoch: 61, train loss: 8.553718566894531, test loss: 5.722173690795898\n",
      "h: 109 | epoch: 62, train loss: 8.553162574768066, test loss: 5.720179080963135\n",
      "h: 109 | epoch: 63, train loss: 8.552682876586914, test loss: 5.718364715576172\n",
      "h: 109 | epoch: 64, train loss: 8.552266120910645, test loss: 5.716711521148682\n",
      "h: 109 | epoch: 65, train loss: 8.55190658569336, test loss: 5.71520471572876\n",
      "h: 109 | epoch: 66, train loss: 8.551594734191895, test loss: 5.713831424713135\n",
      "h: 109 | epoch: 67, train loss: 8.551324844360352, test loss: 5.7125773429870605\n",
      "h: 109 | epoch: 68, train loss: 8.551090240478516, test loss: 5.711432933807373\n",
      "h: 109 | epoch: 69, train loss: 8.550889015197754, test loss: 5.710387229919434\n",
      "h: 109 | epoch: 70, train loss: 8.550712585449219, test loss: 5.709432601928711\n",
      "h: 109 | epoch: 71, train loss: 8.550561904907227, test loss: 5.708558559417725\n",
      "h: 109 | epoch: 72, train loss: 8.550429344177246, test loss: 5.707759857177734\n",
      "h: 109 | epoch: 73, train loss: 8.550315856933594, test loss: 5.707028388977051\n",
      "h: 109 | epoch: 74, train loss: 8.550216674804688, test loss: 5.706357479095459\n",
      "h: 109 | epoch: 75, train loss: 8.550130844116211, test loss: 5.705745697021484\n",
      "h: 109 | epoch: 76, train loss: 8.550057411193848, test loss: 5.705183029174805\n",
      "h: 109 | epoch: 77, train loss: 8.549993515014648, test loss: 5.704668045043945\n",
      "h: 109 | epoch: 78, train loss: 8.54993724822998, test loss: 5.70419454574585\n",
      "h: 109 | epoch: 79, train loss: 8.549888610839844, test loss: 5.703761100769043\n",
      "h: 109 | epoch: 80, train loss: 8.549846649169922, test loss: 5.703364372253418\n",
      "h: 109 | epoch: 81, train loss: 8.549810409545898, test loss: 5.702998161315918\n",
      "h: 109 | epoch: 82, train loss: 8.549779891967773, test loss: 5.702663421630859\n",
      "h: 109 | epoch: 83, train loss: 8.549751281738281, test loss: 5.70235538482666\n",
      "h: 109 | epoch: 84, train loss: 8.549727439880371, test loss: 5.702073574066162\n",
      "h: 109 | epoch: 85, train loss: 8.549707412719727, test loss: 5.701813697814941\n",
      "h: 109 | epoch: 86, train loss: 8.549690246582031, test loss: 5.701573371887207\n",
      "h: 109 | epoch: 87, train loss: 8.549674034118652, test loss: 5.701354503631592\n",
      "h: 109 | epoch: 88, train loss: 8.549660682678223, test loss: 5.7011518478393555\n",
      "h: 109 | epoch: 89, train loss: 8.549649238586426, test loss: 5.700967311859131\n",
      "h: 109 | epoch: 90, train loss: 8.549638748168945, test loss: 5.7007951736450195\n",
      "h: 109 | epoch: 91, train loss: 8.549631118774414, test loss: 5.700638771057129\n",
      "h: 109 | epoch: 92, train loss: 8.549622535705566, test loss: 5.700493812561035\n",
      "h: 109 | epoch: 93, train loss: 8.549615859985352, test loss: 5.7003607749938965\n",
      "h: 109 | epoch: 94, train loss: 8.54961109161377, test loss: 5.700238227844238\n",
      "h: 109 | epoch: 95, train loss: 8.549605369567871, test loss: 5.700125694274902\n",
      "h: 109 | epoch: 96, train loss: 8.549601554870605, test loss: 5.700021743774414\n",
      "h: 109 | epoch: 97, train loss: 8.54959774017334, test loss: 5.699925422668457\n",
      "h: 109 | epoch: 98, train loss: 8.54959487915039, test loss: 5.699837684631348\n",
      "h: 109 | epoch: 99, train loss: 8.549591064453125, test loss: 5.699756145477295\n",
      "h: 110 | epoch: 0, train loss: 44.06698226928711, test loss: 35.93663787841797\n",
      "h: 110 | epoch: 1, train loss: 38.41852569580078, test loss: 31.382394790649414\n",
      "h: 110 | epoch: 2, train loss: 33.66758346557617, test loss: 27.528793334960938\n",
      "h: 110 | epoch: 3, train loss: 29.6699161529541, test loss: 24.26583480834961\n",
      "h: 110 | epoch: 4, train loss: 26.305362701416016, test loss: 21.501441955566406\n",
      "h: 110 | epoch: 5, train loss: 23.473478317260742, test loss: 19.158294677734375\n",
      "h: 110 | epoch: 6, train loss: 21.090076446533203, test loss: 17.171340942382812\n",
      "h: 110 | epoch: 7, train loss: 19.08441734313965, test loss: 15.485730171203613\n",
      "h: 110 | epoch: 8, train loss: 17.39699935913086, test loss: 14.055163383483887\n",
      "h: 110 | epoch: 9, train loss: 15.977699279785156, test loss: 12.840527534484863\n",
      "h: 110 | epoch: 10, train loss: 14.784266471862793, test loss: 11.808744430541992\n",
      "h: 110 | epoch: 11, train loss: 13.781079292297363, test loss: 10.931841850280762\n",
      "h: 110 | epoch: 12, train loss: 12.938085556030273, test loss: 10.186145782470703\n",
      "h: 110 | epoch: 13, train loss: 12.229947090148926, test loss: 9.551624298095703\n",
      "h: 110 | epoch: 14, train loss: 11.635284423828125, test loss: 9.011327743530273\n",
      "h: 110 | epoch: 15, train loss: 11.136078834533691, test loss: 8.550904273986816\n",
      "h: 110 | epoch: 16, train loss: 10.717138290405273, test loss: 8.15820598602295\n",
      "h: 110 | epoch: 17, train loss: 10.365667343139648, test loss: 7.822959899902344\n",
      "h: 110 | epoch: 18, train loss: 10.070882797241211, test loss: 7.536465644836426\n",
      "h: 110 | epoch: 19, train loss: 9.823713302612305, test loss: 7.291359901428223\n",
      "h: 110 | epoch: 20, train loss: 9.616519927978516, test loss: 7.081411838531494\n",
      "h: 110 | epoch: 21, train loss: 9.442880630493164, test loss: 6.9013471603393555\n",
      "h: 110 | epoch: 22, train loss: 9.297391891479492, test loss: 6.746701240539551\n",
      "h: 110 | epoch: 23, train loss: 9.175518035888672, test loss: 6.6136908531188965\n",
      "h: 110 | epoch: 24, train loss: 9.073445320129395, test loss: 6.4991135597229\n",
      "h: 110 | epoch: 25, train loss: 8.987970352172852, test loss: 6.400254726409912\n",
      "h: 110 | epoch: 26, train loss: 8.916407585144043, test loss: 6.314818382263184\n",
      "h: 110 | epoch: 27, train loss: 8.856501579284668, test loss: 6.240850448608398\n",
      "h: 110 | epoch: 28, train loss: 8.806360244750977, test loss: 6.176695823669434\n",
      "h: 110 | epoch: 29, train loss: 8.764395713806152, test loss: 6.120950222015381\n",
      "h: 110 | epoch: 30, train loss: 8.729279518127441, test loss: 6.072419166564941\n",
      "h: 110 | epoch: 31, train loss: 8.699897766113281, test loss: 6.030087947845459\n",
      "h: 110 | epoch: 32, train loss: 8.675313949584961, test loss: 5.9930925369262695\n",
      "h: 110 | epoch: 33, train loss: 8.654748916625977, test loss: 5.960694313049316\n",
      "h: 110 | epoch: 34, train loss: 8.637544631958008, test loss: 5.932268142700195\n",
      "h: 110 | epoch: 35, train loss: 8.623153686523438, test loss: 5.9072771072387695\n",
      "h: 110 | epoch: 36, train loss: 8.611116409301758, test loss: 5.885262966156006\n",
      "h: 110 | epoch: 37, train loss: 8.60104751586914, test loss: 5.86583137512207\n",
      "h: 110 | epoch: 38, train loss: 8.59262752532959, test loss: 5.848647117614746\n",
      "h: 110 | epoch: 39, train loss: 8.58558464050293, test loss: 5.833424091339111\n",
      "h: 110 | epoch: 40, train loss: 8.579693794250488, test loss: 5.81990909576416\n",
      "h: 110 | epoch: 41, train loss: 8.574767112731934, test loss: 5.8078932762146\n",
      "h: 110 | epoch: 42, train loss: 8.570647239685059, test loss: 5.797187805175781\n",
      "h: 110 | epoch: 43, train loss: 8.567201614379883, test loss: 5.787635326385498\n",
      "h: 110 | epoch: 44, train loss: 8.564319610595703, test loss: 5.779097557067871\n",
      "h: 110 | epoch: 45, train loss: 8.561910629272461, test loss: 5.771454334259033\n",
      "h: 110 | epoch: 46, train loss: 8.559893608093262, test loss: 5.764603137969971\n",
      "h: 110 | epoch: 47, train loss: 8.558208465576172, test loss: 5.758450984954834\n",
      "h: 110 | epoch: 48, train loss: 8.55679702758789, test loss: 5.752918243408203\n",
      "h: 110 | epoch: 49, train loss: 8.555617332458496, test loss: 5.747939109802246\n",
      "h: 110 | epoch: 50, train loss: 8.554631233215332, test loss: 5.743451118469238\n",
      "h: 110 | epoch: 51, train loss: 8.55380630493164, test loss: 5.739400386810303\n",
      "h: 110 | epoch: 52, train loss: 8.55311393737793, test loss: 5.735741138458252\n",
      "h: 110 | epoch: 53, train loss: 8.55253791809082, test loss: 5.7324323654174805\n",
      "h: 110 | epoch: 54, train loss: 8.552053451538086, test loss: 5.729435443878174\n",
      "h: 110 | epoch: 55, train loss: 8.551650047302246, test loss: 5.726721286773682\n",
      "h: 110 | epoch: 56, train loss: 8.551311492919922, test loss: 5.724259853363037\n",
      "h: 110 | epoch: 57, train loss: 8.551029205322266, test loss: 5.722025394439697\n",
      "h: 110 | epoch: 58, train loss: 8.550790786743164, test loss: 5.719996452331543\n",
      "h: 110 | epoch: 59, train loss: 8.550594329833984, test loss: 5.718152046203613\n",
      "h: 110 | epoch: 60, train loss: 8.55042839050293, test loss: 5.716474533081055\n",
      "h: 110 | epoch: 61, train loss: 8.550288200378418, test loss: 5.7149481773376465\n",
      "h: 110 | epoch: 62, train loss: 8.550172805786133, test loss: 5.713557720184326\n",
      "h: 110 | epoch: 63, train loss: 8.550074577331543, test loss: 5.712291240692139\n",
      "h: 110 | epoch: 64, train loss: 8.549993515014648, test loss: 5.711136817932129\n",
      "h: 110 | epoch: 65, train loss: 8.549924850463867, test loss: 5.710084438323975\n",
      "h: 110 | epoch: 66, train loss: 8.549867630004883, test loss: 5.7091240882873535\n",
      "h: 110 | epoch: 67, train loss: 8.549820899963379, test loss: 5.708247184753418\n",
      "h: 110 | epoch: 68, train loss: 8.549779891967773, test loss: 5.707446098327637\n",
      "h: 110 | epoch: 69, train loss: 8.549745559692383, test loss: 5.706715106964111\n",
      "h: 110 | epoch: 70, train loss: 8.549718856811523, test loss: 5.706047534942627\n",
      "h: 110 | epoch: 71, train loss: 8.54969596862793, test loss: 5.705436706542969\n",
      "h: 110 | epoch: 72, train loss: 8.549674987792969, test loss: 5.704878330230713\n",
      "h: 110 | epoch: 73, train loss: 8.54965877532959, test loss: 5.7043681144714355\n",
      "h: 110 | epoch: 74, train loss: 8.54964542388916, test loss: 5.703901767730713\n",
      "h: 110 | epoch: 75, train loss: 8.549633026123047, test loss: 5.7034735679626465\n",
      "h: 110 | epoch: 76, train loss: 8.549623489379883, test loss: 5.703083038330078\n",
      "h: 110 | epoch: 77, train loss: 8.549614906311035, test loss: 5.7027268409729\n",
      "h: 110 | epoch: 78, train loss: 8.549609184265137, test loss: 5.70239782333374\n",
      "h: 110 | epoch: 79, train loss: 8.549602508544922, test loss: 5.702099323272705\n",
      "h: 110 | epoch: 80, train loss: 8.549598693847656, test loss: 5.701825141906738\n",
      "h: 110 | epoch: 81, train loss: 8.549593925476074, test loss: 5.701572895050049\n",
      "h: 110 | epoch: 82, train loss: 8.549590110778809, test loss: 5.7013421058654785\n",
      "h: 110 | epoch: 83, train loss: 8.549588203430176, test loss: 5.7011308670043945\n",
      "h: 110 | epoch: 84, train loss: 8.549585342407227, test loss: 5.7009382247924805\n",
      "h: 110 | epoch: 85, train loss: 8.549583435058594, test loss: 5.700760841369629\n",
      "h: 110 | epoch: 86, train loss: 8.549581527709961, test loss: 5.70059871673584\n",
      "h: 110 | epoch: 87, train loss: 8.549581527709961, test loss: 5.700448989868164\n",
      "h: 110 | epoch: 88, train loss: 8.549578666687012, test loss: 5.700313568115234\n",
      "h: 110 | epoch: 89, train loss: 8.549577713012695, test loss: 5.700187683105469\n",
      "h: 110 | epoch: 90, train loss: 8.549576759338379, test loss: 5.700072288513184\n",
      "h: 110 | epoch: 91, train loss: 8.549577713012695, test loss: 5.699967384338379\n",
      "h: 110 | epoch: 92, train loss: 8.549575805664062, test loss: 5.699871063232422\n",
      "h: 110 | epoch: 93, train loss: 8.54957389831543, test loss: 5.699782848358154\n",
      "h: 110 | epoch: 94, train loss: 8.549575805664062, test loss: 5.699701309204102\n",
      "h: 110 | epoch: 95, train loss: 8.54957389831543, test loss: 5.699626922607422\n",
      "h: 110 | epoch: 96, train loss: 8.54957389831543, test loss: 5.699558258056641\n",
      "h: 110 | epoch: 97, train loss: 8.54957389831543, test loss: 5.699495792388916\n",
      "h: 110 | epoch: 98, train loss: 8.54957389831543, test loss: 5.699437141418457\n",
      "h: 110 | epoch: 99, train loss: 8.54957389831543, test loss: 5.699386119842529\n",
      "h: 111 | epoch: 0, train loss: 41.38035583496094, test loss: 33.51959991455078\n",
      "h: 111 | epoch: 1, train loss: 36.58893966674805, test loss: 29.709014892578125\n",
      "h: 111 | epoch: 2, train loss: 32.49964141845703, test loss: 26.435693740844727\n",
      "h: 111 | epoch: 3, train loss: 29.00775718688965, test loss: 23.621728897094727\n",
      "h: 111 | epoch: 4, train loss: 26.02496337890625, test loss: 21.20105743408203\n",
      "h: 111 | epoch: 5, train loss: 23.476469039916992, test loss: 19.117473602294922\n",
      "h: 111 | epoch: 6, train loss: 21.29877471923828, test loss: 17.323074340820312\n",
      "h: 111 | epoch: 7, train loss: 19.437847137451172, test loss: 15.776922225952148\n",
      "h: 111 | epoch: 8, train loss: 17.847646713256836, test loss: 14.444005966186523\n",
      "h: 111 | epoch: 9, train loss: 16.488872528076172, test loss: 13.294332504272461\n",
      "h: 111 | epoch: 10, train loss: 15.327960014343262, test loss: 12.3021879196167\n",
      "h: 111 | epoch: 11, train loss: 14.336214065551758, test loss: 11.44551944732666\n",
      "h: 111 | epoch: 12, train loss: 13.489099502563477, test loss: 10.70539379119873\n",
      "h: 111 | epoch: 13, train loss: 12.765625, test loss: 10.065556526184082\n",
      "h: 111 | epoch: 14, train loss: 12.147834777832031, test loss: 9.512044906616211\n",
      "h: 111 | epoch: 15, train loss: 11.62036418914795, test loss: 9.032865524291992\n",
      "h: 111 | epoch: 16, train loss: 11.17006778717041, test loss: 8.617712020874023\n",
      "h: 111 | epoch: 17, train loss: 10.785703659057617, test loss: 8.25772762298584\n",
      "h: 111 | epoch: 18, train loss: 10.457656860351562, test loss: 7.945301055908203\n",
      "h: 111 | epoch: 19, train loss: 10.177701950073242, test loss: 7.673890590667725\n",
      "h: 111 | epoch: 20, train loss: 9.938812255859375, test loss: 7.437867164611816\n",
      "h: 111 | epoch: 21, train loss: 9.734976768493652, test loss: 7.23239803314209\n",
      "h: 111 | epoch: 22, train loss: 9.561067581176758, test loss: 7.053323268890381\n",
      "h: 111 | epoch: 23, train loss: 9.412691116333008, test loss: 6.897065162658691\n",
      "h: 111 | epoch: 24, train loss: 9.28610897064209, test loss: 6.760540008544922\n",
      "h: 111 | epoch: 25, train loss: 9.178121566772461, test loss: 6.641103267669678\n",
      "h: 111 | epoch: 26, train loss: 9.085996627807617, test loss: 6.536473274230957\n",
      "h: 111 | epoch: 27, train loss: 9.007402420043945, test loss: 6.444683074951172\n",
      "h: 111 | epoch: 28, train loss: 8.940353393554688, test loss: 6.364039421081543\n",
      "h: 111 | epoch: 29, train loss: 8.883150100708008, test loss: 6.293084144592285\n",
      "h: 111 | epoch: 30, train loss: 8.834345817565918, test loss: 6.230554103851318\n",
      "h: 111 | epoch: 31, train loss: 8.792705535888672, test loss: 6.175366401672363\n",
      "h: 111 | epoch: 32, train loss: 8.757174491882324, test loss: 6.126576900482178\n",
      "h: 111 | epoch: 33, train loss: 8.72685432434082, test loss: 6.083376884460449\n",
      "h: 111 | epoch: 34, train loss: 8.700979232788086, test loss: 6.045063018798828\n",
      "h: 111 | epoch: 35, train loss: 8.6788969039917, test loss: 6.011026859283447\n",
      "h: 111 | epoch: 36, train loss: 8.66004753112793, test loss: 5.98074197769165\n",
      "h: 111 | epoch: 37, train loss: 8.643957138061523, test loss: 5.95374870300293\n",
      "h: 111 | epoch: 38, train loss: 8.630220413208008, test loss: 5.929652214050293\n",
      "h: 111 | epoch: 39, train loss: 8.61849308013916, test loss: 5.908105373382568\n",
      "h: 111 | epoch: 40, train loss: 8.608477592468262, test loss: 5.88880729675293\n",
      "h: 111 | epoch: 41, train loss: 8.59992504119873, test loss: 5.871496677398682\n",
      "h: 111 | epoch: 42, train loss: 8.592619895935059, test loss: 5.855944633483887\n",
      "h: 111 | epoch: 43, train loss: 8.586380004882812, test loss: 5.841951847076416\n",
      "h: 111 | epoch: 44, train loss: 8.581048011779785, test loss: 5.829342365264893\n",
      "h: 111 | epoch: 45, train loss: 8.576494216918945, test loss: 5.817962646484375\n",
      "h: 111 | epoch: 46, train loss: 8.572600364685059, test loss: 5.80767822265625\n",
      "h: 111 | epoch: 47, train loss: 8.569272994995117, test loss: 5.798373222351074\n",
      "h: 111 | epoch: 48, train loss: 8.566429138183594, test loss: 5.789942264556885\n",
      "h: 111 | epoch: 49, train loss: 8.563998222351074, test loss: 5.782292366027832\n",
      "h: 111 | epoch: 50, train loss: 8.561919212341309, test loss: 5.7753424644470215\n",
      "h: 111 | epoch: 51, train loss: 8.560141563415527, test loss: 5.769023418426514\n",
      "h: 111 | epoch: 52, train loss: 8.558621406555176, test loss: 5.763270378112793\n",
      "h: 111 | epoch: 53, train loss: 8.557319641113281, test loss: 5.758026123046875\n",
      "h: 111 | epoch: 54, train loss: 8.556207656860352, test loss: 5.753242015838623\n",
      "h: 111 | epoch: 55, train loss: 8.555254936218262, test loss: 5.748871803283691\n",
      "h: 111 | epoch: 56, train loss: 8.55444049835205, test loss: 5.74487829208374\n",
      "h: 111 | epoch: 57, train loss: 8.553743362426758, test loss: 5.7412238121032715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 111 | epoch: 58, train loss: 8.553146362304688, test loss: 5.737879276275635\n",
      "h: 111 | epoch: 59, train loss: 8.55263614654541, test loss: 5.7348127365112305\n",
      "h: 111 | epoch: 60, train loss: 8.55219841003418, test loss: 5.732001304626465\n",
      "h: 111 | epoch: 61, train loss: 8.5518217086792, test loss: 5.729421138763428\n",
      "h: 111 | epoch: 62, train loss: 8.551501274108887, test loss: 5.727051258087158\n",
      "h: 111 | epoch: 63, train loss: 8.551225662231445, test loss: 5.724875450134277\n",
      "h: 111 | epoch: 64, train loss: 8.550990104675293, test loss: 5.722872734069824\n",
      "h: 111 | epoch: 65, train loss: 8.550787925720215, test loss: 5.72103214263916\n",
      "h: 111 | epoch: 66, train loss: 8.550615310668945, test loss: 5.71933650970459\n",
      "h: 111 | epoch: 67, train loss: 8.550467491149902, test loss: 5.717775344848633\n",
      "h: 111 | epoch: 68, train loss: 8.550339698791504, test loss: 5.716338157653809\n",
      "h: 111 | epoch: 69, train loss: 8.550230979919434, test loss: 5.715012073516846\n",
      "h: 111 | epoch: 70, train loss: 8.550138473510742, test loss: 5.713790416717529\n",
      "h: 111 | epoch: 71, train loss: 8.550058364868164, test loss: 5.712663650512695\n",
      "h: 111 | epoch: 72, train loss: 8.549989700317383, test loss: 5.711623191833496\n",
      "h: 111 | epoch: 73, train loss: 8.549930572509766, test loss: 5.710663795471191\n",
      "h: 111 | epoch: 74, train loss: 8.54987907409668, test loss: 5.70977783203125\n",
      "h: 111 | epoch: 75, train loss: 8.549835205078125, test loss: 5.708959102630615\n",
      "h: 111 | epoch: 76, train loss: 8.549798965454102, test loss: 5.708203315734863\n",
      "h: 111 | epoch: 77, train loss: 8.54976749420166, test loss: 5.707503795623779\n",
      "h: 111 | epoch: 78, train loss: 8.549739837646484, test loss: 5.706857681274414\n",
      "h: 111 | epoch: 79, train loss: 8.549715995788574, test loss: 5.70626163482666\n",
      "h: 111 | epoch: 80, train loss: 8.54969596862793, test loss: 5.705708980560303\n",
      "h: 111 | epoch: 81, train loss: 8.549678802490234, test loss: 5.705197811126709\n",
      "h: 111 | epoch: 82, train loss: 8.549663543701172, test loss: 5.704726219177246\n",
      "h: 111 | epoch: 83, train loss: 8.549650192260742, test loss: 5.704288005828857\n",
      "h: 111 | epoch: 84, train loss: 8.549639701843262, test loss: 5.703883647918701\n",
      "h: 111 | epoch: 85, train loss: 8.549630165100098, test loss: 5.70350980758667\n",
      "h: 111 | epoch: 86, train loss: 8.549623489379883, test loss: 5.703162670135498\n",
      "h: 111 | epoch: 87, train loss: 8.549615859985352, test loss: 5.702843189239502\n",
      "h: 111 | epoch: 88, train loss: 8.549609184265137, test loss: 5.702545642852783\n",
      "h: 111 | epoch: 89, train loss: 8.549604415893555, test loss: 5.702271461486816\n",
      "h: 111 | epoch: 90, train loss: 8.549600601196289, test loss: 5.702016830444336\n",
      "h: 111 | epoch: 91, train loss: 8.549596786499023, test loss: 5.701779365539551\n",
      "h: 111 | epoch: 92, train loss: 8.549592971801758, test loss: 5.70156192779541\n",
      "h: 111 | epoch: 93, train loss: 8.549590110778809, test loss: 5.701359748840332\n",
      "h: 111 | epoch: 94, train loss: 8.54958724975586, test loss: 5.701170921325684\n",
      "h: 111 | epoch: 95, train loss: 8.549585342407227, test loss: 5.700998783111572\n",
      "h: 111 | epoch: 96, train loss: 8.549583435058594, test loss: 5.700837135314941\n",
      "h: 111 | epoch: 97, train loss: 8.549581527709961, test loss: 5.700687885284424\n",
      "h: 111 | epoch: 98, train loss: 8.549581527709961, test loss: 5.700550079345703\n",
      "h: 111 | epoch: 99, train loss: 8.549579620361328, test loss: 5.7004218101501465\n",
      "h: 112 | epoch: 0, train loss: 44.44284439086914, test loss: 34.01594161987305\n",
      "h: 112 | epoch: 1, train loss: 37.833702087402344, test loss: 29.011789321899414\n",
      "h: 112 | epoch: 2, train loss: 32.44511795043945, test loss: 24.910011291503906\n",
      "h: 112 | epoch: 3, train loss: 28.04925537109375, test loss: 21.544879913330078\n",
      "h: 112 | epoch: 4, train loss: 24.462032318115234, test loss: 18.78194236755371\n",
      "h: 112 | epoch: 5, train loss: 21.534183502197266, test loss: 16.511798858642578\n",
      "h: 112 | epoch: 6, train loss: 19.144367218017578, test loss: 14.64525032043457\n",
      "h: 112 | epoch: 7, train loss: 17.193756103515625, test loss: 13.109476089477539\n",
      "h: 112 | epoch: 8, train loss: 15.601755142211914, test loss: 11.844934463500977\n",
      "h: 112 | epoch: 9, train loss: 14.302577018737793, test loss: 10.802909851074219\n",
      "h: 112 | epoch: 10, train loss: 13.242494583129883, test loss: 9.94353199005127\n",
      "h: 112 | epoch: 11, train loss: 12.37761402130127, test loss: 9.234136581420898\n",
      "h: 112 | epoch: 12, train loss: 11.672082901000977, test loss: 8.64796257019043\n",
      "h: 112 | epoch: 13, train loss: 11.09660530090332, test loss: 8.163087844848633\n",
      "h: 112 | epoch: 14, train loss: 10.627251625061035, test loss: 7.761526584625244\n",
      "h: 112 | epoch: 15, train loss: 10.244478225708008, test loss: 7.4285478591918945\n",
      "h: 112 | epoch: 16, train loss: 9.932332038879395, test loss: 7.152059078216553\n",
      "h: 112 | epoch: 17, train loss: 9.677785873413086, test loss: 6.9221391677856445\n",
      "h: 112 | epoch: 18, train loss: 9.470210075378418, test loss: 6.730648994445801\n",
      "h: 112 | epoch: 19, train loss: 9.300936698913574, test loss: 6.570903778076172\n",
      "h: 112 | epoch: 20, train loss: 9.162886619567871, test loss: 6.437410831451416\n",
      "h: 112 | epoch: 21, train loss: 9.050294876098633, test loss: 6.325652122497559\n",
      "h: 112 | epoch: 22, train loss: 8.958456039428711, test loss: 6.2319159507751465\n",
      "h: 112 | epoch: 23, train loss: 8.883537292480469, test loss: 6.15314245223999\n",
      "h: 112 | epoch: 24, train loss: 8.822410583496094, test loss: 6.08681058883667\n",
      "h: 112 | epoch: 25, train loss: 8.772529602050781, test loss: 6.0308427810668945\n",
      "h: 112 | epoch: 26, train loss: 8.731817245483398, test loss: 5.983517646789551\n",
      "h: 112 | epoch: 27, train loss: 8.69858169555664, test loss: 5.94342041015625\n",
      "h: 112 | epoch: 28, train loss: 8.671443939208984, test loss: 5.9093732833862305\n",
      "h: 112 | epoch: 29, train loss: 8.649280548095703, test loss: 5.880402565002441\n",
      "h: 112 | epoch: 30, train loss: 8.631172180175781, test loss: 5.855698108673096\n",
      "h: 112 | epoch: 31, train loss: 8.616376876831055, test loss: 5.834588050842285\n",
      "h: 112 | epoch: 32, train loss: 8.60428237915039, test loss: 5.816512107849121\n",
      "h: 112 | epoch: 33, train loss: 8.594392776489258, test loss: 5.80100154876709\n",
      "h: 112 | epoch: 34, train loss: 8.586305618286133, test loss: 5.7876667976379395\n",
      "h: 112 | epoch: 35, train loss: 8.579687118530273, test loss: 5.776180267333984\n",
      "h: 112 | epoch: 36, train loss: 8.574271202087402, test loss: 5.7662672996521\n",
      "h: 112 | epoch: 37, train loss: 8.56983757019043, test loss: 5.75769567489624\n",
      "h: 112 | epoch: 38, train loss: 8.566205978393555, test loss: 5.750271797180176\n",
      "h: 112 | epoch: 39, train loss: 8.563230514526367, test loss: 5.743832588195801\n",
      "h: 112 | epoch: 40, train loss: 8.560791015625, test loss: 5.7382354736328125\n",
      "h: 112 | epoch: 41, train loss: 8.558792114257812, test loss: 5.733365058898926\n",
      "h: 112 | epoch: 42, train loss: 8.55715274810791, test loss: 5.729121208190918\n",
      "h: 112 | epoch: 43, train loss: 8.555807113647461, test loss: 5.7254180908203125\n",
      "h: 112 | epoch: 44, train loss: 8.554701805114746, test loss: 5.72218132019043\n",
      "h: 112 | epoch: 45, train loss: 8.553794860839844, test loss: 5.7193498611450195\n",
      "h: 112 | epoch: 46, train loss: 8.553049087524414, test loss: 5.7168731689453125\n",
      "h: 112 | epoch: 47, train loss: 8.552434921264648, test loss: 5.714699745178223\n",
      "h: 112 | epoch: 48, train loss: 8.551933288574219, test loss: 5.71279239654541\n",
      "h: 112 | epoch: 49, train loss: 8.551518440246582, test loss: 5.711119174957275\n",
      "h: 112 | epoch: 50, train loss: 8.551177978515625, test loss: 5.709647178649902\n",
      "h: 112 | epoch: 51, train loss: 8.550897598266602, test loss: 5.708353519439697\n",
      "h: 112 | epoch: 52, train loss: 8.550665855407715, test loss: 5.707215309143066\n",
      "h: 112 | epoch: 53, train loss: 8.55047607421875, test loss: 5.706213474273682\n",
      "h: 112 | epoch: 54, train loss: 8.550318717956543, test loss: 5.705329418182373\n",
      "h: 112 | epoch: 55, train loss: 8.550189018249512, test loss: 5.704551696777344\n",
      "h: 112 | epoch: 56, train loss: 8.550081253051758, test loss: 5.703866004943848\n",
      "h: 112 | epoch: 57, train loss: 8.549994468688965, test loss: 5.703261375427246\n",
      "h: 112 | epoch: 58, train loss: 8.549921989440918, test loss: 5.702728271484375\n",
      "h: 112 | epoch: 59, train loss: 8.549860954284668, test loss: 5.70225715637207\n",
      "h: 112 | epoch: 60, train loss: 8.549811363220215, test loss: 5.701842308044434\n",
      "h: 112 | epoch: 61, train loss: 8.549771308898926, test loss: 5.701475143432617\n",
      "h: 112 | epoch: 62, train loss: 8.549736976623535, test loss: 5.7011518478393555\n",
      "h: 112 | epoch: 63, train loss: 8.54970932006836, test loss: 5.70086669921875\n",
      "h: 112 | epoch: 64, train loss: 8.549684524536133, test loss: 5.700614929199219\n",
      "h: 112 | epoch: 65, train loss: 8.549666404724121, test loss: 5.700393199920654\n",
      "h: 112 | epoch: 66, train loss: 8.549650192260742, test loss: 5.700196266174316\n",
      "h: 112 | epoch: 67, train loss: 8.549637794494629, test loss: 5.700024604797363\n",
      "h: 112 | epoch: 68, train loss: 8.549626350402832, test loss: 5.699871063232422\n",
      "h: 112 | epoch: 69, train loss: 8.549616813659668, test loss: 5.699736595153809\n",
      "h: 112 | epoch: 70, train loss: 8.549609184265137, test loss: 5.699617385864258\n",
      "h: 112 | epoch: 71, train loss: 8.549603462219238, test loss: 5.699513912200928\n",
      "h: 112 | epoch: 72, train loss: 8.54959774017334, test loss: 5.6994218826293945\n",
      "h: 112 | epoch: 73, train loss: 8.549593925476074, test loss: 5.6993408203125\n",
      "h: 112 | epoch: 74, train loss: 8.549590110778809, test loss: 5.6992692947387695\n",
      "h: 112 | epoch: 75, train loss: 8.549588203430176, test loss: 5.699207305908203\n",
      "h: 112 | epoch: 76, train loss: 8.549585342407227, test loss: 5.699151515960693\n",
      "h: 112 | epoch: 77, train loss: 8.549582481384277, test loss: 5.699103832244873\n",
      "h: 112 | epoch: 78, train loss: 8.549581527709961, test loss: 5.699061393737793\n",
      "h: 112 | epoch: 79, train loss: 8.549580574035645, test loss: 5.699024677276611\n",
      "h: 112 | epoch: 80, train loss: 8.549578666687012, test loss: 5.698991298675537\n",
      "h: 112 | epoch: 81, train loss: 8.549577713012695, test loss: 5.6989641189575195\n",
      "h: 112 | epoch: 82, train loss: 8.549576759338379, test loss: 5.698937892913818\n",
      "h: 112 | epoch: 83, train loss: 8.549575805664062, test loss: 5.698916435241699\n",
      "h: 112 | epoch: 84, train loss: 8.549574851989746, test loss: 5.698897361755371\n",
      "h: 112 | epoch: 85, train loss: 8.549574851989746, test loss: 5.69888162612915\n",
      "h: 112 | epoch: 86, train loss: 8.549574851989746, test loss: 5.698866844177246\n",
      "h: 112 | epoch: 87, train loss: 8.54957389831543, test loss: 5.698853969573975\n",
      "h: 112 | epoch: 88, train loss: 8.54957389831543, test loss: 5.698843955993652\n",
      "h: 112 | epoch: 89, train loss: 8.54957389831543, test loss: 5.69883394241333\n",
      "h: 112 | epoch: 90, train loss: 8.54957389831543, test loss: 5.698826313018799\n",
      "h: 112 | epoch: 91, train loss: 8.54957389831543, test loss: 5.698819160461426\n",
      "h: 112 | epoch: 92, train loss: 8.54957389831543, test loss: 5.698813438415527\n",
      "h: 112 | epoch: 93, train loss: 8.549572944641113, test loss: 5.698807716369629\n",
      "h: 112 | epoch: 94, train loss: 8.54957389831543, test loss: 5.698803424835205\n",
      "h: 112 | epoch: 95, train loss: 8.54957389831543, test loss: 5.6987996101379395\n",
      "h: 112 | epoch: 96, train loss: 8.549572944641113, test loss: 5.698796272277832\n",
      "h: 112 | epoch: 97, train loss: 8.54957389831543, test loss: 5.698794364929199\n",
      "h: 112 | epoch: 98, train loss: 8.549572944641113, test loss: 5.698791027069092\n",
      "h: 112 | epoch: 99, train loss: 8.549572944641113, test loss: 5.698790550231934\n",
      "h: 113 | epoch: 0, train loss: 37.33470916748047, test loss: 30.35711669921875\n",
      "h: 113 | epoch: 1, train loss: 33.16245651245117, test loss: 26.981943130493164\n",
      "h: 113 | epoch: 2, train loss: 29.592172622680664, test loss: 24.077606201171875\n",
      "h: 113 | epoch: 3, train loss: 26.536426544189453, test loss: 21.577335357666016\n",
      "h: 113 | epoch: 4, train loss: 23.920928955078125, test loss: 19.424118041992188\n",
      "h: 113 | epoch: 5, train loss: 21.68238067626953, test loss: 17.569204330444336\n",
      "h: 113 | epoch: 6, train loss: 19.766714096069336, test loss: 15.970817565917969\n",
      "h: 113 | epoch: 7, train loss: 18.127695083618164, test loss: 14.593119621276855\n",
      "h: 113 | epoch: 8, train loss: 16.725723266601562, test loss: 13.405313491821289\n",
      "h: 113 | epoch: 9, train loss: 15.526860237121582, test loss: 12.380934715270996\n",
      "h: 113 | epoch: 10, train loss: 14.50200366973877, test loss: 11.497227668762207\n",
      "h: 113 | epoch: 11, train loss: 13.626190185546875, test loss: 10.734611511230469\n",
      "h: 113 | epoch: 12, train loss: 12.87799072265625, test loss: 10.076242446899414\n",
      "h: 113 | epoch: 13, train loss: 12.239034652709961, test loss: 9.507623672485352\n",
      "h: 113 | epoch: 14, train loss: 11.693554878234863, test loss: 9.016280174255371\n",
      "h: 113 | epoch: 15, train loss: 11.228033065795898, test loss: 8.591480255126953\n",
      "h: 113 | epoch: 16, train loss: 10.830881118774414, test loss: 8.223989486694336\n",
      "h: 113 | epoch: 17, train loss: 10.49216079711914, test loss: 7.905864715576172\n",
      "h: 113 | epoch: 18, train loss: 10.203367233276367, test loss: 7.630270481109619\n",
      "h: 113 | epoch: 19, train loss: 9.957212448120117, test loss: 7.391331672668457\n",
      "h: 113 | epoch: 20, train loss: 9.747459411621094, test loss: 7.183989524841309\n",
      "h: 113 | epoch: 21, train loss: 9.56877326965332, test loss: 7.003901481628418\n",
      "h: 113 | epoch: 22, train loss: 9.416589736938477, test loss: 6.847324371337891\n",
      "h: 113 | epoch: 23, train loss: 9.287009239196777, test loss: 6.711042881011963\n",
      "h: 113 | epoch: 24, train loss: 9.176700592041016, test loss: 6.592292785644531\n",
      "h: 113 | epoch: 25, train loss: 9.082816123962402, test loss: 6.488694190979004\n",
      "h: 113 | epoch: 26, train loss: 9.002927780151367, test loss: 6.398202419281006\n",
      "h: 113 | epoch: 27, train loss: 8.934959411621094, test loss: 6.319052696228027\n",
      "h: 113 | epoch: 28, train loss: 8.877142906188965, test loss: 6.249730110168457\n",
      "h: 113 | epoch: 29, train loss: 8.827971458435059, test loss: 6.188925743103027\n",
      "h: 113 | epoch: 30, train loss: 8.786158561706543, test loss: 6.135519981384277\n",
      "h: 113 | epoch: 31, train loss: 8.75060749053955, test loss: 6.088537693023682\n",
      "h: 113 | epoch: 32, train loss: 8.720382690429688, test loss: 6.047144889831543\n",
      "h: 113 | epoch: 33, train loss: 8.694690704345703, test loss: 6.010621070861816\n",
      "h: 113 | epoch: 34, train loss: 8.67285442352295, test loss: 5.9783406257629395\n",
      "h: 113 | epoch: 35, train loss: 8.654296875, test loss: 5.949766635894775\n",
      "h: 113 | epoch: 36, train loss: 8.638527870178223, test loss: 5.924431800842285\n",
      "h: 113 | epoch: 37, train loss: 8.625127792358398, test loss: 5.9019317626953125\n",
      "h: 113 | epoch: 38, train loss: 8.613744735717773, test loss: 5.881918907165527\n",
      "h: 113 | epoch: 39, train loss: 8.604072570800781, test loss: 5.864090919494629\n",
      "h: 113 | epoch: 40, train loss: 8.595856666564941, test loss: 5.84818172454834\n",
      "h: 113 | epoch: 41, train loss: 8.588876724243164, test loss: 5.8339643478393555\n",
      "h: 113 | epoch: 42, train loss: 8.5829496383667, test loss: 5.8212385177612305\n",
      "h: 113 | epoch: 43, train loss: 8.577914237976074, test loss: 5.8098320960998535\n",
      "h: 113 | epoch: 44, train loss: 8.573638916015625, test loss: 5.799591064453125\n",
      "h: 113 | epoch: 45, train loss: 8.57000732421875, test loss: 5.7903852462768555\n",
      "h: 113 | epoch: 46, train loss: 8.566922187805176, test loss: 5.782097816467285\n",
      "h: 113 | epoch: 47, train loss: 8.56430435180664, test loss: 5.774627208709717\n",
      "h: 113 | epoch: 48, train loss: 8.562081336975098, test loss: 5.767884254455566\n",
      "h: 113 | epoch: 49, train loss: 8.560192108154297, test loss: 5.761789321899414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 113 | epoch: 50, train loss: 8.558588027954102, test loss: 5.756275177001953\n",
      "h: 113 | epoch: 51, train loss: 8.557226181030273, test loss: 5.751279830932617\n",
      "h: 113 | epoch: 52, train loss: 8.556070327758789, test loss: 5.746750831604004\n",
      "h: 113 | epoch: 53, train loss: 8.555089950561523, test loss: 5.74263858795166\n",
      "h: 113 | epoch: 54, train loss: 8.554255485534668, test loss: 5.7389020919799805\n",
      "h: 113 | epoch: 55, train loss: 8.553547859191895, test loss: 5.735504150390625\n",
      "h: 113 | epoch: 56, train loss: 8.552947998046875, test loss: 5.732412338256836\n",
      "h: 113 | epoch: 57, train loss: 8.552438735961914, test loss: 5.729592323303223\n",
      "h: 113 | epoch: 58, train loss: 8.55200481414795, test loss: 5.727022647857666\n",
      "h: 113 | epoch: 59, train loss: 8.551637649536133, test loss: 5.7246785163879395\n",
      "h: 113 | epoch: 60, train loss: 8.551325798034668, test loss: 5.722536087036133\n",
      "h: 113 | epoch: 61, train loss: 8.551061630249023, test loss: 5.720580101013184\n",
      "h: 113 | epoch: 62, train loss: 8.550835609436035, test loss: 5.7187910079956055\n",
      "h: 113 | epoch: 63, train loss: 8.55064582824707, test loss: 5.717154502868652\n",
      "h: 113 | epoch: 64, train loss: 8.550481796264648, test loss: 5.7156572341918945\n",
      "h: 113 | epoch: 65, train loss: 8.550345420837402, test loss: 5.714285373687744\n",
      "h: 113 | epoch: 66, train loss: 8.5502290725708, test loss: 5.7130279541015625\n",
      "h: 113 | epoch: 67, train loss: 8.550128936767578, test loss: 5.711875915527344\n",
      "h: 113 | epoch: 68, train loss: 8.55004596710205, test loss: 5.71082067489624\n",
      "h: 113 | epoch: 69, train loss: 8.54997444152832, test loss: 5.709851264953613\n",
      "h: 113 | epoch: 70, train loss: 8.54991340637207, test loss: 5.708963871002197\n",
      "h: 113 | epoch: 71, train loss: 8.549860954284668, test loss: 5.708146095275879\n",
      "h: 113 | epoch: 72, train loss: 8.54981803894043, test loss: 5.7073974609375\n",
      "h: 113 | epoch: 73, train loss: 8.54978084564209, test loss: 5.706707954406738\n",
      "h: 113 | epoch: 74, train loss: 8.549749374389648, test loss: 5.706076622009277\n",
      "h: 113 | epoch: 75, train loss: 8.549722671508789, test loss: 5.705495357513428\n",
      "h: 113 | epoch: 76, train loss: 8.549699783325195, test loss: 5.704960823059082\n",
      "h: 113 | epoch: 77, train loss: 8.549680709838867, test loss: 5.704470634460449\n",
      "h: 113 | epoch: 78, train loss: 8.549664497375488, test loss: 5.704019546508789\n",
      "h: 113 | epoch: 79, train loss: 8.549650192260742, test loss: 5.703603744506836\n",
      "h: 113 | epoch: 80, train loss: 8.549638748168945, test loss: 5.703222274780273\n",
      "h: 113 | epoch: 81, train loss: 8.549628257751465, test loss: 5.7028703689575195\n",
      "h: 113 | epoch: 82, train loss: 8.549619674682617, test loss: 5.702547550201416\n",
      "h: 113 | epoch: 83, train loss: 8.549612998962402, test loss: 5.702250003814697\n",
      "h: 113 | epoch: 84, train loss: 8.549607276916504, test loss: 5.701977252960205\n",
      "h: 113 | epoch: 85, train loss: 8.549601554870605, test loss: 5.701724052429199\n",
      "h: 113 | epoch: 86, train loss: 8.549596786499023, test loss: 5.7014923095703125\n",
      "h: 113 | epoch: 87, train loss: 8.549592971801758, test loss: 5.701279640197754\n",
      "h: 113 | epoch: 88, train loss: 8.549591064453125, test loss: 5.701082706451416\n",
      "h: 113 | epoch: 89, train loss: 8.549588203430176, test loss: 5.700902938842773\n",
      "h: 113 | epoch: 90, train loss: 8.549585342407227, test loss: 5.700736045837402\n",
      "h: 113 | epoch: 91, train loss: 8.549583435058594, test loss: 5.700582027435303\n",
      "h: 113 | epoch: 92, train loss: 8.549582481384277, test loss: 5.700440406799316\n",
      "h: 113 | epoch: 93, train loss: 8.549579620361328, test loss: 5.700310707092285\n",
      "h: 113 | epoch: 94, train loss: 8.549579620361328, test loss: 5.700190544128418\n",
      "h: 113 | epoch: 95, train loss: 8.549577713012695, test loss: 5.700079917907715\n",
      "h: 113 | epoch: 96, train loss: 8.549576759338379, test loss: 5.699978828430176\n",
      "h: 113 | epoch: 97, train loss: 8.549576759338379, test loss: 5.699885845184326\n",
      "h: 113 | epoch: 98, train loss: 8.549575805664062, test loss: 5.699798107147217\n",
      "h: 113 | epoch: 99, train loss: 8.549575805664062, test loss: 5.699719429016113\n",
      "h: 114 | epoch: 0, train loss: 48.33345031738281, test loss: 38.22003936767578\n",
      "h: 114 | epoch: 1, train loss: 42.12028884887695, test loss: 33.382266998291016\n",
      "h: 114 | epoch: 2, train loss: 36.877891540527344, test loss: 29.278392791748047\n",
      "h: 114 | epoch: 3, train loss: 32.45219802856445, test loss: 25.794612884521484\n",
      "h: 114 | epoch: 4, train loss: 28.714794158935547, test loss: 22.835569381713867\n",
      "h: 114 | epoch: 5, train loss: 25.558195114135742, test loss: 20.321063995361328\n",
      "h: 114 | epoch: 6, train loss: 22.892139434814453, test loss: 18.183460235595703\n",
      "h: 114 | epoch: 7, train loss: 20.640628814697266, test loss: 16.365629196166992\n",
      "h: 114 | epoch: 8, train loss: 18.739561080932617, test loss: 14.81921100616455\n",
      "h: 114 | epoch: 9, train loss: 17.134790420532227, test loss: 13.5032377243042\n",
      "h: 114 | epoch: 10, train loss: 15.780525207519531, test loss: 12.382970809936523\n",
      "h: 114 | epoch: 11, train loss: 14.638031005859375, test loss: 11.428934097290039\n",
      "h: 114 | epoch: 12, train loss: 13.674519538879395, test loss: 10.616104125976562\n",
      "h: 114 | epoch: 13, train loss: 12.86223030090332, test loss: 9.923248291015625\n",
      "h: 114 | epoch: 14, train loss: 12.177674293518066, test loss: 9.332328796386719\n",
      "h: 114 | epoch: 15, train loss: 11.600964546203613, test loss: 8.828042984008789\n",
      "h: 114 | epoch: 16, train loss: 11.115272521972656, test loss: 8.39738655090332\n",
      "h: 114 | epoch: 17, train loss: 10.706375122070312, test loss: 8.029327392578125\n",
      "h: 114 | epoch: 18, train loss: 10.362234115600586, test loss: 7.714498996734619\n",
      "h: 114 | epoch: 19, train loss: 10.072685241699219, test loss: 7.444952487945557\n",
      "h: 114 | epoch: 20, train loss: 9.829137802124023, test loss: 7.213940620422363\n",
      "h: 114 | epoch: 21, train loss: 9.624338150024414, test loss: 7.015730381011963\n",
      "h: 114 | epoch: 22, train loss: 9.452165603637695, test loss: 6.845466613769531\n",
      "h: 114 | epoch: 23, train loss: 9.30745792388916, test loss: 6.699019432067871\n",
      "h: 114 | epoch: 24, train loss: 9.185861587524414, test loss: 6.5728912353515625\n",
      "h: 114 | epoch: 25, train loss: 9.083703994750977, test loss: 6.4641008377075195\n",
      "h: 114 | epoch: 26, train loss: 8.997895240783691, test loss: 6.370125770568848\n",
      "h: 114 | epoch: 27, train loss: 8.92583179473877, test loss: 6.288818836212158\n",
      "h: 114 | epoch: 28, train loss: 8.86532211303711, test loss: 6.218355655670166\n",
      "h: 114 | epoch: 29, train loss: 8.814519882202148, test loss: 6.157186985015869\n",
      "h: 114 | epoch: 30, train loss: 8.771875381469727, test loss: 6.1039910316467285\n",
      "h: 114 | epoch: 31, train loss: 8.73608112335205, test loss: 6.057643890380859\n",
      "h: 114 | epoch: 32, train loss: 8.706042289733887, test loss: 6.0171918869018555\n",
      "h: 114 | epoch: 33, train loss: 8.68083381652832, test loss: 5.981814384460449\n",
      "h: 114 | epoch: 34, train loss: 8.659682273864746, test loss: 5.9508161544799805\n",
      "h: 114 | epoch: 35, train loss: 8.641935348510742, test loss: 5.923604488372803\n",
      "h: 114 | epoch: 36, train loss: 8.627046585083008, test loss: 5.899670600891113\n",
      "h: 114 | epoch: 37, train loss: 8.614555358886719, test loss: 5.878575801849365\n",
      "h: 114 | epoch: 38, train loss: 8.60407829284668, test loss: 5.859950065612793\n",
      "h: 114 | epoch: 39, train loss: 8.595288276672363, test loss: 5.843473434448242\n",
      "h: 114 | epoch: 40, train loss: 8.587915420532227, test loss: 5.828867435455322\n",
      "h: 114 | epoch: 41, train loss: 8.581733703613281, test loss: 5.815899848937988\n",
      "h: 114 | epoch: 42, train loss: 8.576547622680664, test loss: 5.8043622970581055\n",
      "h: 114 | epoch: 43, train loss: 8.572197914123535, test loss: 5.79408073425293\n",
      "h: 114 | epoch: 44, train loss: 8.568549156188965, test loss: 5.7849016189575195\n",
      "h: 114 | epoch: 45, train loss: 8.565488815307617, test loss: 5.776694297790527\n",
      "h: 114 | epoch: 46, train loss: 8.562922477722168, test loss: 5.769344329833984\n",
      "h: 114 | epoch: 47, train loss: 8.560770988464355, test loss: 5.762750625610352\n",
      "h: 114 | epoch: 48, train loss: 8.558965682983398, test loss: 5.756827354431152\n",
      "h: 114 | epoch: 49, train loss: 8.557451248168945, test loss: 5.751498222351074\n",
      "h: 114 | epoch: 50, train loss: 8.556181907653809, test loss: 5.746699333190918\n",
      "h: 114 | epoch: 51, train loss: 8.5551176071167, test loss: 5.742369651794434\n",
      "h: 114 | epoch: 52, train loss: 8.554224014282227, test loss: 5.738460063934326\n",
      "h: 114 | epoch: 53, train loss: 8.553475379943848, test loss: 5.734924793243408\n",
      "h: 114 | epoch: 54, train loss: 8.552846908569336, test loss: 5.731725215911865\n",
      "h: 114 | epoch: 55, train loss: 8.552319526672363, test loss: 5.728826999664307\n",
      "h: 114 | epoch: 56, train loss: 8.551877975463867, test loss: 5.726198196411133\n",
      "h: 114 | epoch: 57, train loss: 8.551506996154785, test loss: 5.723811626434326\n",
      "h: 114 | epoch: 58, train loss: 8.551196098327637, test loss: 5.721644878387451\n",
      "h: 114 | epoch: 59, train loss: 8.550934791564941, test loss: 5.719674110412598\n",
      "h: 114 | epoch: 60, train loss: 8.550714492797852, test loss: 5.717881679534912\n",
      "h: 114 | epoch: 61, train loss: 8.550531387329102, test loss: 5.716248512268066\n",
      "h: 114 | epoch: 62, train loss: 8.550376892089844, test loss: 5.714762210845947\n",
      "h: 114 | epoch: 63, train loss: 8.550247192382812, test loss: 5.713405132293701\n",
      "h: 114 | epoch: 64, train loss: 8.550139427185059, test loss: 5.712169170379639\n",
      "h: 114 | epoch: 65, train loss: 8.550048828125, test loss: 5.71104097366333\n",
      "h: 114 | epoch: 66, train loss: 8.549973487854004, test loss: 5.710010528564453\n",
      "h: 114 | epoch: 67, train loss: 8.549907684326172, test loss: 5.709068775177002\n",
      "h: 114 | epoch: 68, train loss: 8.549856185913086, test loss: 5.7082085609436035\n",
      "h: 114 | epoch: 69, train loss: 8.549809455871582, test loss: 5.707420825958252\n",
      "h: 114 | epoch: 70, train loss: 8.549772262573242, test loss: 5.706703186035156\n",
      "h: 114 | epoch: 71, train loss: 8.549738883972168, test loss: 5.706044673919678\n",
      "h: 114 | epoch: 72, train loss: 8.549712181091309, test loss: 5.705443382263184\n",
      "h: 114 | epoch: 73, train loss: 8.549690246582031, test loss: 5.704891681671143\n",
      "h: 114 | epoch: 74, train loss: 8.549671173095703, test loss: 5.704387187957764\n",
      "h: 114 | epoch: 75, train loss: 8.549656867980957, test loss: 5.703925132751465\n",
      "h: 114 | epoch: 76, train loss: 8.549642562866211, test loss: 5.7035017013549805\n",
      "h: 114 | epoch: 77, train loss: 8.549631118774414, test loss: 5.703114032745361\n",
      "h: 114 | epoch: 78, train loss: 8.549622535705566, test loss: 5.702757835388184\n",
      "h: 114 | epoch: 79, train loss: 8.549613952636719, test loss: 5.702431678771973\n",
      "h: 114 | epoch: 80, train loss: 8.54960823059082, test loss: 5.702132225036621\n",
      "h: 114 | epoch: 81, train loss: 8.549602508544922, test loss: 5.701858997344971\n",
      "h: 114 | epoch: 82, train loss: 8.549596786499023, test loss: 5.701608657836914\n",
      "h: 114 | epoch: 83, train loss: 8.549592971801758, test loss: 5.701378345489502\n",
      "h: 114 | epoch: 84, train loss: 8.549590110778809, test loss: 5.701166152954102\n",
      "h: 114 | epoch: 85, train loss: 8.549588203430176, test loss: 5.700972080230713\n",
      "h: 114 | epoch: 86, train loss: 8.549585342407227, test loss: 5.700794219970703\n",
      "h: 114 | epoch: 87, train loss: 8.549583435058594, test loss: 5.700631141662598\n",
      "h: 114 | epoch: 88, train loss: 8.549581527709961, test loss: 5.700481414794922\n",
      "h: 114 | epoch: 89, train loss: 8.549579620361328, test loss: 5.700343608856201\n",
      "h: 114 | epoch: 90, train loss: 8.549579620361328, test loss: 5.700217247009277\n",
      "h: 114 | epoch: 91, train loss: 8.549577713012695, test loss: 5.700101375579834\n",
      "h: 114 | epoch: 92, train loss: 8.549576759338379, test loss: 5.699995994567871\n",
      "h: 114 | epoch: 93, train loss: 8.549576759338379, test loss: 5.699897289276123\n",
      "h: 114 | epoch: 94, train loss: 8.549575805664062, test loss: 5.699807167053223\n",
      "h: 114 | epoch: 95, train loss: 8.549575805664062, test loss: 5.6997246742248535\n",
      "h: 114 | epoch: 96, train loss: 8.549574851989746, test loss: 5.699649810791016\n",
      "h: 114 | epoch: 97, train loss: 8.549574851989746, test loss: 5.699580192565918\n",
      "h: 114 | epoch: 98, train loss: 8.54957389831543, test loss: 5.699516296386719\n",
      "h: 114 | epoch: 99, train loss: 8.54957389831543, test loss: 5.699456214904785\n",
      "h: 115 | epoch: 0, train loss: 49.36351776123047, test loss: 39.370887756347656\n",
      "h: 115 | epoch: 1, train loss: 42.85041809082031, test loss: 34.21451950073242\n",
      "h: 115 | epoch: 2, train loss: 37.38064956665039, test loss: 29.86428451538086\n",
      "h: 115 | epoch: 3, train loss: 32.78398895263672, test loss: 26.191104888916016\n",
      "h: 115 | epoch: 4, train loss: 28.919368743896484, test loss: 23.087491989135742\n",
      "h: 115 | epoch: 5, train loss: 25.66937828063965, test loss: 20.463634490966797\n",
      "h: 115 | epoch: 6, train loss: 22.935962677001953, test loss: 18.244266510009766\n",
      "h: 115 | epoch: 7, train loss: 20.637006759643555, test loss: 16.36618423461914\n",
      "h: 115 | epoch: 8, train loss: 18.70362091064453, test loss: 14.776208877563477\n",
      "h: 115 | epoch: 9, train loss: 17.077899932861328, test loss: 13.42956256866455\n",
      "h: 115 | epoch: 10, train loss: 15.711138725280762, test loss: 12.288494110107422\n",
      "h: 115 | epoch: 11, train loss: 14.562337875366211, test loss: 11.32115364074707\n",
      "h: 115 | epoch: 12, train loss: 13.596957206726074, test loss: 10.500661849975586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 115 | epoch: 13, train loss: 12.785906791687012, test loss: 9.804327964782715\n",
      "h: 115 | epoch: 14, train loss: 12.104673385620117, test loss: 9.212991714477539\n",
      "h: 115 | epoch: 15, train loss: 11.532612800598145, test loss: 8.710471153259277\n",
      "h: 115 | epoch: 16, train loss: 11.052335739135742, test loss: 8.283102989196777\n",
      "h: 115 | epoch: 17, train loss: 10.649198532104492, test loss: 7.919345855712891\n",
      "h: 115 | epoch: 18, train loss: 10.310874938964844, test loss: 7.609452724456787\n",
      "h: 115 | epoch: 19, train loss: 10.026993751525879, test loss: 7.345186710357666\n",
      "h: 115 | epoch: 20, train loss: 9.788832664489746, test loss: 7.119594573974609\n",
      "h: 115 | epoch: 21, train loss: 9.589052200317383, test loss: 6.9267988204956055\n",
      "h: 115 | epoch: 22, train loss: 9.421486854553223, test loss: 6.761837005615234\n",
      "h: 115 | epoch: 23, train loss: 9.280954360961914, test loss: 6.620509147644043\n",
      "h: 115 | epoch: 24, train loss: 9.16309928894043, test loss: 6.499269962310791\n",
      "h: 115 | epoch: 25, train loss: 9.064271926879883, test loss: 6.39511775970459\n",
      "h: 115 | epoch: 26, train loss: 8.981399536132812, test loss: 6.305515289306641\n",
      "h: 115 | epoch: 27, train loss: 8.911906242370605, test loss: 6.2283124923706055\n",
      "h: 115 | epoch: 28, train loss: 8.853631973266602, test loss: 6.1616926193237305\n",
      "h: 115 | epoch: 29, train loss: 8.804765701293945, test loss: 6.1041107177734375\n",
      "h: 115 | epoch: 30, train loss: 8.763784408569336, test loss: 6.054261684417725\n",
      "h: 115 | epoch: 31, train loss: 8.729413986206055, test loss: 6.011034965515137\n",
      "h: 115 | epoch: 32, train loss: 8.70058536529541, test loss: 5.973487377166748\n",
      "h: 115 | epoch: 33, train loss: 8.67640209197998, test loss: 5.940819263458252\n",
      "h: 115 | epoch: 34, train loss: 8.656115531921387, test loss: 5.912346839904785\n",
      "h: 115 | epoch: 35, train loss: 8.639093399047852, test loss: 5.887490272521973\n",
      "h: 115 | epoch: 36, train loss: 8.624808311462402, test loss: 5.865752696990967\n",
      "h: 115 | epoch: 37, train loss: 8.612818717956543, test loss: 5.846712589263916\n",
      "h: 115 | epoch: 38, train loss: 8.602754592895508, test loss: 5.8300065994262695\n",
      "h: 115 | epoch: 39, train loss: 8.594304084777832, test loss: 5.815324783325195\n",
      "h: 115 | epoch: 40, train loss: 8.587207794189453, test loss: 5.802403450012207\n",
      "h: 115 | epoch: 41, train loss: 8.581245422363281, test loss: 5.79101037979126\n",
      "h: 115 | epoch: 42, train loss: 8.5762357711792, test loss: 5.7809529304504395\n",
      "h: 115 | epoch: 43, train loss: 8.572027206420898, test loss: 5.772060871124268\n",
      "h: 115 | epoch: 44, train loss: 8.568488121032715, test loss: 5.764187812805176\n",
      "h: 115 | epoch: 45, train loss: 8.565512657165527, test loss: 5.757207870483398\n",
      "h: 115 | epoch: 46, train loss: 8.563011169433594, test loss: 5.751011848449707\n",
      "h: 115 | epoch: 47, train loss: 8.560905456542969, test loss: 5.74550724029541\n",
      "h: 115 | epoch: 48, train loss: 8.559133529663086, test loss: 5.74060583114624\n",
      "h: 115 | epoch: 49, train loss: 8.557641983032227, test loss: 5.736240863800049\n",
      "h: 115 | epoch: 50, train loss: 8.556384086608887, test loss: 5.732349395751953\n",
      "h: 115 | epoch: 51, train loss: 8.555326461791992, test loss: 5.728874683380127\n",
      "h: 115 | epoch: 52, train loss: 8.554434776306152, test loss: 5.725772857666016\n",
      "h: 115 | epoch: 53, train loss: 8.553682327270508, test loss: 5.722997665405273\n",
      "h: 115 | epoch: 54, train loss: 8.553049087524414, test loss: 5.720515251159668\n",
      "h: 115 | epoch: 55, train loss: 8.552513122558594, test loss: 5.718292713165283\n",
      "h: 115 | epoch: 56, train loss: 8.552062034606934, test loss: 5.716300964355469\n",
      "h: 115 | epoch: 57, train loss: 8.551680564880371, test loss: 5.71451473236084\n",
      "h: 115 | epoch: 58, train loss: 8.551359176635742, test loss: 5.712912559509277\n",
      "h: 115 | epoch: 59, train loss: 8.55108642578125, test loss: 5.711474895477295\n",
      "h: 115 | epoch: 60, train loss: 8.55085563659668, test loss: 5.710184097290039\n",
      "h: 115 | epoch: 61, train loss: 8.550661087036133, test loss: 5.709023475646973\n",
      "h: 115 | epoch: 62, train loss: 8.550497055053711, test loss: 5.707981586456299\n",
      "h: 115 | epoch: 63, train loss: 8.550357818603516, test loss: 5.7070441246032715\n",
      "h: 115 | epoch: 64, train loss: 8.550238609313965, test loss: 5.706203460693359\n",
      "h: 115 | epoch: 65, train loss: 8.550138473510742, test loss: 5.705445766448975\n",
      "h: 115 | epoch: 66, train loss: 8.550054550170898, test loss: 5.704762935638428\n",
      "h: 115 | epoch: 67, train loss: 8.549982070922852, test loss: 5.704151630401611\n",
      "h: 115 | epoch: 68, train loss: 8.549921989440918, test loss: 5.7036004066467285\n",
      "h: 115 | epoch: 69, train loss: 8.549869537353516, test loss: 5.703103065490723\n",
      "h: 115 | epoch: 70, train loss: 8.549825668334961, test loss: 5.702657699584961\n",
      "h: 115 | epoch: 71, train loss: 8.549788475036621, test loss: 5.702255725860596\n",
      "h: 115 | epoch: 72, train loss: 8.54975700378418, test loss: 5.701894760131836\n",
      "h: 115 | epoch: 73, train loss: 8.54973030090332, test loss: 5.701569080352783\n",
      "h: 115 | epoch: 74, train loss: 8.549707412719727, test loss: 5.701277256011963\n",
      "h: 115 | epoch: 75, train loss: 8.549687385559082, test loss: 5.701014518737793\n",
      "h: 115 | epoch: 76, train loss: 8.549671173095703, test loss: 5.700777530670166\n",
      "h: 115 | epoch: 77, train loss: 8.549656867980957, test loss: 5.700564861297607\n",
      "h: 115 | epoch: 78, train loss: 8.549644470214844, test loss: 5.700374603271484\n",
      "h: 115 | epoch: 79, train loss: 8.549633979797363, test loss: 5.700202465057373\n",
      "h: 115 | epoch: 80, train loss: 8.549625396728516, test loss: 5.700047969818115\n",
      "h: 115 | epoch: 81, train loss: 8.549617767333984, test loss: 5.699909210205078\n",
      "h: 115 | epoch: 82, train loss: 8.549612045288086, test loss: 5.6997857093811035\n",
      "h: 115 | epoch: 83, train loss: 8.549605369567871, test loss: 5.699674129486084\n",
      "h: 115 | epoch: 84, train loss: 8.549601554870605, test loss: 5.699573516845703\n",
      "h: 115 | epoch: 85, train loss: 8.549596786499023, test loss: 5.699484348297119\n",
      "h: 115 | epoch: 86, train loss: 8.549593925476074, test loss: 5.699404239654541\n",
      "h: 115 | epoch: 87, train loss: 8.549591064453125, test loss: 5.699332237243652\n",
      "h: 115 | epoch: 88, train loss: 8.549589157104492, test loss: 5.699267387390137\n",
      "h: 115 | epoch: 89, train loss: 8.549585342407227, test loss: 5.699210166931152\n",
      "h: 115 | epoch: 90, train loss: 8.549583435058594, test loss: 5.699158668518066\n",
      "h: 115 | epoch: 91, train loss: 8.549582481384277, test loss: 5.699112892150879\n",
      "h: 115 | epoch: 92, train loss: 8.549581527709961, test loss: 5.699070930480957\n",
      "h: 115 | epoch: 93, train loss: 8.549580574035645, test loss: 5.699035167694092\n",
      "h: 115 | epoch: 94, train loss: 8.549578666687012, test loss: 5.699002742767334\n",
      "h: 115 | epoch: 95, train loss: 8.549577713012695, test loss: 5.698974132537842\n",
      "h: 115 | epoch: 96, train loss: 8.549577713012695, test loss: 5.698947906494141\n",
      "h: 115 | epoch: 97, train loss: 8.549577713012695, test loss: 5.698925495147705\n",
      "h: 115 | epoch: 98, train loss: 8.549576759338379, test loss: 5.698904991149902\n",
      "h: 115 | epoch: 99, train loss: 8.549575805664062, test loss: 5.698886871337891\n",
      "h: 116 | epoch: 0, train loss: 41.93539810180664, test loss: 33.45832061767578\n",
      "h: 116 | epoch: 1, train loss: 36.41236114501953, test loss: 29.098962783813477\n",
      "h: 116 | epoch: 2, train loss: 31.80060386657715, test loss: 25.43743324279785\n",
      "h: 116 | epoch: 3, train loss: 27.948673248291016, test loss: 22.360177993774414\n",
      "h: 116 | epoch: 4, train loss: 24.731054306030273, test loss: 19.77267074584961\n",
      "h: 116 | epoch: 5, train loss: 22.04340362548828, test loss: 17.595998764038086\n",
      "h: 116 | epoch: 6, train loss: 19.79876136779785, test loss: 15.764184951782227\n",
      "h: 116 | epoch: 7, train loss: 17.92452621459961, test loss: 14.221959114074707\n",
      "h: 116 | epoch: 8, train loss: 16.360023498535156, test loss: 12.922988891601562\n",
      "h: 116 | epoch: 9, train loss: 15.054489135742188, test loss: 11.828405380249023\n",
      "h: 116 | epoch: 10, train loss: 13.965436935424805, test loss: 10.905580520629883\n",
      "h: 116 | epoch: 11, train loss: 13.057299613952637, test loss: 10.127123832702637\n",
      "h: 116 | epoch: 12, train loss: 12.300302505493164, test loss: 9.470029830932617\n",
      "h: 116 | epoch: 13, train loss: 11.669525146484375, test loss: 8.914978981018066\n",
      "h: 116 | epoch: 14, train loss: 11.144109725952148, test loss: 8.44575309753418\n",
      "h: 116 | epoch: 15, train loss: 10.706611633300781, test loss: 8.048724174499512\n",
      "h: 116 | epoch: 16, train loss: 10.342442512512207, test loss: 7.71245813369751\n",
      "h: 116 | epoch: 17, train loss: 10.039410591125488, test loss: 7.427347660064697\n",
      "h: 116 | epoch: 18, train loss: 9.787327766418457, test loss: 7.185323238372803\n",
      "h: 116 | epoch: 19, train loss: 9.577689170837402, test loss: 6.979612827301025\n",
      "h: 116 | epoch: 20, train loss: 9.403396606445312, test loss: 6.804532051086426\n",
      "h: 116 | epoch: 21, train loss: 9.258527755737305, test loss: 6.65529727935791\n",
      "h: 116 | epoch: 22, train loss: 9.138143539428711, test loss: 6.527895450592041\n",
      "h: 116 | epoch: 23, train loss: 9.038128852844238, test loss: 6.4189558029174805\n",
      "h: 116 | epoch: 24, train loss: 8.955053329467773, test loss: 6.325638771057129\n",
      "h: 116 | epoch: 25, train loss: 8.886059761047363, test loss: 6.245560646057129\n",
      "h: 116 | epoch: 26, train loss: 8.828773498535156, test loss: 6.176713943481445\n",
      "h: 116 | epoch: 27, train loss: 8.78121566772461, test loss: 6.117409706115723\n",
      "h: 116 | epoch: 28, train loss: 8.741740226745605, test loss: 6.066219329833984\n",
      "h: 116 | epoch: 29, train loss: 8.708977699279785, test loss: 6.021945476531982\n",
      "h: 116 | epoch: 30, train loss: 8.68178939819336, test loss: 5.983572959899902\n",
      "h: 116 | epoch: 31, train loss: 8.65923023223877, test loss: 5.950246810913086\n",
      "h: 116 | epoch: 32, train loss: 8.64051342010498, test loss: 5.921238899230957\n",
      "h: 116 | epoch: 33, train loss: 8.62498664855957, test loss: 5.895938873291016\n",
      "h: 116 | epoch: 34, train loss: 8.61210823059082, test loss: 5.873823642730713\n",
      "h: 116 | epoch: 35, train loss: 8.601425170898438, test loss: 5.854453086853027\n",
      "h: 116 | epoch: 36, train loss: 8.592565536499023, test loss: 5.83745002746582\n",
      "h: 116 | epoch: 37, train loss: 8.58521842956543, test loss: 5.822495460510254\n",
      "h: 116 | epoch: 38, train loss: 8.57912540435791, test loss: 5.809316158294678\n",
      "h: 116 | epoch: 39, train loss: 8.574071884155273, test loss: 5.7976789474487305\n",
      "h: 116 | epoch: 40, train loss: 8.569883346557617, test loss: 5.78738260269165\n",
      "h: 116 | epoch: 41, train loss: 8.566410064697266, test loss: 5.778256893157959\n",
      "h: 116 | epoch: 42, train loss: 8.563530921936035, test loss: 5.77015495300293\n",
      "h: 116 | epoch: 43, train loss: 8.561142921447754, test loss: 5.762949466705322\n",
      "h: 116 | epoch: 44, train loss: 8.559164047241211, test loss: 5.756529808044434\n",
      "h: 116 | epoch: 45, train loss: 8.557523727416992, test loss: 5.7508015632629395\n",
      "h: 116 | epoch: 46, train loss: 8.556161880493164, test loss: 5.745683193206787\n",
      "h: 116 | epoch: 47, train loss: 8.555034637451172, test loss: 5.741105079650879\n",
      "h: 116 | epoch: 48, train loss: 8.55410099029541, test loss: 5.737001895904541\n",
      "h: 116 | epoch: 49, train loss: 8.553324699401855, test loss: 5.7333221435546875\n",
      "h: 116 | epoch: 50, train loss: 8.552682876586914, test loss: 5.730015754699707\n",
      "h: 116 | epoch: 51, train loss: 8.55215072631836, test loss: 5.727044105529785\n",
      "h: 116 | epoch: 52, train loss: 8.55171012878418, test loss: 5.724370002746582\n",
      "h: 116 | epoch: 53, train loss: 8.55134391784668, test loss: 5.72196102142334\n",
      "h: 116 | epoch: 54, train loss: 8.551040649414062, test loss: 5.719788074493408\n",
      "h: 116 | epoch: 55, train loss: 8.550788879394531, test loss: 5.71782922744751\n",
      "h: 116 | epoch: 56, train loss: 8.550580978393555, test loss: 5.716057777404785\n",
      "h: 116 | epoch: 57, train loss: 8.550409317016602, test loss: 5.714457988739014\n",
      "h: 116 | epoch: 58, train loss: 8.550265312194824, test loss: 5.713011741638184\n",
      "h: 116 | epoch: 59, train loss: 8.550146102905273, test loss: 5.711702823638916\n",
      "h: 116 | epoch: 60, train loss: 8.550049781799316, test loss: 5.710517406463623\n",
      "h: 116 | epoch: 61, train loss: 8.549966812133789, test loss: 5.709443092346191\n",
      "h: 116 | epoch: 62, train loss: 8.54990005493164, test loss: 5.70847225189209\n",
      "h: 116 | epoch: 63, train loss: 8.549844741821289, test loss: 5.707589149475098\n",
      "h: 116 | epoch: 64, train loss: 8.549798011779785, test loss: 5.706789493560791\n",
      "h: 116 | epoch: 65, train loss: 8.549758911132812, test loss: 5.7060627937316895\n",
      "h: 116 | epoch: 66, train loss: 8.549726486206055, test loss: 5.705404758453369\n",
      "h: 116 | epoch: 67, train loss: 8.549700736999512, test loss: 5.704806327819824\n",
      "h: 116 | epoch: 68, train loss: 8.549678802490234, test loss: 5.7042622566223145\n",
      "h: 116 | epoch: 69, train loss: 8.549660682678223, test loss: 5.703770160675049\n",
      "h: 116 | epoch: 70, train loss: 8.54964542388916, test loss: 5.703320503234863\n",
      "h: 116 | epoch: 71, train loss: 8.549633979797363, test loss: 5.702912330627441\n",
      "h: 116 | epoch: 72, train loss: 8.549623489379883, test loss: 5.702541351318359\n",
      "h: 116 | epoch: 73, train loss: 8.549612998962402, test loss: 5.702205657958984\n",
      "h: 116 | epoch: 74, train loss: 8.549607276916504, test loss: 5.70189905166626\n",
      "h: 116 | epoch: 75, train loss: 8.549601554870605, test loss: 5.701620578765869\n",
      "h: 116 | epoch: 76, train loss: 8.549596786499023, test loss: 5.701366901397705\n",
      "h: 116 | epoch: 77, train loss: 8.549592971801758, test loss: 5.701136589050293\n",
      "h: 116 | epoch: 78, train loss: 8.549589157104492, test loss: 5.700926303863525\n",
      "h: 116 | epoch: 79, train loss: 8.549586296081543, test loss: 5.700735092163086\n",
      "h: 116 | epoch: 80, train loss: 8.549585342407227, test loss: 5.700561046600342\n",
      "h: 116 | epoch: 81, train loss: 8.549581527709961, test loss: 5.700402736663818\n",
      "h: 116 | epoch: 82, train loss: 8.549581527709961, test loss: 5.700259208679199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 116 | epoch: 83, train loss: 8.549578666687012, test loss: 5.700127601623535\n",
      "h: 116 | epoch: 84, train loss: 8.549577713012695, test loss: 5.700008392333984\n",
      "h: 116 | epoch: 85, train loss: 8.549577713012695, test loss: 5.699899196624756\n",
      "h: 116 | epoch: 86, train loss: 8.549575805664062, test loss: 5.699800491333008\n",
      "h: 116 | epoch: 87, train loss: 8.549575805664062, test loss: 5.699711322784424\n",
      "h: 116 | epoch: 88, train loss: 8.549574851989746, test loss: 5.699629783630371\n",
      "h: 116 | epoch: 89, train loss: 8.54957389831543, test loss: 5.699554920196533\n",
      "h: 116 | epoch: 90, train loss: 8.54957389831543, test loss: 5.69948673248291\n",
      "h: 116 | epoch: 91, train loss: 8.54957389831543, test loss: 5.699423789978027\n",
      "h: 116 | epoch: 92, train loss: 8.54957389831543, test loss: 5.699368476867676\n",
      "h: 116 | epoch: 93, train loss: 8.54957389831543, test loss: 5.699316024780273\n",
      "h: 116 | epoch: 94, train loss: 8.54957389831543, test loss: 5.6992692947387695\n",
      "h: 116 | epoch: 95, train loss: 8.549572944641113, test loss: 5.6992268562316895\n",
      "h: 116 | epoch: 96, train loss: 8.549572944641113, test loss: 5.699187278747559\n",
      "h: 116 | epoch: 97, train loss: 8.54957389831543, test loss: 5.69915246963501\n",
      "h: 116 | epoch: 98, train loss: 8.54957389831543, test loss: 5.69912052154541\n",
      "h: 116 | epoch: 99, train loss: 8.549572944641113, test loss: 5.699090480804443\n",
      "h: 117 | epoch: 0, train loss: 44.83249282836914, test loss: 36.169071197509766\n",
      "h: 117 | epoch: 1, train loss: 39.27897644042969, test loss: 31.726333618164062\n",
      "h: 117 | epoch: 2, train loss: 34.574607849121094, test loss: 27.94374656677246\n",
      "h: 117 | epoch: 3, train loss: 30.587841033935547, test loss: 24.721202850341797\n",
      "h: 117 | epoch: 4, train loss: 27.208423614501953, test loss: 21.97439193725586\n",
      "h: 117 | epoch: 5, train loss: 24.343605041503906, test loss: 19.632099151611328\n",
      "h: 117 | epoch: 6, train loss: 21.91513442993164, test loss: 17.634037017822266\n",
      "h: 117 | epoch: 7, train loss: 19.85682487487793, test loss: 15.929056167602539\n",
      "h: 117 | epoch: 8, train loss: 18.11261749267578, test loss: 14.473703384399414\n",
      "h: 117 | epoch: 9, train loss: 16.63496971130371, test loss: 13.231035232543945\n",
      "h: 117 | epoch: 10, train loss: 15.383520126342773, test loss: 12.169610977172852\n",
      "h: 117 | epoch: 11, train loss: 14.32398796081543, test loss: 11.262660026550293\n",
      "h: 117 | epoch: 12, train loss: 13.42724895477295, test loss: 10.487377166748047\n",
      "h: 117 | epoch: 13, train loss: 12.668553352355957, test loss: 9.824348449707031\n",
      "h: 117 | epoch: 14, train loss: 12.026880264282227, test loss: 9.257017135620117\n",
      "h: 117 | epoch: 15, train loss: 11.484367370605469, test loss: 8.771293640136719\n",
      "h: 117 | epoch: 16, train loss: 11.025850296020508, test loss: 8.355167388916016\n",
      "h: 117 | epoch: 17, train loss: 10.638452529907227, test loss: 7.998408317565918\n",
      "h: 117 | epoch: 18, train loss: 10.311247825622559, test loss: 7.692303657531738\n",
      "h: 117 | epoch: 19, train loss: 10.034968376159668, test loss: 7.429433345794678\n",
      "h: 117 | epoch: 20, train loss: 9.801756858825684, test loss: 7.203474998474121\n",
      "h: 117 | epoch: 21, train loss: 9.604955673217773, test loss: 7.009051322937012\n",
      "h: 117 | epoch: 22, train loss: 9.438922882080078, test loss: 6.841573238372803\n",
      "h: 117 | epoch: 23, train loss: 9.298883438110352, test loss: 6.69713830947876\n",
      "h: 117 | epoch: 24, train loss: 9.180793762207031, test loss: 6.572417259216309\n",
      "h: 117 | epoch: 25, train loss: 9.081236839294434, test loss: 6.4645819664001465\n",
      "h: 117 | epoch: 26, train loss: 8.997322082519531, test loss: 6.371214389801025\n",
      "h: 117 | epoch: 27, train loss: 8.926604270935059, test loss: 6.290255546569824\n",
      "h: 117 | epoch: 28, train loss: 8.86701774597168, test loss: 6.2199506759643555\n",
      "h: 117 | epoch: 29, train loss: 8.81682014465332, test loss: 6.158801555633545\n",
      "h: 117 | epoch: 30, train loss: 8.774538040161133, test loss: 6.105530738830566\n",
      "h: 117 | epoch: 31, train loss: 8.738927841186523, test loss: 6.059046268463135\n",
      "h: 117 | epoch: 32, train loss: 8.708940505981445, test loss: 6.018413066864014\n",
      "h: 117 | epoch: 33, train loss: 8.683691024780273, test loss: 5.98283576965332\n",
      "h: 117 | epoch: 34, train loss: 8.662434577941895, test loss: 5.951628684997559\n",
      "h: 117 | epoch: 35, train loss: 8.644542694091797, test loss: 5.924208641052246\n",
      "h: 117 | epoch: 36, train loss: 8.629480361938477, test loss: 5.90007209777832\n",
      "h: 117 | epoch: 37, train loss: 8.616804122924805, test loss: 5.878788948059082\n",
      "h: 117 | epoch: 38, train loss: 8.606135368347168, test loss: 5.859988689422607\n",
      "h: 117 | epoch: 39, train loss: 8.59715747833252, test loss: 5.843353271484375\n",
      "h: 117 | epoch: 40, train loss: 8.589603424072266, test loss: 5.828607082366943\n",
      "h: 117 | epoch: 41, train loss: 8.583246231079102, test loss: 5.815513610839844\n",
      "h: 117 | epoch: 42, train loss: 8.577898979187012, test loss: 5.80387020111084\n",
      "h: 117 | epoch: 43, train loss: 8.573400497436523, test loss: 5.793496131896973\n",
      "h: 117 | epoch: 44, train loss: 8.56961441040039, test loss: 5.784241199493408\n",
      "h: 117 | epoch: 45, train loss: 8.56643009185791, test loss: 5.775970935821533\n",
      "h: 117 | epoch: 46, train loss: 8.563751220703125, test loss: 5.768568515777588\n",
      "h: 117 | epoch: 47, train loss: 8.561497688293457, test loss: 5.761936187744141\n",
      "h: 117 | epoch: 48, train loss: 8.559602737426758, test loss: 5.755982875823975\n",
      "h: 117 | epoch: 49, train loss: 8.558008193969727, test loss: 5.750633716583252\n",
      "h: 117 | epoch: 50, train loss: 8.556668281555176, test loss: 5.745820999145508\n",
      "h: 117 | epoch: 51, train loss: 8.55553913116455, test loss: 5.741486072540283\n",
      "h: 117 | epoch: 52, train loss: 8.554590225219727, test loss: 5.737576007843018\n",
      "h: 117 | epoch: 53, train loss: 8.553792953491211, test loss: 5.73404598236084\n",
      "h: 117 | epoch: 54, train loss: 8.553121566772461, test loss: 5.730855941772461\n",
      "h: 117 | epoch: 55, train loss: 8.552556991577148, test loss: 5.727971076965332\n",
      "h: 117 | epoch: 56, train loss: 8.552083015441895, test loss: 5.7253594398498535\n",
      "h: 117 | epoch: 57, train loss: 8.55168342590332, test loss: 5.722992420196533\n",
      "h: 117 | epoch: 58, train loss: 8.551347732543945, test loss: 5.7208452224731445\n",
      "h: 117 | epoch: 59, train loss: 8.551065444946289, test loss: 5.718897342681885\n",
      "h: 117 | epoch: 60, train loss: 8.55082893371582, test loss: 5.717127799987793\n",
      "h: 117 | epoch: 61, train loss: 8.550628662109375, test loss: 5.71552038192749\n",
      "h: 117 | epoch: 62, train loss: 8.550461769104004, test loss: 5.714057922363281\n",
      "h: 117 | epoch: 63, train loss: 8.550318717956543, test loss: 5.7127275466918945\n",
      "h: 117 | epoch: 64, train loss: 8.550200462341309, test loss: 5.711516857147217\n",
      "h: 117 | epoch: 65, train loss: 8.550100326538086, test loss: 5.710412979125977\n",
      "h: 117 | epoch: 66, train loss: 8.550016403198242, test loss: 5.709408760070801\n",
      "h: 117 | epoch: 67, train loss: 8.549945831298828, test loss: 5.708493232727051\n",
      "h: 117 | epoch: 68, train loss: 8.549886703491211, test loss: 5.707657814025879\n",
      "h: 117 | epoch: 69, train loss: 8.549837112426758, test loss: 5.7068963050842285\n",
      "h: 117 | epoch: 70, train loss: 8.549795150756836, test loss: 5.70620059967041\n",
      "h: 117 | epoch: 71, train loss: 8.549759864807129, test loss: 5.705565929412842\n",
      "h: 117 | epoch: 72, train loss: 8.54973030090332, test loss: 5.704986572265625\n",
      "h: 117 | epoch: 73, train loss: 8.549704551696777, test loss: 5.7044572830200195\n",
      "h: 117 | epoch: 74, train loss: 8.549683570861816, test loss: 5.703973770141602\n",
      "h: 117 | epoch: 75, train loss: 8.549665451049805, test loss: 5.7035322189331055\n",
      "h: 117 | epoch: 76, train loss: 8.549651145935059, test loss: 5.703127861022949\n",
      "h: 117 | epoch: 77, train loss: 8.549638748168945, test loss: 5.702759742736816\n",
      "h: 117 | epoch: 78, train loss: 8.549628257751465, test loss: 5.702422142028809\n",
      "h: 117 | epoch: 79, train loss: 8.549619674682617, test loss: 5.702115058898926\n",
      "h: 117 | epoch: 80, train loss: 8.549612045288086, test loss: 5.701831340789795\n",
      "h: 117 | epoch: 81, train loss: 8.549605369567871, test loss: 5.701573848724365\n",
      "h: 117 | epoch: 82, train loss: 8.549600601196289, test loss: 5.701338768005371\n",
      "h: 117 | epoch: 83, train loss: 8.549595832824707, test loss: 5.701121807098389\n",
      "h: 117 | epoch: 84, train loss: 8.549592971801758, test loss: 5.700924873352051\n",
      "h: 117 | epoch: 85, train loss: 8.549590110778809, test loss: 5.700743675231934\n",
      "h: 117 | epoch: 86, train loss: 8.549586296081543, test loss: 5.700578212738037\n",
      "h: 117 | epoch: 87, train loss: 8.549585342407227, test loss: 5.7004265785217285\n",
      "h: 117 | epoch: 88, train loss: 8.549581527709961, test loss: 5.70028829574585\n",
      "h: 117 | epoch: 89, train loss: 8.549581527709961, test loss: 5.700161933898926\n",
      "h: 117 | epoch: 90, train loss: 8.549579620361328, test loss: 5.700046062469482\n",
      "h: 117 | epoch: 91, train loss: 8.549578666687012, test loss: 5.699940204620361\n",
      "h: 117 | epoch: 92, train loss: 8.549577713012695, test loss: 5.6998419761657715\n",
      "h: 117 | epoch: 93, train loss: 8.549577713012695, test loss: 5.699753761291504\n",
      "h: 117 | epoch: 94, train loss: 8.549576759338379, test loss: 5.699671745300293\n",
      "h: 117 | epoch: 95, train loss: 8.549575805664062, test loss: 5.699597358703613\n",
      "h: 117 | epoch: 96, train loss: 8.549574851989746, test loss: 5.69952917098999\n",
      "h: 117 | epoch: 97, train loss: 8.54957389831543, test loss: 5.699465751647949\n",
      "h: 117 | epoch: 98, train loss: 8.54957389831543, test loss: 5.6994099617004395\n",
      "h: 117 | epoch: 99, train loss: 8.54957389831543, test loss: 5.699356555938721\n",
      "h: 118 | epoch: 0, train loss: 49.706687927246094, test loss: 40.44865417480469\n",
      "h: 118 | epoch: 1, train loss: 43.732826232910156, test loss: 35.56183624267578\n",
      "h: 118 | epoch: 2, train loss: 38.648155212402344, test loss: 31.39145851135254\n",
      "h: 118 | epoch: 3, train loss: 34.31464385986328, test loss: 27.827356338500977\n",
      "h: 118 | epoch: 4, train loss: 30.61728286743164, test loss: 24.77748680114746\n",
      "h: 118 | epoch: 5, train loss: 27.459789276123047, test loss: 22.164615631103516\n",
      "h: 118 | epoch: 6, train loss: 24.761249542236328, test loss: 19.923723220825195\n",
      "h: 118 | epoch: 7, train loss: 22.453413009643555, test loss: 17.999923706054688\n",
      "h: 118 | epoch: 8, train loss: 20.47859001159668, test loss: 16.34678077697754\n",
      "h: 118 | epoch: 9, train loss: 18.787878036499023, test loss: 14.924906730651855\n",
      "h: 118 | epoch: 10, train loss: 17.339763641357422, test loss: 13.700860977172852\n",
      "h: 118 | epoch: 11, train loss: 16.098934173583984, test loss: 12.646184921264648\n",
      "h: 118 | epoch: 12, train loss: 15.035326957702637, test loss: 11.73664665222168\n",
      "h: 118 | epoch: 13, train loss: 14.123313903808594, test loss: 10.95158576965332\n",
      "h: 118 | epoch: 14, train loss: 13.341032028198242, test loss: 10.273366928100586\n",
      "h: 118 | epoch: 15, train loss: 12.669815063476562, test loss: 9.686922073364258\n",
      "h: 118 | epoch: 16, train loss: 12.093714714050293, test loss: 9.179378509521484\n",
      "h: 118 | epoch: 17, train loss: 11.599105834960938, test loss: 8.73971176147461\n",
      "h: 118 | epoch: 18, train loss: 11.17432975769043, test loss: 8.358484268188477\n",
      "h: 118 | epoch: 19, train loss: 10.809415817260742, test loss: 8.027620315551758\n",
      "h: 118 | epoch: 20, train loss: 10.495835304260254, test loss: 7.740179538726807\n",
      "h: 118 | epoch: 21, train loss: 10.22628402709961, test loss: 7.4902238845825195\n",
      "h: 118 | epoch: 22, train loss: 9.994508743286133, test loss: 7.272641658782959\n",
      "h: 118 | epoch: 23, train loss: 9.795153617858887, test loss: 7.083052635192871\n",
      "h: 118 | epoch: 24, train loss: 9.623632431030273, test loss: 6.917682647705078\n",
      "h: 118 | epoch: 25, train loss: 9.476011276245117, test loss: 6.77328634262085\n",
      "h: 118 | epoch: 26, train loss: 9.348919868469238, test loss: 6.647074222564697\n",
      "h: 118 | epoch: 27, train loss: 9.239469528198242, test loss: 6.5366363525390625\n",
      "h: 118 | epoch: 28, train loss: 9.145181655883789, test loss: 6.439898490905762\n",
      "h: 118 | epoch: 29, train loss: 9.06393051147461, test loss: 6.355071067810059\n",
      "h: 118 | epoch: 30, train loss: 8.993891716003418, test loss: 6.280606269836426\n",
      "h: 118 | epoch: 31, train loss: 8.933497428894043, test loss: 6.215167999267578\n",
      "h: 118 | epoch: 32, train loss: 8.8814058303833, test loss: 6.157598495483398\n",
      "h: 118 | epoch: 33, train loss: 8.836459159851074, test loss: 6.106896877288818\n",
      "h: 118 | epoch: 34, train loss: 8.79766845703125, test loss: 6.062193393707275\n",
      "h: 118 | epoch: 35, train loss: 8.764178276062012, test loss: 6.022737979888916\n",
      "h: 118 | epoch: 36, train loss: 8.73525619506836, test loss: 5.987874507904053\n",
      "h: 118 | epoch: 37, train loss: 8.710272789001465, test loss: 5.957037925720215\n",
      "h: 118 | epoch: 38, train loss: 8.688684463500977, test loss: 5.9297308921813965\n",
      "h: 118 | epoch: 39, train loss: 8.670023918151855, test loss: 5.905524253845215\n",
      "h: 118 | epoch: 40, train loss: 8.653892517089844, test loss: 5.884042739868164\n",
      "h: 118 | epoch: 41, train loss: 8.63994026184082, test loss: 5.864960670471191\n",
      "h: 118 | epoch: 42, train loss: 8.627872467041016, test loss: 5.8479905128479\n",
      "h: 118 | epoch: 43, train loss: 8.617429733276367, test loss: 5.832884788513184\n",
      "h: 118 | epoch: 44, train loss: 8.608391761779785, test loss: 5.819421291351318\n",
      "h: 118 | epoch: 45, train loss: 8.600566864013672, test loss: 5.807413101196289\n",
      "h: 118 | epoch: 46, train loss: 8.593792915344238, test loss: 5.796689033508301\n",
      "h: 118 | epoch: 47, train loss: 8.58792495727539, test loss: 5.78710412979126\n",
      "h: 118 | epoch: 48, train loss: 8.582841873168945, test loss: 5.778528690338135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 118 | epoch: 49, train loss: 8.578437805175781, test loss: 5.770847797393799\n",
      "h: 118 | epoch: 50, train loss: 8.574621200561523, test loss: 5.763962268829346\n",
      "h: 118 | epoch: 51, train loss: 8.571310997009277, test loss: 5.75778341293335\n",
      "h: 118 | epoch: 52, train loss: 8.568441390991211, test loss: 5.75223445892334\n",
      "h: 118 | epoch: 53, train loss: 8.56595516204834, test loss: 5.7472453117370605\n",
      "h: 118 | epoch: 54, train loss: 8.563796997070312, test loss: 5.7427568435668945\n",
      "h: 118 | epoch: 55, train loss: 8.561924934387207, test loss: 5.738713264465332\n",
      "h: 118 | epoch: 56, train loss: 8.560300827026367, test loss: 5.735069274902344\n",
      "h: 118 | epoch: 57, train loss: 8.558891296386719, test loss: 5.731780529022217\n",
      "h: 118 | epoch: 58, train loss: 8.557668685913086, test loss: 5.728811264038086\n",
      "h: 118 | epoch: 59, train loss: 8.55660629272461, test loss: 5.726129531860352\n",
      "h: 118 | epoch: 60, train loss: 8.555685043334961, test loss: 5.723702430725098\n",
      "h: 118 | epoch: 61, train loss: 8.55488395690918, test loss: 5.721506595611572\n",
      "h: 118 | epoch: 62, train loss: 8.554189682006836, test loss: 5.719517707824707\n",
      "h: 118 | epoch: 63, train loss: 8.553584098815918, test loss: 5.717713832855225\n",
      "h: 118 | epoch: 64, train loss: 8.553060531616211, test loss: 5.716076850891113\n",
      "h: 118 | epoch: 65, train loss: 8.552603721618652, test loss: 5.714592456817627\n",
      "h: 118 | epoch: 66, train loss: 8.55220890045166, test loss: 5.713242053985596\n",
      "h: 118 | epoch: 67, train loss: 8.551864624023438, test loss: 5.7120137214660645\n",
      "h: 118 | epoch: 68, train loss: 8.551565170288086, test loss: 5.710898399353027\n",
      "h: 118 | epoch: 69, train loss: 8.551305770874023, test loss: 5.70988130569458\n",
      "h: 118 | epoch: 70, train loss: 8.551079750061035, test loss: 5.708955764770508\n",
      "h: 118 | epoch: 71, train loss: 8.550883293151855, test loss: 5.70811128616333\n",
      "h: 118 | epoch: 72, train loss: 8.550712585449219, test loss: 5.70734167098999\n",
      "h: 118 | epoch: 73, train loss: 8.550565719604492, test loss: 5.706639289855957\n",
      "h: 118 | epoch: 74, train loss: 8.550436019897461, test loss: 5.705996990203857\n",
      "h: 118 | epoch: 75, train loss: 8.550323486328125, test loss: 5.705411434173584\n",
      "h: 118 | epoch: 76, train loss: 8.550225257873535, test loss: 5.7048749923706055\n",
      "h: 118 | epoch: 77, train loss: 8.550142288208008, test loss: 5.704384803771973\n",
      "h: 118 | epoch: 78, train loss: 8.550066947937012, test loss: 5.7039361000061035\n",
      "h: 118 | epoch: 79, train loss: 8.550003051757812, test loss: 5.703524589538574\n",
      "h: 118 | epoch: 80, train loss: 8.549946784973145, test loss: 5.703148365020752\n",
      "h: 118 | epoch: 81, train loss: 8.549898147583008, test loss: 5.702802658081055\n",
      "h: 118 | epoch: 82, train loss: 8.549856185913086, test loss: 5.702485084533691\n",
      "h: 118 | epoch: 83, train loss: 8.549819946289062, test loss: 5.702195167541504\n",
      "h: 118 | epoch: 84, train loss: 8.549787521362305, test loss: 5.701928615570068\n",
      "h: 118 | epoch: 85, train loss: 8.549759864807129, test loss: 5.701684951782227\n",
      "h: 118 | epoch: 86, train loss: 8.549735069274902, test loss: 5.701460838317871\n",
      "h: 118 | epoch: 87, train loss: 8.549714088439941, test loss: 5.7012529373168945\n",
      "h: 118 | epoch: 88, train loss: 8.54969596862793, test loss: 5.701063632965088\n",
      "h: 118 | epoch: 89, train loss: 8.549680709838867, test loss: 5.700888156890869\n",
      "h: 118 | epoch: 90, train loss: 8.549665451049805, test loss: 5.7007269859313965\n",
      "h: 118 | epoch: 91, train loss: 8.549654006958008, test loss: 5.700579643249512\n",
      "h: 118 | epoch: 92, train loss: 8.549643516540527, test loss: 5.700444221496582\n",
      "h: 118 | epoch: 93, train loss: 8.54963493347168, test loss: 5.700318336486816\n",
      "h: 118 | epoch: 94, train loss: 8.549626350402832, test loss: 5.700202465057373\n",
      "h: 118 | epoch: 95, train loss: 8.5496187210083, test loss: 5.700095176696777\n",
      "h: 118 | epoch: 96, train loss: 8.549612998962402, test loss: 5.699997901916504\n",
      "h: 118 | epoch: 97, train loss: 8.54960823059082, test loss: 5.699907302856445\n",
      "h: 118 | epoch: 98, train loss: 8.549603462219238, test loss: 5.699824333190918\n",
      "h: 118 | epoch: 99, train loss: 8.549599647521973, test loss: 5.6997456550598145\n",
      "h: 119 | epoch: 0, train loss: 46.696022033691406, test loss: 37.59615707397461\n",
      "h: 119 | epoch: 1, train loss: 40.79525375366211, test loss: 32.89799880981445\n",
      "h: 119 | epoch: 2, train loss: 35.80707550048828, test loss: 28.905471801757812\n",
      "h: 119 | epoch: 3, train loss: 31.588287353515625, test loss: 25.510272979736328\n",
      "h: 119 | epoch: 4, train loss: 28.019201278686523, test loss: 22.621456146240234\n",
      "h: 119 | epoch: 5, train loss: 24.99941635131836, test loss: 20.16238784790039\n",
      "h: 119 | epoch: 6, train loss: 22.444427490234375, test loss: 18.06833267211914\n",
      "h: 119 | epoch: 7, train loss: 20.2829532623291, test loss: 16.284481048583984\n",
      "h: 119 | epoch: 8, train loss: 18.45473861694336, test loss: 14.764384269714355\n",
      "h: 119 | epoch: 9, train loss: 16.908794403076172, test loss: 13.468600273132324\n",
      "h: 119 | epoch: 10, train loss: 15.601922988891602, test loss: 12.363638877868652\n",
      "h: 119 | epoch: 11, train loss: 14.497514724731445, test loss: 11.421039581298828\n",
      "h: 119 | epoch: 12, train loss: 13.5645170211792, test loss: 10.616598129272461\n",
      "h: 119 | epoch: 13, train loss: 12.776609420776367, test loss: 9.929733276367188\n",
      "h: 119 | epoch: 14, train loss: 12.11146068572998, test loss: 9.342950820922852\n",
      "h: 119 | epoch: 15, train loss: 11.550146102905273, test loss: 8.841358184814453\n",
      "h: 119 | epoch: 16, train loss: 11.076616287231445, test loss: 8.412302017211914\n",
      "h: 119 | epoch: 17, train loss: 10.677281379699707, test loss: 8.045022010803223\n",
      "h: 119 | epoch: 18, train loss: 10.340622901916504, test loss: 7.730360507965088\n",
      "h: 119 | epoch: 19, train loss: 10.056892395019531, test loss: 7.460537910461426\n",
      "h: 119 | epoch: 20, train loss: 9.817841529846191, test loss: 7.228936195373535\n",
      "h: 119 | epoch: 21, train loss: 9.616490364074707, test loss: 7.029929161071777\n",
      "h: 119 | epoch: 22, train loss: 9.446939468383789, test loss: 6.858736991882324\n",
      "h: 119 | epoch: 23, train loss: 9.304201126098633, test loss: 6.711289882659912\n",
      "h: 119 | epoch: 24, train loss: 9.184063911437988, test loss: 6.584129333496094\n",
      "h: 119 | epoch: 25, train loss: 9.08297348022461, test loss: 6.4743146896362305\n",
      "h: 119 | epoch: 26, train loss: 8.997926712036133, test loss: 6.3793416023254395\n",
      "h: 119 | epoch: 27, train loss: 8.926389694213867, test loss: 6.297080039978027\n",
      "h: 119 | epoch: 28, train loss: 8.866230010986328, test loss: 6.225717067718506\n",
      "h: 119 | epoch: 29, train loss: 8.815645217895508, test loss: 6.163706302642822\n",
      "h: 119 | epoch: 30, train loss: 8.773117065429688, test loss: 6.109732627868652\n",
      "h: 119 | epoch: 31, train loss: 8.737369537353516, test loss: 6.0626726150512695\n",
      "h: 119 | epoch: 32, train loss: 8.707324028015137, test loss: 6.021570205688477\n",
      "h: 119 | epoch: 33, train loss: 8.682074546813965, test loss: 5.9856038093566895\n",
      "h: 119 | epoch: 34, train loss: 8.660859107971191, test loss: 5.954075336456299\n",
      "h: 119 | epoch: 35, train loss: 8.643033981323242, test loss: 5.926386833190918\n",
      "h: 119 | epoch: 36, train loss: 8.628059387207031, test loss: 5.902027130126953\n",
      "h: 119 | epoch: 37, train loss: 8.615478515625, test loss: 5.880553245544434\n",
      "h: 119 | epoch: 38, train loss: 8.604913711547852, test loss: 5.8615922927856445\n",
      "h: 119 | epoch: 39, train loss: 8.596038818359375, test loss: 5.844819068908691\n",
      "h: 119 | epoch: 40, train loss: 8.58858585357666, test loss: 5.829952716827393\n",
      "h: 119 | epoch: 41, train loss: 8.582326889038086, test loss: 5.816755294799805\n",
      "h: 119 | epoch: 42, train loss: 8.577070236206055, test loss: 5.805018901824951\n",
      "h: 119 | epoch: 43, train loss: 8.572657585144043, test loss: 5.7945637702941895\n",
      "h: 119 | epoch: 44, train loss: 8.568950653076172, test loss: 5.785234451293945\n",
      "h: 119 | epoch: 45, train loss: 8.565839767456055, test loss: 5.7768988609313965\n",
      "h: 119 | epoch: 46, train loss: 8.563227653503418, test loss: 5.769436359405518\n",
      "h: 119 | epoch: 47, train loss: 8.56103515625, test loss: 5.762748718261719\n",
      "h: 119 | epoch: 48, train loss: 8.559194564819336, test loss: 5.756745338439941\n",
      "h: 119 | epoch: 49, train loss: 8.557648658752441, test loss: 5.751349449157715\n",
      "h: 119 | epoch: 50, train loss: 8.556352615356445, test loss: 5.746492862701416\n",
      "h: 119 | epoch: 51, train loss: 8.555262565612793, test loss: 5.742117881774902\n",
      "h: 119 | epoch: 52, train loss: 8.554348945617676, test loss: 5.738171100616455\n",
      "h: 119 | epoch: 53, train loss: 8.553582191467285, test loss: 5.7346062660217285\n",
      "h: 119 | epoch: 54, train loss: 8.552936553955078, test loss: 5.731383323669434\n",
      "h: 119 | epoch: 55, train loss: 8.552395820617676, test loss: 5.728466510772705\n",
      "h: 119 | epoch: 56, train loss: 8.551942825317383, test loss: 5.725825786590576\n",
      "h: 119 | epoch: 57, train loss: 8.55156135559082, test loss: 5.723431587219238\n",
      "h: 119 | epoch: 58, train loss: 8.551241874694824, test loss: 5.721259117126465\n",
      "h: 119 | epoch: 59, train loss: 8.550973892211914, test loss: 5.719287395477295\n",
      "h: 119 | epoch: 60, train loss: 8.550748825073242, test loss: 5.717494487762451\n",
      "h: 119 | epoch: 61, train loss: 8.550559043884277, test loss: 5.715865612030029\n",
      "h: 119 | epoch: 62, train loss: 8.55040168762207, test loss: 5.714383125305176\n",
      "h: 119 | epoch: 63, train loss: 8.550267219543457, test loss: 5.713033199310303\n",
      "h: 119 | epoch: 64, train loss: 8.550156593322754, test loss: 5.711804389953613\n",
      "h: 119 | epoch: 65, train loss: 8.55006217956543, test loss: 5.710684776306152\n",
      "h: 119 | epoch: 66, train loss: 8.549983024597168, test loss: 5.709664344787598\n",
      "h: 119 | epoch: 67, train loss: 8.549917221069336, test loss: 5.708733558654785\n",
      "h: 119 | epoch: 68, train loss: 8.549861907958984, test loss: 5.707883358001709\n",
      "h: 119 | epoch: 69, train loss: 8.54981517791748, test loss: 5.707108497619629\n",
      "h: 119 | epoch: 70, train loss: 8.549776077270508, test loss: 5.706401348114014\n",
      "h: 119 | epoch: 71, train loss: 8.54974365234375, test loss: 5.705754280090332\n",
      "h: 119 | epoch: 72, train loss: 8.549715042114258, test loss: 5.705164432525635\n",
      "h: 119 | epoch: 73, train loss: 8.54969310760498, test loss: 5.704625129699707\n",
      "h: 119 | epoch: 74, train loss: 8.549674034118652, test loss: 5.704131603240967\n",
      "h: 119 | epoch: 75, train loss: 8.549657821655273, test loss: 5.703680992126465\n",
      "h: 119 | epoch: 76, train loss: 8.549644470214844, test loss: 5.703268051147461\n",
      "h: 119 | epoch: 77, train loss: 8.549633026123047, test loss: 5.702890872955322\n",
      "h: 119 | epoch: 78, train loss: 8.549622535705566, test loss: 5.702545642852783\n",
      "h: 119 | epoch: 79, train loss: 8.549614906311035, test loss: 5.702229976654053\n",
      "h: 119 | epoch: 80, train loss: 8.54960823059082, test loss: 5.701940536499023\n",
      "h: 119 | epoch: 81, train loss: 8.549602508544922, test loss: 5.701676368713379\n",
      "h: 119 | epoch: 82, train loss: 8.549596786499023, test loss: 5.70143461227417\n",
      "h: 119 | epoch: 83, train loss: 8.549592971801758, test loss: 5.701213359832764\n",
      "h: 119 | epoch: 84, train loss: 8.549589157104492, test loss: 5.701010704040527\n",
      "h: 119 | epoch: 85, train loss: 8.549588203430176, test loss: 5.700824737548828\n",
      "h: 119 | epoch: 86, train loss: 8.549585342407227, test loss: 5.70065450668335\n",
      "h: 119 | epoch: 87, train loss: 8.549582481384277, test loss: 5.700499057769775\n",
      "h: 119 | epoch: 88, train loss: 8.549581527709961, test loss: 5.7003560066223145\n",
      "h: 119 | epoch: 89, train loss: 8.549580574035645, test loss: 5.700225353240967\n",
      "h: 119 | epoch: 90, train loss: 8.549578666687012, test loss: 5.700106143951416\n",
      "h: 119 | epoch: 91, train loss: 8.549577713012695, test loss: 5.699995517730713\n",
      "h: 119 | epoch: 92, train loss: 8.549576759338379, test loss: 5.69989538192749\n",
      "h: 119 | epoch: 93, train loss: 8.549576759338379, test loss: 5.699802875518799\n",
      "h: 119 | epoch: 94, train loss: 8.549575805664062, test loss: 5.699717998504639\n",
      "h: 119 | epoch: 95, train loss: 8.54957389831543, test loss: 5.699642181396484\n",
      "h: 119 | epoch: 96, train loss: 8.54957389831543, test loss: 5.699570655822754\n",
      "h: 119 | epoch: 97, train loss: 8.54957389831543, test loss: 5.699505805969238\n",
      "h: 119 | epoch: 98, train loss: 8.54957389831543, test loss: 5.699445724487305\n",
      "h: 119 | epoch: 99, train loss: 8.54957389831543, test loss: 5.699391841888428\n",
      "h: 120 | epoch: 0, train loss: 51.34101104736328, test loss: 41.090736389160156\n",
      "h: 120 | epoch: 1, train loss: 45.20880126953125, test loss: 36.316078186035156\n",
      "h: 120 | epoch: 2, train loss: 39.96866226196289, test loss: 32.20997619628906\n",
      "h: 120 | epoch: 3, train loss: 35.486141204833984, test loss: 28.674606323242188\n",
      "h: 120 | epoch: 4, train loss: 31.648569107055664, test loss: 25.627532958984375\n",
      "h: 120 | epoch: 5, train loss: 28.3610897064209, test loss: 22.998958587646484\n",
      "h: 120 | epoch: 6, train loss: 25.54351234436035, test loss: 20.72959327697754\n",
      "h: 120 | epoch: 7, train loss: 23.127819061279297, test loss: 18.768936157226562\n",
      "h: 120 | epoch: 8, train loss: 21.056161880493164, test loss: 17.0738525390625\n",
      "h: 120 | epoch: 9, train loss: 19.27923011779785, test loss: 15.607429504394531\n",
      "h: 120 | epoch: 10, train loss: 17.75491714477539, test loss: 14.338029861450195\n",
      "h: 120 | epoch: 11, train loss: 16.447216033935547, test loss: 13.238494873046875\n",
      "h: 120 | epoch: 12, train loss: 15.325307846069336, test loss: 12.285497665405273\n",
      "h: 120 | epoch: 13, train loss: 14.362787246704102, test loss: 11.45897102355957\n",
      "h: 120 | epoch: 14, train loss: 13.537012100219727, test loss: 10.74165153503418\n",
      "h: 120 | epoch: 15, train loss: 12.828569412231445, test loss: 10.118673324584961\n",
      "h: 120 | epoch: 16, train loss: 12.22079849243164, test loss: 9.577224731445312\n",
      "h: 120 | epoch: 17, train loss: 11.699408531188965, test loss: 9.106273651123047\n",
      "h: 120 | epoch: 18, train loss: 11.252134323120117, test loss: 8.696305274963379\n",
      "h: 120 | epoch: 19, train loss: 10.86844539642334, test loss: 8.339109420776367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 120 | epoch: 20, train loss: 10.539306640625, test loss: 8.027608871459961\n",
      "h: 120 | epoch: 21, train loss: 10.256964683532715, test loss: 7.755698204040527\n",
      "h: 120 | epoch: 22, train loss: 10.014766693115234, test loss: 7.518102169036865\n",
      "h: 120 | epoch: 23, train loss: 9.807001113891602, test loss: 7.31027364730835\n",
      "h: 120 | epoch: 24, train loss: 9.628768920898438, test loss: 7.128277778625488\n",
      "h: 120 | epoch: 25, train loss: 9.47586727142334, test loss: 6.968721866607666\n",
      "h: 120 | epoch: 26, train loss: 9.344694137573242, test loss: 6.828669548034668\n",
      "h: 120 | epoch: 27, train loss: 9.232154846191406, test loss: 6.705583095550537\n",
      "h: 120 | epoch: 28, train loss: 9.135597229003906, test loss: 6.59726619720459\n",
      "h: 120 | epoch: 29, train loss: 9.05274486541748, test loss: 6.501824855804443\n",
      "h: 120 | epoch: 30, train loss: 8.981649398803711, test loss: 6.417608737945557\n",
      "h: 120 | epoch: 31, train loss: 8.920639991760254, test loss: 6.343195915222168\n",
      "h: 120 | epoch: 32, train loss: 8.868276596069336, test loss: 6.277351379394531\n",
      "h: 120 | epoch: 33, train loss: 8.823331832885742, test loss: 6.219003200531006\n",
      "h: 120 | epoch: 34, train loss: 8.784753799438477, test loss: 6.167222023010254\n",
      "h: 120 | epoch: 35, train loss: 8.75163459777832, test loss: 6.121199607849121\n",
      "h: 120 | epoch: 36, train loss: 8.723196983337402, test loss: 6.080235481262207\n",
      "h: 120 | epoch: 37, train loss: 8.698780059814453, test loss: 6.043717384338379\n",
      "h: 120 | epoch: 38, train loss: 8.677812576293945, test loss: 6.011114597320557\n",
      "h: 120 | epoch: 39, train loss: 8.65980339050293, test loss: 5.981961250305176\n",
      "h: 120 | epoch: 40, train loss: 8.644335746765137, test loss: 5.955854892730713\n",
      "h: 120 | epoch: 41, train loss: 8.631047248840332, test loss: 5.932441234588623\n",
      "h: 120 | epoch: 42, train loss: 8.619629859924316, test loss: 5.911410331726074\n",
      "h: 120 | epoch: 43, train loss: 8.609820365905762, test loss: 5.892495155334473\n",
      "h: 120 | epoch: 44, train loss: 8.60138988494873, test loss: 5.875454425811768\n",
      "h: 120 | epoch: 45, train loss: 8.594143867492676, test loss: 5.86008358001709\n",
      "h: 120 | epoch: 46, train loss: 8.587916374206543, test loss: 5.846198558807373\n",
      "h: 120 | epoch: 47, train loss: 8.582561492919922, test loss: 5.8336381912231445\n",
      "h: 120 | epoch: 48, train loss: 8.577959060668945, test loss: 5.822261810302734\n",
      "h: 120 | epoch: 49, train loss: 8.573999404907227, test loss: 5.811944007873535\n",
      "h: 120 | epoch: 50, train loss: 8.570595741271973, test loss: 5.802575588226318\n",
      "h: 120 | epoch: 51, train loss: 8.567668914794922, test loss: 5.794057369232178\n",
      "h: 120 | epoch: 52, train loss: 8.565149307250977, test loss: 5.7863054275512695\n",
      "h: 120 | epoch: 53, train loss: 8.562982559204102, test loss: 5.779240131378174\n",
      "h: 120 | epoch: 54, train loss: 8.561119079589844, test loss: 5.772795677185059\n",
      "h: 120 | epoch: 55, train loss: 8.559514999389648, test loss: 5.766909599304199\n",
      "h: 120 | epoch: 56, train loss: 8.558134078979492, test loss: 5.76153039932251\n",
      "h: 120 | epoch: 57, train loss: 8.556946754455566, test loss: 5.7566094398498535\n",
      "h: 120 | epoch: 58, train loss: 8.555925369262695, test loss: 5.752100944519043\n",
      "h: 120 | epoch: 59, train loss: 8.555044174194336, test loss: 5.747969627380371\n",
      "h: 120 | epoch: 60, train loss: 8.554286003112793, test loss: 5.744180202484131\n",
      "h: 120 | epoch: 61, train loss: 8.553633689880371, test loss: 5.740702152252197\n",
      "h: 120 | epoch: 62, train loss: 8.553071975708008, test loss: 5.7375054359436035\n",
      "h: 120 | epoch: 63, train loss: 8.552587509155273, test loss: 5.734566688537598\n",
      "h: 120 | epoch: 64, train loss: 8.55217170715332, test loss: 5.731863975524902\n",
      "h: 120 | epoch: 65, train loss: 8.551813125610352, test loss: 5.729374885559082\n",
      "h: 120 | epoch: 66, train loss: 8.551502227783203, test loss: 5.7270827293396\n",
      "h: 120 | epoch: 67, train loss: 8.551237106323242, test loss: 5.724969863891602\n",
      "h: 120 | epoch: 68, train loss: 8.551007270812988, test loss: 5.7230224609375\n",
      "h: 120 | epoch: 69, train loss: 8.550809860229492, test loss: 5.721225738525391\n",
      "h: 120 | epoch: 70, train loss: 8.550638198852539, test loss: 5.7195658683776855\n",
      "h: 120 | epoch: 71, train loss: 8.550491333007812, test loss: 5.7180352210998535\n",
      "h: 120 | epoch: 72, train loss: 8.55036449432373, test loss: 5.716620922088623\n",
      "h: 120 | epoch: 73, train loss: 8.55025577545166, test loss: 5.715312480926514\n",
      "h: 120 | epoch: 74, train loss: 8.550161361694336, test loss: 5.714104652404785\n",
      "h: 120 | epoch: 75, train loss: 8.550081253051758, test loss: 5.712986469268799\n",
      "h: 120 | epoch: 76, train loss: 8.55001163482666, test loss: 5.711952209472656\n",
      "h: 120 | epoch: 77, train loss: 8.54995059967041, test loss: 5.710993766784668\n",
      "h: 120 | epoch: 78, train loss: 8.549898147583008, test loss: 5.710110187530518\n",
      "h: 120 | epoch: 79, train loss: 8.549854278564453, test loss: 5.709290504455566\n",
      "h: 120 | epoch: 80, train loss: 8.54981517791748, test loss: 5.708529949188232\n",
      "h: 120 | epoch: 81, train loss: 8.549781799316406, test loss: 5.707827091217041\n",
      "h: 120 | epoch: 82, train loss: 8.549753189086914, test loss: 5.707173824310303\n",
      "h: 120 | epoch: 83, train loss: 8.549728393554688, test loss: 5.706571578979492\n",
      "h: 120 | epoch: 84, train loss: 8.549707412719727, test loss: 5.706011772155762\n",
      "h: 120 | epoch: 85, train loss: 8.549688339233398, test loss: 5.7054924964904785\n",
      "h: 120 | epoch: 86, train loss: 8.549673080444336, test loss: 5.705012321472168\n",
      "h: 120 | epoch: 87, train loss: 8.549657821655273, test loss: 5.704565048217773\n",
      "h: 120 | epoch: 88, train loss: 8.549647331237793, test loss: 5.704151630401611\n",
      "h: 120 | epoch: 89, train loss: 8.549636840820312, test loss: 5.703768253326416\n",
      "h: 120 | epoch: 90, train loss: 8.549628257751465, test loss: 5.703412055969238\n",
      "h: 120 | epoch: 91, train loss: 8.549620628356934, test loss: 5.7030816078186035\n",
      "h: 120 | epoch: 92, train loss: 8.549614906311035, test loss: 5.702775001525879\n",
      "h: 120 | epoch: 93, train loss: 8.54960823059082, test loss: 5.702490329742432\n",
      "h: 120 | epoch: 94, train loss: 8.549603462219238, test loss: 5.702227592468262\n",
      "h: 120 | epoch: 95, train loss: 8.549598693847656, test loss: 5.701981544494629\n",
      "h: 120 | epoch: 96, train loss: 8.549595832824707, test loss: 5.701754570007324\n",
      "h: 120 | epoch: 97, train loss: 8.549592018127441, test loss: 5.701542854309082\n",
      "h: 120 | epoch: 98, train loss: 8.549590110778809, test loss: 5.701346397399902\n",
      "h: 120 | epoch: 99, train loss: 8.54958724975586, test loss: 5.701165199279785\n",
      "h: 121 | epoch: 0, train loss: 46.81382369995117, test loss: 38.2821159362793\n",
      "h: 121 | epoch: 1, train loss: 41.45817565917969, test loss: 33.89781188964844\n",
      "h: 121 | epoch: 2, train loss: 36.857093811035156, test loss: 30.11846351623535\n",
      "h: 121 | epoch: 3, train loss: 32.90140914916992, test loss: 26.857784271240234\n",
      "h: 121 | epoch: 4, train loss: 29.49880599975586, test loss: 24.042583465576172\n",
      "h: 121 | epoch: 5, train loss: 26.57088851928711, test loss: 21.61053466796875\n",
      "h: 121 | epoch: 6, train loss: 24.050872802734375, test loss: 19.508386611938477\n",
      "h: 121 | epoch: 7, train loss: 21.881671905517578, test loss: 17.690555572509766\n",
      "h: 121 | epoch: 8, train loss: 20.014381408691406, test loss: 16.117950439453125\n",
      "h: 121 | epoch: 9, train loss: 18.40704345703125, test loss: 14.756959915161133\n",
      "h: 121 | epoch: 10, train loss: 17.02359390258789, test loss: 13.578672409057617\n",
      "h: 121 | epoch: 11, train loss: 15.83299732208252, test loss: 12.558182716369629\n",
      "h: 121 | epoch: 12, train loss: 14.808542251586914, test loss: 11.674027442932129\n",
      "h: 121 | epoch: 13, train loss: 13.927202224731445, test loss: 10.907691955566406\n",
      "h: 121 | epoch: 14, train loss: 13.169137954711914, test loss: 10.243205070495605\n",
      "h: 121 | epoch: 15, train loss: 12.517237663269043, test loss: 9.666769981384277\n",
      "h: 121 | epoch: 16, train loss: 11.956755638122559, test loss: 9.166482925415039\n",
      "h: 121 | epoch: 17, train loss: 11.474973678588867, test loss: 8.732065200805664\n",
      "h: 121 | epoch: 18, train loss: 11.060930252075195, test loss: 8.354635238647461\n",
      "h: 121 | epoch: 19, train loss: 10.705171585083008, test loss: 8.026517868041992\n",
      "h: 121 | epoch: 20, train loss: 10.399555206298828, test loss: 7.7410888671875\n",
      "h: 121 | epoch: 21, train loss: 10.137063980102539, test loss: 7.492626190185547\n",
      "h: 121 | epoch: 22, train loss: 9.911653518676758, test loss: 7.276181221008301\n",
      "h: 121 | epoch: 23, train loss: 9.718119621276855, test loss: 7.0874786376953125\n",
      "h: 121 | epoch: 24, train loss: 9.551982879638672, test loss: 6.922831058502197\n",
      "h: 121 | epoch: 25, train loss: 9.409383773803711, test loss: 6.779041290283203\n",
      "h: 121 | epoch: 26, train loss: 9.287007331848145, test loss: 6.653351783752441\n",
      "h: 121 | epoch: 27, train loss: 9.181997299194336, test loss: 6.543377876281738\n",
      "h: 121 | epoch: 28, train loss: 9.091904640197754, test loss: 6.447056770324707\n",
      "h: 121 | epoch: 29, train loss: 9.014615058898926, test loss: 6.362600803375244\n",
      "h: 121 | epoch: 30, train loss: 8.94831657409668, test loss: 6.288468837738037\n",
      "h: 121 | epoch: 31, train loss: 8.891451835632324, test loss: 6.223325252532959\n",
      "h: 121 | epoch: 32, train loss: 8.842683792114258, test loss: 6.166012287139893\n",
      "h: 121 | epoch: 33, train loss: 8.800862312316895, test loss: 6.11552619934082\n",
      "h: 121 | epoch: 34, train loss: 8.764999389648438, test loss: 6.070998668670654\n",
      "h: 121 | epoch: 35, train loss: 8.73425006866455, test loss: 6.031677722930908\n",
      "h: 121 | epoch: 36, train loss: 8.7078857421875, test loss: 5.996908187866211\n",
      "h: 121 | epoch: 37, train loss: 8.685280799865723, test loss: 5.966123104095459\n",
      "h: 121 | epoch: 38, train loss: 8.665902137756348, test loss: 5.93882942199707\n",
      "h: 121 | epoch: 39, train loss: 8.64928913116455, test loss: 5.914597511291504\n",
      "h: 121 | epoch: 40, train loss: 8.63504695892334, test loss: 5.893056392669678\n",
      "h: 121 | epoch: 41, train loss: 8.622838973999023, test loss: 5.873880386352539\n",
      "h: 121 | epoch: 42, train loss: 8.612373352050781, test loss: 5.85678768157959\n",
      "h: 121 | epoch: 43, train loss: 8.603403091430664, test loss: 5.841529846191406\n",
      "h: 121 | epoch: 44, train loss: 8.595714569091797, test loss: 5.8278913497924805\n",
      "h: 121 | epoch: 45, train loss: 8.589123725891113, test loss: 5.815683841705322\n",
      "h: 121 | epoch: 46, train loss: 8.583473205566406, test loss: 5.804745197296143\n",
      "h: 121 | epoch: 47, train loss: 8.578631401062012, test loss: 5.794928550720215\n",
      "h: 121 | epoch: 48, train loss: 8.574481010437012, test loss: 5.786106586456299\n",
      "h: 121 | epoch: 49, train loss: 8.5709228515625, test loss: 5.778170585632324\n",
      "h: 121 | epoch: 50, train loss: 8.567873001098633, test loss: 5.771021366119385\n",
      "h: 121 | epoch: 51, train loss: 8.56525993347168, test loss: 5.764573097229004\n",
      "h: 121 | epoch: 52, train loss: 8.563019752502441, test loss: 5.75875186920166\n",
      "h: 121 | epoch: 53, train loss: 8.5610990524292, test loss: 5.753488063812256\n",
      "h: 121 | epoch: 54, train loss: 8.559453964233398, test loss: 5.748726844787598\n",
      "h: 121 | epoch: 55, train loss: 8.558042526245117, test loss: 5.744410991668701\n",
      "h: 121 | epoch: 56, train loss: 8.556833267211914, test loss: 5.740498065948486\n",
      "h: 121 | epoch: 57, train loss: 8.555795669555664, test loss: 5.736944198608398\n",
      "h: 121 | epoch: 58, train loss: 8.55490779876709, test loss: 5.733716011047363\n",
      "h: 121 | epoch: 59, train loss: 8.554145812988281, test loss: 5.730780124664307\n",
      "h: 121 | epoch: 60, train loss: 8.553492546081543, test loss: 5.728107452392578\n",
      "h: 121 | epoch: 61, train loss: 8.552934646606445, test loss: 5.725672721862793\n",
      "h: 121 | epoch: 62, train loss: 8.552453994750977, test loss: 5.723452091217041\n",
      "h: 121 | epoch: 63, train loss: 8.552042961120605, test loss: 5.721426010131836\n",
      "h: 121 | epoch: 64, train loss: 8.551691055297852, test loss: 5.719574928283691\n",
      "h: 121 | epoch: 65, train loss: 8.551387786865234, test loss: 5.717883110046387\n",
      "h: 121 | epoch: 66, train loss: 8.551128387451172, test loss: 5.716338157653809\n",
      "h: 121 | epoch: 67, train loss: 8.550908088684082, test loss: 5.714921951293945\n",
      "h: 121 | epoch: 68, train loss: 8.5507173538208, test loss: 5.713625907897949\n",
      "h: 121 | epoch: 69, train loss: 8.550554275512695, test loss: 5.71243953704834\n",
      "h: 121 | epoch: 70, train loss: 8.550413131713867, test loss: 5.71135139465332\n",
      "h: 121 | epoch: 71, train loss: 8.550293922424316, test loss: 5.710352897644043\n",
      "h: 121 | epoch: 72, train loss: 8.550190925598145, test loss: 5.709436893463135\n",
      "h: 121 | epoch: 73, train loss: 8.550104141235352, test loss: 5.7085957527160645\n",
      "h: 121 | epoch: 74, train loss: 8.550027847290039, test loss: 5.707823753356934\n",
      "h: 121 | epoch: 75, train loss: 8.549962997436523, test loss: 5.707115173339844\n",
      "h: 121 | epoch: 76, train loss: 8.549906730651855, test loss: 5.706462860107422\n",
      "h: 121 | epoch: 77, train loss: 8.549859046936035, test loss: 5.7058634757995605\n",
      "h: 121 | epoch: 78, train loss: 8.54981803894043, test loss: 5.705312728881836\n",
      "h: 121 | epoch: 79, train loss: 8.549783706665039, test loss: 5.704805850982666\n",
      "h: 121 | epoch: 80, train loss: 8.549753189086914, test loss: 5.704338550567627\n",
      "h: 121 | epoch: 81, train loss: 8.549728393554688, test loss: 5.703908443450928\n",
      "h: 121 | epoch: 82, train loss: 8.549705505371094, test loss: 5.703514099121094\n",
      "h: 121 | epoch: 83, train loss: 8.549686431884766, test loss: 5.703148365020752\n",
      "h: 121 | epoch: 84, train loss: 8.549670219421387, test loss: 5.702813148498535\n",
      "h: 121 | epoch: 85, train loss: 8.549656867980957, test loss: 5.702503681182861\n",
      "h: 121 | epoch: 86, train loss: 8.549644470214844, test loss: 5.702218532562256\n",
      "h: 121 | epoch: 87, train loss: 8.54963493347168, test loss: 5.701954364776611\n",
      "h: 121 | epoch: 88, train loss: 8.549626350402832, test loss: 5.701712608337402\n",
      "h: 121 | epoch: 89, train loss: 8.549617767333984, test loss: 5.701488971710205\n",
      "h: 121 | epoch: 90, train loss: 8.549612045288086, test loss: 5.701282501220703\n",
      "h: 121 | epoch: 91, train loss: 8.549606323242188, test loss: 5.701091766357422\n",
      "h: 121 | epoch: 92, train loss: 8.549600601196289, test loss: 5.700916290283203\n",
      "h: 121 | epoch: 93, train loss: 8.549596786499023, test loss: 5.700754165649414\n",
      "h: 121 | epoch: 94, train loss: 8.549592971801758, test loss: 5.7006049156188965\n",
      "h: 121 | epoch: 95, train loss: 8.549591064453125, test loss: 5.700467109680176\n",
      "h: 121 | epoch: 96, train loss: 8.549589157104492, test loss: 5.7003397941589355\n",
      "h: 121 | epoch: 97, train loss: 8.549586296081543, test loss: 5.700222492218018\n",
      "h: 121 | epoch: 98, train loss: 8.549583435058594, test loss: 5.700112342834473\n",
      "h: 121 | epoch: 99, train loss: 8.549581527709961, test loss: 5.700012683868408\n",
      "h: 122 | epoch: 0, train loss: 49.34913635253906, test loss: 39.57703399658203\n",
      "h: 122 | epoch: 1, train loss: 42.34175491333008, test loss: 34.095787048339844\n",
      "h: 122 | epoch: 2, train loss: 36.54500198364258, test loss: 29.5267391204834\n",
      "h: 122 | epoch: 3, train loss: 31.746013641357422, test loss: 25.713993072509766\n",
      "h: 122 | epoch: 4, train loss: 27.770954132080078, test loss: 22.52937889099121\n",
      "h: 122 | epoch: 5, train loss: 24.47723960876465, test loss: 19.867198944091797\n",
      "h: 122 | epoch: 6, train loss: 21.74755859375, test loss: 17.639995574951172\n",
      "h: 122 | epoch: 7, train loss: 19.485126495361328, test loss: 15.775288581848145\n",
      "h: 122 | epoch: 8, train loss: 17.609943389892578, test loss: 14.212892532348633\n",
      "h: 122 | epoch: 9, train loss: 16.055795669555664, test loss: 12.902755737304688\n",
      "h: 122 | epoch: 10, train loss: 14.767827033996582, test loss: 11.803243637084961\n",
      "h: 122 | epoch: 11, train loss: 13.7005615234375, test loss: 10.87967300415039\n",
      "h: 122 | epoch: 12, train loss: 12.816276550292969, test loss: 10.103147506713867\n",
      "h: 122 | epoch: 13, train loss: 12.083684921264648, test loss: 9.449578285217285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 122 | epoch: 14, train loss: 11.47683334350586, test loss: 8.898882865905762\n",
      "h: 122 | epoch: 15, train loss: 10.974187850952148, test loss: 8.434301376342773\n",
      "h: 122 | epoch: 16, train loss: 10.557887077331543, test loss: 8.041858673095703\n",
      "h: 122 | epoch: 17, train loss: 10.213128089904785, test loss: 7.709890842437744\n",
      "h: 122 | epoch: 18, train loss: 9.927624702453613, test loss: 7.428652763366699\n",
      "h: 122 | epoch: 19, train loss: 9.691200256347656, test loss: 7.190018653869629\n",
      "h: 122 | epoch: 20, train loss: 9.495423316955566, test loss: 6.987187385559082\n",
      "h: 122 | epoch: 21, train loss: 9.333301544189453, test loss: 6.81448221206665\n",
      "h: 122 | epoch: 22, train loss: 9.199045181274414, test loss: 6.667153835296631\n",
      "h: 122 | epoch: 23, train loss: 9.087861061096191, test loss: 6.5412278175354\n",
      "h: 122 | epoch: 24, train loss: 8.995776176452637, test loss: 6.433375358581543\n",
      "h: 122 | epoch: 25, train loss: 8.919504165649414, test loss: 6.340813159942627\n",
      "h: 122 | epoch: 26, train loss: 8.856322288513184, test loss: 6.261197566986084\n",
      "h: 122 | epoch: 27, train loss: 8.803977966308594, test loss: 6.192570209503174\n",
      "h: 122 | epoch: 28, train loss: 8.760608673095703, test loss: 6.133280277252197\n",
      "h: 122 | epoch: 29, train loss: 8.7246675491333, test loss: 6.081939697265625\n",
      "h: 122 | epoch: 30, train loss: 8.694878578186035, test loss: 6.037380218505859\n",
      "h: 122 | epoch: 31, train loss: 8.670184135437012, test loss: 5.998618125915527\n",
      "h: 122 | epoch: 32, train loss: 8.649709701538086, test loss: 5.964821815490723\n",
      "h: 122 | epoch: 33, train loss: 8.632730484008789, test loss: 5.935286045074463\n",
      "h: 122 | epoch: 34, train loss: 8.618646621704102, test loss: 5.909415245056152\n",
      "h: 122 | epoch: 35, train loss: 8.606962203979492, test loss: 5.886704444885254\n",
      "h: 122 | epoch: 36, train loss: 8.59726619720459, test loss: 5.866722583770752\n",
      "h: 122 | epoch: 37, train loss: 8.589219093322754, test loss: 5.849104881286621\n",
      "h: 122 | epoch: 38, train loss: 8.582537651062012, test loss: 5.833537578582764\n",
      "h: 122 | epoch: 39, train loss: 8.576990127563477, test loss: 5.819756507873535\n",
      "h: 122 | epoch: 40, train loss: 8.572381973266602, test loss: 5.807530403137207\n",
      "h: 122 | epoch: 41, train loss: 8.56855297088623, test loss: 5.796663761138916\n",
      "h: 122 | epoch: 42, train loss: 8.5653715133667, test loss: 5.786988735198975\n",
      "h: 122 | epoch: 43, train loss: 8.562726974487305, test loss: 5.778359413146973\n",
      "h: 122 | epoch: 44, train loss: 8.560528755187988, test loss: 5.770648956298828\n",
      "h: 122 | epoch: 45, train loss: 8.558698654174805, test loss: 5.763749599456787\n",
      "h: 122 | epoch: 46, train loss: 8.55717658996582, test loss: 5.757565975189209\n",
      "h: 122 | epoch: 47, train loss: 8.55591106414795, test loss: 5.752017021179199\n",
      "h: 122 | epoch: 48, train loss: 8.55485725402832, test loss: 5.747030735015869\n",
      "h: 122 | epoch: 49, train loss: 8.553979873657227, test loss: 5.7425432205200195\n",
      "h: 122 | epoch: 50, train loss: 8.55324935913086, test loss: 5.738500595092773\n",
      "h: 122 | epoch: 51, train loss: 8.552640914916992, test loss: 5.734856128692627\n",
      "h: 122 | epoch: 52, train loss: 8.552133560180664, test loss: 5.731564521789551\n",
      "h: 122 | epoch: 53, train loss: 8.55171012878418, test loss: 5.72859001159668\n",
      "h: 122 | epoch: 54, train loss: 8.551358222961426, test loss: 5.7258992195129395\n",
      "h: 122 | epoch: 55, train loss: 8.551063537597656, test loss: 5.723464012145996\n",
      "h: 122 | epoch: 56, train loss: 8.55081844329834, test loss: 5.721257209777832\n",
      "h: 122 | epoch: 57, train loss: 8.550614356994629, test loss: 5.719257354736328\n",
      "h: 122 | epoch: 58, train loss: 8.550443649291992, test loss: 5.717440605163574\n",
      "h: 122 | epoch: 59, train loss: 8.550299644470215, test loss: 5.715792655944824\n",
      "h: 122 | epoch: 60, train loss: 8.550180435180664, test loss: 5.714295387268066\n",
      "h: 122 | epoch: 61, train loss: 8.550081253051758, test loss: 5.712934970855713\n",
      "h: 122 | epoch: 62, train loss: 8.54999828338623, test loss: 5.711695671081543\n",
      "h: 122 | epoch: 63, train loss: 8.549928665161133, test loss: 5.710569858551025\n",
      "h: 122 | epoch: 64, train loss: 8.549871444702148, test loss: 5.709545135498047\n",
      "h: 122 | epoch: 65, train loss: 8.549822807312012, test loss: 5.708611965179443\n",
      "h: 122 | epoch: 66, train loss: 8.549781799316406, test loss: 5.707760810852051\n",
      "h: 122 | epoch: 67, train loss: 8.549748420715332, test loss: 5.706985950469971\n",
      "h: 122 | epoch: 68, train loss: 8.549718856811523, test loss: 5.7062788009643555\n",
      "h: 122 | epoch: 69, train loss: 8.54969596862793, test loss: 5.705633640289307\n",
      "h: 122 | epoch: 70, train loss: 8.549676895141602, test loss: 5.705045700073242\n",
      "h: 122 | epoch: 71, train loss: 8.54965877532959, test loss: 5.7045087814331055\n",
      "h: 122 | epoch: 72, train loss: 8.54964542388916, test loss: 5.704019546508789\n",
      "h: 122 | epoch: 73, train loss: 8.549633979797363, test loss: 5.7035722732543945\n",
      "h: 122 | epoch: 74, train loss: 8.5496244430542, test loss: 5.703164577484131\n",
      "h: 122 | epoch: 75, train loss: 8.549615859985352, test loss: 5.702791690826416\n",
      "h: 122 | epoch: 76, train loss: 8.54960823059082, test loss: 5.702450275421143\n",
      "h: 122 | epoch: 77, train loss: 8.549603462219238, test loss: 5.702138900756836\n",
      "h: 122 | epoch: 78, train loss: 8.54959774017334, test loss: 5.701854705810547\n",
      "h: 122 | epoch: 79, train loss: 8.549593925476074, test loss: 5.701594829559326\n",
      "h: 122 | epoch: 80, train loss: 8.549591064453125, test loss: 5.701355934143066\n",
      "h: 122 | epoch: 81, train loss: 8.549588203430176, test loss: 5.701138973236084\n",
      "h: 122 | epoch: 82, train loss: 8.54958438873291, test loss: 5.7009406089782715\n",
      "h: 122 | epoch: 83, train loss: 8.549583435058594, test loss: 5.700758934020996\n",
      "h: 122 | epoch: 84, train loss: 8.549581527709961, test loss: 5.700592517852783\n",
      "h: 122 | epoch: 85, train loss: 8.549581527709961, test loss: 5.700440406799316\n",
      "h: 122 | epoch: 86, train loss: 8.549578666687012, test loss: 5.7003021240234375\n",
      "h: 122 | epoch: 87, train loss: 8.549577713012695, test loss: 5.700173854827881\n",
      "h: 122 | epoch: 88, train loss: 8.549577713012695, test loss: 5.700057506561279\n",
      "h: 122 | epoch: 89, train loss: 8.549576759338379, test loss: 5.699950218200684\n",
      "h: 122 | epoch: 90, train loss: 8.549575805664062, test loss: 5.69985294342041\n",
      "h: 122 | epoch: 91, train loss: 8.549575805664062, test loss: 5.699765205383301\n",
      "h: 122 | epoch: 92, train loss: 8.549574851989746, test loss: 5.699681758880615\n",
      "h: 122 | epoch: 93, train loss: 8.54957389831543, test loss: 5.699606895446777\n",
      "h: 122 | epoch: 94, train loss: 8.549574851989746, test loss: 5.6995391845703125\n",
      "h: 122 | epoch: 95, train loss: 8.54957389831543, test loss: 5.6994757652282715\n",
      "h: 122 | epoch: 96, train loss: 8.54957389831543, test loss: 5.699418067932129\n",
      "h: 122 | epoch: 97, train loss: 8.549574851989746, test loss: 5.699365139007568\n",
      "h: 122 | epoch: 98, train loss: 8.54957389831543, test loss: 5.69931697845459\n",
      "h: 122 | epoch: 99, train loss: 8.549572944641113, test loss: 5.699274063110352\n",
      "h: 123 | epoch: 0, train loss: 49.72858428955078, test loss: 39.93027114868164\n",
      "h: 123 | epoch: 1, train loss: 43.40203094482422, test loss: 34.925289154052734\n",
      "h: 123 | epoch: 2, train loss: 38.048736572265625, test loss: 30.6669864654541\n",
      "h: 123 | epoch: 3, train loss: 33.5162467956543, test loss: 27.041244506835938\n",
      "h: 123 | epoch: 4, train loss: 29.677295684814453, test loss: 23.952259063720703\n",
      "h: 123 | epoch: 5, train loss: 26.42514419555664, test loss: 21.31930160522461\n",
      "h: 123 | epoch: 6, train loss: 23.669984817504883, test loss: 19.074132919311523\n",
      "h: 123 | epoch: 7, train loss: 21.33603286743164, test loss: 17.158958435058594\n",
      "h: 123 | epoch: 8, train loss: 19.359207153320312, test loss: 15.52473258972168\n",
      "h: 123 | epoch: 9, train loss: 17.685239791870117, test loss: 14.129789352416992\n",
      "h: 123 | epoch: 10, train loss: 16.268125534057617, test loss: 12.938688278198242\n",
      "h: 123 | epoch: 11, train loss: 15.068817138671875, test loss: 11.92127799987793\n",
      "h: 123 | epoch: 12, train loss: 14.054174423217773, test loss: 11.05188274383545\n",
      "h: 123 | epoch: 13, train loss: 13.196062088012695, test loss: 10.308640480041504\n",
      "h: 123 | epoch: 14, train loss: 12.470582962036133, test loss: 9.672921180725098\n",
      "h: 123 | epoch: 15, train loss: 11.857454299926758, test loss: 9.128870010375977\n",
      "h: 123 | epoch: 16, train loss: 11.339456558227539, test loss: 8.662975311279297\n",
      "h: 123 | epoch: 17, train loss: 10.90197467803955, test loss: 8.26373291015625\n",
      "h: 123 | epoch: 18, train loss: 10.532615661621094, test loss: 7.9213361740112305\n",
      "h: 123 | epoch: 19, train loss: 10.22087287902832, test loss: 7.62744665145874\n",
      "h: 123 | epoch: 20, train loss: 9.957833290100098, test loss: 7.374952793121338\n",
      "h: 123 | epoch: 21, train loss: 9.735955238342285, test loss: 7.157809257507324\n",
      "h: 123 | epoch: 22, train loss: 9.548845291137695, test loss: 6.970861911773682\n",
      "h: 123 | epoch: 23, train loss: 9.39109992980957, test loss: 6.809723854064941\n",
      "h: 123 | epoch: 24, train loss: 9.258138656616211, test loss: 6.670656681060791\n",
      "h: 123 | epoch: 25, train loss: 9.146095275878906, test loss: 6.550478935241699\n",
      "h: 123 | epoch: 26, train loss: 9.051697731018066, test loss: 6.446483612060547\n",
      "h: 123 | epoch: 27, train loss: 8.972182273864746, test loss: 6.356356620788574\n",
      "h: 123 | epoch: 28, train loss: 8.905217170715332, test loss: 6.278130531311035\n",
      "h: 123 | epoch: 29, train loss: 8.848828315734863, test loss: 6.210125923156738\n",
      "h: 123 | epoch: 30, train loss: 8.801355361938477, test loss: 6.150910377502441\n",
      "h: 123 | epoch: 31, train loss: 8.761391639709473, test loss: 6.099260330200195\n",
      "h: 123 | epoch: 32, train loss: 8.72775650024414, test loss: 6.054132461547852\n",
      "h: 123 | epoch: 33, train loss: 8.69944953918457, test loss: 6.0146331787109375\n",
      "h: 123 | epoch: 34, train loss: 8.675630569458008, test loss: 5.979996681213379\n",
      "h: 123 | epoch: 35, train loss: 8.65558910369873, test loss: 5.949572563171387\n",
      "h: 123 | epoch: 36, train loss: 8.6387300491333, test loss: 5.922798156738281\n",
      "h: 123 | epoch: 37, train loss: 8.624547004699707, test loss: 5.89919376373291\n",
      "h: 123 | epoch: 38, train loss: 8.612615585327148, test loss: 5.878344535827637\n",
      "h: 123 | epoch: 39, train loss: 8.602581977844238, test loss: 5.859896659851074\n",
      "h: 123 | epoch: 40, train loss: 8.594141960144043, test loss: 5.843545913696289\n",
      "h: 123 | epoch: 41, train loss: 8.587045669555664, test loss: 5.829026222229004\n",
      "h: 123 | epoch: 42, train loss: 8.581076622009277, test loss: 5.8161115646362305\n",
      "h: 123 | epoch: 43, train loss: 8.576058387756348, test loss: 5.8046040534973145\n",
      "h: 123 | epoch: 44, train loss: 8.57183837890625, test loss: 5.794334888458252\n",
      "h: 123 | epoch: 45, train loss: 8.568290710449219, test loss: 5.785154819488525\n",
      "h: 123 | epoch: 46, train loss: 8.565309524536133, test loss: 5.776938438415527\n",
      "h: 123 | epoch: 47, train loss: 8.562799453735352, test loss: 5.769570350646973\n",
      "h: 123 | epoch: 48, train loss: 8.560690879821777, test loss: 5.762955665588379\n",
      "h: 123 | epoch: 49, train loss: 8.558918952941895, test loss: 5.7570085525512695\n",
      "h: 123 | epoch: 50, train loss: 8.557429313659668, test loss: 5.751653671264648\n",
      "h: 123 | epoch: 51, train loss: 8.556177139282227, test loss: 5.746827602386475\n",
      "h: 123 | epoch: 52, train loss: 8.555123329162598, test loss: 5.742473125457764\n",
      "h: 123 | epoch: 53, train loss: 8.554238319396973, test loss: 5.738539218902588\n",
      "h: 123 | epoch: 54, train loss: 8.553494453430176, test loss: 5.734980583190918\n",
      "h: 123 | epoch: 55, train loss: 8.552868843078613, test loss: 5.731760025024414\n",
      "h: 123 | epoch: 56, train loss: 8.552343368530273, test loss: 5.7288408279418945\n",
      "h: 123 | epoch: 57, train loss: 8.551900863647461, test loss: 5.726193904876709\n",
      "h: 123 | epoch: 58, train loss: 8.551529884338379, test loss: 5.723790168762207\n",
      "h: 123 | epoch: 59, train loss: 8.551218032836914, test loss: 5.7216081619262695\n",
      "h: 123 | epoch: 60, train loss: 8.550954818725586, test loss: 5.719624042510986\n",
      "h: 123 | epoch: 61, train loss: 8.550734519958496, test loss: 5.7178192138671875\n",
      "h: 123 | epoch: 62, train loss: 8.550549507141113, test loss: 5.7161760330200195\n",
      "h: 123 | epoch: 63, train loss: 8.550393104553223, test loss: 5.714680194854736\n",
      "h: 123 | epoch: 64, train loss: 8.550262451171875, test loss: 5.713316917419434\n",
      "h: 123 | epoch: 65, train loss: 8.550153732299805, test loss: 5.71207332611084\n",
      "h: 123 | epoch: 66, train loss: 8.550060272216797, test loss: 5.710939407348633\n",
      "h: 123 | epoch: 67, train loss: 8.549982070922852, test loss: 5.70990514755249\n",
      "h: 123 | epoch: 68, train loss: 8.54991626739502, test loss: 5.70896053314209\n",
      "h: 123 | epoch: 69, train loss: 8.549861907958984, test loss: 5.708098888397217\n",
      "h: 123 | epoch: 70, train loss: 8.549817085266113, test loss: 5.707310676574707\n",
      "h: 123 | epoch: 71, train loss: 8.549777030944824, test loss: 5.70659065246582\n",
      "h: 123 | epoch: 72, train loss: 8.549744606018066, test loss: 5.705932140350342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 123 | epoch: 73, train loss: 8.54971694946289, test loss: 5.7053303718566895\n",
      "h: 123 | epoch: 74, train loss: 8.549694061279297, test loss: 5.704780578613281\n",
      "h: 123 | epoch: 75, train loss: 8.549674987792969, test loss: 5.704276084899902\n",
      "h: 123 | epoch: 76, train loss: 8.549657821655273, test loss: 5.7038164138793945\n",
      "h: 123 | epoch: 77, train loss: 8.549646377563477, test loss: 5.703394889831543\n",
      "h: 123 | epoch: 78, train loss: 8.549633026123047, test loss: 5.703008651733398\n",
      "h: 123 | epoch: 79, train loss: 8.549623489379883, test loss: 5.702654838562012\n",
      "h: 123 | epoch: 80, train loss: 8.549615859985352, test loss: 5.702332019805908\n",
      "h: 123 | epoch: 81, train loss: 8.54960823059082, test loss: 5.702034950256348\n",
      "h: 123 | epoch: 82, train loss: 8.549603462219238, test loss: 5.701764106750488\n",
      "h: 123 | epoch: 83, train loss: 8.549598693847656, test loss: 5.701515197753906\n",
      "h: 123 | epoch: 84, train loss: 8.549593925476074, test loss: 5.701288223266602\n",
      "h: 123 | epoch: 85, train loss: 8.549591064453125, test loss: 5.701080322265625\n",
      "h: 123 | epoch: 86, train loss: 8.54958724975586, test loss: 5.700888633728027\n",
      "h: 123 | epoch: 87, train loss: 8.549585342407227, test loss: 5.700714111328125\n",
      "h: 123 | epoch: 88, train loss: 8.549582481384277, test loss: 5.7005534172058105\n",
      "h: 123 | epoch: 89, train loss: 8.549581527709961, test loss: 5.700405597686768\n",
      "h: 123 | epoch: 90, train loss: 8.549580574035645, test loss: 5.700272560119629\n",
      "h: 123 | epoch: 91, train loss: 8.549578666687012, test loss: 5.700149059295654\n",
      "h: 123 | epoch: 92, train loss: 8.549577713012695, test loss: 5.700035095214844\n",
      "h: 123 | epoch: 93, train loss: 8.549577713012695, test loss: 5.6999311447143555\n",
      "h: 123 | epoch: 94, train loss: 8.549576759338379, test loss: 5.699836730957031\n",
      "h: 123 | epoch: 95, train loss: 8.549575805664062, test loss: 5.69974946975708\n",
      "h: 123 | epoch: 96, train loss: 8.549575805664062, test loss: 5.69966983795166\n",
      "h: 123 | epoch: 97, train loss: 8.549574851989746, test loss: 5.699597358703613\n",
      "h: 123 | epoch: 98, train loss: 8.54957389831543, test loss: 5.699528694152832\n",
      "h: 123 | epoch: 99, train loss: 8.549574851989746, test loss: 5.699468612670898\n",
      "h: 124 | epoch: 0, train loss: 41.79180908203125, test loss: 32.959354400634766\n",
      "h: 124 | epoch: 1, train loss: 36.02299499511719, test loss: 28.51938819885254\n",
      "h: 124 | epoch: 2, train loss: 31.259729385375977, test loss: 24.82492446899414\n",
      "h: 124 | epoch: 3, train loss: 27.324594497680664, test loss: 21.747756958007812\n",
      "h: 124 | epoch: 4, train loss: 24.07242774963379, test loss: 19.18243980407715\n",
      "h: 124 | epoch: 5, train loss: 21.38409423828125, test loss: 17.042009353637695\n",
      "h: 124 | epoch: 6, train loss: 19.161569595336914, test loss: 15.25458812713623\n",
      "h: 124 | epoch: 7, train loss: 17.324047088623047, test loss: 13.760685920715332\n",
      "h: 124 | epoch: 8, train loss: 15.80482292175293, test loss: 12.510988235473633\n",
      "h: 124 | epoch: 9, train loss: 14.548800468444824, test loss: 11.464592933654785\n",
      "h: 124 | epoch: 10, train loss: 13.510416030883789, test loss: 10.58753490447998\n",
      "h: 124 | epoch: 11, train loss: 12.652000427246094, test loss: 9.851613998413086\n",
      "h: 124 | epoch: 12, train loss: 11.942391395568848, test loss: 9.233386993408203\n",
      "h: 124 | epoch: 13, train loss: 11.355805397033691, test loss: 8.713370323181152\n",
      "h: 124 | epoch: 14, train loss: 10.870920181274414, test loss: 8.275362014770508\n",
      "h: 124 | epoch: 15, train loss: 10.470101356506348, test loss: 7.905886650085449\n",
      "h: 124 | epoch: 16, train loss: 10.138761520385742, test loss: 7.593729496002197\n",
      "h: 124 | epoch: 17, train loss: 9.864840507507324, test loss: 7.329551696777344\n",
      "h: 124 | epoch: 18, train loss: 9.638371467590332, test loss: 7.105581760406494\n",
      "h: 124 | epoch: 19, train loss: 9.451112747192383, test loss: 6.915340423583984\n",
      "h: 124 | epoch: 20, train loss: 9.296255111694336, test loss: 6.7534308433532715\n",
      "h: 124 | epoch: 21, train loss: 9.168171882629395, test loss: 6.615347385406494\n",
      "h: 124 | epoch: 22, train loss: 9.062214851379395, test loss: 6.4973320960998535\n",
      "h: 124 | epoch: 23, train loss: 8.974542617797852, test loss: 6.396242141723633\n",
      "h: 124 | epoch: 24, train loss: 8.901983261108398, test loss: 6.309450626373291\n",
      "h: 124 | epoch: 25, train loss: 8.841917037963867, test loss: 6.234762191772461\n",
      "h: 124 | epoch: 26, train loss: 8.7921781539917, test loss: 6.170334339141846\n",
      "h: 124 | epoch: 27, train loss: 8.75097942352295, test loss: 6.114623069763184\n",
      "h: 124 | epoch: 28, train loss: 8.71684455871582, test loss: 6.066328525543213\n",
      "h: 124 | epoch: 29, train loss: 8.68855094909668, test loss: 6.0243635177612305\n",
      "h: 124 | epoch: 30, train loss: 8.665094375610352, test loss: 5.9878058433532715\n",
      "h: 124 | epoch: 31, train loss: 8.645635604858398, test loss: 5.955883502960205\n",
      "h: 124 | epoch: 32, train loss: 8.629491806030273, test loss: 5.927937030792236\n",
      "h: 124 | epoch: 33, train loss: 8.616090774536133, test loss: 5.903416633605957\n",
      "h: 124 | epoch: 34, train loss: 8.604961395263672, test loss: 5.881847858428955\n",
      "h: 124 | epoch: 35, train loss: 8.59571647644043, test loss: 5.862834930419922\n",
      "h: 124 | epoch: 36, train loss: 8.588032722473145, test loss: 5.846035957336426\n",
      "h: 124 | epoch: 37, train loss: 8.581644058227539, test loss: 5.831160068511963\n",
      "h: 124 | epoch: 38, train loss: 8.576329231262207, test loss: 5.817960739135742\n",
      "h: 124 | epoch: 39, train loss: 8.571905136108398, test loss: 5.806224822998047\n",
      "h: 124 | epoch: 40, train loss: 8.568222045898438, test loss: 5.795770168304443\n",
      "h: 124 | epoch: 41, train loss: 8.565153121948242, test loss: 5.786438465118408\n",
      "h: 124 | epoch: 42, train loss: 8.562597274780273, test loss: 5.778094291687012\n",
      "h: 124 | epoch: 43, train loss: 8.560464859008789, test loss: 5.7706217765808105\n",
      "h: 124 | epoch: 44, train loss: 8.558687210083008, test loss: 5.763918399810791\n",
      "h: 124 | epoch: 45, train loss: 8.557202339172363, test loss: 5.757894992828369\n",
      "h: 124 | epoch: 46, train loss: 8.555962562561035, test loss: 5.752475738525391\n",
      "h: 124 | epoch: 47, train loss: 8.554926872253418, test loss: 5.747593879699707\n",
      "h: 124 | epoch: 48, train loss: 8.554060935974121, test loss: 5.743189811706543\n",
      "h: 124 | epoch: 49, train loss: 8.553337097167969, test loss: 5.739211559295654\n",
      "h: 124 | epoch: 50, train loss: 8.552732467651367, test loss: 5.735613822937012\n",
      "h: 124 | epoch: 51, train loss: 8.552225112915039, test loss: 5.732358455657959\n",
      "h: 124 | epoch: 52, train loss: 8.551799774169922, test loss: 5.72940731048584\n",
      "h: 124 | epoch: 53, train loss: 8.551444053649902, test loss: 5.726731300354004\n",
      "h: 124 | epoch: 54, train loss: 8.551145553588867, test loss: 5.724302291870117\n",
      "h: 124 | epoch: 55, train loss: 8.550895690917969, test loss: 5.722095489501953\n",
      "h: 124 | epoch: 56, train loss: 8.550686836242676, test loss: 5.720088481903076\n",
      "h: 124 | epoch: 57, train loss: 8.550509452819824, test loss: 5.718264102935791\n",
      "h: 124 | epoch: 58, train loss: 8.550361633300781, test loss: 5.7166008949279785\n",
      "h: 124 | epoch: 59, train loss: 8.550237655639648, test loss: 5.715086936950684\n",
      "h: 124 | epoch: 60, train loss: 8.550132751464844, test loss: 5.7137064933776855\n",
      "h: 124 | epoch: 61, train loss: 8.55004596710205, test loss: 5.712447166442871\n",
      "h: 124 | epoch: 62, train loss: 8.549970626831055, test loss: 5.711297512054443\n",
      "h: 124 | epoch: 63, train loss: 8.549909591674805, test loss: 5.710249900817871\n",
      "h: 124 | epoch: 64, train loss: 8.549856185913086, test loss: 5.709290504455566\n",
      "h: 124 | epoch: 65, train loss: 8.549812316894531, test loss: 5.708415985107422\n",
      "h: 124 | epoch: 66, train loss: 8.549775123596191, test loss: 5.707615375518799\n",
      "h: 124 | epoch: 67, train loss: 8.54974365234375, test loss: 5.706883907318115\n",
      "h: 124 | epoch: 68, train loss: 8.549717903137207, test loss: 5.70621395111084\n",
      "h: 124 | epoch: 69, train loss: 8.54969596862793, test loss: 5.705601215362549\n",
      "h: 124 | epoch: 70, train loss: 8.549675941467285, test loss: 5.705039024353027\n",
      "h: 124 | epoch: 71, train loss: 8.549659729003906, test loss: 5.704525947570801\n",
      "h: 124 | epoch: 72, train loss: 8.549647331237793, test loss: 5.704054832458496\n",
      "h: 124 | epoch: 73, train loss: 8.54963493347168, test loss: 5.703624248504639\n",
      "h: 124 | epoch: 74, train loss: 8.549625396728516, test loss: 5.70322847366333\n",
      "h: 124 | epoch: 75, train loss: 8.549617767333984, test loss: 5.702867031097412\n",
      "h: 124 | epoch: 76, train loss: 8.54961109161377, test loss: 5.702535152435303\n",
      "h: 124 | epoch: 77, train loss: 8.549605369567871, test loss: 5.702230930328369\n",
      "h: 124 | epoch: 78, train loss: 8.549599647521973, test loss: 5.701951026916504\n",
      "h: 124 | epoch: 79, train loss: 8.549596786499023, test loss: 5.701694965362549\n",
      "h: 124 | epoch: 80, train loss: 8.549592971801758, test loss: 5.70145845413208\n",
      "h: 124 | epoch: 81, train loss: 8.549589157104492, test loss: 5.701244354248047\n",
      "h: 124 | epoch: 82, train loss: 8.549586296081543, test loss: 5.701045036315918\n",
      "h: 124 | epoch: 83, train loss: 8.549585342407227, test loss: 5.700863838195801\n",
      "h: 124 | epoch: 84, train loss: 8.549582481384277, test loss: 5.700695991516113\n",
      "h: 124 | epoch: 85, train loss: 8.549581527709961, test loss: 5.70054292678833\n",
      "h: 124 | epoch: 86, train loss: 8.549579620361328, test loss: 5.700402736663818\n",
      "h: 124 | epoch: 87, train loss: 8.549577713012695, test loss: 5.700273513793945\n",
      "h: 124 | epoch: 88, train loss: 8.549577713012695, test loss: 5.7001543045043945\n",
      "h: 124 | epoch: 89, train loss: 8.549577713012695, test loss: 5.70004415512085\n",
      "h: 124 | epoch: 90, train loss: 8.549576759338379, test loss: 5.699944496154785\n",
      "h: 124 | epoch: 91, train loss: 8.549575805664062, test loss: 5.699851036071777\n",
      "h: 124 | epoch: 92, train loss: 8.549575805664062, test loss: 5.699767112731934\n",
      "h: 124 | epoch: 93, train loss: 8.549574851989746, test loss: 5.699688911437988\n",
      "h: 124 | epoch: 94, train loss: 8.549574851989746, test loss: 5.699617862701416\n",
      "h: 124 | epoch: 95, train loss: 8.54957389831543, test loss: 5.699551582336426\n",
      "h: 124 | epoch: 96, train loss: 8.54957389831543, test loss: 5.699490547180176\n",
      "h: 124 | epoch: 97, train loss: 8.54957389831543, test loss: 5.699435710906982\n",
      "h: 124 | epoch: 98, train loss: 8.54957389831543, test loss: 5.699383735656738\n",
      "h: 124 | epoch: 99, train loss: 8.54957389831543, test loss: 5.69933557510376\n",
      "h: 125 | epoch: 0, train loss: 44.31442642211914, test loss: 34.79301452636719\n",
      "h: 125 | epoch: 1, train loss: 38.259185791015625, test loss: 30.183727264404297\n",
      "h: 125 | epoch: 2, train loss: 33.242088317871094, test loss: 26.334552764892578\n",
      "h: 125 | epoch: 3, train loss: 29.081390380859375, test loss: 23.115873336791992\n",
      "h: 125 | epoch: 4, train loss: 25.628448486328125, test loss: 20.42112922668457\n",
      "h: 125 | epoch: 5, train loss: 22.761260986328125, test loss: 18.162410736083984\n",
      "h: 125 | epoch: 6, train loss: 20.379375457763672, test loss: 16.26702117919922\n",
      "h: 125 | epoch: 7, train loss: 18.39990234375, test loss: 14.674715042114258\n",
      "h: 125 | epoch: 8, train loss: 16.754329681396484, test loss: 13.335494995117188\n",
      "h: 125 | epoch: 9, train loss: 15.385940551757812, test loss: 12.207792282104492\n",
      "h: 125 | epoch: 10, train loss: 14.247735977172852, test loss: 11.257027626037598\n",
      "h: 125 | epoch: 11, train loss: 13.300745964050293, test loss: 10.454395294189453\n",
      "h: 125 | epoch: 12, train loss: 12.512636184692383, test loss: 9.775890350341797\n",
      "h: 125 | epoch: 13, train loss: 11.856572151184082, test loss: 9.201488494873047\n",
      "h: 125 | epoch: 14, train loss: 11.310267448425293, test loss: 8.714482307434082\n",
      "h: 125 | epoch: 15, train loss: 10.855219841003418, test loss: 8.300908088684082\n",
      "h: 125 | epoch: 16, train loss: 10.476059913635254, test loss: 7.9491071701049805\n",
      "h: 125 | epoch: 17, train loss: 10.160019874572754, test loss: 7.649327278137207\n",
      "h: 125 | epoch: 18, train loss: 9.896491050720215, test loss: 7.393405914306641\n",
      "h: 125 | epoch: 19, train loss: 9.676664352416992, test loss: 7.1745100021362305\n",
      "h: 125 | epoch: 20, train loss: 9.493210792541504, test loss: 6.986910820007324\n",
      "h: 125 | epoch: 21, train loss: 9.340044975280762, test loss: 6.825807094573975\n",
      "h: 125 | epoch: 22, train loss: 9.21210765838623, test loss: 6.687165260314941\n",
      "h: 125 | epoch: 23, train loss: 9.105189323425293, test loss: 6.567596435546875\n",
      "h: 125 | epoch: 24, train loss: 9.015791893005371, test loss: 6.4642510414123535\n",
      "h: 125 | epoch: 25, train loss: 8.941006660461426, test loss: 6.374727725982666\n",
      "h: 125 | epoch: 26, train loss: 8.878409385681152, test loss: 6.297003269195557\n",
      "h: 125 | epoch: 27, train loss: 8.8259859085083, test loss: 6.229369163513184\n",
      "h: 125 | epoch: 28, train loss: 8.782057762145996, test loss: 6.170380592346191\n",
      "h: 125 | epoch: 29, train loss: 8.745226860046387, test loss: 6.118813514709473\n",
      "h: 125 | epoch: 30, train loss: 8.714329719543457, test loss: 6.073633193969727\n",
      "h: 125 | epoch: 31, train loss: 8.688392639160156, test loss: 6.033957481384277\n",
      "h: 125 | epoch: 32, train loss: 8.666608810424805, test loss: 5.999037742614746\n",
      "h: 125 | epoch: 33, train loss: 8.648300170898438, test loss: 5.968235969543457\n",
      "h: 125 | epoch: 34, train loss: 8.632904052734375, test loss: 5.941005229949951\n",
      "h: 125 | epoch: 35, train loss: 8.619949340820312, test loss: 5.916883945465088\n",
      "h: 125 | epoch: 36, train loss: 8.609042167663574, test loss: 5.895471096038818\n",
      "h: 125 | epoch: 37, train loss: 8.599854469299316, test loss: 5.876423358917236\n",
      "h: 125 | epoch: 38, train loss: 8.592107772827148, test loss: 5.8594465255737305\n",
      "h: 125 | epoch: 39, train loss: 8.585575103759766, test loss: 5.844286918640137\n",
      "h: 125 | epoch: 40, train loss: 8.580061912536621, test loss: 5.830724239349365\n",
      "h: 125 | epoch: 41, train loss: 8.575406074523926, test loss: 5.818570137023926\n",
      "h: 125 | epoch: 42, train loss: 8.57147216796875, test loss: 5.8076581954956055\n",
      "h: 125 | epoch: 43, train loss: 8.568147659301758, test loss: 5.79784631729126\n",
      "h: 125 | epoch: 44, train loss: 8.565333366394043, test loss: 5.789010047912598\n",
      "h: 125 | epoch: 45, train loss: 8.562952995300293, test loss: 5.781039714813232\n",
      "h: 125 | epoch: 46, train loss: 8.560937881469727, test loss: 5.773840427398682\n",
      "h: 125 | epoch: 47, train loss: 8.559229850769043, test loss: 5.767329216003418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 125 | epoch: 48, train loss: 8.557782173156738, test loss: 5.76143217086792\n",
      "h: 125 | epoch: 49, train loss: 8.556554794311523, test loss: 5.756086349487305\n",
      "h: 125 | epoch: 50, train loss: 8.555512428283691, test loss: 5.751232624053955\n",
      "h: 125 | epoch: 51, train loss: 8.554628372192383, test loss: 5.746820449829102\n",
      "h: 125 | epoch: 52, train loss: 8.553876876831055, test loss: 5.742806911468506\n",
      "h: 125 | epoch: 53, train loss: 8.553239822387695, test loss: 5.73915433883667\n",
      "h: 125 | epoch: 54, train loss: 8.55269718170166, test loss: 5.735823154449463\n",
      "h: 125 | epoch: 55, train loss: 8.552234649658203, test loss: 5.732784271240234\n",
      "h: 125 | epoch: 56, train loss: 8.551843643188477, test loss: 5.730010986328125\n",
      "h: 125 | epoch: 57, train loss: 8.551508903503418, test loss: 5.727475643157959\n",
      "h: 125 | epoch: 58, train loss: 8.551224708557129, test loss: 5.725157737731934\n",
      "h: 125 | epoch: 59, train loss: 8.550982475280762, test loss: 5.723036766052246\n",
      "h: 125 | epoch: 60, train loss: 8.550775527954102, test loss: 5.721095085144043\n",
      "h: 125 | epoch: 61, train loss: 8.5506010055542, test loss: 5.719315528869629\n",
      "h: 125 | epoch: 62, train loss: 8.550450325012207, test loss: 5.717684745788574\n",
      "h: 125 | epoch: 63, train loss: 8.550322532653809, test loss: 5.716188430786133\n",
      "h: 125 | epoch: 64, train loss: 8.550212860107422, test loss: 5.714815139770508\n",
      "h: 125 | epoch: 65, train loss: 8.55012035369873, test loss: 5.713555335998535\n",
      "h: 125 | epoch: 66, train loss: 8.550041198730469, test loss: 5.712397575378418\n",
      "h: 125 | epoch: 67, train loss: 8.549973487854004, test loss: 5.711333274841309\n",
      "h: 125 | epoch: 68, train loss: 8.549915313720703, test loss: 5.710355281829834\n",
      "h: 125 | epoch: 69, train loss: 8.54986572265625, test loss: 5.709455490112305\n",
      "h: 125 | epoch: 70, train loss: 8.549822807312012, test loss: 5.708629608154297\n",
      "h: 125 | epoch: 71, train loss: 8.549787521362305, test loss: 5.70786714553833\n",
      "h: 125 | epoch: 72, train loss: 8.54975700378418, test loss: 5.70716667175293\n",
      "h: 125 | epoch: 73, train loss: 8.549729347229004, test loss: 5.706520080566406\n",
      "h: 125 | epoch: 74, train loss: 8.549707412719727, test loss: 5.705925941467285\n",
      "h: 125 | epoch: 75, train loss: 8.549688339233398, test loss: 5.705377578735352\n",
      "h: 125 | epoch: 76, train loss: 8.549671173095703, test loss: 5.704873085021973\n",
      "h: 125 | epoch: 77, train loss: 8.549656867980957, test loss: 5.704408168792725\n",
      "h: 125 | epoch: 78, train loss: 8.549644470214844, test loss: 5.703979015350342\n",
      "h: 125 | epoch: 79, train loss: 8.54963493347168, test loss: 5.703583717346191\n",
      "h: 125 | epoch: 80, train loss: 8.549626350402832, test loss: 5.70321798324585\n",
      "h: 125 | epoch: 81, train loss: 8.5496187210083, test loss: 5.702881336212158\n",
      "h: 125 | epoch: 82, train loss: 8.549612045288086, test loss: 5.702569007873535\n",
      "h: 125 | epoch: 83, train loss: 8.549606323242188, test loss: 5.7022833824157715\n",
      "h: 125 | epoch: 84, train loss: 8.549601554870605, test loss: 5.7020182609558105\n",
      "h: 125 | epoch: 85, train loss: 8.549596786499023, test loss: 5.701775074005127\n",
      "h: 125 | epoch: 86, train loss: 8.549592971801758, test loss: 5.7015485763549805\n",
      "h: 125 | epoch: 87, train loss: 8.549591064453125, test loss: 5.701340198516846\n",
      "h: 125 | epoch: 88, train loss: 8.549588203430176, test loss: 5.70114803314209\n",
      "h: 125 | epoch: 89, train loss: 8.549586296081543, test loss: 5.700970649719238\n",
      "h: 125 | epoch: 90, train loss: 8.54958438873291, test loss: 5.700805187225342\n",
      "h: 125 | epoch: 91, train loss: 8.549582481384277, test loss: 5.700654029846191\n",
      "h: 125 | epoch: 92, train loss: 8.549581527709961, test loss: 5.700512886047363\n",
      "h: 125 | epoch: 93, train loss: 8.549579620361328, test loss: 5.700384140014648\n",
      "h: 125 | epoch: 94, train loss: 8.549578666687012, test loss: 5.7002644538879395\n",
      "h: 125 | epoch: 95, train loss: 8.549577713012695, test loss: 5.700153350830078\n",
      "h: 125 | epoch: 96, train loss: 8.549576759338379, test loss: 5.700051784515381\n",
      "h: 125 | epoch: 97, train loss: 8.549576759338379, test loss: 5.699957370758057\n",
      "h: 125 | epoch: 98, train loss: 8.549576759338379, test loss: 5.699869155883789\n",
      "h: 125 | epoch: 99, train loss: 8.549575805664062, test loss: 5.699788570404053\n",
      "h: 126 | epoch: 0, train loss: 45.67103958129883, test loss: 35.45401382446289\n",
      "h: 126 | epoch: 1, train loss: 38.94597625732422, test loss: 30.318191528320312\n",
      "h: 126 | epoch: 2, train loss: 33.437889099121094, test loss: 26.085681915283203\n",
      "h: 126 | epoch: 3, train loss: 28.92477035522461, test loss: 22.595020294189453\n",
      "h: 126 | epoch: 4, train loss: 25.226276397705078, test loss: 19.714391708374023\n",
      "h: 126 | epoch: 5, train loss: 22.195388793945312, test loss: 17.335865020751953\n",
      "h: 126 | epoch: 6, train loss: 19.711933135986328, test loss: 15.370884895324707\n",
      "h: 126 | epoch: 7, train loss: 17.677494049072266, test loss: 13.746683120727539\n",
      "h: 126 | epoch: 8, train loss: 16.01137351989746, test loss: 12.403406143188477\n",
      "h: 126 | epoch: 9, train loss: 14.647348403930664, test loss: 11.291784286499023\n",
      "h: 126 | epoch: 10, train loss: 13.531044006347656, test loss: 10.371230125427246\n",
      "h: 126 | epoch: 11, train loss: 12.617815017700195, test loss: 9.608316421508789\n",
      "h: 126 | epoch: 12, train loss: 11.870996475219727, test loss: 8.975495338439941\n",
      "h: 126 | epoch: 13, train loss: 11.260492324829102, test loss: 8.450075149536133\n",
      "h: 126 | epoch: 14, train loss: 10.761602401733398, test loss: 8.013354301452637\n",
      "h: 126 | epoch: 15, train loss: 10.3540620803833, test loss: 7.649919033050537\n",
      "h: 126 | epoch: 16, train loss: 10.021257400512695, test loss: 7.347072601318359\n",
      "h: 126 | epoch: 17, train loss: 9.749564170837402, test loss: 7.0943427085876465\n",
      "h: 126 | epoch: 18, train loss: 9.527830123901367, test loss: 6.883105278015137\n",
      "h: 126 | epoch: 19, train loss: 9.346915245056152, test loss: 6.706246852874756\n",
      "h: 126 | epoch: 20, train loss: 9.199346542358398, test loss: 6.557901859283447\n",
      "h: 126 | epoch: 21, train loss: 9.07900333404541, test loss: 6.433229923248291\n",
      "h: 126 | epoch: 22, train loss: 8.98088550567627, test loss: 6.32824182510376\n",
      "h: 126 | epoch: 23, train loss: 8.900903701782227, test loss: 6.239635467529297\n",
      "h: 126 | epoch: 24, train loss: 8.835718154907227, test loss: 6.16468620300293\n",
      "h: 126 | epoch: 25, train loss: 8.782601356506348, test loss: 6.1011433601379395\n",
      "h: 126 | epoch: 26, train loss: 8.739324569702148, test loss: 6.0471367835998535\n",
      "h: 126 | epoch: 27, train loss: 8.704071044921875, test loss: 6.001124382019043\n",
      "h: 126 | epoch: 28, train loss: 8.675355911254883, test loss: 5.961823463439941\n",
      "h: 126 | epoch: 29, train loss: 8.651970863342285, test loss: 5.928169250488281\n",
      "h: 126 | epoch: 30, train loss: 8.632926940917969, test loss: 5.899274826049805\n",
      "h: 126 | epoch: 31, train loss: 8.617420196533203, test loss: 5.8744049072265625\n",
      "h: 126 | epoch: 32, train loss: 8.604795455932617, test loss: 5.852943420410156\n",
      "h: 126 | epoch: 33, train loss: 8.594517707824707, test loss: 5.834377288818359\n",
      "h: 126 | epoch: 34, train loss: 8.586151123046875, test loss: 5.81827449798584\n",
      "h: 126 | epoch: 35, train loss: 8.579339981079102, test loss: 5.804275035858154\n",
      "h: 126 | epoch: 36, train loss: 8.573797225952148, test loss: 5.792074203491211\n",
      "h: 126 | epoch: 37, train loss: 8.56928539276123, test loss: 5.781416893005371\n",
      "h: 126 | epoch: 38, train loss: 8.565614700317383, test loss: 5.772085666656494\n",
      "h: 126 | epoch: 39, train loss: 8.562623977661133, test loss: 5.763901710510254\n",
      "h: 126 | epoch: 40, train loss: 8.560193061828613, test loss: 5.756707191467285\n",
      "h: 126 | epoch: 41, train loss: 8.55821418762207, test loss: 5.750369071960449\n",
      "h: 126 | epoch: 42, train loss: 8.556604385375977, test loss: 5.744777202606201\n",
      "h: 126 | epoch: 43, train loss: 8.555293083190918, test loss: 5.739833831787109\n",
      "h: 126 | epoch: 44, train loss: 8.554226875305176, test loss: 5.735457420349121\n",
      "h: 126 | epoch: 45, train loss: 8.553359031677246, test loss: 5.7315754890441895\n",
      "h: 126 | epoch: 46, train loss: 8.552653312683105, test loss: 5.7281293869018555\n",
      "h: 126 | epoch: 47, train loss: 8.552080154418945, test loss: 5.725064754486084\n",
      "h: 126 | epoch: 48, train loss: 8.55161190032959, test loss: 5.722336769104004\n",
      "h: 126 | epoch: 49, train loss: 8.551231384277344, test loss: 5.719904899597168\n",
      "h: 126 | epoch: 50, train loss: 8.550922393798828, test loss: 5.717735290527344\n",
      "h: 126 | epoch: 51, train loss: 8.55066967010498, test loss: 5.7157979011535645\n",
      "h: 126 | epoch: 52, train loss: 8.55046558380127, test loss: 5.714066028594971\n",
      "h: 126 | epoch: 53, train loss: 8.550299644470215, test loss: 5.712515354156494\n",
      "h: 126 | epoch: 54, train loss: 8.550163269042969, test loss: 5.711128234863281\n",
      "h: 126 | epoch: 55, train loss: 8.550054550170898, test loss: 5.709884166717529\n",
      "h: 126 | epoch: 56, train loss: 8.54996395111084, test loss: 5.708769798278809\n",
      "h: 126 | epoch: 57, train loss: 8.549890518188477, test loss: 5.707769393920898\n",
      "h: 126 | epoch: 58, train loss: 8.549832344055176, test loss: 5.70687198638916\n",
      "h: 126 | epoch: 59, train loss: 8.549783706665039, test loss: 5.706066608428955\n",
      "h: 126 | epoch: 60, train loss: 8.549744606018066, test loss: 5.705341339111328\n",
      "h: 126 | epoch: 61, train loss: 8.549711227416992, test loss: 5.704690933227539\n",
      "h: 126 | epoch: 62, train loss: 8.54968547821045, test loss: 5.704104900360107\n",
      "h: 126 | epoch: 63, train loss: 8.549665451049805, test loss: 5.703580379486084\n",
      "h: 126 | epoch: 64, train loss: 8.54964828491211, test loss: 5.70310640335083\n",
      "h: 126 | epoch: 65, train loss: 8.549633979797363, test loss: 5.702680587768555\n",
      "h: 126 | epoch: 66, train loss: 8.549622535705566, test loss: 5.702296257019043\n",
      "h: 126 | epoch: 67, train loss: 8.549612045288086, test loss: 5.701950550079346\n",
      "h: 126 | epoch: 68, train loss: 8.549606323242188, test loss: 5.7016401290893555\n",
      "h: 126 | epoch: 69, train loss: 8.549600601196289, test loss: 5.701359748840332\n",
      "h: 126 | epoch: 70, train loss: 8.549593925476074, test loss: 5.701107501983643\n",
      "h: 126 | epoch: 71, train loss: 8.549590110778809, test loss: 5.7008795738220215\n",
      "h: 126 | epoch: 72, train loss: 8.54958724975586, test loss: 5.700675010681152\n",
      "h: 126 | epoch: 73, train loss: 8.54958438873291, test loss: 5.700489044189453\n",
      "h: 126 | epoch: 74, train loss: 8.549581527709961, test loss: 5.700323581695557\n",
      "h: 126 | epoch: 75, train loss: 8.549580574035645, test loss: 5.700173377990723\n",
      "h: 126 | epoch: 76, train loss: 8.549578666687012, test loss: 5.700037956237793\n",
      "h: 126 | epoch: 77, train loss: 8.549577713012695, test loss: 5.699915885925293\n",
      "h: 126 | epoch: 78, train loss: 8.549577713012695, test loss: 5.699805736541748\n",
      "h: 126 | epoch: 79, train loss: 8.549575805664062, test loss: 5.699706077575684\n",
      "h: 126 | epoch: 80, train loss: 8.549575805664062, test loss: 5.699616432189941\n",
      "h: 126 | epoch: 81, train loss: 8.549574851989746, test loss: 5.699536323547363\n",
      "h: 126 | epoch: 82, train loss: 8.54957389831543, test loss: 5.699462890625\n",
      "h: 126 | epoch: 83, train loss: 8.54957389831543, test loss: 5.699397563934326\n",
      "h: 126 | epoch: 84, train loss: 8.54957389831543, test loss: 5.699337005615234\n",
      "h: 126 | epoch: 85, train loss: 8.549572944641113, test loss: 5.69928503036499\n",
      "h: 126 | epoch: 86, train loss: 8.54957389831543, test loss: 5.699236869812012\n",
      "h: 126 | epoch: 87, train loss: 8.54957389831543, test loss: 5.699192047119141\n",
      "h: 126 | epoch: 88, train loss: 8.54957389831543, test loss: 5.699154376983643\n",
      "h: 126 | epoch: 89, train loss: 8.54957389831543, test loss: 5.699118614196777\n",
      "h: 126 | epoch: 90, train loss: 8.54957389831543, test loss: 5.6990861892700195\n",
      "h: 126 | epoch: 91, train loss: 8.549572944641113, test loss: 5.6990580558776855\n",
      "h: 126 | epoch: 92, train loss: 8.549572944641113, test loss: 5.699030876159668\n",
      "h: 126 | epoch: 93, train loss: 8.549572944641113, test loss: 5.699007511138916\n",
      "h: 126 | epoch: 94, train loss: 8.549571990966797, test loss: 5.698986530303955\n",
      "h: 126 | epoch: 95, train loss: 8.549572944641113, test loss: 5.698967456817627\n",
      "h: 126 | epoch: 96, train loss: 8.549572944641113, test loss: 5.698949337005615\n",
      "h: 126 | epoch: 97, train loss: 8.549572944641113, test loss: 5.698935031890869\n",
      "h: 126 | epoch: 98, train loss: 8.549571990966797, test loss: 5.698919773101807\n",
      "h: 126 | epoch: 99, train loss: 8.54957389831543, test loss: 5.698907375335693\n",
      "h: 127 | epoch: 0, train loss: 44.32434844970703, test loss: 35.21483612060547\n",
      "h: 127 | epoch: 1, train loss: 38.814353942871094, test loss: 30.875192642211914\n",
      "h: 127 | epoch: 2, train loss: 34.15233612060547, test loss: 27.185449600219727\n",
      "h: 127 | epoch: 3, train loss: 30.206106185913086, test loss: 24.046321868896484\n",
      "h: 127 | epoch: 4, train loss: 26.86496353149414, test loss: 21.37429428100586\n",
      "h: 127 | epoch: 5, train loss: 24.035888671875, test loss: 19.098918914794922\n",
      "h: 127 | epoch: 6, train loss: 21.640491485595703, test loss: 17.160615921020508\n",
      "h: 127 | epoch: 7, train loss: 19.61255645751953, test loss: 15.508906364440918\n",
      "h: 127 | epoch: 8, train loss: 17.896053314208984, test loss: 14.100975036621094\n",
      "h: 127 | epoch: 9, train loss: 16.443523406982422, test loss: 12.90046215057373\n",
      "h: 127 | epoch: 10, train loss: 15.214727401733398, test loss: 11.876458168029785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 127 | epoch: 11, train loss: 14.175532341003418, test loss: 11.002687454223633\n",
      "h: 127 | epoch: 12, train loss: 13.296976089477539, test loss: 10.256805419921875\n",
      "h: 127 | epoch: 13, train loss: 12.554475784301758, test loss: 9.619802474975586\n",
      "h: 127 | epoch: 14, train loss: 11.927178382873535, test loss: 9.075496673583984\n",
      "h: 127 | epoch: 15, train loss: 11.397382736206055, test loss: 8.61013412475586\n",
      "h: 127 | epoch: 16, train loss: 10.950090408325195, test loss: 8.212005615234375\n",
      "h: 127 | epoch: 17, train loss: 10.572568893432617, test loss: 7.871152400970459\n",
      "h: 127 | epoch: 18, train loss: 10.254035949707031, test loss: 7.57910680770874\n",
      "h: 127 | epoch: 19, train loss: 9.985353469848633, test loss: 7.328660488128662\n",
      "h: 127 | epoch: 20, train loss: 9.758784294128418, test loss: 7.113687992095947\n",
      "h: 127 | epoch: 21, train loss: 9.567778587341309, test loss: 6.928976535797119\n",
      "h: 127 | epoch: 22, train loss: 9.406795501708984, test loss: 6.7700910568237305\n",
      "h: 127 | epoch: 23, train loss: 9.271146774291992, test loss: 6.633262634277344\n",
      "h: 127 | epoch: 24, train loss: 9.15687084197998, test loss: 6.515280723571777\n",
      "h: 127 | epoch: 25, train loss: 9.060624122619629, test loss: 6.413416385650635\n",
      "h: 127 | epoch: 26, train loss: 8.979574203491211, test loss: 6.325345993041992\n",
      "h: 127 | epoch: 27, train loss: 8.911334991455078, test loss: 6.249091625213623\n",
      "h: 127 | epoch: 28, train loss: 8.853889465332031, test loss: 6.182967662811279\n",
      "h: 127 | epoch: 29, train loss: 8.805541038513184, test loss: 6.125539779663086\n",
      "h: 127 | epoch: 30, train loss: 8.764854431152344, test loss: 6.075581073760986\n",
      "h: 127 | epoch: 31, train loss: 8.730619430541992, test loss: 6.032051086425781\n",
      "h: 127 | epoch: 32, train loss: 8.701815605163574, test loss: 5.994054794311523\n",
      "h: 127 | epoch: 33, train loss: 8.67758560180664, test loss: 5.96083402633667\n",
      "h: 127 | epoch: 34, train loss: 8.657203674316406, test loss: 5.931737422943115\n",
      "h: 127 | epoch: 35, train loss: 8.64006233215332, test loss: 5.906206130981445\n",
      "h: 127 | epoch: 36, train loss: 8.625645637512207, test loss: 5.88376522064209\n",
      "h: 127 | epoch: 37, train loss: 8.613523483276367, test loss: 5.864003658294678\n",
      "h: 127 | epoch: 38, train loss: 8.603330612182617, test loss: 5.8465728759765625\n",
      "h: 127 | epoch: 39, train loss: 8.594759941101074, test loss: 5.831169128417969\n",
      "h: 127 | epoch: 40, train loss: 8.587553024291992, test loss: 5.817533493041992\n",
      "h: 127 | epoch: 41, train loss: 8.581496238708496, test loss: 5.805441856384277\n",
      "h: 127 | epoch: 42, train loss: 8.576403617858887, test loss: 5.794703483581543\n",
      "h: 127 | epoch: 43, train loss: 8.572122573852539, test loss: 5.785147666931152\n",
      "h: 127 | epoch: 44, train loss: 8.568524360656738, test loss: 5.7766337394714355\n",
      "h: 127 | epoch: 45, train loss: 8.565500259399414, test loss: 5.769033908843994\n",
      "h: 127 | epoch: 46, train loss: 8.562958717346191, test loss: 5.762240409851074\n",
      "h: 127 | epoch: 47, train loss: 8.560821533203125, test loss: 5.756159782409668\n",
      "h: 127 | epoch: 48, train loss: 8.559026718139648, test loss: 5.750709056854248\n",
      "h: 127 | epoch: 49, train loss: 8.557516098022461, test loss: 5.745816230773926\n",
      "h: 127 | epoch: 50, train loss: 8.556248664855957, test loss: 5.741418361663818\n",
      "h: 127 | epoch: 51, train loss: 8.555182456970215, test loss: 5.737460613250732\n",
      "h: 127 | epoch: 52, train loss: 8.554286003112793, test loss: 5.733895778656006\n",
      "h: 127 | epoch: 53, train loss: 8.553533554077148, test loss: 5.730680465698242\n",
      "h: 127 | epoch: 54, train loss: 8.552902221679688, test loss: 5.727776527404785\n",
      "h: 127 | epoch: 55, train loss: 8.552370071411133, test loss: 5.725152492523193\n",
      "h: 127 | epoch: 56, train loss: 8.551922798156738, test loss: 5.722779750823975\n",
      "h: 127 | epoch: 57, train loss: 8.551547050476074, test loss: 5.720629692077637\n",
      "h: 127 | epoch: 58, train loss: 8.551233291625977, test loss: 5.71868371963501\n",
      "h: 127 | epoch: 59, train loss: 8.5509672164917, test loss: 5.716916084289551\n",
      "h: 127 | epoch: 60, train loss: 8.55074405670166, test loss: 5.7153143882751465\n",
      "h: 127 | epoch: 61, train loss: 8.550558090209961, test loss: 5.713858604431152\n",
      "h: 127 | epoch: 62, train loss: 8.550399780273438, test loss: 5.712536811828613\n",
      "h: 127 | epoch: 63, train loss: 8.550268173217773, test loss: 5.711333274841309\n",
      "h: 127 | epoch: 64, train loss: 8.55015754699707, test loss: 5.710239887237549\n",
      "h: 127 | epoch: 65, train loss: 8.550063133239746, test loss: 5.709244251251221\n",
      "h: 127 | epoch: 66, train loss: 8.549985885620117, test loss: 5.708337783813477\n",
      "h: 127 | epoch: 67, train loss: 8.549919128417969, test loss: 5.707510471343994\n",
      "h: 127 | epoch: 68, train loss: 8.549863815307617, test loss: 5.706757545471191\n",
      "h: 127 | epoch: 69, train loss: 8.549817085266113, test loss: 5.706071376800537\n",
      "h: 127 | epoch: 70, train loss: 8.549778938293457, test loss: 5.705446243286133\n",
      "h: 127 | epoch: 71, train loss: 8.549745559692383, test loss: 5.704874515533447\n",
      "h: 127 | epoch: 72, train loss: 8.549718856811523, test loss: 5.704352378845215\n",
      "h: 127 | epoch: 73, train loss: 8.54969596862793, test loss: 5.7038774490356445\n",
      "h: 127 | epoch: 74, train loss: 8.549675941467285, test loss: 5.703442096710205\n",
      "h: 127 | epoch: 75, train loss: 8.549659729003906, test loss: 5.703044891357422\n",
      "h: 127 | epoch: 76, train loss: 8.54964542388916, test loss: 5.7026824951171875\n",
      "h: 127 | epoch: 77, train loss: 8.549633026123047, test loss: 5.70235013961792\n",
      "h: 127 | epoch: 78, train loss: 8.549623489379883, test loss: 5.702048301696777\n",
      "h: 127 | epoch: 79, train loss: 8.549615859985352, test loss: 5.701771259307861\n",
      "h: 127 | epoch: 80, train loss: 8.54960823059082, test loss: 5.7015180587768555\n",
      "h: 127 | epoch: 81, train loss: 8.549603462219238, test loss: 5.701286315917969\n",
      "h: 127 | epoch: 82, train loss: 8.549598693847656, test loss: 5.701075553894043\n",
      "h: 127 | epoch: 83, train loss: 8.549593925476074, test loss: 5.700881481170654\n",
      "h: 127 | epoch: 84, train loss: 8.549590110778809, test loss: 5.7007036209106445\n",
      "h: 127 | epoch: 85, train loss: 8.549588203430176, test loss: 5.700543403625488\n",
      "h: 127 | epoch: 86, train loss: 8.549585342407227, test loss: 5.700394153594971\n",
      "h: 127 | epoch: 87, train loss: 8.549583435058594, test loss: 5.700258255004883\n",
      "h: 127 | epoch: 88, train loss: 8.549581527709961, test loss: 5.700133800506592\n",
      "h: 127 | epoch: 89, train loss: 8.549580574035645, test loss: 5.700020790100098\n",
      "h: 127 | epoch: 90, train loss: 8.549578666687012, test loss: 5.699916839599609\n",
      "h: 127 | epoch: 91, train loss: 8.549577713012695, test loss: 5.699822425842285\n",
      "h: 127 | epoch: 92, train loss: 8.549577713012695, test loss: 5.699734687805176\n",
      "h: 127 | epoch: 93, train loss: 8.549576759338379, test loss: 5.699655055999756\n",
      "h: 127 | epoch: 94, train loss: 8.549575805664062, test loss: 5.699582099914551\n",
      "h: 127 | epoch: 95, train loss: 8.549574851989746, test loss: 5.699514865875244\n",
      "h: 127 | epoch: 96, train loss: 8.549574851989746, test loss: 5.6994547843933105\n",
      "h: 127 | epoch: 97, train loss: 8.549574851989746, test loss: 5.699398517608643\n",
      "h: 127 | epoch: 98, train loss: 8.54957389831543, test loss: 5.699346542358398\n",
      "h: 127 | epoch: 99, train loss: 8.54957389831543, test loss: 5.6992998123168945\n"
     ]
    }
   ],
   "source": [
    "#!!! Note that training in the code below done outside of epoch loop.\n",
    "hs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for h in range(2, hidden_sizes, 1):\n",
    "    hs.append(h)\n",
    "    model = NN(inputDim, h, outputDim)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "    for epoch in range(epochs):\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_train = Variable(torch.from_numpy(x_train).cuda())\n",
    "            labels_train = Variable(torch.from_numpy(y_train).cuda())\n",
    "        else:\n",
    "            inputs_train = Variable(torch.from_numpy(x_train))\n",
    "            labels_train = Variable(torch.from_numpy(y_train))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_test = Variable(torch.from_numpy(x_test).cuda())\n",
    "            labels_test = Variable(torch.from_numpy(y_test).cuda())\n",
    "        else:\n",
    "            inputs_test = Variable(torch.from_numpy(x_test))\n",
    "            labels_test = Variable(torch.from_numpy(y_test))\n",
    "        optimizer.zero_grad()\n",
    "        outputs_train = model(inputs_train)\n",
    "        loss_train = criterion(outputs_train, labels_train)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        outputs_test = model(inputs_test)\n",
    "        loss_test = criterion(outputs_test, labels_test)\n",
    "        print('h: {} | epoch: {}, train loss: {}, test loss: {}'.format(h, epoch, loss_train.item(), loss_test.item()))\n",
    "    train_losses.append(loss_train)\n",
    "    test_losses.append(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.753542  ]\n",
      " [ 3.1920507 ]\n",
      " [ 4.046423  ]\n",
      " [ 5.925515  ]\n",
      " [ 7.9746337 ]\n",
      " [ 0.69138914]\n",
      " [ 8.885186  ]\n",
      " [ 4.985784  ]\n",
      " [ 6.3818727 ]\n",
      " [ 6.7654786 ]\n",
      " [ 3.4692056 ]\n",
      " [ 3.3044915 ]\n",
      " [ 7.589477  ]\n",
      " [ 3.9313571 ]\n",
      " [ 4.7512784 ]\n",
      " [ 3.139624  ]\n",
      " [ 7.976611  ]\n",
      " [ 9.1868    ]\n",
      " [ 7.0140963 ]\n",
      " [ 4.033679  ]\n",
      " [ 1.5257952 ]\n",
      " [ 0.4040706 ]\n",
      " [ 5.6518164 ]\n",
      " [ 3.5419447 ]\n",
      " [ 1.1341014 ]\n",
      " [ 4.049903  ]\n",
      " [ 4.7120366 ]\n",
      " [ 8.14152   ]\n",
      " [ 5.1615415 ]\n",
      " [ 7.0434384 ]\n",
      " [ 1.3335238 ]\n",
      " [ 6.217971  ]\n",
      " [ 5.645223  ]\n",
      " [ 8.624302  ]\n",
      " [ 2.890112  ]\n",
      " [ 2.9914281 ]\n",
      " [-0.1478746 ]\n",
      " [ 8.49522   ]\n",
      " [ 7.467644  ]\n",
      " [ 6.675774  ]\n",
      " [ 7.20904   ]\n",
      " [ 6.7425184 ]\n",
      " [ 5.896552  ]\n",
      " [ 6.5635295 ]\n",
      " [ 7.140519  ]\n",
      " [ 7.6490493 ]\n",
      " [-0.49002442]\n",
      " [ 7.339457  ]\n",
      " [ 8.108717  ]\n",
      " [ 4.0965295 ]\n",
      " [ 5.9281564 ]\n",
      " [ 3.0166218 ]\n",
      " [-0.29306605]\n",
      " [ 6.410546  ]\n",
      " [10.741163  ]\n",
      " [ 2.9183202 ]\n",
      " [ 6.297337  ]\n",
      " [ 4.643355  ]\n",
      " [ 8.999712  ]\n",
      " [ 7.2135115 ]\n",
      " [ 0.4000931 ]\n",
      " [ 5.5305486 ]\n",
      " [ 7.587677  ]\n",
      " [ 4.413444  ]\n",
      " [ 6.3788095 ]\n",
      " [ 4.8792787 ]\n",
      " [ 5.3571887 ]\n",
      " [ 4.2395644 ]\n",
      " [ 2.2266927 ]\n",
      " [ 8.422853  ]\n",
      " [ 4.2338233 ]\n",
      " [ 7.521693  ]\n",
      " [-0.7463114 ]\n",
      " [ 4.7670956 ]\n",
      " [ 4.449384  ]\n",
      " [ 2.536899  ]\n",
      " [10.531451  ]\n",
      " [ 1.4073217 ]\n",
      " [ 1.4614153 ]\n",
      " [ 7.148832  ]\n",
      " [ 3.8976967 ]\n",
      " [ 7.3886576 ]\n",
      " [ 5.962191  ]\n",
      " [ 5.9844503 ]\n",
      " [ 6.271542  ]\n",
      " [ 2.779317  ]\n",
      " [ 6.6236043 ]\n",
      " [ 7.2180142 ]\n",
      " [ 6.7295995 ]\n",
      " [ 2.0960371 ]\n",
      " [ 3.712434  ]\n",
      " [ 5.3204923 ]\n",
      " [ 4.5066457 ]\n",
      " [ 8.885328  ]\n",
      " [ 6.094134  ]\n",
      " [ 7.8912206 ]\n",
      " [ 4.4423823 ]\n",
      " [ 7.7124133 ]\n",
      " [ 3.7838907 ]\n",
      " [ 4.152391  ]\n",
      " [ 7.9956    ]\n",
      " [ 4.9450655 ]\n",
      " [ 7.8889346 ]\n",
      " [ 6.1017785 ]\n",
      " [ 3.715199  ]\n",
      " [ 3.6453538 ]\n",
      " [ 6.8458805 ]\n",
      " [ 1.6729268 ]\n",
      " [ 5.727267  ]\n",
      " [ 5.274898  ]\n",
      " [ 1.8524449 ]\n",
      " [ 4.82242   ]\n",
      " [ 6.226726  ]\n",
      " [-1.1726973 ]\n",
      " [ 2.4257188 ]\n",
      " [ 5.4124446 ]\n",
      " [ 2.472879  ]\n",
      " [ 7.63789   ]\n",
      " [ 9.4920635 ]\n",
      " [ 3.594499  ]\n",
      " [ 5.8375077 ]\n",
      " [ 6.1470838 ]\n",
      " [ 8.58595   ]\n",
      " [ 8.145953  ]\n",
      " [ 1.3971695 ]\n",
      " [ 5.1246533 ]\n",
      " [ 6.493797  ]\n",
      " [ 7.356349  ]\n",
      " [ 5.3641953 ]\n",
      " [ 5.042026  ]\n",
      " [-0.32087037]\n",
      " [10.802676  ]\n",
      " [ 5.8187866 ]\n",
      " [ 1.966053  ]\n",
      " [ 1.5056381 ]\n",
      " [11.708492  ]\n",
      " [ 7.1543083 ]\n",
      " [ 5.723472  ]\n",
      " [ 6.5888677 ]\n",
      " [ 7.9853077 ]\n",
      " [ 2.1995916 ]\n",
      " [ 4.4697948 ]\n",
      " [ 3.8267083 ]\n",
      " [ 4.1945267 ]\n",
      " [ 7.7178035 ]\n",
      " [ 5.2187896 ]\n",
      " [ 0.6248146 ]\n",
      " [ 7.2193446 ]\n",
      " [ 8.877204  ]\n",
      " [11.054256  ]\n",
      " [11.776576  ]\n",
      " [ 5.421554  ]\n",
      " [ 6.096067  ]\n",
      " [ 5.7594247 ]\n",
      " [ 9.160528  ]\n",
      " [ 4.41189   ]\n",
      " [ 8.98435   ]\n",
      " [ 2.8386488 ]\n",
      " [ 4.0193467 ]\n",
      " [ 6.8437314 ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.6817968 ]\n",
      " [-4.315991  ]\n",
      " [ 0.71220577]\n",
      " [ 5.118536  ]\n",
      " [ 7.5204325 ]\n",
      " [ 6.655643  ]\n",
      " [ 8.299387  ]\n",
      " [ 5.6922755 ]\n",
      " [ 6.5698338 ]\n",
      " [ 5.520704  ]\n",
      " [-0.42452773]\n",
      " [ 6.8104906 ]\n",
      " [ 2.0837455 ]\n",
      " [ 5.093297  ]\n",
      " [ 4.148462  ]\n",
      " [ 4.9848027 ]\n",
      " [ 7.469916  ]\n",
      " [ 4.7636185 ]\n",
      " [ 4.3830028 ]\n",
      " [ 7.6808267 ]\n",
      " [ 8.176737  ]\n",
      " [ 4.288291  ]\n",
      " [10.432233  ]\n",
      " [ 4.9239182 ]\n",
      " [ 6.0858073 ]\n",
      " [ 3.3673046 ]\n",
      " [ 7.0227547 ]\n",
      " [ 5.8806777 ]\n",
      " [ 5.4419827 ]\n",
      " [ 2.7147887 ]\n",
      " [ 7.1906505 ]\n",
      " [ 9.076749  ]\n",
      " [ 2.8916688 ]\n",
      " [ 5.887403  ]\n",
      " [ 2.811473  ]\n",
      " [ 3.2037525 ]\n",
      " [ 3.734508  ]\n",
      " [ 4.819898  ]\n",
      " [ 3.8330874 ]\n",
      " [ 6.0467286 ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    if torch.cuda.is_available():\n",
    "        predicted1 = model(Variable(torch.from_numpy(x_test).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted1 = model(Variable(torch.from_numpy(x_test))).data.numpy()\n",
    "    print(predicted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.plot(x_test, y_test, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_test, predicted1, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 100.0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hs, train_losses, label='Train Loss')\n",
    "plt.plot(hs, test_losses, label='Test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 100.0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hs, train_losses)\n",
    "plt.title('Train Loss')\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 100.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hs, test_losses)\n",
    "plt.title('Test Loss')\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for h in range(2, hidden_sizes, 1):\n",
    "        model = NN(inputDim, h, outputDim)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "        for epoch in range(epochs):\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = Variable(torch.from_numpy(x_test).cuda())\n",
    "                labels = Variable(torch.from_numpy(y_test).cuda())\n",
    "            else:\n",
    "                inputs = Variable(torch.from_numpy(x_test))\n",
    "                labels = Variable(torch.from_numpy(y_test))\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print('h: {} | epoch: {}, loss: {}'.format(h, epoch, loss.item()))\n",
    "        test_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12a5cabaf40>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hs, test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
