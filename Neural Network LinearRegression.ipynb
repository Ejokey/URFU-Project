{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import multivariate_normal\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.random.normal(size = 100)\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [3*i + 5 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x24345368610>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        \n",
    "outputDim = 1       \n",
    "learningRate = 0.01 \n",
    "epochs = 500\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.1728, grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 44.172813415527344\n",
      "tensor(42.3788, grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 42.3787841796875\n",
      "tensor(40.6576, grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 40.65764236450195\n",
      "tensor(39.0064, grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 39.006431579589844\n",
      "tensor(37.4223, grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 37.422306060791016\n",
      "tensor(35.9025, grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 35.902530670166016\n",
      "tensor(34.4445, grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 34.44449996948242\n",
      "tensor(33.0457, grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 33.04570007324219\n",
      "tensor(31.7037, grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 31.7037296295166\n",
      "tensor(30.4163, grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 30.416275024414062\n",
      "tensor(29.1811, grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 29.181116104125977\n",
      "tensor(27.9961, grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 27.996143341064453\n",
      "tensor(26.8593, grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 26.859296798706055\n",
      "tensor(25.7686, grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 25.768632888793945\n",
      "tensor(24.7223, grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 24.722274780273438\n",
      "tensor(23.7184, grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 23.71841812133789\n",
      "tensor(22.7553, grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 22.75534439086914\n",
      "tensor(21.8314, grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 21.831382751464844\n",
      "tensor(20.9450, grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 20.944955825805664\n",
      "tensor(20.0945, grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 20.094528198242188\n",
      "tensor(19.2786, grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 19.27864646911621\n",
      "tensor(18.4959, grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 18.49590301513672\n",
      "tensor(17.7450, grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 17.744951248168945\n",
      "tensor(17.0245, grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 17.024499893188477\n",
      "tensor(16.3333, grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 16.333311080932617\n",
      "tensor(15.6702, grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 15.670193672180176\n",
      "tensor(15.0340, grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 15.034008979797363\n",
      "tensor(14.4237, grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 14.423661231994629\n",
      "tensor(13.8381, grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 13.838099479675293\n",
      "tensor(13.2763, grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 13.276323318481445\n",
      "tensor(12.7374, grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 12.737358093261719\n",
      "tensor(12.2203, grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 12.220281600952148\n",
      "tensor(11.7242, grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 11.724204063415527\n",
      "tensor(11.2483, grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 11.248272895812988\n",
      "tensor(10.7917, grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 10.791667938232422\n",
      "tensor(10.3536, grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 10.353607177734375\n",
      "tensor(9.9333, grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 9.933331489562988\n",
      "tensor(9.5301, grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 9.53012466430664\n",
      "tensor(9.1433, grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 9.143289566040039\n",
      "tensor(8.7722, grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 8.772163391113281\n",
      "tensor(8.4161, grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 8.416106224060059\n",
      "tensor(8.0745, grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 8.074507713317871\n",
      "tensor(7.7468, grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 7.746777534484863\n",
      "tensor(7.4324, grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 7.432354927062988\n",
      "tensor(7.1307, grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 7.130699634552002\n",
      "tensor(6.8413, grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 6.841293811798096\n",
      "tensor(6.5636, grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 6.56363582611084\n",
      "tensor(6.2973, grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 6.297250747680664\n",
      "tensor(6.0417, grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 6.041683197021484\n",
      "tensor(5.7965, grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 5.796491622924805\n",
      "tensor(5.5613, grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 5.561253547668457\n",
      "tensor(5.3356, grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 5.335565090179443\n",
      "tensor(5.1190, grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 5.119040012359619\n",
      "tensor(4.9113, grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 4.911304473876953\n",
      "tensor(4.7120, grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 4.712003231048584\n",
      "tensor(4.5208, grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 4.520793437957764\n",
      "tensor(4.3373, grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 4.337345123291016\n",
      "tensor(4.1613, grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 4.161344528198242\n",
      "tensor(3.9925, grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 3.992488145828247\n",
      "tensor(3.8305, grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 3.8304855823516846\n",
      "tensor(3.6751, grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 3.6750597953796387\n",
      "tensor(3.5259, grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 3.5259430408477783\n",
      "tensor(3.3829, grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 3.3828787803649902\n",
      "tensor(3.2456, grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 3.245621681213379\n",
      "tensor(3.1139, grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 3.11393666267395\n",
      "tensor(2.9876, grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 2.987596035003662\n",
      "tensor(2.8664, grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 2.8663840293884277\n",
      "tensor(2.7501, grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 2.750091314315796\n",
      "tensor(2.6385, grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 2.6385183334350586\n",
      "tensor(2.5315, grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 2.5314745903015137\n",
      "tensor(2.4288, grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 2.4287750720977783\n",
      "tensor(2.3302, grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 2.3302435874938965\n",
      "tensor(2.2357, grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 2.2357113361358643\n",
      "tensor(2.1450, grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 2.145014762878418\n",
      "tensor(2.0580, grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 2.057999849319458\n",
      "tensor(1.9745, grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 1.9745157957077026\n",
      "tensor(1.8944, grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 1.8944205045700073\n",
      "tensor(1.8176, grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 1.8175749778747559\n",
      "tensor(1.7438, grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 1.743848443031311\n",
      "tensor(1.6731, grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 1.6731129884719849\n",
      "tensor(1.6052, grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 1.605248212814331\n",
      "tensor(1.5401, grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 1.5401378870010376\n",
      "tensor(1.4777, grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 1.4776691198349\n",
      "tensor(1.4177, grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 1.4177350997924805\n",
      "tensor(1.3602, grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 1.360233187675476\n",
      "tensor(1.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 1.3050642013549805\n",
      "tensor(1.2521, grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 1.2521342039108276\n",
      "tensor(1.2014, grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 1.2013510465621948\n",
      "tensor(1.1526, grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 1.1526284217834473\n",
      "tensor(1.1059, grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 1.105883002281189\n",
      "tensor(1.0610, grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 1.0610342025756836\n",
      "tensor(1.0180, grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 1.018005132675171\n",
      "tensor(0.9767, grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.9767215251922607\n",
      "tensor(0.9371, grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.9371126294136047\n",
      "tensor(0.8991, grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.899111270904541\n",
      "tensor(0.8627, grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.8626511096954346\n",
      "tensor(0.8277, grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.8276699781417847\n",
      "tensor(0.7941, grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.7941083312034607\n",
      "tensor(0.7619, grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.7619081735610962\n",
      "tensor(0.7310, grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.7310140132904053\n",
      "tensor(0.7014, grad_fn=<MseLossBackward>)\n",
      "epoch 100, loss 0.7013732194900513\n",
      "tensor(0.6729, grad_fn=<MseLossBackward>)\n",
      "epoch 101, loss 0.6729347705841064\n",
      "tensor(0.6457, grad_fn=<MseLossBackward>)\n",
      "epoch 102, loss 0.6456503868103027\n",
      "tensor(0.6195, grad_fn=<MseLossBackward>)\n",
      "epoch 103, loss 0.6194722056388855\n",
      "tensor(0.5944, grad_fn=<MseLossBackward>)\n",
      "epoch 104, loss 0.594356119632721\n",
      "tensor(0.5703, grad_fn=<MseLossBackward>)\n",
      "epoch 105, loss 0.5702590942382812\n",
      "tensor(0.5471, grad_fn=<MseLossBackward>)\n",
      "epoch 106, loss 0.5471392273902893\n",
      "tensor(0.5250, grad_fn=<MseLossBackward>)\n",
      "epoch 107, loss 0.5249574184417725\n",
      "tensor(0.5037, grad_fn=<MseLossBackward>)\n",
      "epoch 108, loss 0.5036748647689819\n",
      "tensor(0.4833, grad_fn=<MseLossBackward>)\n",
      "epoch 109, loss 0.48325565457344055\n",
      "tensor(0.4637, grad_fn=<MseLossBackward>)\n",
      "epoch 110, loss 0.4636645019054413\n",
      "tensor(0.4449, grad_fn=<MseLossBackward>)\n",
      "epoch 111, loss 0.4448680877685547\n",
      "tensor(0.4268, grad_fn=<MseLossBackward>)\n",
      "epoch 112, loss 0.4268338084220886\n",
      "tensor(0.4095, grad_fn=<MseLossBackward>)\n",
      "epoch 113, loss 0.4095311462879181\n",
      "tensor(0.3929, grad_fn=<MseLossBackward>)\n",
      "epoch 114, loss 0.3929302990436554\n",
      "tensor(0.3770, grad_fn=<MseLossBackward>)\n",
      "epoch 115, loss 0.3770025372505188\n",
      "tensor(0.3617, grad_fn=<MseLossBackward>)\n",
      "epoch 116, loss 0.36172065138816833\n",
      "tensor(0.3471, grad_fn=<MseLossBackward>)\n",
      "epoch 117, loss 0.3470584452152252\n",
      "tensor(0.3330, grad_fn=<MseLossBackward>)\n",
      "epoch 118, loss 0.33299076557159424\n",
      "tensor(0.3195, grad_fn=<MseLossBackward>)\n",
      "epoch 119, loss 0.3194938004016876\n",
      "tensor(0.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 120, loss 0.3065442740917206\n",
      "tensor(0.2941, grad_fn=<MseLossBackward>)\n",
      "epoch 121, loss 0.29411956667900085\n",
      "tensor(0.2822, grad_fn=<MseLossBackward>)\n",
      "epoch 122, loss 0.28219881653785706\n",
      "tensor(0.2708, grad_fn=<MseLossBackward>)\n",
      "epoch 123, loss 0.2707615792751312\n",
      "tensor(0.2598, grad_fn=<MseLossBackward>)\n",
      "epoch 124, loss 0.2597881853580475\n",
      "tensor(0.2493, grad_fn=<MseLossBackward>)\n",
      "epoch 125, loss 0.24925953149795532\n",
      "tensor(0.2392, grad_fn=<MseLossBackward>)\n",
      "epoch 126, loss 0.23915773630142212\n",
      "tensor(0.2295, grad_fn=<MseLossBackward>)\n",
      "epoch 127, loss 0.22946569323539734\n",
      "tensor(0.2202, grad_fn=<MseLossBackward>)\n",
      "epoch 128, loss 0.2201666235923767\n",
      "tensor(0.2112, grad_fn=<MseLossBackward>)\n",
      "epoch 129, loss 0.21124428510665894\n",
      "tensor(0.2027, grad_fn=<MseLossBackward>)\n",
      "epoch 130, loss 0.20268410444259644\n",
      "tensor(0.1945, grad_fn=<MseLossBackward>)\n",
      "epoch 131, loss 0.19447061419487\n",
      "tensor(0.1866, grad_fn=<MseLossBackward>)\n",
      "epoch 132, loss 0.1865900456905365\n",
      "tensor(0.1790, grad_fn=<MseLossBackward>)\n",
      "epoch 133, loss 0.17902900278568268\n",
      "tensor(0.1718, grad_fn=<MseLossBackward>)\n",
      "epoch 134, loss 0.17177464067935944\n",
      "tensor(0.1648, grad_fn=<MseLossBackward>)\n",
      "epoch 135, loss 0.16481433808803558\n",
      "tensor(0.1581, grad_fn=<MseLossBackward>)\n",
      "epoch 136, loss 0.15813623368740082\n",
      "tensor(0.1517, grad_fn=<MseLossBackward>)\n",
      "epoch 137, loss 0.15172871947288513\n",
      "tensor(0.1456, grad_fn=<MseLossBackward>)\n",
      "epoch 138, loss 0.1455809772014618\n",
      "tensor(0.1397, grad_fn=<MseLossBackward>)\n",
      "epoch 139, loss 0.1396826207637787\n",
      "tensor(0.1340, grad_fn=<MseLossBackward>)\n",
      "epoch 140, loss 0.13402332365512848\n",
      "tensor(0.1286, grad_fn=<MseLossBackward>)\n",
      "epoch 141, loss 0.12859348952770233\n",
      "tensor(0.1234, grad_fn=<MseLossBackward>)\n",
      "epoch 142, loss 0.12338372319936752\n",
      "tensor(0.1184, grad_fn=<MseLossBackward>)\n",
      "epoch 143, loss 0.11838516592979431\n",
      "tensor(0.1136, grad_fn=<MseLossBackward>)\n",
      "epoch 144, loss 0.11358914524316788\n",
      "tensor(0.1090, grad_fn=<MseLossBackward>)\n",
      "epoch 145, loss 0.10898739844560623\n",
      "tensor(0.1046, grad_fn=<MseLossBackward>)\n",
      "epoch 146, loss 0.10457208752632141\n",
      "tensor(0.1003, grad_fn=<MseLossBackward>)\n",
      "epoch 147, loss 0.10033588111400604\n",
      "tensor(0.0963, grad_fn=<MseLossBackward>)\n",
      "epoch 148, loss 0.0962713360786438\n",
      "tensor(0.0924, grad_fn=<MseLossBackward>)\n",
      "epoch 149, loss 0.09237150102853775\n",
      "tensor(0.0886, grad_fn=<MseLossBackward>)\n",
      "epoch 150, loss 0.08862971514463425\n",
      "tensor(0.0850, grad_fn=<MseLossBackward>)\n",
      "epoch 151, loss 0.0850396454334259\n",
      "tensor(0.0816, grad_fn=<MseLossBackward>)\n",
      "epoch 152, loss 0.0815950408577919\n",
      "tensor(0.0783, grad_fn=<MseLossBackward>)\n",
      "epoch 153, loss 0.07829003036022186\n",
      "tensor(0.0751, grad_fn=<MseLossBackward>)\n",
      "epoch 154, loss 0.07511886954307556\n",
      "tensor(0.0721, grad_fn=<MseLossBackward>)\n",
      "epoch 155, loss 0.0720762088894844\n",
      "tensor(0.0692, grad_fn=<MseLossBackward>)\n",
      "epoch 156, loss 0.06915704160928726\n",
      "tensor(0.0664, grad_fn=<MseLossBackward>)\n",
      "epoch 157, loss 0.06635597348213196\n",
      "tensor(0.0637, grad_fn=<MseLossBackward>)\n",
      "epoch 158, loss 0.06366851180791855\n",
      "tensor(0.0611, grad_fn=<MseLossBackward>)\n",
      "epoch 159, loss 0.06108986958861351\n",
      "tensor(0.0586, grad_fn=<MseLossBackward>)\n",
      "epoch 160, loss 0.058615732938051224\n",
      "tensor(0.0562, grad_fn=<MseLossBackward>)\n",
      "epoch 161, loss 0.05624181777238846\n",
      "tensor(0.0540, grad_fn=<MseLossBackward>)\n",
      "epoch 162, loss 0.05396410822868347\n",
      "tensor(0.0518, grad_fn=<MseLossBackward>)\n",
      "epoch 163, loss 0.051778644323349\n",
      "tensor(0.0497, grad_fn=<MseLossBackward>)\n",
      "epoch 164, loss 0.049681879580020905\n",
      "tensor(0.0477, grad_fn=<MseLossBackward>)\n",
      "epoch 165, loss 0.047670021653175354\n",
      "tensor(0.0457, grad_fn=<MseLossBackward>)\n",
      "epoch 166, loss 0.04573969542980194\n",
      "tensor(0.0439, grad_fn=<MseLossBackward>)\n",
      "epoch 167, loss 0.04388749971985817\n",
      "tensor(0.0421, grad_fn=<MseLossBackward>)\n",
      "epoch 168, loss 0.04211043938994408\n",
      "tensor(0.0404, grad_fn=<MseLossBackward>)\n",
      "epoch 169, loss 0.04040531814098358\n",
      "tensor(0.0388, grad_fn=<MseLossBackward>)\n",
      "epoch 170, loss 0.038769327104091644\n",
      "tensor(0.0372, grad_fn=<MseLossBackward>)\n",
      "epoch 171, loss 0.03719953075051308\n",
      "tensor(0.0357, grad_fn=<MseLossBackward>)\n",
      "epoch 172, loss 0.035693470388650894\n",
      "tensor(0.0342, grad_fn=<MseLossBackward>)\n",
      "epoch 173, loss 0.03424840793013573\n",
      "tensor(0.0329, grad_fn=<MseLossBackward>)\n",
      "epoch 174, loss 0.03286173567175865\n",
      "tensor(0.0315, grad_fn=<MseLossBackward>)\n",
      "epoch 175, loss 0.03153125196695328\n",
      "tensor(0.0303, grad_fn=<MseLossBackward>)\n",
      "epoch 176, loss 0.030254680663347244\n",
      "tensor(0.0290, grad_fn=<MseLossBackward>)\n",
      "epoch 177, loss 0.0290298480540514\n",
      "tensor(0.0279, grad_fn=<MseLossBackward>)\n",
      "epoch 178, loss 0.027854623273015022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0267, grad_fn=<MseLossBackward>)\n",
      "epoch 179, loss 0.0267269778996706\n",
      "tensor(0.0256, grad_fn=<MseLossBackward>)\n",
      "epoch 180, loss 0.025645071640610695\n",
      "tensor(0.0246, grad_fn=<MseLossBackward>)\n",
      "epoch 181, loss 0.024606898427009583\n",
      "tensor(0.0236, grad_fn=<MseLossBackward>)\n",
      "epoch 182, loss 0.023610742762684822\n",
      "tensor(0.0227, grad_fn=<MseLossBackward>)\n",
      "epoch 183, loss 0.02265501208603382\n",
      "tensor(0.0217, grad_fn=<MseLossBackward>)\n",
      "epoch 184, loss 0.021738000214099884\n",
      "tensor(0.0209, grad_fn=<MseLossBackward>)\n",
      "epoch 185, loss 0.020858094096183777\n",
      "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "epoch 186, loss 0.020013807341456413\n",
      "tensor(0.0192, grad_fn=<MseLossBackward>)\n",
      "epoch 187, loss 0.01920376718044281\n",
      "tensor(0.0184, grad_fn=<MseLossBackward>)\n",
      "epoch 188, loss 0.01842648722231388\n",
      "tensor(0.0177, grad_fn=<MseLossBackward>)\n",
      "epoch 189, loss 0.017680702731013298\n",
      "tensor(0.0170, grad_fn=<MseLossBackward>)\n",
      "epoch 190, loss 0.016965145245194435\n",
      "tensor(0.0163, grad_fn=<MseLossBackward>)\n",
      "epoch 191, loss 0.016278527677059174\n",
      "tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "epoch 192, loss 0.015619776211678982\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "epoch 193, loss 0.014987676404416561\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "epoch 194, loss 0.014381161890923977\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "epoch 195, loss 0.013799196109175682\n",
      "tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "epoch 196, loss 0.013240768574178219\n",
      "tensor(0.0127, grad_fn=<MseLossBackward>)\n",
      "epoch 197, loss 0.012704937718808651\n",
      "tensor(0.0122, grad_fn=<MseLossBackward>)\n",
      "epoch 198, loss 0.012190897017717361\n",
      "tensor(0.0117, grad_fn=<MseLossBackward>)\n",
      "epoch 199, loss 0.011697608977556229\n",
      "tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "epoch 200, loss 0.01122424378991127\n",
      "tensor(0.0108, grad_fn=<MseLossBackward>)\n",
      "epoch 201, loss 0.010770073160529137\n",
      "tensor(0.0103, grad_fn=<MseLossBackward>)\n",
      "epoch 202, loss 0.010334287770092487\n",
      "tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "epoch 203, loss 0.009916127659380436\n",
      "tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "epoch 204, loss 0.009514928795397282\n",
      "tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "epoch 205, loss 0.009129962883889675\n",
      "tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "epoch 206, loss 0.008760575205087662\n",
      "tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "epoch 207, loss 0.008406157605350018\n",
      "tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "epoch 208, loss 0.008066070266067982\n",
      "tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "epoch 209, loss 0.0077397990971803665\n",
      "tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "epoch 210, loss 0.007426706608384848\n",
      "tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "epoch 211, loss 0.007126304320991039\n",
      "tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "epoch 212, loss 0.006838052999228239\n",
      "tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "epoch 213, loss 0.006561475805938244\n",
      "tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "epoch 214, loss 0.006296062376350164\n",
      "tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "epoch 215, loss 0.006041379179805517\n",
      "tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "epoch 216, loss 0.005797014106065035\n",
      "tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "epoch 217, loss 0.005562514066696167\n",
      "tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "epoch 218, loss 0.005337548442184925\n",
      "tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "epoch 219, loss 0.005121707450598478\n",
      "tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "epoch 220, loss 0.004914530087262392\n",
      "tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "epoch 221, loss 0.004715766757726669\n",
      "tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "epoch 222, loss 0.0045250277034938335\n",
      "tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "epoch 223, loss 0.004342052154242992\n",
      "tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "epoch 224, loss 0.004166469443589449\n",
      "tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "epoch 225, loss 0.00399797922000289\n",
      "tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "epoch 226, loss 0.003836320713162422\n",
      "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "epoch 227, loss 0.0036812028847634792\n",
      "tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "epoch 228, loss 0.0035323691554367542\n",
      "tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "epoch 229, loss 0.0033895554952323437\n",
      "tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "epoch 230, loss 0.0032524915877729654\n",
      "tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "epoch 231, loss 0.003120970446616411\n",
      "tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "epoch 232, loss 0.0029947683215141296\n",
      "tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "epoch 233, loss 0.0028736728709191084\n",
      "tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "epoch 234, loss 0.0027574682608246803\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "epoch 235, loss 0.002645995235070586\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "epoch 236, loss 0.00253902655094862\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "epoch 237, loss 0.0024363582488149405\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "epoch 238, loss 0.002337855286896229\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "epoch 239, loss 0.0022433497942984104\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "epoch 240, loss 0.002152668312191963\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "epoch 241, loss 0.0020656436681747437\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "epoch 242, loss 0.001982161309570074\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "epoch 243, loss 0.0019020288018509746\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "epoch 244, loss 0.0018251341534778476\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "epoch 245, loss 0.0017513480270281434\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "epoch 246, loss 0.001680559478700161\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "epoch 247, loss 0.0016126452246680856\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 248, loss 0.0015474724350497127\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 249, loss 0.0014849177096039057\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 250, loss 0.001424903399311006\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 251, loss 0.001367312390357256\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 252, loss 0.0013120633084326982\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 253, loss 0.0012590480037033558\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 254, loss 0.001208174624480307\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 255, loss 0.0011593655217438936\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 256, loss 0.0011125251185148954\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 257, loss 0.0010675613302737474\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 258, loss 0.0010244122240692377\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 259, loss 0.0009830332128331065\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 260, loss 0.0009433203376829624\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 261, loss 0.0009052148670889437\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 262, loss 0.0008686426444910467\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 263, loss 0.0008335429592989385\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 264, loss 0.0007998701184988022\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 265, loss 0.0007675422239117324\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "epoch 266, loss 0.0007365253404714167\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "epoch 267, loss 0.0007067809347063303\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "epoch 268, loss 0.0006782313575968146\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "epoch 269, loss 0.0006508278893306851\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "epoch 270, loss 0.0006245276890695095\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "epoch 271, loss 0.0005992973456159234\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "epoch 272, loss 0.0005750844720751047\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "epoch 273, loss 0.0005518501857295632\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "epoch 274, loss 0.0005295678856782615\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "epoch 275, loss 0.0005081830313429236\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "epoch 276, loss 0.00048765208339318633\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "epoch 277, loss 0.0004679634002968669\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "epoch 278, loss 0.000449070445029065\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "epoch 279, loss 0.00043093334534205496\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "epoch 280, loss 0.0004135245690122247\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "epoch 281, loss 0.0003968196106143296\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "epoch 282, loss 0.0003808032488450408\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "epoch 283, loss 0.00036542900488711894\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "epoch 284, loss 0.00035067726275883615\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 285, loss 0.0003365135926287621\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 286, loss 0.00032293531694449484\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 287, loss 0.00030988911748863757\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 288, loss 0.0002973756054416299\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 289, loss 0.0002853664627764374\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 290, loss 0.00027384041459299624\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 291, loss 0.0002627858193591237\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 292, loss 0.0002521830901969224\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 293, loss 0.00024200629559345543\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 294, loss 0.00023224296455737203\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 295, loss 0.0002228625089628622\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 296, loss 0.0002138704585377127\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 297, loss 0.0002052398631349206\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 298, loss 0.00019696027447935194\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 299, loss 0.00018900528084486723\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 300, loss 0.00018138086306862533\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 301, loss 0.0001740606821840629\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 302, loss 0.00016703511937521398\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 303, loss 0.00016029832477215677\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "epoch 304, loss 0.0001538260403322056\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 305, loss 0.00014762376667931676\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 306, loss 0.00014166932669468224\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 307, loss 0.00013595740892924368\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 308, loss 0.00013047012907918543\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 309, loss 0.00012520841846708208\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 310, loss 0.00012015453830827028\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 311, loss 0.00011530531628523022\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 312, loss 0.00011065252328990027\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 313, loss 0.00010618919623084366\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 314, loss 0.00010190141620114446\n",
      "tensor(9.7792e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 315, loss 9.779209358384833e-05\n",
      "tensor(9.3846e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 316, loss 9.384587610838935e-05\n",
      "tensor(9.0058e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 317, loss 9.005792526295409e-05\n",
      "tensor(8.6426e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 318, loss 8.64255489432253e-05\n",
      "tensor(8.2940e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 319, loss 8.294030703837052e-05\n",
      "tensor(7.9591e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 320, loss 7.959146751090884e-05\n",
      "tensor(7.6384e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 321, loss 7.638386159669608e-05\n",
      "tensor(7.3302e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 322, loss 7.330216612899676e-05\n",
      "tensor(7.0344e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 323, loss 7.034446025500074e-05\n",
      "tensor(6.7506e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 324, loss 6.750552711309865e-05\n",
      "tensor(6.4782e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 325, loss 6.478235445683822e-05\n",
      "tensor(6.2171e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 326, loss 6.217110785655677e-05\n",
      "tensor(5.9661e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 327, loss 5.9661062550731e-05\n",
      "tensor(5.7256e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 328, loss 5.725626033381559e-05\n",
      "tensor(5.4947e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 329, loss 5.494747165357694e-05\n",
      "tensor(5.2729e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 330, loss 5.2729417802765965e-05\n",
      "tensor(5.0601e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 331, loss 5.060056719230488e-05\n",
      "tensor(4.8557e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 332, loss 4.855710722040385e-05\n",
      "tensor(4.6599e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 333, loss 4.659877231460996e-05\n",
      "tensor(4.4720e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 334, loss 4.4720349251292646e-05\n",
      "tensor(4.2915e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 335, loss 4.291540972189978e-05\n",
      "tensor(4.1185e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 336, loss 4.118511060369201e-05\n",
      "tensor(3.9524e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 337, loss 3.952355109504424e-05\n",
      "tensor(3.7933e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 338, loss 3.7932662962703034e-05\n",
      "tensor(3.6406e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 339, loss 3.6405861465027556e-05\n",
      "tensor(3.4940e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 340, loss 3.4939883335027844e-05\n",
      "tensor(3.3533e-05, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 341, loss 3.35331387759652e-05\n",
      "tensor(3.2179e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 342, loss 3.217923585907556e-05\n",
      "tensor(3.0882e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 343, loss 3.088153607677668e-05\n",
      "tensor(2.9637e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 344, loss 2.9637325496878475e-05\n",
      "tensor(2.8441e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 345, loss 2.8440595997381024e-05\n",
      "tensor(2.7294e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 346, loss 2.729448715399485e-05\n",
      "tensor(2.6194e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 347, loss 2.6193685698672198e-05\n",
      "tensor(2.5140e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 348, loss 2.5140207071672194e-05\n",
      "tensor(2.4129e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 349, loss 2.4128945369739085e-05\n",
      "tensor(2.3157e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 350, loss 2.3157459509093314e-05\n",
      "tensor(2.2225e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 351, loss 2.2225221982807852e-05\n",
      "tensor(2.1329e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 352, loss 2.1329229639377445e-05\n",
      "tensor(2.0472e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 353, loss 2.047227644652594e-05\n",
      "tensor(1.9645e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 354, loss 1.964484363270458e-05\n",
      "tensor(1.8853e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 355, loss 1.8853143046726473e-05\n",
      "tensor(1.8095e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 356, loss 1.8094635379384272e-05\n",
      "tensor(1.7365e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 357, loss 1.7364540326525457e-05\n",
      "tensor(1.6666e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 358, loss 1.666560820012819e-05\n",
      "tensor(1.5994e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 359, loss 1.599368260940537e-05\n",
      "tensor(1.5351e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 360, loss 1.535053706902545e-05\n",
      "tensor(1.4733e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 361, loss 1.4732694580743555e-05\n",
      "tensor(1.4139e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 362, loss 1.4138636288407724e-05\n",
      "tensor(1.3568e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 363, loss 1.356789653073065e-05\n",
      "tensor(1.3020e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 364, loss 1.3020201549807098e-05\n",
      "tensor(1.2495e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 365, loss 1.2494652764871716e-05\n",
      "tensor(1.1990e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 366, loss 1.1990405255346559e-05\n",
      "tensor(1.1507e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 367, loss 1.1506662303872872e-05\n",
      "tensor(1.1043e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 368, loss 1.1042910955438856e-05\n",
      "tensor(1.0599e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 369, loss 1.0598925655358471e-05\n",
      "tensor(1.0173e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 370, loss 1.0173252121603582e-05\n",
      "tensor(9.7628e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 371, loss 9.762824447534513e-06\n",
      "tensor(9.3698e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 372, loss 9.369805411552079e-06\n",
      "tensor(8.9915e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 373, loss 8.991469258035067e-06\n",
      "tensor(8.6294e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 374, loss 8.629403055238072e-06\n",
      "tensor(8.2805e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 375, loss 8.280469046439976e-06\n",
      "tensor(7.9474e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 376, loss 7.947364792926237e-06\n",
      "tensor(7.6272e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 377, loss 7.627167633472709e-06\n",
      "tensor(7.3212e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 378, loss 7.321153589145979e-06\n",
      "tensor(7.0266e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 379, loss 7.026626462902641e-06\n",
      "tensor(6.7439e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 380, loss 6.743945959897246e-06\n",
      "tensor(6.4720e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 381, loss 6.472013410530053e-06\n",
      "tensor(6.2105e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 382, loss 6.210483206814388e-06\n",
      "tensor(5.9597e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 383, loss 5.959726422588574e-06\n",
      "tensor(5.7209e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 384, loss 5.7208585531043354e-06\n",
      "tensor(5.4916e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 385, loss 5.491633601195645e-06\n",
      "tensor(5.2693e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 386, loss 5.26934218214592e-06\n",
      "tensor(5.0565e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 387, loss 5.05648222315358e-06\n",
      "tensor(4.8524e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 388, loss 4.8523811528866645e-06\n",
      "tensor(4.6568e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 389, loss 4.6567893150495365e-06\n",
      "tensor(4.4696e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 390, loss 4.469561190489912e-06\n",
      "tensor(4.2900e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 391, loss 4.290037395549007e-06\n",
      "tensor(4.1164e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 392, loss 4.116423497180222e-06\n",
      "tensor(3.9505e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 393, loss 3.9504752749053296e-06\n",
      "tensor(3.7918e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 394, loss 3.791817334786174e-06\n",
      "tensor(3.6400e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 395, loss 3.6399899272510083e-06\n",
      "tensor(3.4933e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 396, loss 3.4932579637825256e-06\n",
      "tensor(3.3533e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 397, loss 3.3532619454490487e-06\n",
      "tensor(3.2180e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 398, loss 3.218047140762792e-06\n",
      "tensor(3.0891e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 399, loss 3.0890514608472586e-06\n",
      "tensor(2.9645e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 400, loss 2.964457735288306e-06\n",
      "tensor(2.8457e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 401, loss 2.845672725015902e-06\n",
      "tensor(2.7309e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 402, loss 2.730899495873018e-06\n",
      "tensor(2.6204e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 403, loss 2.620357236082782e-06\n",
      "tensor(2.5152e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 404, loss 2.5151848603854887e-06\n",
      "tensor(2.4138e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 405, loss 2.4137993932527024e-06\n",
      "tensor(2.3161e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 406, loss 2.3161444460129132e-06\n",
      "tensor(2.2233e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 407, loss 2.223273668278125e-06\n",
      "tensor(2.1337e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 408, loss 2.133704583684448e-06\n",
      "tensor(2.0476e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 409, loss 2.0476229565247195e-06\n",
      "tensor(1.9645e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 410, loss 1.9645335669338237e-06\n",
      "tensor(1.8861e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 411, loss 1.886093627945229e-06\n",
      "tensor(1.8104e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 412, loss 1.8104240098182345e-06\n",
      "tensor(1.7376e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 413, loss 1.7376390815115883e-06\n",
      "tensor(1.6676e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 414, loss 1.6676115137670422e-06\n",
      "tensor(1.6005e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 415, loss 1.6005417364794994e-06\n",
      "tensor(1.5358e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 416, loss 1.5358335758719477e-06\n",
      "tensor(1.4738e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 417, loss 1.4738294566996046e-06\n",
      "tensor(1.4143e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 418, loss 1.414276312061702e-06\n",
      "tensor(1.3570e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 419, loss 1.3570080454883282e-06\n",
      "tensor(1.3022e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 420, loss 1.3022475968682556e-06\n",
      "tensor(1.2496e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 421, loss 1.2495871715145768e-06\n",
      "tensor(1.1992e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 422, loss 1.199153984998702e-06\n",
      "tensor(1.1508e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 423, loss 1.15084981189284e-06\n",
      "tensor(1.1045e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 424, loss 1.1045372048101854e-06\n",
      "tensor(1.0602e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 425, loss 1.0601678468447062e-06\n",
      "tensor(1.0177e-06, grad_fn=<MseLossBackward>)\n",
      "epoch 426, loss 1.0177188869420206e-06\n",
      "tensor(9.7703e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 427, loss 9.77025138126919e-07\n",
      "tensor(9.3740e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 428, loss 9.374019782626419e-07\n",
      "tensor(8.9948e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 429, loss 8.994811082629894e-07\n",
      "tensor(8.6319e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 430, loss 8.631895411781443e-07\n",
      "tensor(8.2868e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 431, loss 8.286833121928794e-07\n",
      "tensor(7.9568e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 432, loss 7.95682183252211e-07\n",
      "tensor(7.6338e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 433, loss 7.633796599293419e-07\n",
      "tensor(7.3255e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 434, loss 7.325501769628318e-07\n",
      "tensor(7.0325e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 435, loss 7.03252396760945e-07\n",
      "tensor(6.7525e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 436, loss 6.752512149432732e-07\n",
      "tensor(6.4794e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 437, loss 6.479438638962165e-07\n",
      "tensor(6.2198e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 438, loss 6.219793249329086e-07\n",
      "tensor(5.9720e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 439, loss 5.972011649646447e-07\n",
      "tensor(5.7306e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 440, loss 5.730597081310407e-07\n",
      "tensor(5.5007e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 441, loss 5.500713200490281e-07\n",
      "tensor(5.2827e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 442, loss 5.282710731080442e-07\n",
      "tensor(5.0702e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 443, loss 5.070171482657315e-07\n",
      "tensor(4.8680e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 444, loss 4.868022074333567e-07\n",
      "tensor(4.6705e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 445, loss 4.670496878134145e-07\n",
      "tensor(4.4836e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 446, loss 4.4836008328275057e-07\n",
      "tensor(4.3007e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 447, loss 4.3006673422496533e-07\n",
      "tensor(4.1275e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 448, loss 4.1274506656918675e-07\n",
      "tensor(3.9636e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 449, loss 3.9635801840631757e-07\n",
      "tensor(3.8038e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 450, loss 3.803809818236914e-07\n",
      "tensor(3.6532e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 451, loss 3.6531997693600715e-07\n",
      "tensor(3.5055e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 452, loss 3.5055163039032777e-07\n",
      "tensor(3.3666e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 453, loss 3.366633336554514e-07\n",
      "tensor(3.2309e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 454, loss 3.2308977893080737e-07\n",
      "tensor(3.1035e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 455, loss 3.103463370734971e-07\n",
      "tensor(2.9777e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 456, loss 2.97772942303709e-07\n",
      "tensor(2.8613e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 457, loss 2.8613115432563063e-07\n",
      "tensor(2.7467e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 458, loss 2.746733400726953e-07\n",
      "tensor(2.6338e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 459, loss 2.6337727376812836e-07\n",
      "tensor(2.5287e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 460, loss 2.528672666812781e-07\n",
      "tensor(2.4260e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 461, loss 2.4260185682578594e-07\n",
      "tensor(2.3299e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 462, loss 2.3299297424728138e-07\n",
      "tensor(2.2357e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 463, loss 2.235702964981101e-07\n",
      "tensor(2.1484e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 464, loss 2.1483668888322427e-07\n",
      "tensor(2.0621e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 465, loss 2.06207388941948e-07\n",
      "tensor(1.9784e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 466, loss 1.9784049243298796e-07\n",
      "tensor(1.9005e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 467, loss 1.9005169349384232e-07\n",
      "tensor(1.8239e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 468, loss 1.8238803534131875e-07\n",
      "tensor(1.7491e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 469, loss 1.749069582501761e-07\n",
      "tensor(1.6794e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 470, loss 1.6794363943972712e-07\n",
      "tensor(1.6118e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 471, loss 1.6118110579554923e-07\n",
      "tensor(1.5456e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 472, loss 1.5455785273843503e-07\n",
      "tensor(1.4842e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 473, loss 1.484150544683871e-07\n",
      "tensor(1.4242e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 474, loss 1.4242104384720733e-07\n",
      "tensor(1.3656e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 475, loss 1.3655920838573365e-07\n",
      "tensor(1.3115e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 476, loss 1.3114828334437334e-07\n",
      "tensor(1.2587e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 477, loss 1.2587373987571482e-07\n",
      "tensor(1.2068e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 478, loss 1.2068265675679868e-07\n",
      "tensor(1.1594e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 479, loss 1.1593638760132308e-07\n",
      "tensor(1.1134e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 480, loss 1.1133738553326111e-07\n",
      "tensor(1.0679e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 481, loss 1.0678778039618919e-07\n",
      "tensor(1.0265e-07, grad_fn=<MseLossBackward>)\n",
      "epoch 482, loss 1.0264515992730594e-07\n",
      "tensor(9.8598e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 483, loss 9.85977592904419e-08\n",
      "tensor(9.4605e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 484, loss 9.460501360081253e-08\n",
      "tensor(9.0745e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 485, loss 9.074476281512034e-08\n",
      "tensor(8.7219e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 486, loss 8.72187513323297e-08\n",
      "tensor(8.3781e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 487, loss 8.378128057984213e-08\n",
      "tensor(8.0428e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 488, loss 8.042840704547416e-08\n",
      "tensor(7.7140e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 489, loss 7.714048422258202e-08\n",
      "tensor(7.4134e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 490, loss 7.413414238044425e-08\n",
      "tensor(7.1227e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 491, loss 7.122658018943184e-08\n",
      "tensor(6.8375e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 492, loss 6.837450428065495e-08\n",
      "tensor(6.5597e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 493, loss 6.559741194678281e-08\n",
      "tensor(6.2877e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 494, loss 6.287745435429315e-08\n",
      "tensor(6.0442e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 495, loss 6.044157174756037e-08\n",
      "tensor(5.8055e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 496, loss 5.805530278735205e-08\n",
      "tensor(5.5716e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 497, loss 5.571622452293923e-08\n",
      "tensor(5.3441e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 498, loss 5.344084996750098e-08\n",
      "tensor(5.1203e-08, grad_fn=<MseLossBackward>)\n",
      "epoch 499, loss 5.120322299489999e-08\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.4140706 ]\n",
      " [ 1.4268999 ]\n",
      " [ 9.29785   ]\n",
      " [ 4.0618386 ]\n",
      " [ 2.8380432 ]\n",
      " [ 7.6612387 ]\n",
      " [ 7.578516  ]\n",
      " [ 3.0902357 ]\n",
      " [ 5.046871  ]\n",
      " [-1.7281885 ]\n",
      " [ 8.449847  ]\n",
      " [ 7.975584  ]\n",
      " [ 7.8597193 ]\n",
      " [-1.0639067 ]\n",
      " [ 3.997563  ]\n",
      " [ 5.0061374 ]\n",
      " [ 6.216128  ]\n",
      " [ 5.867048  ]\n",
      " [ 8.963208  ]\n",
      " [ 0.3591237 ]\n",
      " [ 4.391851  ]\n",
      " [ 3.031899  ]\n",
      " [ 5.5800395 ]\n",
      " [ 6.6600785 ]\n",
      " [ 8.954188  ]\n",
      " [ 3.5918841 ]\n",
      " [ 7.0264196 ]\n",
      " [-0.45123148]\n",
      " [ 4.450464  ]\n",
      " [ 8.176651  ]\n",
      " [ 3.8062768 ]\n",
      " [ 6.012083  ]\n",
      " [ 8.142479  ]\n",
      " [ 8.137558  ]\n",
      " [ 7.5909023 ]\n",
      " [ 4.6335125 ]\n",
      " [ 5.3739166 ]\n",
      " [ 4.03141   ]\n",
      " [ 7.5247755 ]\n",
      " [12.172575  ]\n",
      " [ 5.2283783 ]\n",
      " [ 3.3004658 ]\n",
      " [ 5.108207  ]\n",
      " [-1.2250733 ]\n",
      " [ 5.7431498 ]\n",
      " [ 2.3083456 ]\n",
      " [ 4.589403  ]\n",
      " [ 5.0546494 ]\n",
      " [ 7.2659965 ]\n",
      " [ 5.6455803 ]\n",
      " [ 7.5227776 ]\n",
      " [ 0.66240597]\n",
      " [ 0.7939148 ]\n",
      " [ 4.6970315 ]\n",
      " [ 3.3550756 ]\n",
      " [ 4.5659294 ]\n",
      " [ 6.0618305 ]\n",
      " [ 4.8932447 ]\n",
      " [ 6.6969767 ]\n",
      " [ 9.636702  ]\n",
      " [ 2.0771098 ]\n",
      " [ 4.7887506 ]\n",
      " [ 5.9236774 ]\n",
      " [ 4.3742943 ]\n",
      " [ 8.101147  ]\n",
      " [-2.201489  ]\n",
      " [11.091518  ]\n",
      " [ 1.5719311 ]\n",
      " [ 5.6354246 ]\n",
      " [ 7.1139183 ]\n",
      " [ 2.6435063 ]\n",
      " [ 6.3859444 ]\n",
      " [ 7.112441  ]\n",
      " [ 6.5702868 ]\n",
      " [ 2.221054  ]\n",
      " [11.023237  ]\n",
      " [ 5.6806617 ]\n",
      " [ 1.5418482 ]\n",
      " [ 6.8956976 ]\n",
      " [ 5.118319  ]\n",
      " [ 6.3929424 ]\n",
      " [-5.6906347 ]\n",
      " [ 8.96305   ]\n",
      " [ 5.4576683 ]\n",
      " [ 5.4933653 ]\n",
      " [ 3.7095113 ]\n",
      " [ 7.3018603 ]\n",
      " [ 7.9545054 ]\n",
      " [ 5.81228   ]\n",
      " [ 9.17569   ]\n",
      " [ 5.2393064 ]\n",
      " [ 3.7999039 ]\n",
      " [ 1.9162693 ]\n",
      " [ 3.2456496 ]\n",
      " [ 7.449534  ]\n",
      " [ 4.7539444 ]\n",
      " [ 3.9654975 ]\n",
      " [ 6.584627  ]\n",
      " [ 1.792856  ]\n",
      " [ 3.4641576 ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
